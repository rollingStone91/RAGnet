{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2dfced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import load_dataset\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "from langchain.chat_models import ChatOllama\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# 自定义 LangChain 的 Embeddings 类封装\n",
    "class LlamaCppEmbeddings(Embeddings):\n",
    "    def __init__(self, model_path: str):\n",
    "        self.llm = Llama(model_path=model_path, embedding=True)\n",
    "\n",
    "    def embed_documents(self, texts: list[str]):\n",
    "        # return [self.llm.embed(text)[\"data\"][0][\"embedding\"] for text in texts]\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            result = self.llm.embed(text)\n",
    "            if isinstance(result, list) and isinstance(result[0], list):\n",
    "                embeddings.append(result[0])\n",
    "            else:\n",
    "                embeddings.append(result)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        # return self.llm.embed(text)[\"data\"][0][\"embedding\"]\n",
    "        result = self.llm.embed(text)\n",
    "        return result[0] if isinstance(result, list) and isinstance(result[0], list) else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c32364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proof():\n",
    "    def __init__(self, document: Document, vector: List[np.ndarray], score: float):\n",
    "        self.document = document\n",
    "        self.vector = vector\n",
    "        self.score = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec073e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    \"\"\"\n",
    "    轻量级rag客户端，负责数据集加载、向量存储构建与检索。\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path: str = \"./models/Qwen3-Embedding/Qwen3-Embedding-0.6B-Q8_0.gguf\", \n",
    "                vectorstore_path: str = \"faiss_db\"): # dashscope_api_key: str,使用api调用embedding模型\n",
    "        os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"TRUE\")\n",
    "        self.vectorstore_path = vectorstore_path\n",
    "\n",
    "        # 如果使用Qwen/Qwen3-Embedding-4B\n",
    "        # self.embeddings = HuggingFaceEmbeddings(\n",
    "        #     model_name=model_name,\n",
    "        #     model_kwargs={\"device\": \"cuda\"},  # 如果是CPU，可以改成 \"cpu\"\n",
    "        #     encode_kwargs={\"normalize_embeddings\": True}\n",
    "        # )\n",
    "        self.embeddings = LlamaCppEmbeddings(model_path=model_path)\n",
    "        self.db: FAISS = None\n",
    "\n",
    "    def _chunk_text(self, text: str, chunk_size=800, overlap= 200) -> list[str]:\n",
    "        \"\"\"\n",
    "        将文本分块处理，使用递归字符分割器。\n",
    "        \"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \"。\", \".\", \"！\", \"？\", \"!\", \"?\", \" \", \"\"],\n",
    "            length_function=len\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "    \n",
    "    def _semantic_chunk_docs(self, docs: list[Document],\n",
    "                             buffer_size=3,\n",
    "                             breakpoint_threshold_type=\"percentile\",\n",
    "                             sentence_split_regex=r\"(?<=[.?!])\\s+\") -> list[Document]:\n",
    "        \"\"\"\n",
    "        使用语义分块器对文档进行分块处理。\n",
    "        \"\"\"\n",
    "        splitter = SemanticChunker(\n",
    "            embeddings=self.embeddings,\n",
    "            buffer_size=buffer_size,\n",
    "            breakpoint_threshold_type=breakpoint_threshold_type,\n",
    "            sentence_split_regex=sentence_split_regex\n",
    "        )\n",
    "        return splitter.split_documents(docs)\n",
    "\n",
    "    # 读取PDF文件并提取文本内容\n",
    "    def _read_pdfs(self, pdf_paths: List[str]) -> List[Document]:\n",
    "        docs = []\n",
    "        for i, path in enumerate(pages):\n",
    "            loader = PyPDFLoader(path)\n",
    "            pages = loader.load_and_split()\n",
    "            for page in pages:\n",
    "                docs.append(Document(page_content=page.page_content, metadata={'source': path, 'doc_id': i}))\n",
    "        return docs\n",
    "\n",
    "    # 读取JSON文件夹中的所有文件\n",
    "    def _load_json_folder(self, folder_path: str) -> List[Document]:\n",
    "        docs = []\n",
    "        i = 0\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if not filename.endswith('.json'):\n",
    "                continue\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            with open(filepath, encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            content = f\"{data.get('title', '')}\\n{data.get('content', '')}\".strip()\n",
    "            if content:\n",
    "                docs.append(Document(page_content=content, metadata={'source': filepath, 'doc_id': i}))\n",
    "                i+=1\n",
    "        return docs\n",
    "    \n",
    "    # 在线读取数据集\n",
    "    def _streaming_load_dataset(self, sample_size=100, language='en', date_version='20231101') -> List[str]:\n",
    "        # 启用streaming模式在线读取huggingface datasets\n",
    "        dataset = load_dataset(\"wikimedia/wikipedia\", f'{date_version}.{language}', streaming=True)\n",
    "        docs = []\n",
    "        for i, item in enumerate(dataset['train']):\n",
    "            if i >= sample_size:\n",
    "                break\n",
    "            text = item.get('text', '')\n",
    "            title = item.get('title', '')\n",
    "            if not text:\n",
    "                continue\n",
    "            # snippet = text[:5000]\n",
    "            meta = {'source': f'wikipedia://{language}/{item.get(\"id\")}', 'doc_id': i}\n",
    "            docs.append(Document(page_content=f\"{title}\\n{text}\", metadata=meta))\n",
    "        print(f\"Streamed {len(docs)} Wikipedia docs.\")\n",
    "        return docs\n",
    "    \n",
    "    def build_vectorstore(self, sample_size=100, batch_size=10, \n",
    "                          streaming=False, folder_path=None, pdf_paths:List[str]=None,\n",
    "                          buffer_size=3, threshold_type=\"percentile\", sentence_split_regex=r\"(?<=[.?!])\\s+\"):\n",
    "        docs = []\n",
    "        if streaming:\n",
    "            # 在线读取数据集\n",
    "            docs.extend(self._streaming_load_dataset(sample_size))\n",
    "        elif folder_path is not None and pdf_paths is None:\n",
    "            # 从指定文件夹加载JSON文件\n",
    "            docs.extend(self._load_json_folder(folder_path))\n",
    "        elif pdf_paths is not None:\n",
    "            # 从PDF文件加载\n",
    "            docs.extend(self._read_pdfs(pdf_paths))\n",
    "\n",
    "        # 使用 SemanticChunker 分块\n",
    "        chunks = self._semantic_chunk_docs(docs, buffer_size, threshold_type, sentence_split_regex)\n",
    "        print(f\"Total chunks after semantic split: {len(chunks)}\")\n",
    "\n",
    "        # 构建 FAISS\n",
    "        texts, metadatas = [], []\n",
    "        for i, doc in enumerate(chunks):\n",
    "            texts.append(doc.page_content)\n",
    "            metadatas.append({\n",
    "                \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "                \"doc_id\": doc.metadata.get(\"doc_id\", \"\"),\n",
    "                \"faiss_id\": i\n",
    "            })\n",
    "            if len(texts) >= batch_size or i == len(chunks) - 1:\n",
    "                if self.db is None:\n",
    "                    self.db = FAISS.from_texts(texts, embedding=self.embeddings, metadatas=metadatas, normalize_L2=True)\n",
    "                else:\n",
    "                    self.db.add_texts(texts, metadatas=metadatas)\n",
    "                texts.clear()\n",
    "                metadatas.clear()\n",
    "                print(f\"Inserted batch up to chunk {i+1}/{len(chunks)}\")\n",
    "\n",
    "        # 保存向量库\n",
    "        if self.db:\n",
    "            self.db.save_local(self.vectorstore_path)\n",
    "            print(f\"Vectorstore saved to {self.vectorstore_path}\")\n",
    "        else:\n",
    "            print(\"No data processed.\")\n",
    "\n",
    "    def load_vectorstore(self) -> None:\n",
    "        \"\"\"\n",
    "        加载已保存的向量存储\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.vectorstore_path):\n",
    "            raise FileNotFoundError(f\"Vectorstore directory '{self.vectorstore_path}' not found.\")\n",
    "        self.db = FAISS.load_local(\n",
    "            self.vectorstore_path,\n",
    "            embeddings=self.embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(f\"Vectorstore {self.vectorstore_path} loaded.\")\n",
    "\n",
    "    def retrieve(self, query:str, top_k=4):\n",
    "        \"\"\"\n",
    "        通过query在FAISS向量库中检索k个最相似文档，\n",
    "        返回每个Document对象、其特征向量及相似度得分\n",
    "        \"\"\"\n",
    "        # 检查向量库是否已加载\n",
    "        if self.db is None:\n",
    "            raise ValueError(\"Vectorstore尚未加载，请先调用load_vectorstore或build_vectorstore\")\n",
    "\n",
    "        # 查询向量并归一化\n",
    "        query_vec = np.array(self.embeddings.embed_query(query), dtype=np.float32)\n",
    "        query_norm = query_vec / np.linalg.norm(query_vec)\n",
    "\n",
    "        # 使用 FAISS 内积搜索（等价于余弦相似度）\n",
    "        scores, indices = self.db.index.search(query_norm.reshape(1, -1), top_k)\n",
    "\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            doc_id = self.db.index_to_docstore_id[idx]\n",
    "            doc = self.db.docstore.search(doc_id)\n",
    "\n",
    "            faiss_index = int(idx)\n",
    "            vec = self.db.index.reconstruct(faiss_index).tolist()\n",
    "            score = float(scores[0][i])  # 余弦相似度\n",
    "            results.append(Proof(doc, vec, score))\n",
    "        \n",
    "        return results, query_norm.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client(vectorstore_path=\"./common_sense_db\"), Client(vectorstore_path=\"./computer_science_coding_related_db\"),\n",
    "               Client(vectorstore_path=\"./law_related_db\"), Client(vectorstore_path=\"./medicine_related_db\")]\n",
    "folder_paths = [\"./classified_dataset/common_sense\", \"./classified_dataset/computer_science_coding_related\",\n",
    "             \"./classified_dataset/law_related\", \"./classified_dataset/medicine_related\"]\n",
    "for c, f in zip(clients, folder_paths):\n",
    "    print(f\"Building vectorstore for {f}...\")\n",
    "    c.build_vectorstore(folder_path=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    \"\"\"\n",
    "    Server 类，负责：\n",
    "    1) 接收客户端选择的上下文数据\n",
    "    2) 验证数据完整性（通过 Proof 信息）\n",
    "    3) 调用 Ollama 部署的 Qwen3:4B 模型生成答案\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"qwen3:4b\"):\n",
    "        self.llm = ChatOllama(model=model_name)\n",
    "\n",
    "    def verify_documents(self):\n",
    "        return\n",
    "\n",
    "    def build_prompt(self, query: str, contexts: List[str]) -> str:\n",
    "        \"\"\"构造 Prompt，将 query 和上下文拼接\"\"\"\n",
    "        prompt = \"You are an AI assistant. Use the following contexts to answer the question:\\n\"\n",
    "        for i, c in enumerate(contexts, 1):\n",
    "            prompt += f\"Context {i}: {c}\\n\"\n",
    "        prompt += f\"Question: {query}\\nAnswer:\"\n",
    "        return prompt\n",
    "\n",
    "    def generate_answer(self, query: str, contexts: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        验证 Proof 后调用模型生成答案\n",
    "        \"\"\"\n",
    "        # if not self.verify_documents(contexts, proofs):\n",
    "        #     raise ValueError(\"Proof verification failed! Data may be tampered.\")\n",
    "        prompt = self.build_prompt(query, contexts)\n",
    "        response = self.llm.predict(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed47d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
