{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2dfced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation_tools import load_sampled_dataset\n",
    "from client import Client\n",
    "from server import Server\n",
    "from validation_tools import load_sampled_dataset, compute_prf\n",
    "from evaluate import load as load_metric\n",
    "import pandas as pd\n",
    "\n",
    "# DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f854717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_natural_questions(sample):\n",
    "    question = sample[\"question\"][\"text\"]\n",
    "    print(f\"Processing question: {question}\")\n",
    "    gold_answers = sample.get(\"answers\", {}).get(\"text\", [])\n",
    "    print(f\"Gold answers: {gold_answers}\")\n",
    "    if not gold_answers:\n",
    "        gold_answers = [\"\"]  # 占位\n",
    "    return question, gold_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5371cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trivia_qa(sample):\n",
    "    if \"question\" in sample:\n",
    "        question = sample[\"question\"]\n",
    "    elif \"query\" in sample:\n",
    "        question = sample[\"query\"]\n",
    "    else:\n",
    "        raise KeyError(\"无法在样本中找到 question 字段\")\n",
    "    print(f\"Processing question: {question}\")\n",
    "\n",
    "    # answer 可能是字符串，也可能是list\n",
    "    if \"answer\" in sample:\n",
    "        gold_answers = sample[\"answer\"].get('aliases', [])\n",
    "    elif \"answers\" in sample:\n",
    "        gold_answers = sample[\"answers\"].get('aliases', [])\n",
    "    else:\n",
    "        gold_answers = []\n",
    "    print(f\"Gold answers: {gold_answers}\")\n",
    "    return question, gold_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squad(sample):\n",
    "    if \"question\" in sample:\n",
    "        question = sample[\"question\"]\n",
    "    else:\n",
    "        raise KeyError(\"无法在样本中找到 question 字段\")\n",
    "    print(f\"Processing question: {question}\")\n",
    "\n",
    "    if \"answer\" in sample:\n",
    "        gold_answers = sample[\"answer\"].get('text', [])\n",
    "    elif \"answers\" in sample:\n",
    "        gold_answers = sample[\"answers\"].get('text', [])\n",
    "    else:\n",
    "        gold_answers = []\n",
    "    print(f\"Gold answers: {gold_answers}\")\n",
    "    return question, gold_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99387b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_questions(sample):\n",
    "    if \"question\" in sample:\n",
    "        question = sample[\"question\"]\n",
    "    else:\n",
    "        raise KeyError(\"无法在样本中找到 question 字段\")\n",
    "    print(f\"Processing question: {question}\")\n",
    "\n",
    "    if \"answer\" in sample:\n",
    "        gold_answers = sample[\"answer\"]\n",
    "    elif \"answers\" in sample:\n",
    "        gold_answers = sample[\"answers\"]\n",
    "    else:\n",
    "        gold_answers = []\n",
    "    print(f\"Gold answers: {gold_answers}\")\n",
    "    return question, gold_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mmlu(sample):\n",
    "    if \"question\" in sample:\n",
    "        question = sample[\"question\"]\n",
    "    else:\n",
    "        raise KeyError(\"无法在样本中找到 question 字段\")\n",
    "    choices = sample[\"choices\"]\n",
    "    options = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)])\n",
    "\n",
    "    # 拼接\n",
    "    query = f\"{question}\\n\\nChoices:\\n{options}\\n\\nWhich one is correct?\"\n",
    "    print(f\"Processing question: {query}\")\n",
    "\n",
    "    if \"answer\" in sample:\n",
    "        gold_answers = sample[\"answer\"]\n",
    "    elif \"answers\" in sample:\n",
    "        gold_answers = sample[\"answers\"]\n",
    "    else:\n",
    "        gold_answers = []\n",
    "    print(f\"Gold answers: {gold_answers}\")\n",
    "    return query, gold_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6837387",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"natural_questions\": load_sampled_dataset(\"google-research-datasets/natural_questions\", split=\"validation\", sample_size=100),\n",
    "    \"trivia_qa\": load_sampled_dataset(\"mandarjoshi/trivia_qa\", \"rc\", split=\"validation\", sample_size=100),\n",
    "    \"squad\": load_sampled_dataset(\"rajpurkar/squad\", split=\"validation\", sample_size=100),\n",
    "    \"web_questions\": load_sampled_dataset(\"stanfordnlp/web_questions\", split=\"test\", sample_size=100),\n",
    "    \"mmlu\": load_sampled_dataset(\"cais/mmlu\", \"all\", split=\"validation\", sample_size=100),\n",
    "    \"strategy_qa\": load_sampled_dataset(\"wics/strategy-qa\", split=\"validation\", sample_size=100),\n",
    "    \"hotpot_qa\": load_sampled_dataset(\"hotpot_qa\", split=\"validation\", sample_size=100)  # 移除 fullwiki\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_natural_questions(clients: list[Client], server: Server, top_k=5, dataset_name = 'natural_questions',\n",
    "                                output_csv: str = \"natural_questions_results.csv\"): \n",
    "    results = []\n",
    "    samples = datasets.get(dataset_name)\n",
    "    for idx, sample in enumerate(samples):\n",
    "        # 提取question和gold answers\n",
    "        question, gold_answers = get_natural_questions(sample)\n",
    "\n",
    "        # 调用 LLM Server\n",
    "        latency, answer = server.multi_client_generate(question, clients, top_k)\n",
    "\n",
    "        # 构造 SQuAD 格式输入\n",
    "        prediction = [{\"id\": str(idx), \"prediction_text\": answer}]\n",
    "        reference  = [{\"id\": str(idx), \"answers\": {\"text\": gold_answers, \"answer_start\": [0]}}]\n",
    "\n",
    "        # 计算 EM 和 F1\n",
    "        # 加载指标\n",
    "        metric_qa = load_metric(\"squad\")\n",
    "        qa_metrics = metric_qa.compute(predictions=prediction, references=reference)\n",
    "        em_score = qa_metrics[\"exact_match\"]\n",
    "        f1_score = qa_metrics[\"f1\"]\n",
    "\n",
    "        # 计算自定义 P/R/F1  \n",
    "        precision, recall, pr_f1 = compute_prf(answer, gold_answers)\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"gold_answers\": gold_answers,\n",
    "            \"answer\": answer,\n",
    "            \"exact_match\": em_score,\n",
    "            \"squad_f1\": f1_score,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"pr_f1\": pr_f1, # 自定义的F1分数\n",
    "            \"latency\": latency\n",
    "        })\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"[{idx}] EM={em_score}, F1={f1_score:.2f}, P={precision:.2f}, R={recall:.2f}, latency={latency:.2f}s\")\n",
    "\n",
    "    # 保存到 CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved Natural Questions results to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client(vectorstore_path=\"./common_sense_db\"), Client(vectorstore_path=\"./computer_science_coding_related_db\"),\n",
    "            Client(vectorstore_path=\"./law_related_db\"), Client(vectorstore_path=\"./medicine_related_db\")]\n",
    "for c in clients:\n",
    "    c.load_vectorstore()\n",
    "server = Server(model_name=\"qwen3:4b\")\n",
    "evaluate_natural_questions(clients, server, top_k=5, dataset_name='natural_questions', output_csv=\"natural_questions_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
