[
    [
        "AbiWord\nAbiWord () is a free and open-source software word processor. It is written in C++ and since version 3 it is based on GTK+ 3. The name \"AbiWord\" is derived from the root of the Spanish word \"abierto\", meaning \"open\".\n\nAbiWord was originally started by SourceGear Corporation as the first part of a proposed AbiSuite but was adopted by open source developers after SourceGear changed its business focus and ceased development. It now runs on Linux, ReactOS, Solaris, AmigaOS 4.0 (through its Cygwin X11 engine), MeeGo (on the Nokia N9 smartphone), Maemo (on the Nokia N810), QNX and other operating systems. Development of a version for Microsoft Windows has temporarily ended due to lack of maintainers (the latest released versions are 2.8.6 and 2.9.4 beta).",
        "The macOS port has remained on version 2.4 since 2005, although the current version does run non-natively on macOS through XQuartz.\n\nAbiWord is part of the AbiSource project which develops a number of office-related technologies. Abiword is one of the rare text processing software which allows local users to edit simultaneously the same shared document in a local network, without the requirement of an Internet connection, since 2009.",
        "Features\nAbiWord supports both basic word processing features such as lists, indents and character formats, and more sophisticated features including tables, styles, page headers and footers, footnotes, templates, multiple views, page columns, spell checking, and grammar checking. Starting with version 2.8.0, AbiWord includes a collaboration plugin that allows integration with AbiCollab.net, a Web-based service that permits multiple users to work on the same document in real time, in full synchronization. The Presentation view of AbiWord, which permits easy display of presentations created in AbiWord on \"screen-sized\" pages, is another feature not often found in word processors.",
        "Interface\nAbiWord generally works similarly to classic versions (pre-Office 2007) of Microsoft Word, as direct ease of migration was a high priority early goal. While many interface similarities remain, cloning the Word interface is no longer a top priority. The interface is intended to follow user interface guidelines for each respective platform.",
        "File formats\nAbiWord comes with several import and export filters providing partial support for such formats as HTML, Microsoft Word (.doc), Office Open XML (.docx), OpenDocument Text (.odt), Rich Text Format (.rtf), and text documents (.txt). LaTeX is supported for export only. Plug-in filters are available to deal with many other formats, notably WordPerfect documents. The native file format, .abw, uses XML, so as to mitigate vendor lock-in concerns with respect to interoperability and digital archiving.",
        "Grammar checking\nThe AbiWord project includes a US English-only grammar checking plugin using Link Grammar. AbiWord had grammar checking before any other open source word processor, although a grammar checker was later added to OpenOffice.org. Link Grammar is both a theory of syntax and an open source parser which is now developed by the AbiWord project.\n\nSee also\n\nList of free and open-source software packages\nList of word processors\n Comparison of word processors\n Office Open XML software\n OpenDocument software\n\nReferences\n\nExternal links",
        "List of free and open-source software packages\nList of word processors\n Comparison of word processors\n Office Open XML software\n OpenDocument software\n\nReferences\n\nExternal links\n\n \n \n \n \n \n\n1998 software\nCross-platform free software\nFree software programmed in C++\nFree word processors\nLinux word processors\nMacOS word processors\nOffice software that uses GTK\nPortable software\nSoftware using the GPL license\nWindows word processors"
    ],
    [
        "ABM\nABM or Abm may refer to:\n\nCompanies\n ABM Industries, a US facility management provider\n ABM Intelligence, a UK software company\n Advantage Business Media, a US digital marketing and information services company\n Associated British Maltsters, acquired by Dalgety plc\n\nComputing\n Advanced Bit Manipulation, an instruction set extension for x86\n Agent-based model, a computational model for simulating autonomous agents\n Asynchronous Balanced Mode, an HDLC communication mode\n\nMilitary\n Air Battle Manager, US Air Force rated officer position\n Anti-ballistic missile\n Anti-Ballistic Missile Treaty, 1972 arms control treaty between the US and USSR",
        "Military\n Air Battle Manager, US Air Force rated officer position\n Anti-ballistic missile\n Anti-Ballistic Missile Treaty, 1972 arms control treaty between the US and USSR\n\nOrganizations\n Abahlali baseMjondolo, movement of South African shack dwellers\n Anglican Board of Mission - Australia, the national mission agency of the Anglican Church of Australia\n Ansar Bait al-Maqdis, an Egyptian jihadist group",
        "Other uses\n Abanyom language of Nigeria, ISO 639-3 code\n ABM (video game), 1980 video game\n Account-based marketing, strategic approach to business marketing\n Activity-based management, method of identifying and evaluating activities that a business performs\n Agaricus blazei Murill, a species of mushroom\n Automated banking machine, Canadian term for automated teller machine\n IATA airport code for Northern Peninsula Airport, in Queensland, Australia\n Atmospheric Black Metal"
    ],
    [
        "Abstraction\nAbstraction is a conceptual process wherein general rules and concepts are derived from the usage and classification of specific examples, literal (real or concrete) signifiers, first principles, or other methods.\n\n\"An abstraction\" is the outcome of this process—a concept that acts as a common noun for all subordinate concepts and connects any related concepts as a group, field, or category.",
        "\"An abstraction\" is the outcome of this process—a concept that acts as a common noun for all subordinate concepts and connects any related concepts as a group, field, or category.\n\nConceptual abstractions may be formed by filtering the information content of a concept or an observable phenomenon, selecting only those aspects which are relevant for a particular purpose. For example, abstracting a leather soccer ball to the more general idea of a ball selects only the information on general ball attributes and behavior, excluding but not eliminating the other phenomenal and cognitive characteristics of that particular ball. In a type–token distinction, a type (e.g., a 'ball') is more abstract than its tokens (e.g., 'that leather soccer ball').",
        "Abstraction in its secondary use is a material process, discussed in the themes below.\n\nOrigins\n\nThinking in abstractions is considered by anthropologists, archaeologists, and sociologists to be one of the key traits in modern human behaviour, which is believed to have developed between 50,000 and 100,000 years ago. Its development is likely to have been closely connected with the development of human language, which (whether spoken or written) appears to both involve and facilitate abstract thinking.",
        "History\nAbstraction involves induction of ideas or the synthesis of particular facts into one general theory about something. It is the opposite of specification, which is the analysis or breaking-down of a general idea or abstraction into concrete facts. Abstraction can be illustrated by Francis Bacon's Novum Organum (1620), a book of modern scientific philosophy written in the late Jacobean era of England to encourage modern thinkers to collect specific facts before making any generalizations.",
        "Bacon used and promoted induction as an abstraction tool; it complemented but was distinct from the ancient deductive-thinking approach that had dominated the intellectual world since the times of Greek philosophers like Thales, Anaximander, and Aristotle. Thales (–546 BCE) believed that everything in the universe comes from one main substance, water. He deduced or specified from a general idea, \"everything is water,\" to the specific forms of water such as ice, snow, fog, and rivers.",
        "Modern scientists used the approach of abstraction (going from particular facts collected into one general idea). Newton (1642–1727) derived the motion of the planets from Copernicus' (1473–1543) simplification, that the sun is the center of our solar system; Kepler (1571–1630) compressed thousands of measurements into one expression to finally conclude that Mars moves in an elliptical orbit about the sun; Galileo (1564–1642) repeated one hundred specific experiments into the law of falling bodies.\n\nThemes\n\nCompression",
        "An abstraction can be seen as a compression process, mapping multiple different pieces of constituent data to a single piece of abstract data; based on similarities in the constituent data, for example, many different physical cats map to the abstraction \"CAT\". This conceptual scheme emphasizes the inherent equality of both constituent and abstract data, thus avoiding problems arising from the distinction between \"abstract\" and \"concrete\". In this sense the process of abstraction entails the identification of similarities between objects, and the process of associating these objects with an abstraction (which is itself an object).\nFor example, picture 1 below illustrates the concrete relationship \"Cat sits on Mat\".",
        "For example, picture 1 below illustrates the concrete relationship \"Cat sits on Mat\".\nChains of abstractions can be construed, moving from neural impulses arising from sensory perception to basic abstractions such as color or shape, to experiential abstractions such as a specific cat, to semantic abstractions such as the \"idea\" of a CAT, to classes of objects such as \"mammals\" and even categories such as \"object\" as opposed to \"action\".\nFor example, graph 1 below expresses the abstraction \"agent sits on location\". This conceptual scheme entails no specific hierarchical taxonomy (such as the one mentioned involving cats and mammals), only a progressive exclusion of detail.",
        "Instantiation\n\nNon-existent things in any particular place and time are often seen as abstract. By contrast, instances, or members, of such an abstract thing might exist in many different places and times.",
        "Those abstract things are then said to be multiply instantiated, in the sense of picture 1, picture 2, etc., shown below. It is not sufficient, however, to define abstract ideas as those that can be instantiated and to define abstraction as the movement in the opposite direction to instantiation. Doing so would make the concepts \"cat\" and \"telephone\" abstract ideas since despite their varying appearances, a particular cat or a particular telephone is an instance of the concept \"cat\" or the concept \"telephone\". Although the concepts \"cat\" and \"telephone\" are abstractions, they are not abstract in the sense of the objects in graph 1 below. We might look at other graphs, in a progression from cat to mammal to animal, and see that animal is more abstract than mammal; but on the other hand",
        "in the sense of the objects in graph 1 below. We might look at other graphs, in a progression from cat to mammal to animal, and see that animal is more abstract than mammal; but on the other hand mammal is a harder idea to express, certainly in relation to marsupial or monotreme.",
        "Perhaps confusingly, some philosophies refer to tropes (instances of properties) as abstract particulars—e.g., the particular redness of a particular apple is an abstract particular. This is similar to qualia and sumbebekos.\n\nMaterial process\n\nStill retaining the primary meaning of '' or 'to draw away from', the abstraction of money, for example, works by drawing away from the particular value of things allowing completely incommensurate objects to be compared (see the section on 'Physicality' below). Karl Marx's writing on the commodity abstraction recognizes a parallel process.",
        "The state (polity) as both concept and material practice exemplifies the two sides of this process of abstraction. Conceptually, 'the current concept of the state is an abstraction from the much more concrete early-modern use as the standing or status of the prince, his visible estates'. At the same time, materially, the 'practice of statehood is now constitutively and materially more abstract than at the time when princes ruled as the embodiment of extended power'.",
        "Ontological status\nThe way that physical objects, like rocks and trees, have being differs from the way that properties of abstract concepts or relations have being, for example the way the concrete, particular, individuals pictured in picture 1 exist differs from the way the concepts illustrated in graph 1 exist. That difference accounts for the ontological usefulness of the word \"abstract\". The word applies to properties and relations to mark the fact that, if they exist, they do not exist in space or time, but that instances of them can exist, potentially in many different places and times.\n Abstraction this, abstraction\n\nPhysicality",
        "A physical object (a possible referent of a concept or word) is considered concrete (not abstract) if it is a particular individual that occupies a particular place and time. However, in the secondary sense of the term 'abstraction', this physical object can carry materially abstracting processes. For example, record-keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in containers. According to , these clay containers contained tokens, the total of which were the count of objects being transferred. The containers thus served as something of a bill of lading or an accounts book. In order to avoid breaking open the containers for the count, marks were placed on the outside of the",
        "transferred. The containers thus served as something of a bill of lading or an accounts book. In order to avoid breaking open the containers for the count, marks were placed on the outside of the containers. These physical marks, in other words, acted as material abstractions of a materially abstract process of accounting, using conceptual abstractions (numbers) to communicate its meaning.",
        "Abstract things are sometimes defined as those things that do not exist in reality or exist only as sensory experiences, like the color red. That definition, however, suffers from the difficulty of deciding which things are real (i.e. which things exist in reality). For example, it is difficult to agree to whether concepts like God, the number three, and goodness are real, abstract, or both.",
        "An approach to resolving such difficulty is to use predicates as a general term for whether things are variously real, abstract, concrete, or of a particular property (e.g., good). Questions about the properties of things are then propositions about predicates, which propositions remain to be evaluated by the investigator. In the graph 1 below, the graphical relationships like the arrows joining boxes and ellipses might denote predicates.",
        "Referencing and referring\nAbstractions sometimes have ambiguous referents. For example, \"happiness\" can mean experiencing various positive emotions, but can also refer to life satisfaction and subjective well-being. Likewise, \"architecture\" refers not only to the design of safe, functional buildings, but also to elements of creation and innovation which aim at elegant solutions to construction problems, to the use of space, and to the attempt to evoke an emotional response in the builders, owners, viewers and users of the building.",
        "Simplification and ordering\nAbstraction uses a strategy of simplification, wherein formerly concrete details are left ambiguous, vague, or undefined; thus effective communication about things in the abstract requires an intuitive or common experience between the communicator and the communication recipient. This is true for all verbal/abstract communication.",
        "For example, many different things can be red. Likewise, many things sit on surfaces (as in picture 1, to the right). The property of redness and the relation sitting-on are therefore abstractions of those objects. Specifically, the conceptual diagram graph 1 identifies only three boxes, two ellipses, and four arrows (and their five labels), whereas the picture 1 shows much more pictorial detail, with the scores of implied relationships as implicit in the picture rather than with the nine explicit details in the graph.",
        "Graph 1 details some explicit relationships between the objects of the diagram. For example, the arrow between the agent and CAT:Elsie depicts an example of an is-a relationship, as does the arrow between the location and the MAT. The arrows between the gerund/present participle SITTING and the nouns agent and location express the diagram's basic relationship; \"agent is SITTING on location\"; Elsie is an instance of CAT.",
        "Although the description sitting-on (graph 1) is more abstract than the graphic image of a cat sitting on a mat (picture 1), the delineation of abstract things from concrete things is somewhat ambiguous; this ambiguity or vagueness is characteristic of abstraction. Thus something as simple as a newspaper might be specified to six levels, as in Douglas Hofstadter's illustration of that ambiguity, with a progression from abstract to concrete in Gödel, Escher, Bach (1979):\n\nAn abstraction can thus encapsulate each of these levels of detail with no loss of generality. But perhaps a detective or philosopher/scientist/engineer might seek to learn about something, at progressively deeper levels of detail, to solve a crime or a puzzle.",
        "Thought processes\nIn philosophical terminology, abstraction is the thought process wherein ideas are distanced from objects. But an idea can be symbolized.\n\nAs used in different disciplines\n\nIn art",
        "Typically, abstraction is used in the arts as a synonym for abstract art in general. Strictly speaking, it refers to art unconcerned with the literal depiction of things from the visible world—it can, however, refer to an object or image which has been distilled from the real world, or indeed, another work of art. Artwork that reshapes the natural world for expressive purposes is called abstract; that which derives from, but does not imitate a recognizable subject is called nonobjective abstraction. In the 20th century the trend toward abstraction coincided with advances in science, technology, and changes in urban life, eventually reflecting an interest in psychoanalytic theory. Later still, abstraction was manifest in more purely formal terms, such as color, freedom from objective",
        "and changes in urban life, eventually reflecting an interest in psychoanalytic theory. Later still, abstraction was manifest in more purely formal terms, such as color, freedom from objective context, and a reduction of form to basic geometric designs.",
        "In computer science",
        "Computer scientists use abstraction to make models that can be used and re-used without having to re-write all the program code for each new application on every different type of computer. They communicate their solutions with the computer by writing source code in some particular computer language which can be translated into machine code for different types of computers to execute. Abstraction allows program designers to separate a framework (categorical concepts related to computing problems) from specific instances which implement details. This means that the program code can be written so that code does not have to depend on the specific details of supporting applications, operating system software, or hardware, but on a categorical concept of the solution. A solution to the problem",
        "that code does not have to depend on the specific details of supporting applications, operating system software, or hardware, but on a categorical concept of the solution. A solution to the problem can then be integrated into the system framework with minimal additional work. This allows programmers to take advantage of another programmer's work, while requiring only an abstract understanding of the implementation of another's work, apart from the problem that it solves.",
        "In general semantics \nAbstractions and levels of abstraction play an important role in the theory of general semantics originated by Alfred Korzybski. Anatol Rapoport wrote \"Abstracting is a mechanism by which an infinite variety of experiences can be mapped on short noises (words).\"\n\nIn history \nFrancis Fukuyama defines history as \"a deliberate attempt of abstraction in which we separate out important from unimportant events\".\n\nIn linguistics",
        "In history \nFrancis Fukuyama defines history as \"a deliberate attempt of abstraction in which we separate out important from unimportant events\".\n\nIn linguistics\n\nResearchers in linguistics frequently apply abstraction so as to allow an analysis of the phenomena of language at the desired level of detail. A commonly used abstraction, the phoneme, abstracts speech sounds in such a way as to neglect details that cannot serve to differentiate meaning. Other analogous kinds of abstractions (sometimes called \"emic units\") considered by linguists include morphemes, graphemes, and lexemes.",
        "Abstraction also arises in the relation between syntax, semantics, and pragmatics. Pragmatics involves considerations that make reference to the user of the language; semantics considers expressions and what they denote (the designata) abstracted from the language user; and syntax considers only the expressions themselves, abstracted from the designate.\n\nIn mathematics\n\nAbstraction in mathematics is the process of extracting the underlying structures, patterns or properties of a mathematical concept or object, removing any dependence on real-world objects with which it might originally have been connected, and generalizing it so that it has wider applications or matching among other abstract descriptions of equivalent phenomena.",
        "The advantages of abstraction in mathematics are:\n It reveals deep connections between different areas of mathematics.\n Known results in one area can suggest conjectures in another related area.\n Techniques and methods from one area can be applied to prove results in other related area.\nPatterns from one mathematical object can be generalized to other similar objects in the same class.\n\nThe main disadvantage of abstraction is that highly abstract concepts are more difficult to learn, and might require a degree of mathematical maturity and experience before they can be assimilated.",
        "The main disadvantage of abstraction is that highly abstract concepts are more difficult to learn, and might require a degree of mathematical maturity and experience before they can be assimilated.\n\nIn music\nIn music, the term abstraction can be used to describe improvisatory approaches to interpretation, and may sometimes indicate abandonment of tonality. Atonal music has no key signature, and is characterized by the exploration of internal numeric relationships.\n\nIn neurology",
        "In neurology\n\nA recent meta-analysis suggests that the verbal system has a greater engagement with abstract concepts when the perceptual system is more engaged in processing concrete concepts. This is because abstract concepts elicit greater brain activity in the inferior frontal gyrus and middle temporal gyrus compared to concrete concepts which elicit greater activity in the posterior cingulate, precuneus, fusiform gyrus, and parahippocampal gyrus. Other research into the human brain suggests that the left and right hemispheres differ in their handling of abstraction. For example, one meta-analysis reviewing human brain lesions has shown a left hemisphere bias during tool usage.\n\nIn philosophy",
        "In philosophy\n\nAbstraction in philosophy is the process (or, to some, the alleged process) in concept formation of recognizing some set of common features in individuals, and on that basis forming a concept of that feature. The notion of abstraction is important to understanding some philosophical controversies surrounding empiricism and the problem of universals. It has also recently become popular in formal logic under predicate abstraction. Another philosophical tool for the discussion of abstraction is thought space.\n\nJohn Locke defined abstraction in An Essay Concerning Human Understanding:",
        "'So words are used to stand as outward marks of our internal ideas, which are taken from particular things; but if every particular idea that we take in had its own special name, there would be no end to names. To prevent this, the mind makes particular ideas received from particular things become general; which it does by considering them as they are in the mind—mental appearances—separate from all other existences, and from the circumstances of real existence, such as time, place, and so on. This procedure is called abstraction. In it, an idea taken from a particular thing becomes a general representative of all of the same kind, and its name becomes a general name that is applicable to any existing thing that fits that abstract idea.' (2.11.9)",
        "In psychology\nCarl Jung's definition of abstraction broadened its scope beyond the thinking process to include exactly four mutually exclusive, different complementary psychological functions: sensation, intuition, feeling, and thinking. Together they form a structural totality of the differentiating abstraction process. Abstraction operates in one of these functions when it excludes the simultaneous influence of the other functions and other irrelevancies, such as emotion. Abstraction requires selective use of this structural split of abilities in the psyche. The opposite of abstraction is concretism. Abstraction is one of Jung's 57 definitions in Chapter XI of Psychological Types.",
        "In social theory\nSocial theorists deal with abstraction both as an ideational and as a material process. Alfred Sohn-Rethel (1899–1990) asked: \"Can there be abstraction other than by thought?\" He used the example of commodity abstraction to show that abstraction occurs in practice as people create systems of abstract exchange that extend beyond the immediate physicality of the object and yet have real and immediate consequences. This work was extended through the 'Constitutive Abstraction' approach of writers associated with the Journal Arena. Two books that have taken this theme of the abstraction of social relations as an organizing process in human history are Nation Formation: Towards a Theory of Abstract Community (1996)",
        "and an associated volume published in 2006, Globalism, Nationalism, Tribalism: Bringing Theory Back In.\nThese books argue that a nation is an abstract community bringing together strangers who will never meet as such; thus constituting materially real and substantial, but abstracted and mediated relations. The books suggest that contemporary processes of globalization and mediatization have contributed to materially abstracting relations between people, with major consequences for how humans live their lives.",
        "One can readily argue that abstraction is an elementary methodological tool in several disciplines of social science. These disciplines have definite and different concepts of \"man\" that highlight those aspects of man and his behaviour by idealization that are relevant for the given human science. For example,  is the man as sociology abstracts and idealizes it, depicting man as a social being. Moreover, we could talk about  (the man who can extend his biologically determined intelligence thanks to new technologies), or  (who is simply creative).",
        "Abstraction (combined with Weberian idealization) plays a crucial role in economics - hence abstractions such as \"the market\"\nand the generalized concept of \"business\".",
        "Breaking away from directly experienced reality was a common trend in 19th-century sciences (especially physics), and this was the effort which fundamentally determined the way economics tried (and still tries) to approach the economic aspects of social life. It is abstraction we meet in the case of both Newton's physics and the neoclassical theory, since the goal was to grasp the unchangeable and timeless essence of phenomena. For example, Newton created the concept of the material point by following the abstraction method so that he abstracted from the dimension and shape of any perceptible object, preserving only inertial and translational motion. Material point is the ultimate and common feature of all bodies. Neoclassical economists created the indefinitely abstract notion of homo",
        "object, preserving only inertial and translational motion. Material point is the ultimate and common feature of all bodies. Neoclassical economists created the indefinitely abstract notion of homo economicus by following the same procedure. Economists abstract from all individual and personal qualities in order to get to those characteristics that embody the essence of economic activity. Eventually, it is the substance of the economic man that they try to grasp. Any characteristic beyond it only disturbs the functioning of this essential core.",
        "See also\n\nReferences\n\nCitations\n\nSources\n\n \n Sohn-Rethel, Alfred (1977) Intellectual and manual labour: A critique of epistemology, Humanities Press.\n .\n\nFurther reading\n \n \n .\n\nExternal links\n\n \n \n Internet Encyclopedia of Philosophy: Gottlob Frege\n Discussion at The Well concerning Abstraction hierarchy\n\n \nConcepts in epistemology\nConcepts in metaphilosophy\nConcepts in metaphysics\nThought"
    ],
    [
        "Accumulator (computing)\nIn a computer's central processing unit (CPU), the accumulator is a register in which intermediate arithmetic logic unit results are stored. \n\nWithout a register like an accumulator, it would be necessary to write the result of each calculation (addition, multiplication, shift, etc.) to main memory, perhaps only to be read right back again for use in the next operation. \n\nAccess to main memory is slower than access to a register like an accumulator because the technology used for the large main memory is slower (but cheaper) than that used for a register. Early electronic computer systems were often split into two groups, those with accumulators and those without.",
        "Modern computer systems often have multiple general-purpose registers that can operate as accumulators, and the term is no longer as common as it once was. However, to simplify their design, a number of special-purpose processors still use a single accumulator.\n\nBasic concept \nMathematical operations often take place in a stepwise fashion, using the results from one operation as the input to the next. For instance, a manual calculation of a worker's weekly payroll might look something like:",
        "look up the number of hours worked from the employee's time card\n look up the pay rate for that employee from a table\n multiply the hours by the pay rate to get their basic weekly pay\n multiply their basic pay by a fixed percentage to account for income tax\n subtract that number from their basic pay to get their weekly pay after tax\n multiply that result by another fixed percentage to account for retirement plans\n subtract that number from their basic pay to get their weekly pay after all deductions",
        "A computer program carrying out the same task would follow the same basic sequence of operations, although the values being looked up would all be stored in computer memory. In early computers, the number of hours would likely be held on a punch card and the pay rate in some other form of memory, perhaps a magnetic drum. Once the multiplication is complete, the result needs to be placed somewhere. On a \"drum machine\" this would likely be back to the drum, an operation that takes considerable time. And then the very next operation has to read that value back in, which introduces another considerable delay.",
        "Accumulators dramatically improve performance in systems like these by providing a scratchpad area where the results of one operation can be fed to the next one for little or no performance penalty. In the example above, the basic weekly pay would be calculated and placed in the accumulator, which could then immediately be used by the income tax calculation. This removes one save and one read operation from the sequence, operations that generally took tens to hundreds of times as long as the multiplication itself.",
        "Accumulator machines\nAn accumulator machine, also called a 1-operand machine, or a CPU with accumulator-based architecture, is a kind of CPU where, although it may have several registers, the CPU mostly stores the results of calculations in one special register, typically called \"the accumulator\". Almost all  computers were accumulator machines with only the high-performance \"supercomputers\" having multiple registers. Then as mainframe systems gave way to microcomputers, accumulator architectures were again popular with the MOS 6502 being a notable example. Many 8-bit microcontrollers that are still popular as of 2014, such as the PICmicro and 8051, are accumulator-based machines.",
        "Modern CPUs are typically 2-operand or 3-operand machines. The additional operands specify which one of many general-purpose registers (also called \"general-purpose accumulators\") are used as the source and destination for calculations. These CPUs are not considered \"accumulator machines\".",
        "The characteristic which distinguishes one register as being the accumulator of a computer architecture is that the accumulator (if the architecture were to have one) would be used as an implicit operand for arithmetic instructions. For instance, a CPU might have an instruction like: ADD memaddress that adds the value read from memory location memaddress to the value in the accumulator, placing the result back in the accumulator. The accumulator is not identified in the instruction by a register number; it is implicit in the instruction and no other register can be specified in the instruction.  Some architectures use a particular register as an accumulator in some instructions, but other instructions use register numbers for explicit operand specification.",
        "History of the computer accumulator \nAny system that uses a single \"memory\" to store the result of multiple operations can be considered an accumulator. J. Presper Eckert refers to even the earliest adding machines of Gottfried Leibniz and Blaise Pascal as accumulator-based systems. Percy Ludgate was the first to conceive a multiplier-accumulator (MAC) in his Analytical Machine of 1909.\n\nHistorical convention dedicates a register to \"the accumulator\", an \"arithmetic organ\" that literally accumulates its number during a sequence of arithmetic operations:",
        "Historical convention dedicates a register to \"the accumulator\", an \"arithmetic organ\" that literally accumulates its number during a sequence of arithmetic operations:\n\n\"The first part of our arithmetic organ ... should be a parallel storage organ which can receive a number and add it to the one already in it, which is also able to clear its contents and which can store what it contains. We will call such an organ an Accumulator. It is quite conventional in principle in past and present computing machines of the most varied types, e.g. desk multipliers, standard IBM counters, more modern relay machines, the ENIAC\" (Goldstine and von Neumann, 1946; p. 98 in Bell and Newell 1971).",
        "Just a few of the instructions are, for example (with some modern interpretation):\n Clear accumulator and add number from memory location X\n Clear accumulator and subtract number from memory location X\n Add number copied from memory location X to the contents of the accumulator\n Subtract number copied from memory location X from the contents of the accumulator\n Clear accumulator and shift contents of register into accumulator",
        "No convention exists regarding the names for operations from registers to accumulator and from accumulator to registers. Tradition (e.g. Donald Knuth's (1973) hypothetical MIX computer), for example, uses two instructions called load accumulator from register/memory (e.g. \"LDA r\") and store accumulator to register/memory (e.g. \"STA r\"). Knuth's model has many other instructions as well.\n\nNotable accumulator-based computers",
        "The 1945 configuration of ENIAC had 20 accumulators, which could operate in parallel.  Each one could store an eight decimal digit number and add to it (or subtract from it) a number it received. Most of IBM's early binary \"scientific\" computers, beginning with the vacuum tube IBM 701 in 1952, used a single 36-bit accumulator, along with a separate multiplier/quotient register to handle operations with longer results. The IBM 650, a decimal machine, had one 10 digit distributor and two ten-digit accumulators; the IBM 7070, a later, transistorized decimal machine had three accumulators.  The IBM System/360, and Digital Equipment Corporation's PDP-6, had 16 general-purpose registers, although the PDP-6 and its successor, the PDP-10, call them accumulators.",
        "The 12-bit PDP-8 was one of the first minicomputers to use accumulators, and inspired many later machines. The PDP-8 had but one accumulator. The HP 2100 and Data General Nova had 2 and 4 accumulators. The Nova was created when this follow-on to the PDP-8 was rejected in favor of what would become the PDP-11. The Nova provided four accumulators, AC0-AC3, although AC2 and AC3 could also be used to provide offset addresses, tending towards more generality of usage for the registers. The PDP-11 had 8 general-purpose registers, along the lines of the System/360 and PDP-10; most later CISC and RISC machines provided multiple general-purpose registers.",
        "Early 4-bit and 8-bit microprocessors such as the 4004, 8008 and numerous others, typically had single accumulators. The 8051 microcontroller has two, a primary accumulator and a secondary accumulator, where the second is used by instructions only when multiplying (MUL AB) or dividing (DIV AB); the former splits the 16-bit result between the two 8-bit accumulators, whereas the latter stores the quotient on the primary accumulator A and the remainder in the secondary accumulator B. As a direct descendant of the 8008, the 8080, and the 8086, the modern ubiquitous Intel x86 processors still uses the primary accumulator EAX and the secondary accumulator EDX for multiplication and division of large numbers. For instance, MUL ECX will multiply the 32-bit registers ECX and EAX and split the",
        "uses the primary accumulator EAX and the secondary accumulator EDX for multiplication and division of large numbers. For instance, MUL ECX will multiply the 32-bit registers ECX and EAX and split the 64-bit result between EAX and EDX. However, MUL and DIV are special cases; other arithmetic-logical instructions (ADD, SUB, CMP, AND, OR, XOR, TEST) may specify any of the eight registers EAX, ECX, EDX, EBX, ESP, EBP, ESI, EDI as the accumulator (i.e. left operand and destination).  This is also supported for multiply if the upper half of the result is not required.  x86 is thus a fairly general register architecture, despite being based on an accumulator model.  The 64-bit extension of x86, x86-64, has been further generalized to 16 instead of 8 general registers.",
        "References \n\nGoldstine, Herman H., and von Neumann, John, \"Planning and Coding of the Problems for an Electronic Computing Instrument\", Rep. 1947, Institute for Advanced Study, Princeton. Reprinted on pp. 92–119 in Bell, C. Gordon and Newell, Allen (1971), Computer Structures: Readings and Examples, McGraw-Hill Book Company, New York. }. A veritable treasure-trove of detailed descriptions of ancient machines including photos.\n\nCentral processing unit\nDigital registers"
    ],
    [
        "Ackermann function\nIn computability theory, the Ackermann function, named after Wilhelm Ackermann, is one of the simplest and earliest-discovered examples of a total computable function that is not primitive recursive. All primitive recursive functions are total and computable, but the Ackermann function illustrates that not all total computable functions are primitive recursive.",
        "After Ackermann's publication of his function (which had three non-negative integer arguments), many authors modified it to suit various purposes, so that today \"the Ackermann function\" may refer to any of numerous variants of the original function. One common version is the two-argument Ackermann–Péter function developed by Rózsa Péter and Raphael Robinson. Its value grows very rapidly; for example,  results in , an integer of 19,729 decimal digits.",
        "History\nIn the late 1920s, the mathematicians Gabriel Sudan and Wilhelm Ackermann, students of David Hilbert, were studying the foundations of computation. Both Sudan and Ackermann are credited with discovering total computable functions (termed simply \"recursive\" in some references) that are not primitive recursive. Sudan published the lesser-known Sudan function, then shortly afterwards and independently, in 1928, Ackermann published his function  (the Greek letter phi). Ackermann's three-argument function, , is defined such that for , it reproduces the basic operations of addition, multiplication, and exponentiation as\n\nand for p > 2 it extends these basic operations in a way that can be compared to the hyperoperations:",
        "and for p > 2 it extends these basic operations in a way that can be compared to the hyperoperations:\n\n(Aside from its historic role as a total-computable-but-not-primitive-recursive function, Ackermann's original function is seen to extend the basic arithmetic operations beyond exponentiation, although not as seamlessly as do variants of Ackermann's function that are specifically designed for that purpose—such as Goodstein's hyperoperation sequence.)\n\nIn On the Infinite, David Hilbert hypothesized that the Ackermann function was not primitive recursive, but it was Ackermann, Hilbert's personal secretary and former student, who actually proved the hypothesis in his paper On Hilbert's Construction of the Real Numbers.",
        "Rózsa Péter and Raphael Robinson later developed a two-variable version of the Ackermann function that became preferred by almost all authors.\n\nThe generalized hyperoperation sequence, e.g. , is a version of Ackermann function as well.\n\nIn 1963 R.C. Buck based an intuitive two-variable  variant  on the hyperoperation sequence:\n\nCompared to most other versions Buck's function has no unessential offsets:\n\nMany other versions of Ackermann function have been investigated.\n\nDefinition\n\nDefinition: as m-ary function \nAckermann's original three-argument function  is defined recursively as follows for nonnegative integers  and :",
        "Definition\n\nDefinition: as m-ary function \nAckermann's original three-argument function  is defined recursively as follows for nonnegative integers  and :\n\nOf the various two-argument versions, the one developed by Péter and Robinson (called \"the\" Ackermann function by most authors) is defined for nonnegative integers  and  as follows:\n\nThe Ackermann function has also been expressed in relation to the hyperoperation sequence:\n\nor, written in Knuth's up-arrow notation (extended to integer indices ):\n\nor, equivalently, in terms of Buck's function F:\n\nDefinition: as iterated 1-ary function \nDefine  as the n-th iterate of :\n\nIteration is the process of composing a function with itself a certain number of times. Function composition is an associative operation, so .",
        "Iteration is the process of composing a function with itself a certain number of times. Function composition is an associative operation, so .\n\nConceiving the Ackermann function as a sequence of unary functions, one can set .\n\nThe function then becomes a sequence  of unary functions, defined from iteration:\n\nComputation\nThe recursive definition of the Ackermann function can naturally be transposed to a term rewriting system (TRS).\n\nTRS, based on 2-ary function\nThe definition of the 2-ary Ackermann function leads to the obvious reduction rules \n\nExample\n\nCompute \n\nThe reduction sequence is \n\nTo compute  one can use a stack, which initially contains the elements .\n\nThen repeatedly the two top elements are replaced according to the rules\n\nSchematically, starting from :",
        "To compute  one can use a stack, which initially contains the elements .\n\nThen repeatedly the two top elements are replaced according to the rules\n\nSchematically, starting from :\n\n WHILE stackLength <> 1\n {\n    POP 2 elements;\n    PUSH 1 or 2 or 3 elements, applying the rules r1, r2, r3\n }\n\nThe pseudocode is published in .\n\nFor example, on input ,\n\nRemarks\nThe leftmost-innermost strategy is implemented in 225 computer languages on Rosetta Code.\nFor all  the computation of  takes no more than  steps.\n pointed out that in the computation of  the maximum length of the stack is , as long as .\nTheir own algorithm, inherently iterative, computes  within  time and within  space.",
        "TRS, based on iterated 1-ary function\nThe definition of the iterated 1-ary Ackermann functions leads to different reduction rules\n\nAs function composition is associative, instead of rule r6 one can define\n\nLike in the previous section the computation of  can be implemented with a stack.\n\nInitially the stack contains the three elements .\n\nThen repeatedly the three top elements are replaced according to the rules\n\nSchematically, starting from :\n WHILE stackLength <> 1\n {\n    POP 3 elements;\n    PUSH 1 or 3 or 5 elements, applying the rules r4, r5, r6;\n }\n\nExample\n\nOn input  the successive stack configurations are\n\nThe corresponding equalities are\n\nWhen reduction rule r7 is used instead of rule r6, the replacements in the stack will follow\n\nThe successive stack configurations will then be",
        "The corresponding equalities are\n\nWhen reduction rule r7 is used instead of rule r6, the replacements in the stack will follow\n\nThe successive stack configurations will then be\n\nThe corresponding equalities are",
        "Remarks\nOn any given input the TRSs presented so far converge in the same number of steps. They also use the same reduction rules (in this comparison the rules r1, r2, r3 are considered \"the same as\" the rules r4, r5, r6/r7 respectively). For example, the reduction of  converges in 14 steps: 6 × r1, 3 × r2, 5 × r3. The reduction of  converges in the same 14 steps: 6 × r4, 3 × r5, 5 × r6/r7. The TRSs differ in the order in which the reduction rules are applied.",
        "When  is computed following the rules {r4, r5, r6}, the maximum length of the stack stays below . When reduction rule r7 is used instead of rule r6, the maximum length of the stack is only . The length of the stack reflects the recursion depth. As the reduction according to the rules {r4, r5, r7} involves a smaller maximum depth of recursion, this computation is more efficient in that respect.",
        "TRS, based on hyperoperators\nAs  — or  — showed explicitly, the Ackermann function can be expressed in terms of the hyperoperation sequence:\n\nor, after removal of the constant 2 from the parameter list, in terms of Buck's function\n  \n\nBuck's function , a variant of Ackermann function by itself, can be computed with the following reduction rules:\n\nInstead of rule b6 one can define the rule \n\nTo compute the Ackermann function it suffices to add three reduction rules\n\nThese rules take care of the base case A(0,n), the alignment (n+3) and the fudge (-3).\n\nExample\n\nCompute \n\nThe matching equalities are\nwhen the TRS with the reduction rule  is applied:\n\nwhen the TRS with the reduction rule  is applied:",
        "Remarks\nThe computation of  according to the rules {b1 - b5, b6, r8 - r10} is deeply recursive. The maximum depth of nested s is . The culprit is the order in which iteration is executed: . The first  disappears only after the whole sequence is unfolded.\nThe computation according to the rules {b1 - b5, b7, r8 - r10} is more efficient in that respect. The iteration  simulates the repeated loop over a block of code. The nesting is limited to , one recursion level per iterated function.  showed this correspondence.",
        "These considerations concern the recursion depth only. Either way of iterating leads to the same number of reduction steps, involving the same rules (when the rules b6 and b7 are considered \"the same\"). The reduction of  for instance converges in 35 steps: 12 × b1, 4 × b2, 1 × b3, 4 × b5, 12 × b6/b7, 1 × r9, 1 × r10. The modus iterandi only affects the order in which the reduction rules are applied.\nA real gain of execution time can only be achieved by not recalculating subresults over and over again. Memoization is an optimization technique where the results of function calls are cached and returned when the same inputs occur again. See for instance .  published a cunning algorithm which computes  within  time and within  space.",
        "Huge numbers\nTo demonstrate how the computation of  results in many steps and in a large number:\n\nTable of values \nComputing the Ackermann function can be restated in terms of an infinite table. First, place the natural numbers along the top row. To determine a number in the table, take the number immediately to the left. Then use that number to look up the required number in the column given by that number and one row up. If there is no number to its left, simply look at the column headed \"1\" in the previous row. Here is a small upper-left portion of the table:\n\nThe numbers here which are only expressed with recursive exponentiation or Knuth arrows are very large and would take up too much space to notate in plain decimal digits.",
        "The numbers here which are only expressed with recursive exponentiation or Knuth arrows are very large and would take up too much space to notate in plain decimal digits.\n\nDespite the large values occurring in this early section of the table, some even larger numbers have been defined, such as Graham's number, which cannot be written with any small number of Knuth arrows. This number is constructed with a technique similar to applying the Ackermann function to itself recursively.\n\nThis is a repeat of the above table, but with the values replaced by the relevant expression from the function definition to show the pattern clearly:\n\nProperties",
        "General remarks \nIt may not be immediately obvious that the evaluation of  always terminates. However, the recursion is bounded because in each recursive application either  decreases, or  remains the same and  decreases. Each time that  reaches zero,  decreases, so  eventually reaches zero as well. (Expressed more technically, in each case the pair  decreases in the lexicographic order on pairs, which is a well-ordering, just like the ordering of single non-negative integers; this means one cannot go down in the ordering infinitely many times in succession.) However, when  decreases there is no upper bound on how much  can increase — and it will often increase greatly.",
        "For small values of m like 1, 2, or 3, the Ackermann function grows relatively slowly with respect to n (at most exponentially). For , however, it grows much more quickly; even  is about 2.00353, and the decimal expansion of  is very large by any typical measure, about 2.12004.\nAn interesting aspect is that the only arithmetic operation it ever uses is addition of 1. Its fast growing power is based solely on nested recursion. This also implies that its running time is at least proportional to its output, and so is also extremely huge. In actuality, for most cases the running time is far larger than the output; see above.",
        "A single-argument version  that increases both  and  at the same time dwarfs every primitive recursive function, including very fast-growing functions such as the exponential function, the factorial function, multi- and superfactorial functions, and even functions defined using Knuth's up-arrow notation (except when the indexed up-arrow is used). It can be seen that  is roughly comparable to  in the fast-growing hierarchy. This extreme growth can be exploited to show that  which is obviously computable on a machine with infinite memory such as a Turing machine and so is a computable function, grows faster than any primitive recursive function and is therefore not primitive recursive.",
        "Not primitive recursive \n\nThe Ackermann function grows faster than any primitive recursive function and therefore is not itself primitive recursive. The sketch of the proof is this: a primitive recursive function defined using up to k recursions must grow slower than , the (k+1)-th function in the fast-growing hierarchy, but the Ackermann function grows at least as fast as .\n\nSpecifically, one shows that to every primitive recursive function  there exists a non-negative integer  such that for all non-negative integers ,\n\nOnce this is established, it follows that  itself is not primitive recursive, since otherwise putting  would lead to the contradiction \n\nThe proof proceeds as follows: define the class  of all functions that grow slower than the Ackermann function",
        "The proof proceeds as follows: define the class  of all functions that grow slower than the Ackermann function\n\nand show that  contains all primitive recursive functions. The latter is achieved by showing that  contains the constant functions, the successor function, the projection functions and that it is closed under the operations of function composition and primitive recursion.\n\nInverse \nSince the function  considered above grows very rapidly, its inverse function, f, grows very slowly. This inverse Ackermann function f−1 is usually denoted by α. In fact, α(n) is less than 5 for any practical input size n, since  is on the order of .",
        "This inverse appears in the time complexity of some algorithms, such as the disjoint-set data structure and Chazelle's algorithm for minimum spanning trees. Sometimes Ackermann's original function or other variations are used in these settings, but they all grow at similarly high rates. In particular, some modified functions simplify the expression by eliminating the −3 and similar terms.\n\nA two-parameter variation of the inverse Ackermann function can be defined as follows, where  is the floor function:",
        "A two-parameter variation of the inverse Ackermann function can be defined as follows, where  is the floor function:\n\nThis function arises in more precise analyses of the algorithms mentioned above, and gives a more refined time bound. In the disjoint-set data structure, m represents the number of operations while n represents the number of elements; in the minimum spanning tree algorithm, m represents the number of edges while n represents the number of vertices. Several slightly different definitions of  exist; for example,  is sometimes replaced by n, and the floor function is sometimes replaced by a ceiling.\n\nOther studies might define an inverse function of one where m is set to a constant, such that the inverse applies to a particular row.",
        "Other studies might define an inverse function of one where m is set to a constant, such that the inverse applies to a particular row. \n\nThe inverse of the Ackermann function is primitive recursive.\n\nUse as benchmark\nThe Ackermann function, due to its definition in terms of extremely deep recursion, can be used as a benchmark of a compiler's ability to optimize recursion. The first published use of Ackermann's function in this way was in 1970 by Dragoș Vaida and, almost simultaneously, in 1971, by Yngve Sundblad.\n\nSundblad's seminal paper was taken up by Brian Wichmann (co-author of the Whetstone benchmark) in a trilogy of papers written between 1975 and 1982.\n\nSee also",
        "Sundblad's seminal paper was taken up by Brian Wichmann (co-author of the Whetstone benchmark) in a trilogy of papers written between 1975 and 1982.\n\nSee also\n\n Computability theory\n Double recursion\n Fast-growing hierarchy\n Goodstein function\n Primitive recursive function\n Recursion (computer science)\n\nNotes\n\nReferences\n\nBibliography\n\nExternal links\n \n \n \n An animated Ackermann function calculator\n Ackerman function implemented using a for loop\n \n Ackermann functions. Includes a table of some values.\n  describes several variations on the definition of A.\n \n \n The Ackermann function written in different programming languages, (on Rosetta Code)\n ) Some study and programming.\n\nArithmetic\nLarge integers\nSpecial functions\nTheory of computation\nComputability theory"
    ],
    [
        "Acorn Electron\nThe Acorn Electron (nicknamed the Elk inside Acorn and beyond) was a lower-cost alternative to the BBC Micro educational/home computer, also developed by Acorn Computers Ltd, to provide many of the features of that more expensive machine at a price more competitive with that of the ZX Spectrum. It had 32 kilobytes of RAM, and its ROM included BBC BASIC II together with the operating system. Announced in 1982 for a possible release the same year, it was eventually introduced on 25 August 1983 priced at £199.",
        "The Electron was able to save and load programs onto audio cassette via a supplied cable that connected it to any standard tape recorder that had the correct sockets. It was capable of bitmapped graphics, and could use either a television set, a colour (RGB) monitor or a monochrome monitor as its display. Several expansions were made available to provide many of the capabilities omitted from the BBC Micro. Acorn introduced a general-purpose expansion unit, the Plus 1, offering analogue joystick and parallel ports, together with cartridge slots into which ROM cartridges, providing software, or other kinds of hardware expansions, such as disc interfaces, could be inserted. Acorn also produced a dedicated disc expansion, the Plus 3, featuring a disc controller and 3.5-inch floppy drive.",
        "For a short period, the Electron was reportedly the best selling micro in the United Kingdom, with an estimated 200,000 to 250,000 machines sold over its entire commercial lifespan. With production effectively discontinued by Acorn as early as 1985, and with the machine offered in bundles with games and expansions, later being substantially discounted by retailers, a revival in demand for the Electron supported a market for software and expansions without Acorn's involvement, with its market for games also helping to sustain the continued viability of games production for the BBC Micro.\n\nHistory",
        "History\n\nAfter Acorn Computers released the BBC Micro, executives believed that the company needed a less-expensive computer for the mass market. In May 1982, when asked about the recently announced Sinclair ZX Spectrum's potential to hurt sales of the BBC Micro, priced at £125 for the 16K model compared to around twice that price for the 16K BBC Model A, Acorn co-founder Hermann Hauser responded that in the third quarter of that year Acorn would release a new £120–150 computer which \"will probably be called the Electron\", a form of \"miniaturised BBC Micro\", having 32 KB of RAM and 32 KB of ROM, with \"higher resolution graphics than those offered by the Spectrum\".",
        "Acorn co-founder Chris Curry also emphasised the Electron's role as being \"designed to compete with the Spectrum... to get the starting price very low, but not preclude expansion in the long term.\" In order to reduce component costs, and to prevent cloning, the company reduced the number of chips in the Electron from the 102 on the BBC Micro's motherboard to \"something like 12 to 14 chips\" with most functionality on a single 2,400-gate Uncommitted Logic Array (ULA). Reports during the second half of 1982 indicated a potential December release, with Curry providing qualified confirmation of such plans, together with an accurate depiction of the machine's form and capabilities, noting that the \"massive ULA\" would be the \"dominant factor\" in any pre-Christmas release. However, as the end of",
        "plans, together with an accurate depiction of the machine's form and capabilities, noting that the \"massive ULA\" would be the \"dominant factor\" in any pre-Christmas release. However, as the end of the year approached, with the ULA not ready for \"main production\", the launch of the Electron was to be delayed until the spring.",
        "By June 1983, with the planned March release having passed, the launch of the Electron had been rescheduled for the Acorn User Exhibition in August 1983, and the machine was indeed launched at the event. The company expected to ship the Electron before Christmas, and sell 100,000 by February 1984. The price at launch - £199 - remained unchanged from that stated in an announcement earlier in the year, with the machine's nickname within Acorn - the \"Elk\" - also being reported publicly for perhaps the first time.",
        "Reviews were generally favourable, starting with positive impressions based on the physical design of the computer, with one reviewer noting, \"The Electron is beautifully designed and built — quite a shock compared to the BBC. Its designer case will look great on the coffee table.\" Praise was also forthcoming for the Electron's keyboard which was regarded as being better than most of its low-cost peers, with only the VIC-20 being comparable. In one review, the keyboard was even regarded as better than the one in the BBC Micro. The provision of rapid BASIC keyword entry though the combination of the  key with various letter keys was also welcomed as a helpful aid to prevent typing errors by \"most users\", while \"touch typists\" were still able to type out the keywords in full.",
        "Reviewers also welcomed the machine's excellent graphics compared to its rivals, noting that \"the graphics are much more flexible and the maximum resolution is many times that of the Spectrum's\". The provision of screen modes supporting 80 columns of readable text and graphics resolutions of  was described as \"unrivalled by every machine up to the BBC Model B itself\", although the absence of a teletext mode was considered regrettable. Although valued for its low memory usage characteristics in the BBC Micro, one reviewer considered the absence of a \"software simulation of a teletext screen\" to be a \"lazy omission\" even if it would have to be \"awfully slow and take up piles of memory\".",
        "While its speed was acceptable compared to its immediate competition, the Electron was, however, rather slower than the BBC Micro with one review noting that games designed for the BBC Micro ran \"at less than half the speed, with very significant effects on their appeal\". The reduced performance can be attributed to the use of a 4-bit wide memory system instead of the 8-bit wide memory system of the BBC Micro to reduce cost. Due to needing two accesses to the memory instead of one to fetch each byte, along with contention with the video hardware also needing access, reading or writing RAM was much slower than on the BBC Micro. Reviewers were also disappointed by the single-channel sound, noting that \"BBC-style music\" and its \"imitations of various musical instruments\" would not be",
        "RAM was much slower than on the BBC Micro. Reviewers were also disappointed by the single-channel sound, noting that \"BBC-style music\" and its \"imitations of various musical instruments\" would not be possible, the latter due to the inability of the sound system to vary the amplitude of sounds.",
        "Despite some uncertainty about Acorn's target demographic for the Electron, some noted the potential for the machine in education given its robustness, but also given its price, noting that the high price of BBC Model B machines seemed \"rarely justified by their actual practical applications in schools\". The introduction of the Electron was seen as potentially leading to competition between Acorn's different models within the schools market rather than creating a broader audience for them, although the potential for more computers in schools, giving more \"hands-on\" experience for students, was welcomed. Nevertheless, reviewers anticipated that the Electron would sell well at the lower end of the market, with projected sales of 100,000 units by Christmas 1983, helped by the Electron's",
        "was welcomed. Nevertheless, reviewers anticipated that the Electron would sell well at the lower end of the market, with projected sales of 100,000 units by Christmas 1983, helped by the Electron's software compatibility with the BBC Micro and the already established reputation of its predecessor. With parents potentially being convinced of the Electron's educational value, some reviewers foresaw a conflict between parents and \"discerning children\", the latter merely wanting to play games and preferring models with sound and graphics capabilities more appropriate for gaming. Although Acorn had based its expansion into the United States on the BBC Micro, the company did have plans to introduce the Electron at a later time, with Chris Curry having indicated \"a very heavy push overseas\"",
        "had based its expansion into the United States on the BBC Micro, the company did have plans to introduce the Electron at a later time, with Chris Curry having indicated \"a very heavy push overseas\" involving both the BBC Micro and Electron. A model for the US market was described in an official book, The Acorn Guide to the Electron, but this model was never produced.",
        "Production difficulties at Astec in Malaysia delayed the machine's introduction, forcing Acorn to look to other manufacturers such as AB Electronics in Wales and Wongs in Hong Kong (an original equipment manufacturer making over 30 million circuit boards a year, along with power supplies and plastic housings, for companies such as IBM, Xerox, Atari, and Apple, including units made for Acorn for the BBC Micro). By October 1983, Acorn had received orders for more than 150,000 units, but had production targets of only 25,000 a month before Christmas, meaning that the existing backlog would take more than six months to fulfil. Demand for the Electron was high but only two of WH Smith's London branches had inventory. Ultimately, manufacturing in Malaysia ceased with the anticipated but",
        "take more than six months to fulfil. Demand for the Electron was high but only two of WH Smith's London branches had inventory. Ultimately, manufacturing in Malaysia ceased with the anticipated but unspecified number of units having been produced, this having been originally reported as 100,000 units. Acorn's marketing manager, Tom Hohenberg, admitted in early 1984 that \"a lot of the trouble stemmed from the ULA\" in getting production to the desired levels, but that such difficulties had been resolved, although Acorn faced an order backlog of almost a quarter of a million units.",
        "As the company increased production during 1984, however, the British home computer market greatly weakened. Hohenberg later noted that after the 1983 Christmas season, Electron deliveries had increased to meet a demand that was no longer there, with the market having \"completely dried up\". Acorn's Christmas 1984 sales were greatly below expectations and by March 1985 the company had reduced the Electron's price to £129. With the company's unsuccessful expansion into the United States abandoned, Acorn's financial situation had deteriorated sufficiently to prompt Olivetti to rescue the company by taking a 49.30% ownership stake. Renewed efforts were made to sell the machine, bundling it with Acorn's own expansions and software, such as one package adding the Plus 1 expansion, joysticks and",
        "by taking a 49.30% ownership stake. Renewed efforts were made to sell the machine, bundling it with Acorn's own expansions and software, such as one package adding the Plus 1 expansion, joysticks and a ROM cartridge game to the base machine for a total price of £219. Acorn committed to supporting the machine \"until the end of 1986\", continuing to supply it (as the Merlin M2105) to British Telecom as part of the Healthnet communications system, with small-scale manufacturing continuing while existing stocks were being run down.",
        "By autumn, retailers appeared eager to discount the computer, with prices in stores as low as £100, reportedly less than the distributor prices of the summer months. As the Christmas season approached, Dixons Retail acquired the remaining Electron inventory to sell, bundled with a cassette recorder and software, at a retail price of £99.95. This deal, from the perspective of a year later, apparently played a significant part in helping to reduce Acorn's unsold inventory from a value of £18 million to around £7.9 million, and in combination with \"streamlining corporate activities and reducing overheads\", had helped to reduce Acorn's losses from over £20 million to less than £3 million.",
        "The deal effectively brought to an end Acorn's interest in the Electron and the lower-cost end of the home computing market, but empowered third-party suppliers whose \"inventiveness and initiative\" was noted as being in contrast with Acorn's lack of interest in the product and the \"false promises\" made to its users. However, Acorn subsequently released the Master Compact a model in the Master series of microcomputers with fewer BBC Micro-style ports and a similar expansion connector to that used by the Electron with the home audience specifically in mind. Indeed, prior to its release, the Master Compact had been perceived as the successor to the Electron. Superficial similarities between the Compact and Acorn Communicator, together with technical similarities between the Electron",
        "the Master Compact had been perceived as the successor to the Electron. Superficial similarities between the Compact and Acorn Communicator, together with technical similarities between the Electron (particularly when expanded in the form of the Merlin M2105) and the Communicator, may also have driven rumours of an updated Electron model. A more substantial emphasis on the \"home, music and hobby sectors\" came with the appointment of a dedicated marketing manager in 1989 following the launch of the BBC A3000 in the Acorn Archimedes range.",
        "Although the Electron presented challenges to developers in terms of the amount of memory available to programs and, particularly for those writing or porting games to the machine, a reduction in hardware features useful for controlling or presenting content on the screen, developers often discovered creative workarounds to deliver commercially successful products, making the business of writing conversions a viable one for some developers.",
        "Several features that would later be associated with the BBC Master and Archimedes first appeared as features of Electron expansion units, including ROM cartridge slots and the Advanced Disc Filing System, a hierarchical improvement to the BBC's original Disc Filing System. Having been envisaged as the basis of a portable computer with \"a very strong emphasis on communications\" during its development, supporting both modem and Econet interfaces, the BT Merlin M2105 product subsequently combined the Electron with communications functionality, and the Acorn Communicator developed such concepts further, introducing networking support.",
        "The availability of the Electron at discounted prices from 1985 onwards led to increased demand for third-party software and expansions for the machine. While it may not have been as popular as the Spectrum, Commodore 64 or Amstrad CPC, it did sell in sufficient numbers to ensure that new software titles from established producers were being produced right up until the early 1990s, with mainstream publications dedicated to the machine having effectively supported it for five years beyond the point at which Acorn's own support had ceased.",
        "Hardware expansions\nSince the Electron provided only a selection of video output ports, a cassette port and the expansion connector, a range of additional expansions were produced to offer ports and connections to various peripherals. The first expansions were largely joystick and printer interfaces or sideways ROM boards. For instance, First Byte Computers developed an interface and software which allowed a \"switched\" joystick to be used with the majority of software titles. This interface became very popular and was sold by W.H. Smiths, Boots, Comet and hundreds of independent computer dealers, selling as many as 23,000 units over a two-year period, helped by a bundling agreement with Dixons.",
        "Acorn's own expansion strategy was led by the Plus 1 which offered a combination of ports and cartridge connectors, followed by the Plus 3 disc drive unit, but by early 1986 the more general range of expansions had broadened to include floppy drive and RS423 interfaces, Teletext adapters, and other fundamental enhancements to the base machine.",
        "Multi-function expansions\nSince the Electron's expansion connector was the basis of practically all external hardware expansions for the machine, unless an expansion propagated this connector to others, as was done by the Acorn Plus 3, the capabilities of any given expansion would limit the capabilities of the expanded machine. Thus, expansions offering a single function, such as joystick ports or a printer port, would need to be disconnected if other capabilities were needed, and then reconnected later. Consequently, multi-function expansions offering a combination of different capabilities offered a significant degree of convenience as well as avoiding wear on the expansion connector.",
        "Alongside announcements of Acorn's then-unreleased Plus 1, Solidisk previewed a General Purpose Interface for the Electron in early 1984 offering a Centronics printer port, switched joystick port, user port, sideways ROM sockets, and mini-cartridge sockets supported by the 6522 versatile interface adapter (VIA) chip. The Plus 1 itself was released in mid-1984, introducing the influential cartridge format for expansions ultimately used by several other companies.\n\nAcorn Plus 1",
        "Acorn Plus 1\n\nThe Acorn Plus 1 added two ROM cartridge slots, an analogue interface (supporting four channels) and a Centronics parallel port, priced at £59.90. The analogue interface was normally used for joysticks, although trackball and graphics tablet devices were available, and the parallel interface was typically used to connect a printer. Game ROM cartridges would boot automatically. Languages in paged ROM cartridges would take precedence over BASIC. (The slot at the front of the interface took priority if both were populated.)",
        "Access to ROM occurred at 2 MHz until RAM access was required, so theoretically programs released on ROM could run up to twice as fast as those released on tape or disc. Despite this, all of the games released on ROM were packaged as ROM filing system cartridges, from which the micro would load programs into main memory in exactly the same way as if it were loading from tape. This meant that programs did not need to be modified for their new memory location and could be written in BASIC but gave no execution speed benefits. Six ROM cartridge titles were announced for the launch of the Plus 1: three arcade games, one adventure game, one educational title, and the Lisp language implementation, the latter being a genuine language ROM that \"takes the place of the BASIC ROM\" and is instantly",
        "three arcade games, one adventure game, one educational title, and the Lisp language implementation, the latter being a genuine language ROM that \"takes the place of the BASIC ROM\" and is instantly available when switching on.",
        "The cartridge slots provided additional control lines (compared to the lines available via the edge connector on the rear of the Electron) to ease implementation of ROM cartridges. Acorn described the hardware extension possibilities in promotional literature, giving an RS423 cartridge as an example of this capability of the Plus 1.",
        "Additional peripheral cartridges were produced by companies such as Advanced Computer Products (and subsequently PRES) whose Advanced ROM Adaptor (ARA) and Advanced Sideways RAM (ASR) products provided sideways ROM and RAM capabilities,  allowing ROM- or EPROM-based software to be accessed to provide languages, utilities and applications. ROM or EPROM devices containing the software could be physically inserted into empty ZIF sockets, or the software would be loaded from ROM image files (typically provided on disk) into RAM devices fitted in such sockets. Such RAM could potentially be powered by a battery and thus be able to retain its contents when the computer itself was powered off. Both such arrangements exposed the software in the same sideways memory region.",
        "Such cartridge support enabled the Electron to provide the same functionality as that offered by the expansion ROM slots under the keyboard and on the bottom-left of the BBC Micro B keyboard. However, the need to use cartridge sockets for other peripherals encouraged PRES to develop the Advanced Plus 6 (AP6): an internal RAM and ROM board for the Plus 1 providing six sockets that could be freely used for ROM, EPROM and RAM devices. Installation of the AP6 unit required some modifications to the Plus 1, undertaken either by the user or by PRES, and the product could also be enhanced with the Advanced Plus 7 offering battery-backed RAM support for two 16 KB banks.",
        "The addition of the Plus 1 added a number of new *FX or OSBYTE calls that allowed the OS to read values from the analogue interface and write to the parallel interface.\n\nThe Plus 1 needed memory page &D for its workspace, and the unit added some processing overhead when enabled, both of these things causing issues with the loading and running of software, particularly cassette-based games. To disable the Plus 1, after pressing BREAK, the following commands could be issued:\n\n *FX163,128,1\n ?&212=&D6\n ?&213=&F1\n ?&2AC=0\n\nAn official application note described a similar set of commands to \"remove the Plus One completely from the address map disabling the Centronics and A/D ports (additionally disabling the RS423 cartridge if fitted)\".\n\nFurther developments",
        "After Acorn's change of focus away from the Electron, and with a shortage of Plus 1 units available to purchase, Advanced Computer Products secured the rights to manufacture the unit under licence from Acorn, obtaining the injection moulds and tooling, thus restarting production in 1987 after Acorn's own production of the unit had ceased in November 1985. The Advanced Plus 2 (AP2) ROM was later sold by PRES as a replacement ROM for the Plus 1, of whose 8 KB utility ROM only 3 KB had been used, thus providing an opportunity for a more comprehensive ROM to be developed. The AP2 added ROM management commands familiar from the BBC Master series, support for various sideways RAM products from PRES, disc formatting and verification utilities for different ADFS versions, a command to disable the",
        "commands familiar from the BBC Master series, support for various sideways RAM products from PRES, disc formatting and verification utilities for different ADFS versions, a command to disable the Plus 1 entirely, and improvements that made tape loading more reliable in \"high memory\" screen modes.",
        "Slogger Rombox Plus",
        "Following on from Slogger's earlier Rombox product an expansion similar in profile to the Plus 1 but offering eight ROM sockets and propagating the expansion connector to other units the Rombox Plus was positioned more directly as a competitor to the Plus 1 in that it offered two cartridge slots and a Centronics print port alongside four ROM sockets. Priced at £49.95, the unit was mostly compatible with cartridges designed for the Plus 1 although one reviewer reported physical issues with some expansion cartridges, suggesting some manufacturing inconsistencies given other users' more positive experiences, but indicated that it was still \"worth considering as an alternative to the Plus 1\". One review reported that the Cumana Electron Filing System cartridge had an edge connector that would",
        "experiences, but indicated that it was still \"worth considering as an alternative to the Plus 1\". One review reported that the Cumana Electron Filing System cartridge had an edge connector that would not physically fit inside the slot in the Rombox Plus unit; this along with a perceived lack of robustness of the case being their only major reservations about the product. The product's support for utilising 8 KB RAM devices as a printer buffer, with buffer management provided by the built-in EPROM, was noted as a particular advantage over the Plus 1.",
        "Slogger Remote Expansion and Plus 2\nIn early 1989, Slogger announced its \"remote expansion\" (RX) system for the Electron, providing a separate case with power supply to house expansions and disc drives, able to support the weight of a monitor or television. Intended for the RX system, the Plus 2 offered two ROM cartridge slots compatible with the Plus 1, three ROM sockets, and RS423 and user port capabilities. One application of the user port was to connect a mouse, utilised by Slogger's version of the Stop Press desktop publishing package by Advanced Memory Systems.",
        "Software Bargains Plus 1",
        "In mid-1989, Software Bargains announced an expansion providing different levels of Plus 1 functionality, offered as a bare printed circuit board without casing and in three different variants: the basic model offered one cartridge port and was bundled with View and Viewsheet cartridges for £29.95; an extended model offered one cartridge port and a printer port with the two bundled cartridges for £36.95; the full model offered two cartridge ports, printer port and the bundled cartridges for £39.95. Various board upgrade options were also offered between the variants, with the product being described mainly as a vehicle to expose the bundled software packages to as many as 150,000 owners of the estimated 200,000 Electrons in the UK who \"have not yet been able to acquire or use View or",
        "described mainly as a vehicle to expose the bundled software packages to as many as 150,000 owners of the estimated 200,000 Electrons in the UK who \"have not yet been able to acquire or use View or Viewsheet\". The lack of casing was considered the most significant disadvantage, with the absence of the analogue port deemed less critical due to a general lack of support for joysticks in many games.",
        "Communications and networking\nTo support connectivity, Acorn announced a Plus 2 network interface with availability scheduled for early 1985, together with a RS423 cartridge for the Plus 1. Neither of these products were delivered as announced.\n\nAcorn Plus 2\nThe Acorn Plus 2 interface was due to provide Econet capability. This interface did not make it to market. However, an Econet interface was produced by Barson Computers in Australia and possibly other individuals and businesses.\n\nAcorn Plus 4\nThe Acorn Plus 4 interface was due to provide a modem communications capability.\n\nAndyk RS423 cartridge\nAndyk announced an RS423 cartridge for the Plus 1 providing a serial port, alongside other products, in late 1985. It was priced at £34.99.",
        "Andyk RS423 cartridge\nAndyk announced an RS423 cartridge for the Plus 1 providing a serial port, alongside other products, in late 1985. It was priced at £34.99.\n\nPace Tellstar/Nightingale\nOriginally reported in mid-1985 as a collaboration between Acorn and Pace Micro Technology, but launched in early 1986, Pace offered a communications product consisting of a RS423 cartridge, bundled with a Nightingale modem and Tellstar communications software, offered at a discounted price of £145.",
        "Jafa Systems RS423 cartridge\nJafa Systems announced an RS423 cartridge in late 1989 to \"fill a two year gap in the market\", offering a serial connector compatible with the BBC Micro together with an on-board socket for 8 KB or 16 KB EPROM devices or for 32 KB RAM, the latter being configured to present two sideways RAM banks to the system. Write protect functionality was supported to prevent certain ROM software from attempting to overwrite itself if stored in RAM. The cartridge board itself was priced at around £30, with a case costing £5 extra, and an optional 32 KB RAM adding another £20. Support for the E00 ADFS offered by PRES for that company's AP3 disc system was indicated as an application for the sideways RAM.",
        "Slogger Plus 2 RS423 interface\nSlogger provided an RS423 interface as an option for its own Plus 2 expansion, announced in early 1989.",
        "Disc interfaces",
        "The first disc interface to be announced for the Electron was Pace's Le Box in 1984, offering a single-sided 100 KB floppy drive controlled by the 8271 controller and accessed using the Amcom Disc Filing System, with pricing at £299 plus VAT including the drive or around £199 without. The unit also provided eight sideways ROM sockets and was intended to sit under the Electron itself. The unit was connected via cabling to the expansion edge connector and included its own power supply, and other drives including switchable 40/80 track drives offering up to 400 KB capacity were dealer-supplied options. Although the product was meant to be on sale at the Acorn User Show in August 1984, and had been advertised, it was \"discontinued\" in early 1985 before getting to market, with a Pace",
        "options. Although the product was meant to be on sale at the Acorn User Show in August 1984, and had been advertised, it was \"discontinued\" in early 1985 before getting to market, with a Pace representative indicating that prohibitive pricing of the 8271 chips (each at \"over £80 at times\") had left the company considering a re-launch of the product should the pricing situation become more favourable.",
        "Following on from Acorn's Plus 3 interface, Cumana, Solidisk, Advanced Computer Products and Slogger all offered disc interfaces for the Electron. Unlike disc systems on the BBC Micro and the Acorn Plus 3, many of the systems released for the Electron did not claim RAM workspace (and raise the PAGE variable affecting applications above the default of &E00), making it easier to use cassette-based software transferred to disc and to run larger programs from disc.",
        "Low-cost alternatives to disc systems, briefly made fashionable by press coverage of the Sinclair Microdrive, were reportedly under development by expansion suppliers such as Solidisk, and finished products such as the Phloopy looped tape system were offered for the Electron. Reliability issues were described with the Phloopy, and the product was apparently short-lived.\n\nAcorn Plus 3\n\nLaunched in late 1984 for a price of £229, the Acorn Plus 3 was a hardware module that connected independently of the Plus 1 and provided a \"self contained disc interface and 3.5 inch single sided disc drive\" offering over 300 KB of storage per disc using the newly introduced Advanced Disc Filing System (ADFS). The Plus 3 was also reportedly produced with a double-sided drive fitted.",
        "An expansion connector for a second 3.5- or 5.25-inch drive was also provided by the unit, with such drives needing to provide a Shugart-compatible connector and their own power supply. The original Electron edge connector was repeated on the back of the Plus 3, allowing the Plus 1 or other compatible expansion to be connected in conjunction with the Plus 3.",
        "The double-density drive of the Plus 3 was driven using a WD1770 drive controller by the ADFS. (The Plus 3 had been rumoured to offer Acorn's DFS and to feature a 8272 double-density disk controller before its launch.) Because the WD1770 is capable of single-density mode and uses the same IBM360-derived floppy disc format as the Intel 8271 found in the BBC Micro, it was also possible to use the Disc Filing System with an alternate ROM, such as the ACP 1770 DFS.",
        "The Plus 3 reset PAGE to &1D00, reducing the amount of free RAM available to the user. The ADFS system could be temporarily disabled (and PAGE reset to &E00) via the  command. Later products such as the PRES E00 ADFS remedied the memory demands of the ADFS, along with other issues suffered by the software as delivered with the Plus 3. If using the Plus 3 in screen modes 0–3, the pseudo-variable  would be thrown off, as the interrupts were disabled during disk access in these modes. The screen would also blank during disc accesses.",
        "Disks had to be manually mounted and dismounted using the  /  commands, or using the ++ key combination. Disks could also be booted from via the standard + key-combination, if the !BOOT file was present on the disk. This behaviour was the same as on the BBC Micro.\n\nThe Plus 3 included an uprated square black power supply unit with mains cord, manufactured by STC, designed and manufactured in England to  and , that was designed to power the Plus 3, in addition to the Electron and the Plus 1 interface as well. This replaced the original cream-coloured \"wall wart\" style power supply, designed to  and manufactured in Hong Kong.",
        "Original Part no: 0201,113; input 220/240 V AC/50 Hz; output 19 V AC/0.737 A/14 W:  Usage: Electron, Electron+Plus1\n Uprated Part no: 0865,010; input 240 V AC/50 Hz 50 W; output 21 V AC/1.75 A/36.75 W: Usage: Electron+Plus3, Electron+Plus3+Plus1\n\nRepair note: If the internal power-supply connector, used to power the existing internal 3.5-inch drive is damaged, and requires replacement, then the original AMP 800-930 4-pin connector, which was already in short supply during the original production run, may be replaced with a Molex 5264 50-37-5043 \"Mini-SPOX\" connector as an alternative.\n\nAdvanced Plus 3",
        "Advanced Plus 3\n\nDesigned and produced by Baildon Electronics and sold by PRES, the Advanced Plus 3 (AP3) was a Plus 1 cartridge interface using the WD1770 controller, supplied with Acorn's ADFS and a single-sided 3½-inch disc drive for £99 plus VAT, offering equivalent functionality to the Acorn Plus 3. Announced in late 1987, the product was made possible by an agreement between ACP and Acorn to license the ADFS software. As with many disc interfaces for the Electron, since the interface provided a connector for the drive, this made it possible to connect a 5¼\" floppy disc drive (more common amongst BBC Micro owners) or the more typical 3½-inch drive.",
        "PRES later released a version of ADFS with support for PAGE at &E00, this being achieved by using RAM provided by the Advanced Battery Backed RAM (ABR) cartridge. This version also fixed two notable bugs in Acorn's ADFS, eliminating unreliability when accessing the first tracks on a disc which had previously necessitated the writing of a file (ZYSYSHELP) as a workaround, and switching off the text cursor during disc compaction which had previously caused disc corruption (since the disc data would be processed using screen memory during this operation, and the cursor would modify that data when blinking). The ROM image was supplied on disc for £17.19, whereas a bundle of the ROM and ABR cartridge was £50.95.",
        "In 1989, the Advanced Plus 3 Mark 2 was launched, offering a double-sided drive in place of the single-sided drive previously offered. This meant that the storage capacity of each disc was increased from the 320 KB of the original Plus 3 to 640 KB (this being supported by ADFS on the Master Compact).",
        "Cumana Floppy Disc System\nEarly in 1985, Cumana released a cartridge-based interface providing support for double-density storage, a real-time clock and calendar for timestamping of files, and a spare ROM socket for user-fitted sideways ROMs. The filing system used was Cumana's own QFS, supporting 89 files per disc, PAGE at &E00, a non-hierarchical catalogue, ten-character filenames, with a format not directly compatible with either of Acorn's DFS or ADFS. The interface itself cost £149.95 when originally announced, but settled at around £115.95 including VAT, also being offered in a promotional bundle with a 5¼-inch drive for £224.15 including VAT. Later pricing put the interface at £74.95 including VAT.",
        "Solidisk EFS\nIn mid-1985, Solidisk released a cartridge-based interface with support for single and double density storage and providing Acorn DFS and ADFS compatibility, 16 KB of on-board sideways RAM, and a connector for a Winchester hard drive. The cartridge itself cost £59, with a bundle including a double-sided, double-density, 3½-inch drive costing £200. A 20 MB hard drive was offered at a price of £805.",
        "Advanced Plus 4\nAnnounced in early 1986, the Advanced Plus 4 (AP4) from Advanced Computer Products was a cartridge-based interface employing the WD1770 controller and featuring ACP's 1770 DFS product, providing compatibility with Acorn's DFS from the BBC Micro and thereby supporting seven-character filenames and up to 31 files per disc. However, 8 KB of on-board static RAM was used as workspace for the filing system, keeping PAGE at &E00. An extra ROM socket was provided for a user-fitted sideways ROM, and being a 1770-based interface, it was reported that Acorn's ADFS could be used instead, although since it was not aware of the additional RAM, PAGE would be raised to &1D00 as it would be when using Acorn's Plus 3. The interface was priced at £69.55 plus VAT.",
        "Slogger Electron Disc System and Pegasus 400\nSlogger, an established producer of expansions and a reseller of other disc systems, introduced the Electron Disc System in early 1987, priced at £74.95, featuring the Cumana Floppy Disc System interface, which was combined with an Acorn-compatible DFS, SEDFS, having the capability of reading 40-track discs on 80-track drives plus support for Slogger's tape-to-disc conversion products, and reported as offering \"virtual 100 per cent 8271 emulation\" for compatibility with traditional DFS software. The SEDFS ROM was also available separately for existing Cumana interface owners, priced at £24.95.",
        "The SEDFS was later bundled with Slogger's own cartridge-based interface and a 40/80-track switchable drive offering up to 400 KB storage per disc, with the bundle taking the Pegasus 400 name, introduced as part of a sales tour towards the end of 1987. This package of interface and drive cost £130. The precise DFS variant used by the Pegasus 400 system kept PAGE at &E00 and introduced \"typeahead\" support, permitting keystroke buffering during disc activity on systems with the Turbo-Driver or Master RAM Board fitted and enabled.",
        "Slogger/Elektuur Turbo boards\nAnnounced in early 1986, the Slogger Turbo-Driver was a professionally fitted upgrade priced at £42. The board itself plugged into the CPU and BASIC ROM sockets on the main circuit board of the Electron, which merely involved removing socketed components on very early Electron models, but required desoldering work and therefore benefited from a fitting service for later units. The performance benefit of fitting the board was to make some programs, particularly those running in the high bandwidth modes (0 to 3), run up to three times faster.",
        "The direct origins of the Slogger product appear to be a board designed by Andyk Limited, announced as the Fast Electron Board in late 1985 with a price of £29.99, whereas the Elektuur modification was described in an article in Dutch Electronics magazine Elektuur and intended for users to perform at home.",
        "The Slogger and Elektuur Turbo boards were born out of a hack initially devised at Acorn. By shadowing the lowest 8 KB of RAM with a static RAM chip outside of reach of the ULA, the CPU could always access it at 2 MHz. The tradeoff was that the screen could not be located in that 8 KB. In practice the operating system ROMs always put the screen into the top 20 KB and as a result this probably only broke compatibility with around 2% of software. Speeding up the low portion of memory is particularly useful on 6502 derived machines because that processor has a faster addressing for the first 256 bytes and so it is common for software to put any variables involved in time-critical sections of program into that region.",
        "The cost of the 64 Kbit SRAM chip would have been more than that of doubling the four 64 Kbit DRAM chips to give 8-bit RAM access, fixing both the modest memory and poor performance issues of the Electron.",
        "Slogger Master RAM Board\nIntroduced at around the start of 1987 and priced at £64.95 fitted or £54.95 as a kit, the Master RAM Board offered the familiar turbo mode from the Slogger Turbo-Driver alongside a shadow mode providing 32 KB of static shadow RAM in addition to the existing 32 KB, thus giving 64 KB in total. So-called \"legally written software\", this being software using the operating system calls and not writing directly to the screen, could function without significant modification, making substantially more memory available for BASIC, View, Viewsheet, language ROMs and many other applications. By providing extra storage this modification also allowed some games and applications intended for the BBC Micro to function on the Electron despite the lack of a native Mode 7.",
        "Applications could not directly address video memory in shadow mode without modification, so it was incompatible with most games, although there is no inherent reason why a game could not be written to function in shadow mode. A switch mounted through the case switched between normal, turbo and shadow modes.\n\nTowards the end of the Electron's commercial lifetime, the Turbo-Driver and Master RAM Boards were offered already fitted to new Electrons in an attempt to increase sales. For a time, Jafa Systems manufactured their own equivalent of the Master RAM Board in order to support their own product range.",
        "Mode 7 display expansions\nOne of the features of the BBC Micro that was absent in the Electron was the Teletext-style Mode 7 display. The omission of this display mode was remarkable because it had a very low memory requirement (just 1000 bytes) and many BBC programs used it to maximise available memory for program code and data while also providing a colourful 40-column textual display with simple low-resolution graphical decorations.",
        "Such display capabilities, desirable in their own right on low-memory computers, were also desirable for delivering content through low-bandwidth communications channels such as that from Teletext and Viewdata services. However, access to such services can be considered to be a separate capability, and the BBC Micro needed to be upgraded to complement its display capability with the Teletext or Prestel adapters to receive such over-the-air or online content.\n\nJafa Systems provided a number of solutions to remedy the absence of a Teletext display capability. Morley Electronics instead chose to offer an expansion combining the display and reception capabilities.",
        "Sir Computers\nIn late 1984, Sir Computers announced a Mode 7 adapter unit that plugged into the Electron's expansion connector. Unfortunately, Sir Computers ceased trading before the product was brought to market.",
        "Jafa Systems",
        "Released in 1987 at a price of £89, the Mode 7 Mark 1 Display Unit was a separate unit \"about the size, shape and colour of the Plus One or a Slogger ROMbox\" that connected to the Electron's expansion connector and featured a Motorola 6845 display controller and Mullard SAA5050 character generator to replicate the main elements of the BBC Micro's Teletext display solution. This only used 1 KB of memory for the display, with the expansion listening to display memory write accesses and buffering the data in its own memory. A ROM was included to extend the operating system to allow activation of Mode 7 as a genuine screen mode and to provide extra commands and to support keyboard shortcuts used on the BBC Micro to emit Teletext control sequences. To support the output of both the Mode 7",
        "of Mode 7 as a genuine screen mode and to provide extra commands and to support keyboard shortcuts used on the BBC Micro to emit Teletext control sequences. To support the output of both the Mode 7 display and the existing video output, a lead connected the Electron's RGB output to the expansion, with the expansion providing only RF (television) output.",
        "Conscious of the relatively high price of the Mark 1 unit, John Wike of Jafa devised and, at the end of 1988, introduced a software-based Mode 7 Simulator, priced at £25, supplied on a ROM cartridge that rendered the Mode 7 display in a low-resolution, 8-colour graphics mode. Although cheap and effective in enabling use of some software that only used official operating system routines for text output, this solution proved very slow because the Electron had to be placed into the high-bandwidth Mode 2 display to be able to show eight colours at once. In doing so, the CPU spent a lot of time drawing representations of Mode 7 characters and graphics that in a hardware solution would be achieved without any demand on the CPU. It also used up 20 KB of RAM for the graphics display rather than",
        "representations of Mode 7 characters and graphics that in a hardware solution would be achieved without any demand on the CPU. It also used up 20 KB of RAM for the graphics display rather than the 1000 bytes of a hardware Mode 7.",
        "A conceptually similar predecessor to the software-based simulator was published by Electron User in early 1987, offering a monochrome Mode 4 simulation of the Teletext display, using the lower 25 character lines of the screen to show the Teletext output, reserving several lines at the top of the screen for a representation of Mode 7 used to prepare the eventual visual output. However, the program did not support direct access to Mode 7 memory locations. The author noted that a Mode 2 version would have been possible but would have required a redesigned character set and \"too much memory\".",
        "A further refinement of the hardware solution was introduced in 1989 with the Mode 7 Mark 2 Display Unit, which retained the SAA5050 character generator but omitted the 6845 display controller, and was fitted internally in the Electron itself instead of being housed in an external unit, although some kind of ROM expansion unit was needed to hold the driver/utilities ROM. It used software to ensure that the SAA5050 was fed with the correct character data. A software ROM would put the machine into a two-colour, 40-column graphics mode (thus providing one byte per character), and as the ULA read display data from memory in the usual fashion, the SAA5050 would listen to the data it was reading and produce a Mode 7 interpretation of the same information, this being achieved by fitting a board",
        "display data from memory in the usual fashion, the SAA5050 would listen to the data it was reading and produce a Mode 7 interpretation of the same information, this being achieved by fitting a board on top of the ULA connecting to its pins. When necessary the hardware would switch between the conventional Electron graphics output and the Mode 7 output being produced by the add-on, feeding it to the Electron's built-in video output sockets via the red, green and blue lines on the motherboard.",
        "The disadvantage to this system is that while the SAA5050 would expect to be repeatedly fed the same 40 bytes of data for every display scanline of each character row, the ULA would read a different set of 40 bytes for every display scanline in order to produce a full graphics display. A software ROM worked around this by duplicating the data intended for a Mode 7 display in memory. Although this produced a Mode 7 that had less of an impact upon CPU performance than a software solution, gave the same visual quality as the BBC Micro, and supported direct access to Mode 7 screen addresses as well as accesses via operating system routines, it still used 10 KB of memory for the display and reduced the amount of readily-usable application memory  (as indicated by HIMEM) by another 6 KB.",
        "However, with users increasingly able to rely on expansions such as the Slogger Master RAM board to provide more memory, and with this combination of expansions acknowledged throughout the user manual, the emphasis of the Mode 7 Simulator and Mark 2 Display Unit was arguably to deliver the actual display capabilities for those applications that needed them, instead of using Mode 7 as a way of economising with regard to memory usage, and to do so at a reasonable price. In this latter regard, the Mark 2 model was available as a kit costing £25 or as an assembled product (requiring some soldering) costing £49, with a fitting service available for £10.",
        "The Jafa interfaces did not provide a Teletext or Viewdata reception capability, but the Mark 2 was explicitly stated to work in conjunction with the Morley Electronics Teletext Adapter. Meanwhile, the manual for the Mark 2 noted that the product would provide the functionality of a Viewdata terminal if combined with Jafa's RS423 cartridge.",
        "Morley Electronics",
        "Morley Electronics produced a Teletext Adaptor expansion for the BBC Micro and the Electron. Since the BBC Micro has the Mode 7 display capability, the model aimed at the BBC Micro merely provided the content reception capability needed to receive and decode Teletext signals, connecting to the user port and power supply. However, the Electron models provided both display and reception capabilities, doing so by routing either the RGB or UHF signals (depending on the model) through the unit in order to introduce the Mode 7 output produced by the unit, also connecting via a cartridge. The Teletext display capabilities in the Electron models exceeded those of the BBC Micro, with one reviewer noting that the enhanced capabilities permitted \"black text on a coloured background, something I've",
        "display capabilities in the Electron models exceeded those of the BBC Micro, with one reviewer noting that the enhanced capabilities permitted \"black text on a coloured background, something I've always wanted to do on my Beeb\". The UHF model of the Electron adapter also supported overlaying of Teletext onto video and framing of video.",
        "Second Processor expansions\nAcorn did demonstrate a prototype \"Tube\" interface for the Electron alongside the Plus 3 interface at the Compec exhibition in November 1984, although this was never brought to market directly by Acorn.",
        "Advanced Plus 5",
        "Despite Acorn's withdrawal from the Electron peripheral market, Baildon Electronics developed the Advanced Plus 5 (AP5) expansion, featuring Tube, 1 MHz bus and user port interfaces, which plugged into a Plus 1 cartridge socket. This provided a sufficient level of compatibility that both the 6502 and Z80 second processor products from Acorn were shown to work, providing a Tube implementation that was \"as faithful as you can get\", with it also being noted that the Electron being available for as little as £50 at that point in its commercial lifespan was a \"very cheap way of getting a CP/M machine\". Some differences in the memory map of the Electron meant that BBC Micro software would need modifications to work on the Electron with AP5. The price of the unit in late 1986 was £66.70.",
        "The additional facilities of the AP5 alongside the Tube interface permitted various expansions for the BBC Micro to be made available for the Electron. These included the Hybrid Music 5000 and the AMX Mouse.",
        "PMS Electron Second Processor",
        "In 1986, Permanent Memory Systems (PMS) announced a second processor product for the Electron, the PMS-E2P, as a self-contained cartridge for use with the Plus 1 containing a second 2 MHz 6502A processor plus 64 KB of RAM, priced at £89. This was based on a product originally developed by John Wike of Jafa Systems. Available as a kit or in assembled form, it could even be adapted to connect directly to the Electron's expansion connector, thus avoiding the need to even have a Plus 1 expansion, although this would require the user to find other solutions for attaching peripherals. The implementation of the interface between the Electron and second processor was said to adhere closely to Acorn's recommendations, noting that any hardware or software compatibility issues were likely to be the",
        "of the interface between the Electron and second processor was said to adhere closely to Acorn's recommendations, noting that any hardware or software compatibility issues were likely to be the fault of other vendors not similarly adhering to Acorn's guidelines. PMS supplied Acorn's Hi-BASIC with the E2P, permitting the use of as much as 44 KB of the second processor's RAM with BASIC programs. The company also made a version of Computer Concepts' Wordwise Plus available for the E2P, priced at £39.95.",
        "Sound system expansions\nDespite the Electron having only limited sound generation capabilities, few expansions were offered to overcome the machine's limitations.\n\nMillsgrade Voxbox\nAdvertised in late 1985, the Voxbox by Millsgrade Limited was an expansion connecting to the Electron's expansion connector that provided allophone-based speech synthesis, with driver software provided on cassette. The supplied software supported the definition of spoken words built up from the allophones these allophones or sounds being stored in the expansion's own ROM and for catalogues of words to be created and saved. A program was supplied that extended BASIC to allow the use of the synthesiser in user programs. The expansion used the General Instrument SP0256A-AL2 speech synthesis chip.",
        "Sound Expansion cartridge\nOriginally announced in 1987 by Project Expansions to be priced at around £40, the Sound Expansion cartridge could be fitted in a Plus 1 (or compatible) slot and provide sound output equivalent to that of the BBC Micro, with Superior Software's Speech bundled as a \"limited offer\". A product of the same name and with similar functionality was subsequently sold by Complex Software for around £55, employing its own adjustable speaker in the cartridge unit. Superior Software had announced a version of Speech for the unexpanded Acorn Electron in 1986, but this was never released.",
        "Hybrid Music 5000\nHybrid Technology's Music 5000 was adapted and released by PRES for use with the 1 MHz bus of the Advanced Plus 5 expansion, with the Music 5000 itself priced at £113.85. The only functional differences between the Electron adaptation and the original BBC Micro unit involved the use of Mode 6 for the display and the reduced performance of the Electron imposing some limitations on processing in programs written for the system, although this was not thought to prevent most programs for the system from working on the Electron version.",
        "Merlin M2105\nAn unusual variant of the Electron was sold by British Telecom Business Systems as the BT Merlin M2105 Communications Terminal, being previewed by British Telecom at the Communications '84 show. This consisted of a rebadged Electron plus a large expansion unit containing 32 KB of battery-backed RAM (making up 64 KB of RAM in total), up to 64 KB of ROM resident in four sockets (making up to 96 KB of ROM in total), a Centronics printer port, an RS423 serial port, a modem, and the speech generator previously offered for the BBC Micro. The ROM firmware provided dial-up communications facilities, text editing and text messaging functions. The complete product included a monitor and dot-matrix printer.",
        "Initially trialled in a six-month pilot at 50 florists, with the intention of rolling out to all 2,500 members of the UK network, these were used by the Interflora florists network in the UK for over a decade. Used mostly for sending messages, despite providing support for other applications, limited availability of the product led Interflora to look for alternatives after five years, although users appeared to be happy with the product as it was.",
        "This generic product combination of the Electron and accompanying expansion was apparently known as the Chain during development, itself having a different board layout, with British Telecom having intended the M2105 to be a product supporting access to an online service known as Healthnet. This service aimed to improve and speed up communications within hospitals so that patients could be treated and discharged more quickly, and to facilitate transfers of information to doctors and health workers outside hospitals, with communications taking place over conventional telephone lines. The service was to be introduced in the Hammersmith and Fulham district health authority, with installation starting at Charing Cross Hospital. The Electron was said to be particularly suitable for deployment",
        "was to be introduced in the Hammersmith and Fulham district health authority, with installation starting at Charing Cross Hospital. The Electron was said to be particularly suitable for deployment in this application in that it had a \"large expansion bus\", ostensibly making the machine amenable to the necessary adaptations required for the role, together with its \"price, and the fact it has a real keyboard\". As a Healthnet terminal, the M2105 was intended to support the exchange of forms, letters and memos.",
        "The adoption of an Acorn product in this role was perhaps also unusual in that much of BT's Merlin range of this era had been supplied by ICL, notably the M2226 small business computer and M3300 \"communicating word processor\". Nevertheless, the M2105 offered interoperability with other BT products such as the QWERTYphone which was able to receive messages from the M2105 and the Merlin Tonto.",
        "The hardware specifications of the M2105, observed from manufactured units, include the 6502 CPU (SY6502 or R6502), ULA and 32 KB of dynamic RAM fitted in the Electron main unit, plus 32 KB of static RAM, two 6522 VIA devices for interfacing, AM2910PC modem, SCN2681A UART, and TMS5220 plus TMS6100 for speech synthesis. The speech synthesis was used for the \"voice response\" function which answered incoming voice calls by playing a synthesised message to the caller. The components chosen and the capabilities provided (excluding speech synthesis) are broadly similar to those featured by the Acorn Communicator which was another product of Acorn's custom systems division.",
        "The product documentation indicates a specification with 48 KB of RAM plus 16 KB of \"non volatile CMOS RAM\" and 96 KB of ROM, although this particular composition of RAM is apparently contradicted by the RAM devices present on surviving M2105 machines. However, the earlier Chain variant of the board does appear to provide only 16 KB of static RAM using two HM6264LP-15 chips, also providing an extra 16 KB of dynamic RAM using eight MK4516-15 chips, suggesting that the product evolved during development.",
        "Technical information\nMuch of the core functionality of the BBC Micro the video and memory controller, cassette input/output, timers and sound generation was replicated using a single customised ULA chip designed by Acorn in conjunction with Ferranti, albeit with only one sound channel instead of three (and one noise channel), and without the character-based Teletext Mode 7.",
        "The edge connector on the rear of the Electron exposes all address and data bus lines from the CPU, including the upper eight bits of the address bus, in contrast to the limited selection available via the BBC Micro's expansion ports, with the One Megahertz Bus as the principal mechanism for general purpose expansion on the BBC Micro only providing the lower eight bits of the address bus. In addition, various control signals provided by the CPU and ULA are exposed via the Electron's expansion connector.",
        "For Issue 1–4 motherboards, the ULA had an issue similar to those experienced by other socketed CPUs. Over time, the thermal heating and cooling could cause the ULA to rise slightly out of its socket just enough to cause the machine to start exhibiting 'hanging' or other startup-failure issues, such as a continuous 'startup beep'. This was despite a metal cover, and locking-bar mechanism designed to prevent this from occurring. Pushing down on the metal cover to reseat the ULA was normally sufficient to rectify these issues. Issue 5 and 6 boards utilized a different ULA type, this being known as the Aberdeen ULA (as opposed to the earlier Ferranti ULA) which was mounted on a board that was directly soldered to the main board, with the chip being covered by epoxy resin \"insulating",
        "known as the Aberdeen ULA (as opposed to the earlier Ferranti ULA) which was mounted on a board that was directly soldered to the main board, with the chip being covered by epoxy resin \"insulating material\". This arrangement dispensed with the 68-pin socket, and this new type of ULA was expected to be \"less prone to failure\". This type of ULA was also used on the German release of the Electron mainboard which is designated by the marking \"GERMAN ELECTRON Issue 1\" on the mainboard rather than just \"ELECTRON\" as for the UK model.",
        "The keyboard includes a form of quick keyword input, similar to that used on the Sinclair ZX Spectrum, through use of the  key in combination with other keys labelled with BASIC keywords. However, unlike the Spectrum, this method of rapid keyword entry is optional, and keywords can be entered manually if preferred.",
        "The ULA mediates access to 32 KB of addressable RAM using 4 64-kilobit RAM chips (4164), sharing the RAM between the CPU and the video signal generation (or screen refresh) performed by the ULA itself. Two accesses have to be made to the RAM to get each byte (albeit with a single RAS), delivering a maximum transfer rate to or from RAM of one byte per 2 MHz cycle. In generating the video signal, the ULA is able to take advantage of this 2 MHz bandwidth when producing the picture for the high-bandwidth screen modes. Due to signalling constraints, the CPU can only access RAM at 1 MHz, even when it is not competing with the video system.",
        "When the ULA is consuming all of the RAM bandwidth during the active portion of a display line, the CPU is unable to access the RAM. (The Electron uses the Synertek variant of the 6502 processor as that allowed the clock to be stopped for this 40 microsecond period.) In other modes the CPU and video accesses are interleaved with each accessor acquiring bytes at 1 MHz.",
        "In contrast, the BBC Micro employs one or two sets of eight 16-kilobit devices, with the RAM running at twice the speed (4 MHz) of the CPU (2 MHz), allowing the video system (screen refresh) and CPU memory accesses to be interleaved, with each accessor able to transfer bytes at 2 MHz. The RAM access limitations imposed by the Electron's ULA therefore reduce the effective CPU speed by as much as a factor of four relative to the BBC Micro in the more demanding display modes, and as much as a factor of two otherwise. Byte transfers from ROM occur at 2 MHz, however.\n\nHardware\n\nThe hardware specification according to official documentation, combined with more technical documentation and analysis is as follows:",
        "CPU: Synertek SY6502A\n Clock rate: variable. CPU runs at 2 MHz when accessing ROM and 1 MHz when accessing RAM. The CPU is also periodically halted.\n Glue logic: Ferranti Semiconductor Custom ULA\n RAM: 32 KB\n ROM: 32 KB\n Graphics modes: 160×256 pixels (20×32 characters) in 4 or 16 colours, 320×256 (40×32 characters) in 2 or 4 colours, 640×256 (80x32 characters) in 2 colours\n Text modes: 40x25 characters in 2 colours, 80x25 characters in 2 colours\n Colours: 8 colours (TTL combinations of RGB primaries) + 8 flashing versions of the same colours\n Sound: 1 channel of sound, 7 octaves; built-in speaker. Software emulation of noise channel supported\n Keyboard: 56 key \"full travel QWERTY keyboard\"\n Dimensions: 16×34×6.5 cm",
        "Sound: 1 channel of sound, 7 octaves; built-in speaker. Software emulation of noise channel supported\n Keyboard: 56 key \"full travel QWERTY keyboard\"\n Dimensions: 16×34×6.5 cm\n I/O ports: Expansion port, tape recorder connector (1200 baud CUTS variation on the Kansas City standard for data encoding, via a 7-pin circular DIN connector), aerial TV connector (RF modulator), composite video and RGB monitor output\n Power supply: External PSU, 19V AC",
        "The composite video output provides a greyscale image on the standard machine, but an internal modification allows a colour image to be produced, albeit with a degradation in picture quality. Acorn ostensibly intended the composite output to be a high-quality output for monochrome monitors, with the RGB output being the preferred high-quality output for colour images.\n\nQuirks\n\nLike the BBC Micro, the Electron was constrained by limited memory resources. Of the 32 KB RAM, 3½ KB was allocated to the OS at startup and at least 10 KB was taken up by the display buffer in contiguous display modes.",
        "Although programs running on the BBC Micro could use the machine's 6522 chip to trigger interrupts at certain points in the update of each display frame, using these events to change the palette and potentially switching all colours to black, thus blanking regions of the screen and hiding non-graphical data that had been stored in screen memory, the Electron lacked such hardware capabilities as standard. However, it was possible to take advantage of the characteristics of interrupts that were provided, permitting palette changes after the top 100 lines of each display frame, thus facilitating the blanking of either the top 100 or bottom 156 lines of the display. Many games took advantage of this, gaining storage by leaving non-graphical data in the disabled area.",
        "Other games would simply load non-graphical data into the display and leave it visible as regions of apparently randomly coloured pixels. One notable example is Superior Software's Citadel.\n\nAlthough page flipping was a hardware possibility, the limited memory forced most applications to do all their drawing directly to the visible screen, often resulting in graphical flicker or visible redraw. A notable exception is Players' Joe Blade series.\n\nTricks\n\nFiretrack: smooth vertical scrolling\nAlthough programs can alter the position of the screen in memory, the non-linear format of the display means that vertical scrolling can only be done in blocks of 8 pixels without further work.",
        "Firetrack, released on a compilation by Superior Software, exploits a division in the way the Electron handles its display of the seven available graphics modes, two are configured so that the final two of every ten scanlines are blank and are not based on the contents of RAM. If 16 scanlines of continuous graphical data are written to a character-block-aligned portion of the screen then they will appear as a continuous block in most modes but in the two non-continuous modes they will be displayed as two blocks of eight scanlines, separated in the middle by two blank scanlines.",
        "In order to keep track of its position within the display, the Electron maintains an internal display address counter. The same counter is used in both the continuous and non-continuous graphics modes and switching modes mid-frame does not cause any adjustment to the counter.\n\nFiretrack switches from a non-continuous to a continuous graphics mode part way down the display. By using the palette to mask the top area of the display and taking care about when it changes mode it can shift the continuous graphics at the bottom of the display down in two pixel increments because the internal display counter is not incremented on blank scanlines during non-continuous graphics modes.\n\nExile: sampled speech\nExile turns the Electron's one channel output into a digital speaker for PCM output.",
        "Exile: sampled speech\nExile turns the Electron's one channel output into a digital speaker for PCM output.\n\nThe speaker can be programmatically switched on or off at any time but is permanently attached to a hardware counter so is normally only able to output a square wave. But if set to a frequency outside the human audible range then the ear can't perceive the square wave, only the difference between the speaker being switched on and off. This gives the effect of a simple toggle speaker similar to that seen in the 48 KB Sinclair ZX Spectrum. Exile uses this to output 1-bit audio samples.",
        "Frak! and Zalaga: Polyphonic music\nAs part of their copy protection, illegal copies of Aardvark Software's Frak! and Zalaga would cause a pseudo-polyphonic rendition of Trumpet Hornpipe, the Captain Pugwash theme tune, to play endlessly rather than loading the game properly (Pugwash being a pirate). On the Electron version of Frak!, the tune was the main theme from \"Benny Hill\" (Boots Randolph's \"Yakety Sax\"). The polyphony was achieved via fast note-switching to achieve the necessary chords.",
        "Software",
        "A range of titles were made available on cassette at the launch of the Electron through the Acornsoft publishing arm of Acorn, including a number of games, the Forth and Lisp languages, and a handful of other educational and productivity titles. Acorn's decision to provide the Electron with a degree of compatibility with the BBC Micro meant that a number of titles already available for the older machine could be expected to run on its new machine, with only minor cosmetic issues occurring when running some titles. Of the Acornsoft languages, the existing Forth and Lisp language releases worked on the Electron (these being re-released specially for the machine), together with BCPL and Microtext (which remained BBC-only releases). Games such as Chess and Snooker, plus a number of other",
        "on the Electron (these being re-released specially for the machine), together with BCPL and Microtext (which remained BBC-only releases). Games such as Chess and Snooker, plus a number of other titles were also established as being compatible prior to launch. Various applications in Acornsoft's View suite, together with the languages Comal, Logo and ISO Pascal, were reported as being compatible with the Electron, as were some titles from BBC Soft and other developers.",
        "Languages\nA significant selling point for the Electron was its built-in BBC BASIC interpreter, providing a degree of familiarity from the BBC Micro along with a level of compatibility with the earlier machine. However, as had been the case with the BBC Micro, support for other languages was quickly forthcoming, facilitated by the common heritage of the two systems.",
        "In addition to the early releases, Forth and Lisp, Acornsoft released the Pascal subset, S-Pascal, on cassette and followed up with an ISO Pascal implementation on ROM cartridge, the latter providing two 16 KB ROMs containing a program editor and a Pascal compiler producing intermediate code that required Pascal run-time routines to be loaded. As a more minimal implementation, S-Pascal made use of the machine's built-in BASIC program editing facilities and provided a compiler generating assembly language that would then be assembled, generating machine code for direct execution. ISO Pascal had Oxford Pascal as a direct competitor offering a range of features differentiating it from Acornsoft's product, notably a compiler that could produce a stand-alone \"relocatable 6502 machine-code",
        "had Oxford Pascal as a direct competitor offering a range of features differentiating it from Acornsoft's product, notably a compiler that could produce a stand-alone \"relocatable 6502 machine-code file\". Acornsoft later released the ISO Pascal Stand Alone Generator product for the BBC Micro and Master series, permitting the generation of executable programs embedding \"sections of the interpreter\" required by each program, with such executables being subject to various licensing restrictions.",
        "Acornsoft Forth, aiming for compliance with the Forth-79 standard, was regarded as \"an excellent implementation of the language\". It saw competition from Skywave Software's Multi-Forth 83 which was delivered on a ROM chip, supported the Forth-83 standard, and provided a multitasking environment. Future availability of Multi-Forth 83 on ROM cartridge was advertised.",
        "With the launch of the Plus 1, Acornsoft Lisp was also made available on cartridge. This Lisp implementation provided only the \"bare essentials\" of a Lisp system that \"a small micro such as the Electron\" could hope to be able to support. However, with the interpreter and initialised workspace being loaded from cassette into RAM in the earlier release, one stated advantage of the ROM version was the availability of more memory for use by programs, with the immediacy of a Lisp system provided as a language ROM being an implicit benefit.",
        "Acornsoft provided two products offering different degrees of support for the Logo programming language. Turtle Graphics was a cassette-based product, available alongside Forth, Lisp and S-Pascal amongst the first titles released for the Electron, featuring a subset of Logo focused on the interactive aspects of the language. Acornsoft Logo was provided on ROM cartridge and offered a vocabulary of over 200 commands as part of a more comprehensive implementation of the language, exposing its list processing foundations. Turtle Graphics was substantially cheaper than Logo: by 1987, the former had been reportedly discounted to under £3 whereas the latter cost \"less than £30\". Unlike other Acornsoft language products, however, Logo was supplied with \"two thick manuals\".",
        "Applications",
        "Acornsoft made a number of applications available for the Electron. In early 1985, the View word processor and ViewSheet spreadsheet applications, familiar from the BBC Micro, were released on ROM cartridge for use with the Electron expanded with a Plus 1, priced at £49.50 each. By running directly from ROM, these applications were able to dedicate all of the machine's available RAM to their documents, and using general filing system mechanisms, documents could be loaded from and saved to cassette or disc, although disc users could also use commands that took advantage of that faster, random-access medium. Cassette-based operation was still regarded as \"perfectly feasible\" since the software itself did not need to be loaded, with loading and saving operations in View achieving about 800",
        "medium. Cassette-based operation was still regarded as \"perfectly feasible\" since the software itself did not need to be loaded, with loading and saving operations in View achieving about 800 words per minute and in ViewSheet achieving around 200 cells per minute.",
        "When using View in Mode 6, providing a 40-column, 25-line display occupying 8 KB of memory, around 20 KB of RAM was available to cassette-based systems or to disc-based systems using products such as the Cumana Floppy Disc System that also maintained PAGE at &E00, this corresponding to about 10 or 11 A4 pages of text. In Mode 3, providing an 80-column, 25-line display occupying 16 KB, around 6 or 7 A4 pages of text could be retained in memory. Acorn's Plus 3 disc system reduced this workspace by a further 4 KB. However, documents could be broken up into sections to be processed individually by View. Operation in the 80-column Mode 0 and Mode 3 was reported as being \"sometimes slow\" due to the Electron's hardware architecture, but View supported horizontal scrolling across documents,",
        "by View. Operation in the 80-column Mode 0 and Mode 3 was reported as being \"sometimes slow\" due to the Electron's hardware architecture, but View supported horizontal scrolling across documents, permitting the use of a 40-column mode to edit wider documents.",
        "ViewSheet could also operate in different display modes, with spreadsheets of approximately 1600 cells being editable in Mode 6 and around 800 cells in Mode 3. A windowing system was provided that permitted ten different views of a spreadsheet to be displayed on screen at once, and recalculation operations were reported to be \"around ten seconds for quite a large model\". Reviewers considered the View and ViewSheet applications to be \"professional\" and to \"compare well with similar software sold for much more expensive machines\" such as the IBM PC, with WordStar being noted as a broadly similar package to View. Compatibility with the same programs on the BBC Micro made a complete Electron-based system an attractive, low-cost, entry-level word processing and spreadsheet system. However,",
        "package to View. Compatibility with the same programs on the BBC Micro made a complete Electron-based system an attractive, low-cost, entry-level word processing and spreadsheet system. However, View's printing support was criticised as inadequate without the use of a companion printer driver program.",
        "Acornsoft did not release its ViewStore database program specifically for the Electron, but the software was reported as being compatible, albeit with function key combinations different to those documented for the BBC Micro. However, Acornsoft did release a product, Database, on 3.5-inch diskette for use with the Electron upgraded with the Plus 3 expansion. The product provided a suite of programs for the creation, maintenance and analysis of structured data files, visualising records using a card index user interface metaphor, and supporting sorting and searching operations on the stored data.",
        "Slogger, an established provider of expansions, also produced productivity applications such as Starword, a word processor, and Starstore, a database. Starword provided separate command and editing modes familiar from Acornsoft's View, also supporting 132-column documents and horizontal scrolling for the editing of such wider documents. Along with other operations familiar from View, such as search and replace functions, block-based editing, and control over text justification, it had built-in support for customising documents for output using a mail merge function. Available on ROM for fitting to a ROM expansion such as Slogger's Rombox or inside a separately purchased ROM cartridge, and reportedly developed specifically for the Electron, Starword was considered \"comprehensive and",
        "fitting to a ROM expansion such as Slogger's Rombox or inside a separately purchased ROM cartridge, and reportedly developed specifically for the Electron, Starword was considered \"comprehensive and powerful\".",
        "Starstore, also available on ROM, provided a database management suite primarily aimed at users of cassette storage, with databases being entirely resident in RAM. It supported database definition, data editing, searching, sorting and printing activities. Various features complemented Starword, such as mail merge integration. Starstore II followed on as an alternative to, as opposed to a direct successor of, the earlier Starstore product by requiring a disc-based system and permitting databases to be as large as the amount of free space on any given disc. Its user interface was improved over the earlier product, offering pop-up menus and cursor-based navigation.",
        "Computer Concepts' Wordwise Plus, developed from the company's earlier Wordwise product for the BBC Micro and launched in early 1985, was made available for use with the Electron expanded with the E2P-6502 second processor cartridge. The original Wordwise product was incompatible with the Electron due to its use of Mode 7 (the BBC Micro's 40-column Teletext display mode), and being supplied on a ROM chip, it could also not be readily added to the Electron without appropriate expansions. Available from Permanent Memory Systems, producers of the E2P-6502 cartridge, the Electron version of the software was the Hi-Wordwise Plus variant, supplied on disc instead of ROM, and designed to run on the second processor and to use the expanded memory provided in that environment. The program used the",
        "was the Hi-Wordwise Plus variant, supplied on disc instead of ROM, and designed to run on the second processor and to use the expanded memory provided in that environment. The program used the Electron's 40-column Mode 6 display.",
        "Expansion manufacturers Advanced Computer Products and Slogger both made solutions available based on products from Advanced Memory Systems. ACP released a bundle of the AMX Mouse and AMX Art software for use with its Advanced Plus 5 expansion, also requiring a DFS-compatible disc system. Slogger produced a version of the desktop publishing package Stop Press for the Electron, requiring a DFS-compatible disc system, two spare ROM sockets, a mouse, and a suitable user port expansion, with Slogger producing its own user port expansion cartridge. Competing with these products but requiring only a disc system, AVP's Pixel Perfect offered a rudimentary desktop publishing solution, utilising the computer's high-resolution Mode 0 display.\n\nGames",
        "Games\n\nOf the twelve software titles announced by Acornsoft for the Electron at the machine's launch, six were games titles: Snapper, Monsters (a clone of Space Panic), Meteors (a clone of Asteroids), Starship Command, Chess, and the combined title Draughts and Reversi. When the Plus 1 expansion was launched in 1984, three of these titles - Hopper, Snapper and Starship Command - were among the six ROM cartridge titles available at launch, together with the adventure Countdown to Doom. Acornsoft would continue to release games including those based on existing arcade games such as Arcadians (based on Galaxian) and Hopper (based on Frogger), as well as original titles such as Free Fall and Elite.",
        "Micro Power, already an established BBC Micro games publisher, also entered the Electron market at a relatively early stage, offering ten initial titles either converted from the BBC Micro, in the case of Escape from Moonbase Alpha and Killer Gorilla, or \"completely re-written\", in the case of Moonraider (due to differences in the screen handling between the machines). Superior Software, also a significant publisher for the BBC Micro, routinely released games for both machines, notably a licensed version of Atari's Tempest in 1985, but also successful original titles such as the Repton series of games, Citadel, Thrust and Galaforce. Superior's role in games publishing for the Acorn machines expanded in 1986 when the company acquired the right to use the Acornsoft brand, leading to the",
        "of games, Citadel, Thrust and Galaforce. Superior's role in games publishing for the Acorn machines expanded in 1986 when the company acquired the right to use the Acornsoft brand, leading to the co-branding of games and compilations released by the company and the re-release of existing Acornsoft titles with this branding, Elite among them. The company would subsequently release another \"masterpiece\" with bundled novella - the 1988 game Exile - as well as numerous conversions and compilations.",
        "By 1988, the \"big three\" full-price games publishers for the Acorn 8-bit market were identified as Superior Software, Audiogenic (ASL) and Tynesoft, with Top Ten and Alternative Software being the significant budget publishers, and other \"strong contenders\" being Godax, Mandarin and Bug Byte, this assessment made from the perspective of an established games author evaluating trustworthy publishers for aspiring authors. Commercial considerations motivated authors to make their games available for the Electron due to its importance in sales terms, representing \"around half of the Acorn market\", with it being regarded as \"almost compulsory for any mainstream game\" to have an Electron version \"unless your game is a state-of-the-art masterpiece\", with Revs, Cholo and Sentinel cited as such BBC",
        "with it being regarded as \"almost compulsory for any mainstream game\" to have an Electron version \"unless your game is a state-of-the-art masterpiece\", with Revs, Cholo and Sentinel cited as such BBC Micro exclusives. Although the Electron imposed additional technical constraints on authors accustomed to the BBC Micro, some authors were able to use this to their creative advantage. For instance, of Frak! it was noted that the \"Electron version is more popular, and considered better than the BBC version because it has a screen designer included\".",
        "Although not as well supported by the biggest software publishers as rivals like the Commodore 64 and Sinclair ZX Spectrum, a good range of games were available for the Electron including popular multi-format games such as Chuckie Egg. There were also many popular games officially converted to the Electron from arcade machines (including Crystal Castles, Tempest, Commando, Paperboy and Yie Ar Kung-Fu) and other home computer systems (including Impossible Mission, Jet Set Willy, The Way of the Exploding Fist, Tetris, The Last Ninja, Barbarian, Ballistix, Predator, Hostages and SimCity).",
        "Despite Acorn themselves effectively shelving the Electron in 1985, games continued to be developed and released by professional software houses until the early 1990s. There were around 1,400 games released for the Acorn Electron, several thousand extra public domain titles were released on disc through Public Domain libraries. Notable enterprises which produced discs of such software are BBC PD, EUG (Electron User Group) and HeadFirst PD.\n\nEmulation\nSeveral emulators of the machine exist: ElectrEm for Windows/Linux/macOS, Elkulator for Windows/Linux/DOS, ElkJS is a browser-based (JavaScript/HTML5) emulator, and the multi-system emulators MESS and Clock Signal feature support for the Electron. Electron software is predominantly archived in the UEF file format.",
        "There are also two known publicly documented FPGA based recreations of the Acorn Electron hardware: ElectronFPGA for the Papilio Duo hardware and the Acorn-Electron core for the FPGA Arcade \"Replay\" board. In addition, an implementation of the ULA for the Lattice ICE40 series has been made available.\n\nDesign team\nThe operating system ROM locations 0xFC00-0xFFFF contain the details of some members of the Electron's design team, these differing somewhat from those listed in the corresponding message in the BBC Model B ROM:\n\nAdditionally, the last bytes of both the BASIC ROM and the Plus 3 interface's ADFS v1.0 ROM include the word \"Roger\", thought to be a reference to Roger Wilson.\n\nThe case was designed by industrial designer Allen Boothroyd of Cambridge Product Design Ltd.",
        "The case was designed by industrial designer Allen Boothroyd of Cambridge Product Design Ltd.\n\nSee also\n Electron User, the most popular Acorn Electron focused magazine\n\nReferences\n\nNotes\n\nExternal links\n\n Stairway To Hell\n Acorn Electron tape archive - playUEF@8bitkick\n Acorn Electron World\n Electron MODE 7 Photos\n\nElectron\n6502-based home computers\nHome computers\nComputer-related introductions in 1983\nComputers designed in the United Kingdom"
    ],
    [
        "Active Directory\nActive Directory (AD) is a directory service developed by Microsoft for Windows domain networks. Windows Server operating systems include it as a set of processes and services. Originally, only centralized domain management used Active Directory. However, it ultimately became an umbrella title for various directory-based identity-related services.",
        "A domain controller is a server running the Active Directory Domain Service (AD DS) role. It authenticates and authorizes all users and computers in a Windows domain-type network, assigning and enforcing security policies for all computers and installing or updating software. For example, when a user logs into a computer part of a Windows domain, Active Directory checks the submitted username and password and determines whether the user is a system administrator or a non-admin user. Furthermore, it allows the management and storage of information, provides authentication and authorization mechanisms, and establishes a framework to deploy other related services: Certificate Services, Active Directory Federation Services, Lightweight Directory Services, and Rights Management Services.",
        "Active Directory uses Lightweight Directory Access Protocol (LDAP) versions 2 and 3, Microsoft's version of Kerberos, and DNS.\n\nRobert R. King defined it in the following way:",
        "Robert R. King defined it in the following way:\n\nHistory\nLike many information-technology efforts, Active Directory originated out of a democratization of design using Requests for Comments (RFCs). The Internet Engineering Task Force (IETF) oversees the RFC process and has accepted numerous RFCs initiated by widespread participants. For example, LDAP underpins Active Directory. Also, X.500 directories and the Organizational Unit preceded the Active Directory concept that uses those methods. The LDAP concept began to emerge even before the founding of Microsoft in April 1975, with RFCs as early as 1971. RFCs contributing to LDAP include RFC 1823 (on the LDAP API, August 1995), RFC 2307, RFC 3062, and RFC 4533.",
        "Microsoft previewed Active Directory in 1999, released it first with Windows 2000 Server edition, and revised it to extend functionality and improve administration in Windows Server 2003. Active Directory support was also added to Windows 95, Windows 98, and Windows NT 4.0 via patch, with some unsupported features. Additional improvements came with subsequent versions of Windows Server. In Windows Server 2008, Microsoft added further services to Active Directory, such as Active Directory Federation Services.  The part of the directory in charge of managing domains, which was a core part of the operating system, was renamed Active Directory Domain Services (ADDS) and became a server role like others. \"Active Directory\" became the umbrella title of a broader range of directory-based",
        "of the operating system, was renamed Active Directory Domain Services (ADDS) and became a server role like others. \"Active Directory\" became the umbrella title of a broader range of directory-based services. According to Byron Hynes, everything related to identity was brought under Active Directory's banner.",
        "Active Directory Services\nActive Directory Services consist of multiple directory services. The best known is Active Directory Domain Services, commonly abbreviated as AD DS or simply AD.\n\nDomain Services\nActive Directory Domain Services (AD DS) is the foundation of every Windows domain network. It stores information about domain members, including devices and users, verifies their credentials, and defines their access rights. The server running this service is called a domain controller. A domain controller is contacted when a user logs into a device, accesses another device across the network, or runs a line-of-business Metro-style app sideloaded into a machine.",
        "Other Active Directory services (excluding LDS, as described below) and most Microsoft server technologies rely on or use Domain Services; examples include Group Policy, Encrypting File System, BitLocker, Domain Name Services, Remote Desktop Services, Exchange Server, and SharePoint Server.\n\nThe self-managed Active Directory DS must be distinct from managed Azure AD DS, a cloud product.",
        "The self-managed Active Directory DS must be distinct from managed Azure AD DS, a cloud product.\n\nLightweight Directory Services\nActive Directory Lightweight Directory Services (AD LDS), previously called Active Directory Application Mode (ADAM), implements the LDAP protocol for AD DS. It runs as a service on Windows Server and offers the same functionality as AD DS, including an equal API. However, AD LDS does not require the creation of domains or domain controllers. It provides a Data Store for storing directory data and a Directory Service with an LDAP Directory Service Interface. Unlike AD DS, multiple AD LDS instances can operate on the same server.",
        "Certificate Services\nActive Directory Certificate Services (AD CS) establishes an on-premises public key infrastructure. It can create, validate, revoke and perform other similar actions, public key certificates for internal uses of an organization. These certificates can be used to encrypt files (when used with Encrypting File System), emails (per S/MIME standard), and network traffic (when used by virtual private networks, Transport Layer Security protocol or IPSec protocol).\n\nAD CS predates Windows Server 2008, but its name was simply Certificate Services.\n\nAD CS requires an AD DS infrastructure.\n\nFederation Services",
        "Active Directory Federation Services (AD FS) is a single sign-on service. With an AD FS infrastructure in place, users may use several web-based services (e.g. internet forum, blog, online shopping, webmail) or network resources using only one set of credentials stored at a central location, as opposed to having to be granted a dedicated set of credentials for each service. AD FS uses many popular open standards to pass token credentials such as SAML, OAuth or OpenID Connect. AD FS supports encryption and signing of SAML assertions. AD FS's purpose is an extension of that of AD DS: The latter enables users to authenticate with and use the devices that are part of the same network, using one set of credentials. The former enables them to use the same set of credentials in a different",
        "enables users to authenticate with and use the devices that are part of the same network, using one set of credentials. The former enables them to use the same set of credentials in a different network.",
        "As the name suggests, AD FS works based on the concept of federated identity.\n\nAD FS requires an AD DS infrastructure, although its federation partner may not.\n\nRights Management Services",
        "AD FS requires an AD DS infrastructure, although its federation partner may not.\n\nRights Management Services\n\nActive Directory Rights Management Services (AD RMS), previously known as Rights Management Services or RMS before Windows Server 2008, is server software that allows for information rights management, included with Windows Server. It uses encryption and selective denial to restrict access to various documents, such as corporate e-mails, Microsoft Word documents, and web pages. It also limits the operations authorized users can perform on them, such as viewing, editing, copying, saving, or printing. IT administrators can create pre-set templates for end users for convenience, but end users can still define who can access the content and what actions they can take.",
        "Logical structure\nActive Directory is a service comprising a database and executable code. It is responsible for managing requests and maintaining the database. The Directory System Agent is the executable part, a set of Windows services and processes that run on Windows 2000 and later. Accessing the objects in Active Directory databases is possible through various interfaces such as LDAP, ADSI, messaging API, and Security Accounts Manager services.",
        "Objects used\nActive Directory structures consist of information about objects classified into two categories: resources (such as printers) and security principals (which include user or computer accounts and groups). Each security principal is assigned a unique security identifier (SID). An object represents a single entity, such as a user, computer, printer, or group, along with its attributes. Some objects may even contain other objects within them. Each object has a unique name, and its definition is a set of characteristics and information by a schema, which determines the storage in the Active Directory.",
        "Administrators can extend or modify the schema using the schema object when needed. However, because each schema object is integral to the definition of Active Directory objects, deactivating or changing them can fundamentally alter or disrupt a deployment. Modifying the schema affects the entire system automatically, and new objects cannot be deleted, only deactivated. Changing the schema usually requires planning.",
        "Forests, trees, and domains\nIn an Active Directory network, the framework that holds objects has different levels: the forest, tree, and domain. Domains within a deployment contain objects stored in a single replicable database, and the DNS name structure identifies their domains, the namespace. A domain is a logical group of network objects such as computers, users, and devices that share the same Active Directory database.",
        "On the other hand, a tree is a collection of domains and domain trees in a contiguous namespace linked in a transitive trust hierarchy. The forest is at the top of the structure, a collection of trees with a standard global catalog, directory schema, logical structure, and directory configuration. The forest is a secure boundary that limits access to users, computers, groups, and other objects.",
        "Organizational units",
        "The objects held within a domain can be grouped into organizational units (OUs). OUs can provide hierarchy to a domain, ease its administration, and can resemble the organization's structure in managerial or geographical terms. OUs can contain other OUs—domains are containers in this sense. Microsoft recommends using OUs rather than domains for structure and simplifying the implementation of policies and administration. The OU is the recommended level at which to apply group policies, which are Active Directory objects formally named group policy objects (GPOs), although policies can also be applied to domains or sites (see below). The OU is the level at which administrative powers are commonly delegated, but delegation can be performed on individual objects or attributes as well.",
        "Organizational units do not each have a separate namespace. As a consequence, for compatibility with Legacy NetBios implementations, user accounts with an identical sAMAccountName are not allowed within the same domain even if the accounts objects are in separate OUs. This is because sAMAccountName, a user object attribute, must be unique within the domain. However, two users in different OUs can have the same common name (CN), the name under which they are stored in the directory itself such as \"fred.staff-ou.domain\" and \"fred.student-ou.domain\", where \"staff-ou\" and \"student-ou\" are the OUs.",
        "In general, the reason for this lack of allowance for duplicate names through hierarchical directory placement is that Microsoft primarily relies on the principles of NetBIOS, which is a flat-namespace method of network object management that, for Microsoft software, goes all the way back to Windows NT 3.1 and MS-DOS LAN Manager. Allowing for duplication of object names in the directory, or completely removing the use of NetBIOS names, would prevent backward compatibility with legacy software and equipment. However, disallowing duplicate object names in this way is a violation of the LDAP RFCs on which Active Directory is supposedly based.",
        "As the number of users in a domain increases, conventions such as \"first initial, middle initial, last name\" (Western order) or the reverse (Eastern order) fail for common family names like Li (李), Smith or Garcia. Workarounds include adding a digit to the end of the username. Alternatives include creating a separate ID system of unique employee/student ID numbers to use as account names in place of actual users' names and allowing users to nominate their preferred word sequence within an acceptable use policy.",
        "Because duplicate usernames cannot exist within a domain, account name generation poses a significant challenge for large organizations that cannot be easily subdivided into separate domains, such as students in a public school system or university who must be able to use any computer across the network.\n\nShadow groups\n\nIn Microsoft's Active Directory, OUs do not confer access permissions, and objects placed within OUs are not automatically assigned access privileges based on their containing OU. It represents a design limitation specific to Active Directory, and other competing directories, such as Novell NDS, can set access privileges through object placement within an OU.",
        "Active Directory requires a separate step for an administrator to assign an object in an OU as a group member also within that OU. Using only the OU location to determine access permissions is unreliable since the entity might not have been assigned to the group object for that OU yet.",
        "A common workaround for an Active Directory administrator is to write a custom PowerShell or Visual Basic script to automatically create and maintain a user group for each OU in their Directory. The scripts run periodically to update the group to match the OU's account membership. However, they cannot instantly update the security groups anytime the directory changes, as occurs in competing directories, as security is directly implemented into the Directory. Such groups are known as shadow groups. Once created, these shadow groups are selectable in place of the OU in the administrative tools. Microsoft's Server 2008 Reference documentation mentions shadow groups but does not provide instructions on creating them. Additionally, there are no available server methods or console snap-ins for",
        "Microsoft's Server 2008 Reference documentation mentions shadow groups but does not provide instructions on creating them. Additionally, there are no available server methods or console snap-ins for managing these groups.",
        "An organization must determine the structure of its information infrastructure by dividing it into one or more domains and top-level OUs. This decision is critical and can base on various models such as business units, geographical locations, IT service, object type, or a combination of these models. The immediate purpose of organizing OUs is to simplify administrative delegation and, secondarily, to apply group policies. It's important to note that while OUs serve as an administrative boundary, the forest itself is the only security boundary. All other domains must trust any administrator in the forest to maintain security.",
        "Partitions\nThe Active Directory database is organized in partitions, each holding specific object types and following a particular replication pattern. Microsoft often refers to these partitions as 'naming contexts. The 'Schema' partition defines object classes and attributes within the forest. The 'Configuration' partition contains information on the physical structure and configuration of the forest (such as the site topology). Both replicate all domains in the forest. The 'Domain' partition holds all objects created in that domain and replicates only within it.",
        "Physical structure\nSites are physical (rather than logical) groupings defined by one or more IP subnets. AD also defines connections, distinguishing low-speed (e.g., WAN, VPN) from high-speed (e.g., LAN) links. Site definitions are independent of the domain and OU structure and are shared across the forest. Sites play a crucial role in managing network traffic created by replication and directing clients to their nearest domain controllers (DCs). Microsoft Exchange Server 2007 uses the site topology for mail routing. Administrators can also define policies at the site level.",
        "The Active Directory information is physically held on one or more peer domain controllers, replacing the NT PDC/BDC model. Each DC has a copy of the Active Directory. Member servers joined to Active Directory that are not domain controllers are called Member Servers. In the domain partition, a group of objects acts as copies of domain controllers set up as global catalogs. These global catalog servers offer a comprehensive list of all objects located in the forest.",
        "Global Catalog servers replicate all objects from all domains to themselves, providing an international listing of entities in the forest. However, to minimize replication traffic and keep the GC's database small, only selected attributes of each object are replicated, called the partial attribute set (PAS). The PAS can be modified by modifying the schema and marking features for replication to the GC. Earlier versions of Windows used NetBIOS to communicate. Active Directory is fully integrated with DNS and requires TCP/IP—DNS. To fully operate, the DNS server must support SRV resource records, also known as service records.",
        "Replication\nActive Directory uses multi-master replication to synchronize changes, meaning replicas pull changes from the server where the change occurred rather than being pushed to them. The Knowledge Consistency Checker (KCC) uses defined sites to manage traffic and create a replication topology of site links. Intra-site replication occurs frequently and automatically due to change notifications, which prompt peers to begin a pull replication cycle. Replication intervals between different sites are usually less consistent and don't usually use change notifications. However, it's possible to set it up to be the same as replication between locations on the same network if needed.",
        "Each DS3, T1, and ISDN link can have a cost, and the KCC alters the site link topology accordingly. Replication may occur transitively through several site links on same-protocol site link bridges if the price is low. However, KCC automatically costs a direct site-to-site link lower than transitive connections. A bridgehead server in each zone can send updates to other DCs in the exact location to replicate changes between sites. To configure replication for Active Directory zones, activate DNS in the domain based on the site.",
        "To replicate Active Directory, Remote Procedure Calls (RPC) over IP (RPC/IP) are used. SMTP is used to replicate between sites but only for modifications in the Schema, Configuration, or Partial Attribute Set (Global Catalog) GCs. It's not suitable for reproducing the default Domain partition.\n\nImplementation\nGenerally, a network utilizing Active Directory has more than one licensed Windows server computer. Backup and restore of Active Directory are possible for a network with a single domain controller. However, Microsoft recommends more than one domain controller to provide automatic failover protection of the directory. Domain controllers are ideally single-purpose for directory operations only and should not run any other software or role.",
        "Since certain Microsoft products, like SQL Server and Exchange, can interfere with the operation of a domain controller, isolation of these products on additional Windows servers is advised. Combining them can complicate the configuration and troubleshooting of the domain controller or the other installed software more complex. If planning to implement Active Directory, a business should purchase multiple Windows server licenses to have at least two separate domain controllers. Administrators should consider additional domain controllers for performance or redundancy and individual servers for tasks like file storage, Exchange, and SQL Server since this will guarantee that all server roles are adequately supported.",
        "One way to lower the physical hardware costs is by using virtualization. However, for proper failover protection, Microsoft recommends not running multiple virtualized domain controllers on the same physical hardware.\n\nDatabase\nThe Active-Directory database, the directory store, in Windows 2000 Server uses the JET Blue-based Extensible Storage Engine (ESE98). Each domain controller's database is limited to 16 terabytes and 2 billion objects (but only 1 billion security principals). Microsoft has created NTDS databases with more than 2 billion objects. NT4's Security Account Manager could support up to 40,000 objects. It has two main tables: the data table and the link table. Windows Server 2003 added a third main table for security descriptor single instancing.",
        "Programs may access the features of Active Directory via the COM interfaces provided by Active Directory Service Interfaces.\n\nTrusting\nTo allow users in one domain to access resources in another, Active Directory uses trusts.\n\nTrusts inside a forest are automatically created when domains are created. The forest sets the default boundaries of trust, and implicit, transitive trust is automatic for all domains within a forest.",
        "Terminology\nOne-way trust\nOne domain allows access to users on another domain, but the other domain does not allow access to users on the first domain.\nTwo-way trust\nTwo domains allow access to users on both domains.\nTrusted domain\nThe domain that is trusted; whose users have access to the trusting domain.\nTransitive trust\nA trust that can extend beyond two domains to other trusted domains in the forest.\nIntransitive trust\nA one way trust that does not extend beyond two domains.\nExplicit trust\nA trust that an admin creates. It is not transitive and is one way only.\nCross-link trust\nAn explicit trust between domains in different trees or the same tree when a descendant/ancestor (child/parent) relationship does not exist between the two domains.\nShortcut",
        "Cross-link trust\nAn explicit trust between domains in different trees or the same tree when a descendant/ancestor (child/parent) relationship does not exist between the two domains.\nShortcut\nJoins two domains in different trees, transitive, one- or two-way.\nForest trust\nApplies to the entire forest. Transitive, one- or two-way.\nRealm\nCan be transitive or nontransitive (intransitive), one- or two-way.\nExternal\nConnect to other forests or non-Active Directory domains. Nontransitive, one- or two-way.\nPAM trust\nA one-way trust used by Microsoft Identity Manager from a (possibly low-level) production forest to a (Windows Server 2016 functionality level) 'bastion' forest, which issues time-limited group memberships.",
        "Management tools\nMicrosoft Active Directory management tools include:\nActive Directory Administrative Center (Introduced with Windows Server 2012 and above),\nActive Directory Users and Computers, \nActive Directory Domains and Trusts, \nActive Directory Sites and Services, \nADSI Edit, \nLocal Users and Groups, \nActive Directory Schema snap-ins for Microsoft Management Console (MMC),\nSysInternals ADExplorer\n\nThese management tools may not provide enough functionality for efficient workflow in large environments. Some third-party tools extend the administration and management capabilities. They provide essential features for a more convenient administration process, such as automation, reports, integration with other services, etc.",
        "Unix integration\nVarying levels of interoperability with Active Directory can be achieved on most Unix-like operating systems (including Unix, Linux, Mac OS X or Java and Unix-based programs) through standards-compliant LDAP clients, but these systems usually do not interpret many attributes associated with Windows components, such as Group Policy and support for one-way trusts.\n\nThird parties offer Active Directory integration for Unix-like platforms, including:\n PowerBroker Identity Services, formerly Likewise (BeyondTrust, formerly Likewise Software) – Allows a non-Windows client to join Active Directory\n ADmitMac (Thursby Software Systems)\n Samba (free software under GPLv3) – Can act as a domain controller",
        "The schema additions shipped with Windows Server 2003 R2 include attributes that map closely enough to RFC 2307 to be generally usable. The reference implementation of RFC 2307, nss_ldap and pam_ldap provided by PADL.com, support these attributes directly. The default schema for group membership complies with RFC 2307bis (proposed). Windows Server 2003 R2 includes a Microsoft Management Console snap-in that creates and edits the attributes.",
        "An alternative option is to use another directory service as non-Windows clients authenticate to this while Windows Clients authenticate to Active Directory. Non-Windows clients include 389 Directory Server (formerly Fedora Directory Server, FDS), ViewDS v7.2 XML Enabled Directory, and Sun Microsystems Sun Java System Directory Server. The latter two are both able to perform two-way synchronization with Active Directory and thus provide a \"deflected\" integration.\n\nAnother option is to use OpenLDAP with its translucent overlay, which can extend entries in any remote LDAP server with additional attributes stored in a local database. Clients pointed at the local database see entries containing both the remote and local attributes, while the remote database remains completely untouched.",
        "Administration (querying, modifying, and monitoring) of Active Directory can be achieved via many scripting languages, including PowerShell, VBScript, JScript/JavaScript, Perl, Python, and Ruby. Free and non-free Active Directory administration tools can help to simplify and possibly automate Active Directory management tasks.\n\nSince October 2017 Amazon AWS offers integration with Microsoft Active Directory.\n\nSee also\n AGDLP (implementing role based access controls using nested groups)\n Apple Open Directory\n Flexible single master operation\n\n FreeIPA\n List of LDAP software\n System Security Services Daemon (SSSD)\n Univention Corporate Server\n\nReferences\n\nExternal links",
        "Microsoft Technet: White paper: Active Directory Architecture (Single technical document that gives an overview about Active Directory.)\n Microsoft Technet: Detailed description of Active Directory on Windows Server 2003\n Microsoft MSDN Library: [MS-ADTS]: Active Directory Technical Specification (part of the Microsoft Open Specification Promise)\n Active Directory Application Mode (ADAM)\n Microsoft MSDN: [AD-LDS]: Active Directory Lightweight Directory Services\n Microsoft TechNet: [AD-LDS]: Active Directory Lightweight Directory Services\n Microsoft MSDN: Active Directory Schema\n Microsoft TechNet: Understanding Schema\n Microsoft TechNet Magazine: Extending the Active Directory Schema\n Microsoft MSDN: Active Directory Certificate Services",
        "Microsoft TechNet: Understanding Schema\n Microsoft TechNet Magazine: Extending the Active Directory Schema\n Microsoft MSDN: Active Directory Certificate Services\n Microsoft TechNet: Active Directory Certificate Services",
        "Directory services\nPublic key infrastructure\nMicrosoft server technology\nWindows components\nWindows 2000"
    ],
    [
        "Active Server Pages\nActive Server Pages (ASP) is Microsoft's first server-side scripting language and engine for dynamic web pages.\n\nIt was first released in December 1996, before being superseded in January 2002 by ASP.NET.\n\nHistory \nInitially released as an add-on to Internet Information Services (IIS) via the Windows NT 4.0 Option Pack (1996), it is included as a component of Windows Server (since the initial release of Windows 2000 Server). There have been three versions of ASP, each introduced with different versions of IIS:\n\n ASP 1.0 was released in December 1996 as part of IIS 3.0\n ASP 2.0 was released in September 1997 as part of IIS 4.0\n ASP 3.0 was released in November 2000 as part of IIS 5.0",
        "ASP 1.0 was released in December 1996 as part of IIS 3.0\n ASP 2.0 was released in September 1997 as part of IIS 4.0\n ASP 3.0 was released in November 2000 as part of IIS 5.0\n\nASP 2.0 provides six built-in objects: Application, ASPError, Request, Response, Server, and Session. Session object, for example, represents a session that maintains the state of variables from page to page. The Active Scripting engine's support of the Component Object Model enables ASP websites to access functionality in compiled libraries such as dynamic-link libraries.",
        "ASP 3.0 does not differ greatly from ASP 2.0 but it does offer some additional enhancements such as Server.Transfer method, Server.Execute method, and an enhanced ASPError object. ASP 3.0 also enables buffering by default and optimized the engine for better performance.\n\nASP was supported until 14 January 2020 on Windows 7. The use of ASP pages will be supported on Windows 8 for a minimum of 10 years from the Windows 8 release date. ASP is currently supported in all available versions of IIS.",
        "Architecture \nASP uses scripting on the server to generate content that is sent to the client's web browser via HTTP response. The ASP interpreter reads and executes all script code between <% and %> tags, the result of which is content generation. These scripts were written using VBScript, JScript, or PerlScript. The @Language directive, the  syntax or server configuration can be used to select the language. In the example below, Response.Write Now() is in an HTML page; it would be dynamically replaced by the current time of the server.",
        "Web pages with the .asp filename extension use ASP, although some web sites disguise their choice of scripting language for security purposes by using the more common .htm or .html extensions. Pages with the .aspx extension use compiled ASP.NET; however, ASP.NET pages may still include some ASP scripting. The introduction of ASP.NET led to use of the term Classic ASP for the original technology.\n\nSun Java System ASP (formerly ChiliSoft ASP) was a popular and reportedly complete emulator, but it has been discontinued.\n\nThe Server object\nThe server object allows connections to databases (ADO), filesystem, and use of components installed on the server.\n\n<%\nDim oAdoCon, oAdoRec, oAdoStm, oCdoCon, oCdoMsg, oSciDic, oSciFsm, oMswAdr",
        "<%\nDim oAdoCon, oAdoRec, oAdoStm, oCdoCon, oCdoMsg, oSciDic, oSciFsm, oMswAdr\n\nSet oAdoCon = Server.CreateObject(\"ADODB.Connection\")\nSet oAdoRec = Server.CreateObject(\"ADODB.Recordset\")\nSet oAdoStm = Server.CreateObject(\"ADODB.Stream\")\nSet oCdoCon = Server.CreateObject(\"CDO.Configuration\")\nSet oCdoMsg = Server.CreateObject(\"CDO.Message\")\nSet oSciDic = Server.CreateObject(\"Scripting.Dictionary\")\nSet oSciFsm = Server.CreateObject(\"Scripting.FileSystemObject\")\nSet oMswAdr = Server.CreateObject(\"MSWC.Swingbridge\")\n%>\n\nThe Application object\nThis object stores global variables, which are variables accessible to all users.\n\n<%\nApplication(\"Ali\") = \"My ASP Application\"\nResponse.Write \"Welcome to \" & Server.HTMLEncode(Application(\"Ali\")) & \"!\"\n%>",
        "<%\nApplication(\"Ali\") = \"My ASP Application\"\nResponse.Write \"Welcome to \" & Server.HTMLEncode(Application(\"Ali\")) & \"!\"\n%>\n\nThe Session object\nStores variables accessible only to a single visitor, which are local variables.\n\n<%\nIf Len(Request.QueryString(\"name\")) > 0 Then\n     Session(\"name\") = Request.QueryString(\"name\") \nEnd If\n\nResponse.Write \"Welcome \" & Server.HTMLEncode(Session(\"name\")) & \"!\"\n%>\n\nThe session object is file based and multiple concurrent read and/or write requests will be blocked and processed in turn.\n\nThe Err object\nAllows the management and fixing of non-fatal errors.\n\n<%\nOn Error Resume Next\n\nResponse.Write 1 / 0 ' Division by zero",
        "The Err object\nAllows the management and fixing of non-fatal errors.\n\n<%\nOn Error Resume Next\n\nResponse.Write 1 / 0 ' Division by zero\n\nIf Err.Number <> 0 Then\n     Response.Write \"Error Code: \" & Server.HTMLEncode(Err.Number) & \"<br />\"\n     Response.Write \"Error Source: \" & Server.HTMLEncode(Err.Source) & \"<br />\"\n     Response.Write \"Error Description: \" & Server.HTMLEncode(Err.Description) & \"<br />\"\n     Err.Clear \nEnd If \n%>\n\nSee also\n ASP.NET\n Template processor\n Comparison of web template engines\n Jakarta Server Pages\n PHP\n Common Gateway Interface\n\nReferences\n\nExternal links\n\n ASP on MSDN\n Microsoft Support for ASP on Windows\n Classic ASP Applications on IIS 7.0 and IIS 7.5 Overview\n Primitive Classic ASP Framework (XML, JSON, BENCODE)\n\nMicrosoft server technology"
    ],
    [
        "Ada (programming language)\nAda is a structured, statically typed, imperative, and object-oriented high-level programming language, inspired by Pascal and other languages. It has built-in language support for design by contract (DbC), extremely strong typing, explicit concurrency, tasks, synchronous message passing, protected objects, and non-determinism. Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international technical standard, jointly defined by the International Organization for Standardization (ISO), and the International Electrotechnical Commission (IEC). , the standard, called Ada 2012 informally, is ISO/IEC 8652:2012.",
        "Ada was originally designed by a team led by French computer scientist Jean Ichbiah of Honeywell under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede over 450 programming languages used by the DoD at that time. Ada was named after Ada Lovelace (1815–1852), who has been credited as the first computer programmer.\n\nFeatures \nAda was originally designed for embedded and real-time systems. The Ada 95 revision, designed by S. Tucker Taft of Intermetrics between 1992 and 1995, improved support for systems, numerical, financial, and object-oriented programming (OOP).",
        "Features of Ada include: strong typing, modular programming mechanisms (packages), run-time checking, parallel processing (tasks, synchronous message passing, protected objects, and nondeterministic select statements), exception handling, and generics. Ada 95 added support for object-oriented programming, including dynamic dispatch.",
        "The syntax of Ada minimizes choices of ways to perform basic operations, and prefers English keywords (such as \"or else\" and \"and then\") to symbols (such as \"||\" and \"&&\"). Ada uses the basic arithmetical operators \"+\", \"-\", \"*\", and \"/\", but avoids using other symbols. Code blocks are delimited by words such as \"declare\", \"begin\", and \"end\", where the \"end\" (in most cases) is followed by the identifier of the block it closes (e.g., if ... end if, loop ... end loop). In the case of conditional blocks this avoids a dangling else that could pair with the wrong nested if-expression in other languages like C or Java.",
        "Ada is designed for developing very large software systems. Ada packages can be compiled separately. Ada package specifications (the package interface) can also be compiled separately without the implementation to check for consistency. This makes it possible to detect problems early during the design phase, before implementation starts.",
        "A large number of compile-time checks are supported to help avoid bugs that would not be detectable until run-time in some other languages or would require explicit checks to be added to the source code.  For example, the syntax requires explicitly named closing of blocks to prevent errors due to mismatched end tokens. The adherence to strong typing allows detecting many common software errors (wrong parameters, range violations, invalid references, mismatched types, etc.) either during compile-time, or otherwise during run-time. As concurrency is part of the language specification, the compiler can in some cases detect potential deadlocks. Compilers also commonly check for misspelled identifiers, visibility of packages, redundant declarations, etc. and can provide warnings and useful",
        "can in some cases detect potential deadlocks. Compilers also commonly check for misspelled identifiers, visibility of packages, redundant declarations, etc. and can provide warnings and useful suggestions on how to fix the error.",
        "Ada also supports run-time checks to protect against access to unallocated memory, buffer overflow errors, range violations, off-by-one errors, array access errors, and other detectable bugs. These checks can be disabled in the interest of runtime efficiency, but can often be compiled efficiently. It also includes facilities to help program verification. For these reasons, Ada is widely used in critical systems, where any anomaly might lead to very serious consequences, e.g., accidental death, injury or severe financial loss. Examples of systems where Ada is used include avionics, air traffic control, railways, banking, military and space technology.",
        "Ada's dynamic memory management is high-level and type-safe. Ada has no generic or untyped pointers; nor does it implicitly declare any pointer type. Instead, all dynamic memory allocation and deallocation must occur via explicitly declared access types. Each access type has an associated storage pool that handles the low-level details of memory management; the programmer can either use the default storage pool or define new ones (this is particularly relevant for Non-Uniform Memory Access). It is even possible to declare several different access types that all designate the same type but use different storage pools. Also, the language provides for accessibility checks, both at compile time and at run time, that ensures that an access value cannot outlive the type of the object it points",
        "different storage pools. Also, the language provides for accessibility checks, both at compile time and at run time, that ensures that an access value cannot outlive the type of the object it points to.",
        "Though the semantics of the language allow automatic garbage collection of inaccessible objects, most implementations do not support it by default, as it would cause unpredictable behaviour in real-time systems.  Ada does support a limited form of region-based memory management; also, creative use of storage pools can provide for a limited form of automatic garbage collection, since destroying a storage pool also destroys all the objects in the pool.",
        "A double-dash (\"--\"), resembling an em dash, denotes comment text.  Comments stop at end of line, to prevent unclosed comments from accidentally voiding whole sections of source code.  Disabling a whole block of code now requires the prefixing of each line (or column) individually with \"--\". While clearly denoting disabled code with a column of repeated \"--\" down the page, this renders the experimental dis/re-enablement of large blocks a more drawn out process.\n\nThe semicolon (\";\") is a statement terminator, and the null or no-operation statement is null;. A single ; without a statement to terminate is not allowed.",
        "The semicolon (\";\") is a statement terminator, and the null or no-operation statement is null;. A single ; without a statement to terminate is not allowed.\n\nUnlike most ISO standards, the Ada language definition (known as the Ada Reference Manual or ARM, or sometimes the Language Reference Manual or LRM) is free content. Thus, it is a common reference for Ada programmers, not only programmers implementing Ada compilers. Apart from the reference manual, there is also an extensive rationale document which explains the language design and the use of various language constructs. This document is also widely used by programmers. When the language was revised, a new rationale document was written.",
        "One notable free software tool that is used by many Ada programmers to aid them in writing Ada source code is the GNAT Programming Studio, and GNAT which is part of the GNU Compiler Collection.",
        "History \nIn the 1970s the US Department of Defense (DoD) became concerned by the number of different programming languages being used for its embedded computer system projects, many of which were obsolete or hardware-dependent, and none of which supported safe modular programming. In 1975, a working group, the High Order Language Working Group (HOLWG), was formed with the intent to reduce this number by finding or creating a programming language generally suitable for the department's and the UK Ministry of Defence's requirements. After many iterations beginning with an original straw-man proposal the eventual programming language was named Ada. The total number of high-level programming languages in use for such projects fell from over 450 in 1983 to 37 by 1996.",
        "HOLWG crafted the Steelman language requirements, a series of documents stating the requirements they felt a programming language should satisfy. Many existing languages were formally reviewed, but the team concluded in 1977 that no existing language met the specifications.",
        "Requests for proposals for a new programming language were issued and four contractors were hired to develop their proposals under the names of Red (Intermetrics led by Benjamin Brosgol), Green (Honeywell, led by Jean Ichbiah), Blue (SofTech, led by John Goodenough) and Yellow (SRI International, led by Jay Spitzen). In April 1978, after public scrutiny, the Red and Green proposals passed to the next phase. In May 1979, the Green proposal, designed by Jean Ichbiah at Honeywell, was chosen and given the name Ada—after Augusta Ada, Countess of Lovelace. This proposal was influenced by the language LIS that Ichbiah and his group had developed in the 1970s. The preliminary Ada reference manual was published in ACM SIGPLAN Notices in June 1979. The Military Standard reference manual was",
        "language LIS that Ichbiah and his group had developed in the 1970s. The preliminary Ada reference manual was published in ACM SIGPLAN Notices in June 1979. The Military Standard reference manual was approved on December 10, 1980 (Ada Lovelace's birthday), and given the number MIL-STD-1815 in honor of Ada Lovelace's birth year. In 1981, C. A. R. Hoare took advantage of his Turing Award speech to criticize Ada for being overly complex and hence unreliable, but subsequently seemed to recant in the foreword he wrote for an Ada textbook.",
        "Ada attracted much attention from the programming community as a whole during its early days. Its backers and others predicted that it might become a dominant language for general purpose programming and not only defense-related work. Ichbiah publicly stated that within ten years, only two programming languages would remain: Ada and Lisp.  Early Ada compilers struggled to implement the large, complex language, and both compile-time and run-time performance tended to be slow and tools primitive.   Compiler vendors expended most of their efforts in passing the massive, language-conformance-testing, government-required Ada Compiler Validation Capability (ACVC) validation suite that was required in another novel feature of the Ada language effort. The Jargon File, a dictionary of computer",
        "government-required Ada Compiler Validation Capability (ACVC) validation suite that was required in another novel feature of the Ada language effort. The Jargon File, a dictionary of computer hacker slang originating in 1975–1983, notes in an entry on Ada that \"it is precisely what one might expect given that kind of endorsement by fiat; designed by committee...difficult to use, and overall a disastrous, multi-billion-dollar boondoggle...Ada Lovelace...would almost certainly blanch at the use her name has been latterly put to; the kindest thing that has been said about it is that there is probably a good small language screaming to get out from inside its vast, elephantine bulk.\"",
        "The first validated Ada implementation was the NYU Ada/Ed translator, certified on April 11, 1983. NYU Ada/Ed is implemented in the high-level set language SETL. Several commercial companies began offering Ada compilers and associated development tools, including Alsys, TeleSoft, DDC-I, Advanced Computer Techniques, Tartan Laboratories, Irvine Compiler, TLD Systems, and Verdix. Computer manufacturers who had a significant business in the defense, aerospace, or related industries, also offered Ada compilers and tools on their platforms; these included Concurrent Computer Corporation, Cray Research, Inc., Digital Equipment Corporation, Harris Computer Systems, and Siemens Nixdorf Informationssysteme AG.",
        "In 1991, the US Department of Defense began to require the use of Ada (the Ada mandate) for all software, though exceptions to this rule were often granted.  The Department of Defense Ada mandate was effectively removed in 1997, as the DoD began to embrace commercial off-the-shelf (COTS) technology. Similar requirements existed in other NATO countries: Ada was required for NATO systems involving command and control and other functions, and Ada was the mandated or preferred language for defense-related applications in countries such as Sweden, Germany, and Canada.",
        "By the late 1980s and early 1990s, Ada compilers had improved in performance, but there were still barriers to fully exploiting Ada's abilities, including a tasking model that was different from what most real-time programmers were used to.",
        "Because of Ada's safety-critical support features, it is now used not only for military applications, but also in commercial projects where a software bug can have severe consequences, e.g., avionics and air traffic control, commercial rockets such as the Ariane 4 and 5, satellites and other space systems, railway transport and banking.",
        "For example, the Primary Flight Control System, the fly-by-wire system software in the Boeing 777, was written in Ada, as were the fly-by-wire systems for the aerodynamically unstable Eurofighter, Gripen, F-22 Raptor and the DFCS replacement flight control system for the F-14 Tomcat. The Canadian Automated Air Traffic System was written in 1 million lines of Ada (SLOC count). It featured advanced distributed processing, a distributed Ada database, and object-oriented design. Ada is also used in other air traffic systems, e.g., the UK's next-generation Interim Future Area Control Tools Support () air traffic control system is designed and implemented using SPARK Ada.",
        "It is also used in the French TVM in-cab signalling system on the TGV high-speed rail system, and the metro suburban trains in Paris, London, Hong Kong and New York City.",
        "Standardization \nPreliminary Ada can be found in ACM Sigplan Notices Vol 14, No 6, June 1979.\n\nAda was first published in 1980 as an ANSI standard ANSI/MIL-STD 1815. As this very first version held many errors and inconsistencies (see Summary of Ada Language Changes), the revised edition was published in 1983 as ANSI/MIL-STD 1815A. Without any further changes, it became an ISO standard in 1987, ISO 8652:1987. This version of the language is commonly known as Ada 83, from the date of its adoption by ANSI, but is sometimes referred to also as Ada 87, from the date of its adoption by ISO. This is the Ada 83 Reference Manual. There is also a French translation; DIN translated it into German as DIN 66268 in 1988.",
        "Ada 95, the joint ISO/IEC/ANSI standard ISO/IEC 8652:1995 (see Ada 95 RM) was published in February 1995, making it the first ISO standard object-oriented programming language. To help with the standard revision and future acceptance, the US Air Force funded the development of the GNAT Compiler. Presently, the GNAT Compiler is part of the GNU Compiler Collection.\n\nWork has continued on improving and updating the technical content of the Ada language. A Technical Corrigendum to Ada 95 was published in October 2001, ISO/IEC 8652:1995/Corr 1:2001 (see Ada 95 RM with TC 1), and a major Amendment, ISO/IEC 8652:1995/Amd 1:2007 (see Ada 2005 RM) was published on March 9, 2007, commonly known as Ada 2005 because work on the new standard was finished that year.",
        "At the Ada-Europe 2012 conference in Stockholm, the Ada Resource Association (ARA) and Ada-Europe announced the completion of the design of the latest version of the Ada language and the submission of the reference manual to the ISO/IEC JTC 1/SC 22/WG 9 of the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) for approval. ISO/IEC 8652:2012 (see Ada 2012 RM) was published in December 2012, known as Ada 2012. A technical corrigendum was published ISO/IEC 8652:2012/COR 1:2016 (see RM 2012 with TC 1).",
        "Despite the names Ada 83, 95 etc., legally there is only one Ada standard, the one of the last ISO/IEC standard: with the acceptance of a new standard version, the previous one becomes withdrawn. The other names are just informal ones referencing a certain edition.\n\nOther related standards include ISO/IEC 8651-3:1988 Information processing systems—Computer graphics—Graphical Kernel System (GKS) language bindings—Part 3: Ada.",
        "Other related standards include ISO/IEC 8651-3:1988 Information processing systems—Computer graphics—Graphical Kernel System (GKS) language bindings—Part 3: Ada.\n\nLanguage constructs \nAda is an ALGOL-like programming language featuring control structures with reserved words such as if, then, else, while, for, and so on. However, Ada also has many data structuring facilities and other abstractions which were not included in the original ALGOL 60, such as type definitions, records, pointers, enumerations. Such constructs were in part inherited from or inspired by Pascal.",
        "\"Hello, world!\" in Ada \nA common example of a language's syntax is the Hello world program:\n(hello.adb)\nwith Ada.Text_IO;\nprocedure Hello is\nbegin\n   Ada.Text_IO.Put_Line (\"Hello, world!\");\nend Hello;\nThis program can be compiled by using the freely available open source compiler GNAT, by executing\ngnatmake hello.adb",
        "Data types \nAda's type system is not based on a set of predefined primitive types but allows users to declare their own types. This declaration in turn is not based on the internal representation of the type but on describing the goal which should be achieved. This allows the compiler to determine a suitable memory size for the type, and to check for violations of the type definition at compile time and run time (i.e., range violations, buffer overruns, type consistency, etc.). Ada supports numerical types defined by a range, modulo types, aggregate types (records and arrays), and enumeration types. Access types define a reference to an instance of a specified type; untyped pointers are not permitted.\nSpecial types provided by the language are task types and protected types.",
        "For example, a date might be represented as:\n\ntype Day_type   is range    1 ..   31;\ntype Month_type is range    1 ..   12;\ntype Year_type  is range 1800 .. 2100;\ntype Hours is mod 24;\ntype Weekday is (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday);\n\ntype Date is\n   record\n     Day   : Day_type;\n     Month : Month_type;\n     Year  : Year_type;\n   end record;\n\nImportant to note: Day_type, Month_type, Year_type, Hours are incompatible types, meaning that for instance the following expression is illegal:\n\nToday: Day_type := 4;\nCurrent_Month: Month_type := 10;\n... Today + Current_Month ...  -- illegal\n\nThe predefined plus-operator can only add values of the same type, so the expression is illegal. \n\nTypes can be refined by declaring subtypes:",
        "The predefined plus-operator can only add values of the same type, so the expression is illegal. \n\nTypes can be refined by declaring subtypes:\n\nsubtype Working_Hours is Hours range 0 .. 12;            -- at most 12 Hours to work a day\nsubtype Working_Day is Weekday range Monday .. Friday;   -- Days to work\n\nWork_Load: constant array(Working_Day) of Working_Hours  -- implicit type declaration\n   := (Friday => 6, Monday => 4, others => 10);           -- lookup table for working hours with initialization\n\nTypes can have modifiers such as limited, abstract, private etc. Private types do not show their inner structure; objects of limited types cannot be copied. Ada 95 adds further features for object-oriented extension of types.",
        "Control structures \nAda is a structured programming language, meaning that the flow of control is structured into standard statements. All standard constructs and deep-level early exit are supported, so the use of the also supported \"go to\" commands is seldom needed.\n\n-- while a is not equal to b, loop.\nwhile a /= b loop\n  Ada.Text_IO.Put_Line (\"Waiting\");\nend loop;\n\nif a > b then\n  Ada.Text_IO.Put_Line (\"Condition met\");\nelse\n  Ada.Text_IO.Put_Line (\"Condition not met\");\nend if;\n\nfor i in 1 .. 10 loop\n  Ada.Text_IO.Put (\"Iteration: \");\n  Ada.Text_IO.Put (i);\n  Ada.Text_IO.Put_Line;\nend loop;\n\nloop\n  a := a + 1;\n  exit when a = 10;\nend loop;",
        "for i in 1 .. 10 loop\n  Ada.Text_IO.Put (\"Iteration: \");\n  Ada.Text_IO.Put (i);\n  Ada.Text_IO.Put_Line;\nend loop;\n\nloop\n  a := a + 1;\n  exit when a = 10;\nend loop;\n\ncase i is\n  when 0 => Ada.Text_IO.Put (\"zero\");\n  when 1 => Ada.Text_IO.Put (\"one\");\n  when 2 => Ada.Text_IO.Put (\"two\");\n  -- case statements have to cover all possible cases:\n  when others => Ada.Text_IO.Put (\"none of the above\");\nend case;",
        "for aWeekday in Weekday'Range loop               -- loop over an enumeration\n   Put_Line ( Weekday'Image(aWeekday) );         -- output string representation of an enumeration\n   if aWeekday in Working_Day then               -- check of a subtype of an enumeration\n      Put_Line ( \" to work for \" &\n               Working_Hours'Image (Work_Load(aWeekday)) ); -- access into a lookup table\n   end if;\nend loop;\n\nPackages, procedures and functions \nAmong the parts of an Ada program are packages, procedures and functions.\n\nExample:\nPackage specification (example.ads)\npackage Example is\n     type Number is range 1 .. 11;\n     procedure Print_and_Increment (j: in out Number);\nend Example;\nPackage body (example.adb)\nwith Ada.Text_IO;\npackage body Example is\n\n  i : Number := Number'First;",
        "i : Number := Number'First;\n\n  procedure Print_and_Increment (j: in out Number) is\n\n    function Next (k: in Number) return Number is\n    begin\n      return k + 1;\n    end Next;\n\n  begin\n    Ada.Text_IO.Put_Line ( \"The total is: \" & Number'Image(j) );\n    j := Next (j);\n  end Print_and_Increment;\n\n-- package initialization executed when the package is elaborated\nbegin\n  while i < Number'Last loop\n    Print_and_Increment (i);\n  end loop;\nend Example;\nThis program can be compiled, e.g., by using the freely available open-source compiler GNAT, by executing\ngnatmake -z example.adb\n\nPackages, procedures and functions can nest to any depth, and each can also be the logical outermost block.",
        "Packages, procedures and functions can nest to any depth, and each can also be the logical outermost block.\n\nEach package, procedure or function can have its own declarations of constants, types, variables, and other procedures, functions and packages, which can be declared in any order.\n\nConcurrency \nAda has language support for task-based concurrency. The fundamental concurrent unit in Ada is a task, which is a built-in limited type. Tasks are specified in two parts – the task declaration defines the task interface (similar to a type declaration), the task body specifies the implementation of the task. Depending on the implementation, Ada tasks are either mapped to operating system threads or processes, or are scheduled internally by the Ada runtime.",
        "Tasks can have entries for synchronisation (a form of synchronous message passing). Task entries are declared in the task specification. Each task entry can have one or more accept statements within the task body. If the control flow of the task reaches an accept statement, the task is blocked until the corresponding entry is called by another task (similarly, a calling task is blocked until the called task reaches the corresponding accept statement). Task entries can have parameters similar to procedures, allowing tasks to synchronously exchange data. In conjunction with select statements it is possible to define guards on accept statements (similar to Dijkstra's guarded commands).",
        "Ada also offers protected objects for mutual exclusion. Protected objects are a monitor-like construct, but use guards instead of conditional variables for signaling (similar to conditional critical regions). Protected objects combine the data encapsulation and safe mutual exclusion from monitors, and entry guards from conditional critical regions.  The main advantage over classical monitors is that conditional variables are not required for signaling, avoiding potential deadlocks due to incorrect locking semantics. Like tasks, the protected object is a built-in limited type, and it also has a declaration part and a body.",
        "A protected object consists of encapsulated private data (which can only be accessed from within the protected object), and procedures, functions and entries which are guaranteed to be mutually exclusive (with the only exception of functions, which are required to be side effect free and can therefore run concurrently with other functions). A task calling a protected object is blocked if another task is currently executing inside the same protected object, and released when this other task leaves the protected object. Blocked tasks are queued on the protected object ordered by time of arrival.",
        "Protected object entries are similar to procedures, but additionally have guards. If a guard evaluates to false, a calling task is blocked and added to the queue of that entry; now another task can be admitted to the protected object, as no task is currently executing inside the protected object. Guards are re-evaluated whenever a task leaves the protected object, as this is the only time when the evaluation of guards can have changed.\n\nCalls to entries can be requeued to other entries with the same signature. A task that is requeued is blocked and added to the queue of the target entry; this means that the protected object is released and allows admission of another task.",
        "The select statement in Ada can be used to implement non-blocking entry calls and accepts, non-deterministic selection of entries (also with guards), time-outs and aborts.\n\nThe following example illustrates some concepts of concurrent programming in Ada.\nwith Ada.Text_IO; use Ada.Text_IO;\n\nprocedure Traffic is\n\n   type Airplane_ID is range 1..10;             -- 10 airplanes\n\n   task type Airplane (ID: Airplane_ID);        -- task representing airplanes, with ID as initialisation parameter\n   type Airplane_Access is access Airplane;     -- reference type to Airplane",
        "task type Airplane (ID: Airplane_ID);        -- task representing airplanes, with ID as initialisation parameter\n   type Airplane_Access is access Airplane;     -- reference type to Airplane\n\n   protected type Runway is                     -- the shared runway (protected to allow concurrent access)\n      entry Assign_Aircraft (ID: Airplane_ID);  -- all entries are guaranteed mutually exclusive\n      entry Cleared_Runway (ID: Airplane_ID);\n      entry Wait_For_Clear;\n   private\n      Clear: Boolean := True;                   -- protected private data - generally more than only a flag...\n   end Runway;\n   type Runway_Access is access all Runway;",
        "-- the air traffic controller task takes requests for takeoff and landing\n   task type Controller (My_Runway: Runway_Access) is\n      -- task entries for synchronous message passing\n      entry Request_Takeoff (ID: in Airplane_ID; Takeoff: out Runway_Access);\n      entry Request_Approach(ID: in Airplane_ID; Approach: out Runway_Access);\n   end Controller;\n\n   --  allocation of instances\n   Runway1    : aliased Runway;              -- instantiate a runway\n   Controller1: Controller (Runway1'Access); -- and a controller to manage it",
        "--  allocation of instances\n   Runway1    : aliased Runway;              -- instantiate a runway\n   Controller1: Controller (Runway1'Access); -- and a controller to manage it\n\n   ------ the implementations of the above types ------\n   protected body Runway is\n      entry Assign_Aircraft (ID: Airplane_ID)\n when Clear is   -- the entry guard - calling tasks are blocked until the condition is true\n      begin\n       Clear := False;\n       Put_Line (Airplane_ID'Image (ID) & \" on runway \");\n      end;\n\n      entry Cleared_Runway (ID: Airplane_ID)\n when not Clear is\n      begin\n         Clear := True;\n         Put_Line (Airplane_ID'Image (ID) & \" cleared runway \");\n      end;",
        "entry Cleared_Runway (ID: Airplane_ID)\n when not Clear is\n      begin\n         Clear := True;\n         Put_Line (Airplane_ID'Image (ID) & \" cleared runway \");\n      end;\n\n      entry Wait_For_Clear\n when Clear is\n      begin\n         null;      -- no need to do anything here - a task can only enter if \"Clear\" is true\n      end;\n   end Runway;",
        "task body Controller is\n   begin\n      loop\n         My_Runway.Wait_For_Clear;   -- wait until runway is available (blocking call)\n         select                      -- wait for two types of requests (whichever is runnable first)\n            when Request_Approach'count = 0 =>  -- guard statement - only accept if there are no tasks queuing on Request_Approach\n             accept Request_Takeoff (ID: in Airplane_ID; Takeoff: out Runway_Access)\n             do                                 -- start of synchronized part\n               My_Runway.Assign_Aircraft (ID);  -- reserve runway (potentially blocking call if protected object busy or entry guard false)\n               Takeoff := My_Runway;            -- assign \"out\" parameter value to tell airplane which runway",
        "Takeoff := My_Runway;            -- assign \"out\" parameter value to tell airplane which runway\n             end Request_Takeoff;               -- end of the synchronised part\n         or\n            accept Request_Approach (ID: in Airplane_ID; Approach: out Runway_Access) do\n               My_Runway.Assign_Aircraft (ID);\n               Approach := My_Runway;\n            end Request_Approach;\n         or                          -- terminate if no tasks left who could call\n            terminate;\n         end select;\n      end loop;\n   end;",
        "task body Airplane is\n      Rwy : Runway_Access;\n   begin\n      Controller1.Request_Takeoff (ID, Rwy); -- This call blocks until Controller task accepts and completes the accept block\n      Put_Line (Airplane_ID'Image (ID) & \"  taking off...\");\n      delay 2.0;\n      Rwy.Cleared_Runway (ID);               -- call will not block as \"Clear\" in Rwy is now false and no other tasks should be inside protected object\n      delay 5.0; -- fly around a bit...\n      loop\n         select   -- try to request a runway\n            Controller1.Request_Approach (ID, Rwy); -- this is a blocking call - will run on controller reaching accept block and return on completion\n            exit; -- if call returned we're clear for landing - leave select block and proceed...\n         or",
        "exit; -- if call returned we're clear for landing - leave select block and proceed...\n         or\n            delay 3.0;  -- timeout - if no answer in 3 seconds, do something else (everything in following block)\n            Put_Line (Airplane_ID'Image (ID) & \"   in holding pattern\");  -- simply print a message\n         end select;\n      end loop;\n      delay 4.0;  -- do landing approach...\n      Put_Line (Airplane_ID'Image (ID) & \"            touched down!\");\n      Rwy.Cleared_Runway (ID);  -- notify runway that we're done here.\n   end;",
        "New_Airplane: Airplane_Access;\n\nbegin\n   for I in Airplane_ID'Range loop  -- create a few airplane tasks\n      New_Airplane := new Airplane (I); -- will start running directly after creation\n      delay 4.0;\n   end loop;\nend Traffic;\n\nPragmas \nA pragma is a compiler directive that conveys information to the compiler to allow specific manipulating of compiled output.  Certain pragmas are built into the language, while others are implementation-specific.\n\nExamples of common usage of compiler pragmas would be to disable certain features, such as run-time type checking or array subscript boundary checking, or to instruct the compiler to insert object code instead of a function call (as C/C++ does with inline functions).\n\nGenerics\n\nSee also",
        "Generics\n\nSee also \n\n Ada compilers\n APSE – a specification for a programming environment to support software development in Ada\n Ravenscar profile – a subset of the Ada tasking features designed for safety-critical hard real-time computing\n SPARK – a programming language consisting of a highly restricted subset of Ada, annotated with meta-information describing desired component behavior and individual runtime requirements\n\nReferences",
        "References\n\nInternational standards \n ISO/IEC 8652: Information technology—Programming languages—Ada\n ISO/IEC 15291: Information technology—Programming languages—Ada Semantic Interface Specification (ASIS)\n ISO/IEC 18009: Information technology—Programming languages—Ada: Conformity assessment of a language processor (ACATS)\n IEEE Standard 1003.5b-1996, the POSIX Ada binding\n Ada Language Mapping Specification, the CORBA interface description language (IDL) to Ada mapping\n\nRationale \nThese documents have been published in various forms, including print.\n  Also available apps.dtic.mil, pdf\n\nBooks \n\n 795 pages.",
        "Rationale \nThese documents have been published in various forms, including print.\n  Also available apps.dtic.mil, pdf\n\nBooks \n\n 795 pages.\n\nArchives \n Ada Programming Language Materials, 1981–1990. Charles Babbage Institute, University of Minnesota. Includes literature on software products designed for the Ada language; U.S. government publications, including Ada 9X project reports, technical reports, working papers, newsletters; and user group information.\n\nExternal links \n\n Ada - C/C++ changer - MapuSoft\n DOD Ada programming language (ANSI/MIL STD 1815A-1983) specification\n JTC1/SC22/WG9 ISO home of Ada Standards",
        "External links \n\n Ada - C/C++ changer - MapuSoft\n DOD Ada programming language (ANSI/MIL STD 1815A-1983) specification\n JTC1/SC22/WG9 ISO home of Ada Standards\n \n\n \nProgramming languages\n.NET programming languages\nAvionics programming languages\nHigh Integrity Programming Language\nMulti-paradigm programming languages\nProgramming language standards\nProgramming languages created in 1980\nProgramming languages with an ISO standard\nStatically typed programming languages\nSystems programming languages\n1980 software\nHigh-level programming languages\nAda Lovelace"
    ],
    [
        "Ada Lovelace\nAugusta Ada King, Countess of Lovelace (née Byron; 10 December 1815 – 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation.",
        "Ada Byron was the only legitimate child of poet Lord Byron and reformer Lady Byron. All Lovelace's half-siblings, Lord Byron's other children, were born out of wedlock to other women. Byron separated from his wife a month after Ada was born and left England forever.  He died in Greece when Ada was eight. Her mother remained bitter and promoted Ada's interest in mathematics and logic in an effort to prevent her from developing her father's perceived insanity. Despite this, Ada remained interested in him, naming her two sons Byron and Gordon. Upon her death, she was buried next to him at her request. Although often ill in her childhood, Ada pursued her studies assiduously. She married William King in 1835. King was made Earl of Lovelace in 1838, Ada thereby becoming Countess of Lovelace.",
        "Her educational and social exploits brought her into contact with scientists such as Andrew Crosse, Charles Babbage, Sir David Brewster, Charles Wheatstone, Michael Faraday, and the author Charles Dickens, contacts which she used to further her education. Ada described her approach as \"poetical science\" and herself as an \"Analyst (& Metaphysician)\".\n\nWhen she was eighteen, her mathematical talents led her to a long working relationship and friendship with fellow British mathematician Charles Babbage, who is known as \"the father of computers\". She was in particular interested in Babbage's work on the Analytical Engine. Lovelace first met him in June 1833, through their mutual friend, and her private tutor, Mary Somerville.",
        "Between 1842 and 1843, Ada translated an article by the military engineer Luigi Menabrea (later Prime Minister of Italy) about the Analytical Engine, supplementing it with an elaborate set of seven notes, simply called \"Notes\".",
        "Lovelace's notes are important in the early history of computers, especially since the seventh one contained what many consider to be the first computer program—that is, an algorithm designed to be carried out by a machine. Other historians reject this perspective and point out that Babbage's personal notes from the years 1836/1837 contain the first programs for the engine. She also developed a vision of the capability of computers to go beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities. Her mindset of \"poetical science\" led her to ask questions about the Analytical Engine (as shown in her notes) examining how individuals and society relate to technology as a collaborative tool.\n\nBiography",
        "Biography\n\nChildhood\nLord Byron expected his child to be a \"glorious boy\" and was disappointed when Lady Byron gave birth to a girl. The child was named after Byron's half-sister, Augusta Leigh, and was called \"Ada\" by Byron himself. On 16 January 1816, at Lord Byron's command, Lady Byron left for her parents' home at Kirkby Mallory, taking their five-week-old daughter with her. Although English law at the time granted full custody of children to the father in cases of separation, Lord Byron made no attempt to claim his parental rights, but did request that his sister keep him informed of Ada's welfare.",
        "On 21 April, Lord Byron signed the deed of separation, although very reluctantly, and left England for good a few days later. Aside from an acrimonious separation, Lady Byron continued throughout her life to make allegations about her husband's immoral behaviour. This set of events made Lovelace infamous in Victorian society. Ada did not have a relationship with her father. He died in 1824 when she was eight years old. Her mother was the only significant parental figure in her life. Lovelace was not shown the family portrait of her father until her 20th birthday.",
        "Lovelace did not have a close relationship with her mother. She was often left in the care of her maternal grandmother Judith, Hon. Lady Milbanke, who doted on her. However, because of societal attitudes of the time—which favoured the husband in any separation, with the welfare of any child acting as mitigation—Lady Byron had to present herself as a loving mother to the rest of society. This included writing anxious letters to Lady Milbanke about her daughter's welfare, with a cover note saying to retain the letters in case she had to use them to show maternal concern. In one letter to Lady Milbanke, she referred to her daughter as \"it\": \"I talk to it for your satisfaction, not my own, and shall be very glad when you have it under your own.\" Lady Byron had her teenage daughter watched by",
        "she referred to her daughter as \"it\": \"I talk to it for your satisfaction, not my own, and shall be very glad when you have it under your own.\" Lady Byron had her teenage daughter watched by close friends for any sign of moral deviation. Lovelace dubbed these observers the \"Furies\" and later complained they exaggerated and invented stories about her.",
        "Lovelace was often ill, beginning in early childhood. At the age of eight, she experienced headaches that obscured her vision. In June 1829, she was paralyzed after a bout of measles. She was subjected to continuous bed rest for nearly a year, something which may have extended her period of disability. By 1831, she was able to walk with crutches. Despite the illnesses, she developed her mathematical and technological skills.",
        "Ada Byron had an affair with a tutor in early 1833. She tried to elope with him after she was caught, but the tutor's relatives recognised her and contacted her mother. Lady Byron and her friends covered the incident up to prevent a public scandal. Lovelace never met her younger half-sister, Allegra, the daughter of Lord Byron and Claire Clairmont. Allegra died in 1822 at the age of five. Lovelace did have some contact with Elizabeth Medora Leigh, the daughter of Byron's half-sister Augusta Leigh, who purposely avoided Lovelace as much as possible when introduced at court.\n\nAdult years",
        "Lovelace became close friends with her tutor Mary Somerville, who introduced her to Charles Babbage in 1833. She had a strong respect and affection for Somerville, and they corresponded for many years. Other acquaintances included the scientists Andrew Crosse, Sir David Brewster, Charles Wheatstone, Michael Faraday and the author Charles Dickens. She was presented at Court at the age of seventeen \"and became a popular belle of the season\" in part because of her \"brilliant mind\". By 1834 Ada was a regular at Court and started attending various events. She danced often and was able to charm many people, and was described by most people as being dainty, although John Hobhouse, Byron's friend, described her as \"a large, coarse-skinned young woman but with something of my friend's features,",
        "people, and was described by most people as being dainty, although John Hobhouse, Byron's friend, described her as \"a large, coarse-skinned young woman but with something of my friend's features, particularly the mouth\". This description followed their meeting on 24 February 1834 in which Ada made it clear to Hobhouse that she did not like him, probably due to her mother's influence, which led her to dislike all of her father's friends. This first impression was not to last, and they later became friends.",
        "On 8 July 1835, she married William, 8th Baron King, becoming Lady King. They had three homes: Ockham Park, Surrey; a Scottish estate on Loch Torridon in Ross-shire; and a house in London. They spent their honeymoon at Worthy Manor in Ashley Combe near Porlock Weir, Somerset. The Manor had been built as a hunting lodge in 1799 and was improved by King in preparation for their honeymoon. It later became their summer retreat and was further improved during this time. From 1845, the family's main house was Horsley Towers, built in the Tudorbethan fashion by the architect of the Houses of Parliament, Charles Barry, and later greatly enlarged to Lovelace's own designs.",
        "They had three children: Byron (born 1836); Anne Isabella (called Annabella, born 1837); and Ralph Gordon (born 1839). Immediately after the birth of Annabella, Lady King experienced \"a tedious and suffering illness, which took months to cure\". Ada was a descendant of the extinct Barons Lovelace and in 1838, her husband was made Earl of Lovelace and Viscount Ockham, meaning Ada became the Countess of Lovelace. In 1843–44, Ada's mother assigned William Benjamin Carpenter to teach Ada's children and to act as a \"moral\" instructor for Ada. He quickly fell for her and encouraged her to express any frustrated affections, claiming that his marriage meant he would never act in an \"unbecoming\" manner. When it became clear that Carpenter was trying to start an affair, Ada cut it off.",
        "In 1841, Lovelace and Medora Leigh (the daughter of Lord Byron's half-sister Augusta Leigh) were told by Ada's mother that Ada's father was also Medora's father. On 27 February 1841, Ada wrote to her mother: \"I am not in the least astonished. In fact, you merely confirm what I have for years and years felt scarcely a doubt about, but should have considered it most improper in me to hint to you that I in any way suspected.\" She did not blame the incestuous relationship on Byron, but instead blamed Augusta Leigh: \"I fear she is more inherently wicked than he ever was.\" In the 1840s, Ada flirted with scandals: firstly, from a relaxed approach to extra-marital relationships with men, leading to rumours of affairs; and secondly, from her love of gambling. She apparently lost more than £3,000",
        "with scandals: firstly, from a relaxed approach to extra-marital relationships with men, leading to rumours of affairs; and secondly, from her love of gambling. She apparently lost more than £3,000 on the horses during the later 1840s. The gambling led to her forming a syndicate with male friends, and an ambitious attempt in 1851 to create a mathematical model for successful large bets. This went disastrously wrong, leaving her thousands of pounds in debt to the syndicate, forcing her to admit it all to her husband. She had a shadowy relationship with Andrew Crosse's son John from 1844 onwards. John Crosse destroyed most of their correspondence after her death as part of a legal agreement. She bequeathed him the only heirlooms her father had personally left to her. During her final",
        "onwards. John Crosse destroyed most of their correspondence after her death as part of a legal agreement. She bequeathed him the only heirlooms her father had personally left to her. During her final illness, she would panic at the idea of the younger Crosse being kept from visiting her.",
        "Education",
        "From 1832, when she was seventeen, her mathematical abilities began to emerge, and her interest in mathematics dominated the majority of her adult life. Her mother's obsession with rooting out any of the insanity of which she accused Byron was one of the reasons that Ada was taught mathematics from an early age. She was privately educated in mathematics and science by William Frend, William King, and Mary Somerville, the noted 19th-century researcher and scientific author. In the 1840s, the mathematician Augustus De Morgan extended her \"much help in her mathematical studies\" including study of advanced calculus topics including the \"numbers of Bernoulli\" (that formed her celebrated algorithm for Babbage's Analytical Engine). In a letter to Lady Byron, De Morgan suggested that Ada's skill",
        "advanced calculus topics including the \"numbers of Bernoulli\" (that formed her celebrated algorithm for Babbage's Analytical Engine). In a letter to Lady Byron, De Morgan suggested that Ada's skill in mathematics might lead her to become \"an original mathematical investigator, perhaps of first-rate eminence\".",
        "Lovelace often questioned basic assumptions through integrating poetry and science. Whilst studying differential calculus, she wrote to De Morgan:\n\nI may remark that the curious transformations many formulae can undergo, the unsuspected and to a beginner apparently impossible identity of forms exceedingly dissimilar at first sight, is I think one of the chief difficulties in the early part of mathematical studies. I am often reminded of certain sprites and fairies one reads of, who are at one's elbows in one shape now, and the next minute in a form most dissimilar.",
        "Lovelace believed that intuition and imagination were critical to effectively applying mathematical and scientific concepts. She valued metaphysics as much as mathematics, viewing both as tools for exploring \"the unseen worlds around us\".\n\nDeath",
        "Death\n\nLovelace died at the age of 36 on 27 November 1852, from uterine cancer. The illness lasted several months, in which time Annabella took command over whom Ada saw, and excluded all of her friends and confidants. Under her mother's influence, Ada had a religious transformation and was coaxed into repenting of her previous conduct and making Annabella her executor. She lost contact with her husband after confessing something to him on 30 August which caused him to abandon her bedside. It is not known what she told him. She was buried, at her request, next to her father at the Church of St. Mary Magdalene in Hucknall, Nottinghamshire. A memorial plaque, written in Latin, to her and her father is in the chapel attached to Horsley Towers.",
        "Work",
        "Throughout her life, Lovelace was strongly interested in scientific developments and fads of the day, including phrenology and mesmerism. After her work with Babbage, Lovelace continued to work on other projects. In 1844, she commented to a friend Woronzow Greig about her desire to create a mathematical model for how the brain gives rise to thoughts and nerves to feelings (\"a calculus of the nervous system\"). She never achieved this, however. In part, her interest in the brain came from a long-running pre-occupation, inherited from her mother, about her \"potential\" madness. As part of her research into this project, she visited the electrical engineer Andrew Crosse in 1844 to learn how to carry out electrical experiments. In the same year, she wrote a review of a paper by Baron Karl von",
        "into this project, she visited the electrical engineer Andrew Crosse in 1844 to learn how to carry out electrical experiments. In the same year, she wrote a review of a paper by Baron Karl von Reichenbach, Researches on Magnetism, but this was not published and does not appear to have progressed past the first draft. In 1851, the year before her cancer struck, she wrote to her mother mentioning \"certain productions\" she was working on regarding the relation of maths and music.",
        "Lovelace first met Charles Babbage in June 1833, through their mutual friend Mary Somerville. Later that month, Babbage invited Lovelace to see the prototype for his difference engine. She became fascinated with the machine and used her relationship with Somerville to visit Babbage as often as she could. Babbage was impressed by Lovelace's intellect and analytic skills. He called her \"The Enchantress of Number\". In 1843, he wrote to her:",
        "During a nine-month period in 1842–43, Lovelace translated the Italian mathematician Luigi Menabrea's article on Babbage's newest proposed machine, the Analytical Engine. With the article, she appended a set of notes. Explaining the Analytical Engine's function was a difficult task; many other scientists did not grasp the concept and the British establishment had shown little interest in it. Lovelace's notes even had to explain how the Analytical Engine differed from the original Difference Engine. Her work was well received at the time; the scientist Michael Faraday described himself as a supporter of her writing.",
        "The notes are around three times longer than the article itself and include (in Note G), in complete detail, a method for calculating a sequence of Bernoulli numbers using the Analytical Engine, which might have run correctly had it ever been built (only Babbage's Difference Engine has been built, completed in London in 2002). Based on this work, Lovelace is now considered by many to be the first computer programmer and her method has been called the world's first computer program. Others dispute this because some of Charles Babbage's earlier writings could be considered computer programs.",
        "Note G also contains Lovelace's dismissal of artificial intelligence. She wrote that \"The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths.\" This objection has been the subject of much debate and rebuttal, for example by Alan Turing in his paper \"Computing Machinery and Intelligence\". Most modern computer scientists argue that this view is outdated and that computer software can develop in ways that cannot necessarily be anticipated by programmers.",
        "Lovelace and Babbage had a minor falling out when the papers were published, when he tried to leave his own statement (criticising the government's treatment of his Engine) as an unsigned preface, which could have been mistakenly interpreted as a joint declaration. When Taylor's Scientific Memoirs ruled that the statement should be signed, Babbage wrote to Lovelace asking her to withdraw the paper. This was the first that she knew he was leaving it unsigned, and she wrote back refusing to withdraw the paper. The historian Benjamin Woolley theorised that \"His actions suggested he had so enthusiastically sought Ada's involvement, and so happily indulged her ... because of her 'celebrated name'.\" Their friendship recovered, and they continued to correspond. On 12 August 1851, when she was",
        "sought Ada's involvement, and so happily indulged her ... because of her 'celebrated name'.\" Their friendship recovered, and they continued to correspond. On 12 August 1851, when she was dying of cancer, Lovelace wrote to him asking him to be her executor, though this letter did not give him the necessary legal authority. Part of the terrace at Worthy Manor was known as Philosopher's Walk; it was there that Lovelace and Babbage were reputed to have walked while discussing mathematical principles.",
        "First published computer program",
        "In 1840, Babbage was invited to give a seminar at the University of Turin about his Analytical Engine. Luigi Menabrea, a young Italian engineer and the future Prime Minister of Italy, transcribed Babbage's lecture into French, and this transcript was subsequently published in the Bibliothèque universelle de Genève in October 1842.\nBabbage's friend Charles Wheatstone commissioned Ada Lovelace to translate Menabrea's paper into English. She then augmented the paper with notes, which were added to the translation. Ada Lovelace spent the better part of a year doing this, assisted with input from Babbage. These notes, which are more extensive than Menabrea's paper, were then published in the September 1843 edition of Taylor's Scientific Memoirs under the initialism AAL.",
        "Ada Lovelace's notes were labelled alphabetically from A to G. In Note G, she describes an algorithm for the Analytical Engine to compute Bernoulli numbers. It is considered to be the first published algorithm ever specifically tailored for implementation on a computer, and Ada Lovelace has often been cited as the first computer programmer for this reason. The engine was never completed and so her program was never tested.\n\nIn 1953, more than a century after her death, Ada Lovelace's notes on Babbage's Analytical Engine were republished as an appendix to B. V. Bowden's Faster than Thought: A Symposium on Digital Computing Machines. The engine has now been recognised as an early model for a computer and her notes as a description of a computer and software.",
        "Insight into potential of computing devices\nIn her notes, Ada Lovelace emphasised the difference between the Analytical Engine and previous calculating machines, particularly its ability to be programmed to solve problems of any complexity. She realised the potential of the device extended far beyond mere number crunching. In her notes, she wrote:",
        "This analysis was an important development from previous ideas about the capabilities of computing devices and anticipated the implications of modern computing one hundred years before they were realised. Walter Isaacson ascribes Ada's insight regarding the application of computing to any process based on logical symbols to an observation about textiles: \"When she saw some mechanical looms that used punchcards to direct the weaving of beautiful patterns, it reminded her of how Babbage's engine used punched cards to make calculations.\" This insight is seen as significant by writers such as Betty Toole and Benjamin Woolley, as well as the programmer John Graham-Cumming, whose project Plan 28 has the aim of constructing the first complete Analytical Engine.",
        "According to the historian of computing and Babbage specialist Doron Swade:",
        "Ada saw something that Babbage in some sense failed to see. In Babbage's world his engines were bound by number...What Lovelace saw...was that number could represent entities other than quantity. So once you had a machine for manipulating numbers, if those numbers represented other things, letters, musical notes, then the machine could manipulate symbols of which number was one instance, according to rules. It is this fundamental transition from a machine which is a number cruncher to a machine for manipulating symbols according to rules that is the fundamental transition from calculation to computation—to general-purpose computation—and looking back from the present high ground of modern computing, if we are looking and sifting history for that transition, then that transition was made",
        "computation—to general-purpose computation—and looking back from the present high ground of modern computing, if we are looking and sifting history for that transition, then that transition was made explicitly by Ada in that 1843 paper.",
        "Distinction between mechanism and logical structure\nLovelace recognized the difference between the details of the computing mechanism, as covered in a 1834 article on the Difference Engine, \nand the logical structure of the Analytical Engine, on which the article she was reviewing dwelt. She noted that different specialists might be required in each area.",
        "The [1834 article] chiefly treats it under its mechanical aspect, entering but slightly into the mathematical principles of which that engine is the representative, but giving, in considerable length, many details of the mechanism and contrivances by means of which it tabulates the various orders of differences. M. Menabrea, on the contrary, exclusively developes the analytical view; taking it for granted that mechanism is able to perform certain processes, but without attempting to explain how; and devoting his whole attention to explanations and illustrations of the manner in which analytical laws can be so arranged and combined as to bring every branch of that vast subject within the grasp of the assumed powers of mechanism. It is obvious that, in the invention of a calculating engine,",
        "laws can be so arranged and combined as to bring every branch of that vast subject within the grasp of the assumed powers of mechanism. It is obvious that, in the invention of a calculating engine, these two branches of the subject are equally essential fields of investigation... They are indissolubly connected, though so different in their intrinsic nature, that perhaps the same mind might not be likely to prove equally profound or successful in both.",
        "Controversy over contribution\nThough Lovelace is often referred to as the first computer programmer, some biographers, computer scientists and historians of computing suggest otherwise.\n\nAllan G. Bromley, in the 1990 article Difference and Analytical Engines:\n\nBruce Collier wrote that Lovelace \"made a considerable contribution to publicizing the Analytical Engine, but there is no evidence that she advanced the design or theory of it in any way\".",
        "Eugene Eric Kim and Betty Alexandra Toole consider it \"incorrect\" to regard Lovelace as the first computer programmer, as Babbage wrote the initial programs for his Analytical Engine, although the majority were never published. Bromley notes several dozen sample programs prepared by Babbage between 1837 and 1840, all substantially predating Lovelace's notes. Dorothy K. Stein regards Lovelace's notes as \"more a reflection of the mathematical uncertainty of the author, the political purposes of the inventor, and, above all, of the social and cultural context in which it was written, than a blueprint for a scientific development\".",
        "Doron Swade has said that Ada only published the first computer program instead of actually writing it, but agrees that she was the only person to see the potential of the analytical engine as a machine capable of expressing entities other than quantities.",
        "In his book, Idea Makers, Stephen Wolfram defends Lovelace's contributions. While acknowledging that Babbage wrote several unpublished algorithms for the Analytical Engine prior to Lovelace's notes, Wolfram argues that \"there's nothing as sophisticated—or as clean—as Ada's computation of the Bernoulli numbers. Babbage certainly helped and commented on Ada's work, but she was definitely the driver of it.\" Wolfram then suggests that Lovelace's main achievement was to distill from Babbage's correspondence \"a clear exposition of the abstract operation of the machine—something which Babbage never did\".\n\nCommemoration",
        "Commemoration\n\nThe computer language Ada, created on behalf of the United States Department of Defense, was named after Lovelace. The reference manual for the language was approved on 10 December 1980 and the Department of Defense Military Standard for the language, MIL-STD-1815, was given the number of the year of her birth.\n\nIn 1981, the Association for Women in Computing inaugurated its Ada Lovelace Award. , the British Computer Society (BCS) has awarded the Lovelace Medal, and in 2008 initiated an annual competition for women students. BCSWomen sponsors the Lovelace Colloquium, an annual conference for women undergraduates. Ada College is a further-education college in Tottenham Hale, London, focused on digital skills.",
        "Ada Lovelace Day is an annual event celebrated on the second Tuesday of October, which began in 2009. Its goal is to \"... raise the profile of women in science, technology, engineering, and maths,\" and to \"create new role models for girls and women\" in these fields. Events have included Wikipedia edit-a-thons with the aim of improving the representation of women on Wikipedia in terms of articles and editors to reduce unintended gender bias on Wikipedia.\n\nThe Ada Initiative was a non-profit organisation dedicated to increasing the involvement of women in the free culture and open source movements.",
        "The Ada Initiative was a non-profit organisation dedicated to increasing the involvement of women in the free culture and open source movements.\n\nThe Engineering in Computer Science and Telecommunications College building in Zaragoza University is called the Ada Byron Building. The computer centre in the village of Porlock, near where Lovelace lived, is named after her. Ada Lovelace House is a council-owned building in Kirkby-in-Ashfield, Nottinghamshire, near where Lovelace spent her infancy.\n\nIn 2012, a Google Doodle and blog post honoured her on her birthday.",
        "In 2012, a Google Doodle and blog post honoured her on her birthday.\n\nIn 2013, Ada Developers Academy was founded and named after her. The mission of Ada Developers Academy is to diversify tech by providing women and gender diverse people the skills, experience, and community support to become professional software developers to change the face of tech.\n\nOn 17 September 2013, the BBC Radio 4 biography programme Great Lives devoted an episode to Ada Lovelace; she was sponsored by TV presenter Konnie Huq.\n\nAs of November 2015, all new British passports have included an illustration of Lovelace and Babbage.\n\nIn 2017, a Google Doodle honoured her with other women on International Women's Day.",
        "As of November 2015, all new British passports have included an illustration of Lovelace and Babbage.\n\nIn 2017, a Google Doodle honoured her with other women on International Women's Day.\n\nOn 2 February 2018, Satellogic, a high-resolution Earth observation imaging and analytics company, launched a ÑuSat type micro-satellite named in honour of Ada Lovelace.\n\nIn March 2018, The New York Times published a belated obituary for Ada Lovelace.\n\nOn 27 July 2018, Senator Ron Wyden submitted, in the United States Senate, the designation of 9 October 2018 as National Ada Lovelace Day: \"To honor the life and contributions of Ada Lovelace as a leading woman in science and mathematics\". The resolution (S.Res.592) was considered, and agreed to without amendment and with a preamble by unanimous consent.",
        "In November 2020 it was announced that Trinity College Dublin whose library had previously held forty busts, all of them of men, was commissioning four new busts of women, one of whom was to be Lovelace.\n\nIn March 2022, a statue of Ada Lovelace was installed at the site of the former Ergon House in the City of Westminster, London, honoring its scientific history. The redevelopment was part of a complex with Imperial Chemical House. The statue was sculpted by Etienne and Mary Millner and based on the portrait by Margaret Sarah Carpenter.  The sculpture was unveiled on International Women's Day, 2022. It stands on the 7th floor of Millbank Quarter overlooking the junction of Dean Bradley Street and Horseferry Road.",
        "In September 2022, Nvidia announced the Ada Lovelace graphics processing unit (GPU) microarchitecture.\n\nIn July 2023, The UK's Royal Mint issued four commemorative £2 coins in various metals to \"honour the innovative contributions of computer science visionary Ada Lovelace and her legacy as a female trailblazer.\"\n\nBicentenary (2015)\nThe bicentenary of Ada Lovelace's birth was celebrated with a number of events, including:",
        "Bicentenary (2015)\nThe bicentenary of Ada Lovelace's birth was celebrated with a number of events, including:\n\n The Ada Lovelace Bicentenary Lectures on Computability, Israel Institute for Advanced Studies, 20 December 2015 – 31 January 2016.\n Ada Lovelace Symposium, University of Oxford, 13–14 October 2015.\nAda.Ada.Ada, a one-woman show about the life and work of Ada Lovelace (using an LED dress), premiered at Edinburgh International Science Festival on 11 April 2015, and continued to touring internationally to promote diversity on STEM at technology conferences, businesses, government and educational organisations.\n\nSpecial exhibitions were displayed by the Science Museum in London, England and the Weston Library (part of the Bodleian Library) in Oxford, England.\n\nIn popular culture",
        "Special exhibitions were displayed by the Science Museum in London, England and the Weston Library (part of the Bodleian Library) in Oxford, England.\n\nIn popular culture\n\nNovels and plays\n\nLovelace is portrayed in Romulus Linney's 1977 play Childe Byron. In Tom Stoppard's 1993 play Arcadia, the precocious teenage genius Thomasina Coverly—a character \"apparently based\" on Ada Lovelace (the play also involves Lord Byron)—comes to understand chaos theory, and theorises the second law of thermodynamics, before either is officially recognised.",
        "In the 1990 steampunk novel The Difference Engine by William Gibson and Bruce Sterling, Lovelace delivers a lecture on the \"punched cards\" programme which proves Gödel's incompleteness theorems decades before their actual discovery. Lovelace and Mary Shelley as teenagers are the central characters in Jordan Stratford's steampunk series, The Wollstonecraft Detective Agency.\n\nLovelace features in John Crowley's 2005 novel, Lord Byron's Novel: The Evening Land, as an unseen character whose personality is forcefully depicted in her annotations and anti-heroic efforts to archive her father's lost novel.",
        "The 2015 play Ada and the Engine by Lauren Gunderson portrays Lovelace and Charles Babbage in unrequited love, and it imagines a post-death meeting between Lovelace and her father. Lovelace and Babbage are also the main characters in Sydney Padua's webcomic and graphic novel The Thrilling Adventures of Lovelace and Babbage. The comic features extensive footnotes on the history of Ada Lovelace, and many lines of dialogue are drawn from actual correspondence.\n\nLovelace is a primary character in the 2019 Big Finish Doctor Who audio play The Enchantress of Numbers, starring Tom Baker as the Fourth Doctor and Jane Slavin as his current companion, WPC Ann Kelso. Lovelace is played by Finty Williams.",
        "In 2019, Lovelace is a featured character in the play STEM FEMMES by Philadelphia theater company Applied Mechanics.\n\nFilm and television\n\nIn the 1997 film Conceiving Ada, a computer scientist obsessed with Ada finds a way of communicating with her in the past by means of \"undying information waves\".\n\nLovelace, identified as Ada Augusta Byron, is portrayed by Lily Lesser in the second season of The Frankenstein Chronicles. She is employed as an \"analyst\" to provide the workings of a life-sized humanoid automaton. The brass workings of the machine are reminiscent of Babbage's analytical engine. Her employment is described as keeping her occupied until she returns to her studies in advanced mathematics.",
        "Lovelace and Babbage appear as characters in the second season of the ITV series Victoria (2017). Emerald Fennell portrays Lovelace in the episode, \"The Green-Eyed Monster.\"\n\n\"Lovelace\" is the name given to the operating system designed by the character Cameron Howe in Halt and Catch Fire.\n\nLovelace features as a character in \"Spyfall, Part 2\", the second episode of Doctor Who, series 12, which first aired on BBC One on 5 January 2020. The character was portrayed by Sylvie Briggs, alongside characterisations of Charles Babbage and Noor Inayat Khan.\n\nSoftware\n\nThe Cardano cryptocurrency platform, launched in 2017, uses Ada as the name for the cryptocurrency and Lovelace as the smallest sub-unit of an Ada.",
        "Software\n\nThe Cardano cryptocurrency platform, launched in 2017, uses Ada as the name for the cryptocurrency and Lovelace as the smallest sub-unit of an Ada.\n\nIn 2021, Lovelace was directly honoured in the codename of Nvidia's new GPU architecture featured in its RTX 4000 series. Ada Lovelace is the first Nvidia architecture to feature both a first and last name.\n\nPublications\n Lovelace, Ada King. Ada, the Enchantress of Numbers: A Selection from the Letters of Lord Byron's Daughter and her Description of the First Computer. Mill Valley, CA: Strawberry Press, 1992. .\n\nAlso available on Wikisource: The Menebrea article, The notes by Ada Lovelace.",
        "Also available on Wikisource: The Menebrea article, The notes by Ada Lovelace.\n\nPublication history \nSix copies of the 1843 first edition of Sketch of the Analytical Engine with Ada Lovelace's \"Notes\" have been located. Three are held at Harvard University, one at the University of Oklahoma, and one at the United States Air Force Academy. On 20 July 2018, the sixth copy was sold at auction to an anonymous buyer for £95,000. A digital facsimile of one of the copies in the Harvard University Library is available online.\n\nIn December 2016, a letter written by Ada Lovelace was forfeited by Martin Shkreli to the New York State Department of Taxation and Finance for unpaid taxes owed by Shkreli.\n\nSee also",
        "In December 2016, a letter written by Ada Lovelace was forfeited by Martin Shkreli to the New York State Department of Taxation and Finance for unpaid taxes owed by Shkreli.\n\nSee also\n\n Ai-Da – humanoid robot, completed in 2019\n Code: Debugging the Gender Gap\n List of pioneers in computer science\n Timeline of women in science\n Women in computing\n Women in STEM fields\n\nExplanatory notes\n\nReferences\n\nCitations\n\nGeneral and cited sources\n\n .\n .\n .\n .\n .\n .\n \n \n .\n  With notes upon the memoir by the translator.\n Miller, Clair Cain. \"Ada Lovelace, 1815–1852,\" New York Times, 8 March 2018.\n .\n .\n .\n .\n .\n \n .\n .",
        ".\n .\n .\n .\n .\n .\n \n \n .\n  With notes upon the memoir by the translator.\n Miller, Clair Cain. \"Ada Lovelace, 1815–1852,\" New York Times, 8 March 2018.\n .\n .\n .\n .\n .\n \n .\n .\n\nFurther reading\n Jennifer Chiaverini, 2017,  Enchantress of Numbers, Dutton, 426 pp.\n Christopher Hollings, Ursula Martin, and Adrian Rice, 2018, Ada Lovelace: The Making of a Computer Scientist, Bodleian Library, 114 pp.\n Miranda Seymour, 2018, In Byron's Wake: The Turbulent Lives of Byron's Wife and Daughter: Annabella Milbanke and Ada Lovelace, Pegasus, 547 pp.\n Jenny Uglow (22 November 2018), \"Stepping Out of Byron's Shadow\", The New York Review of Books, vol. LXV, no. 18, pp. 30–32.\n\nExternal links",
        "External links \n\n \"Ada's Army gets set to rewrite history at Inspirefest 2018\" by Luke Maxwell, 4 August 2018\n \n \"Untangling the Tale of Ada Lovelace\" by Stephen Wolfram, December 2015",
        "1815 births\n1852 deaths\n19th-century British women scientists\n19th-century English writers\n19th-century English mathematicians\n19th-century English women writers\n19th-century British inventors\n19th-century English nobility\nAda (programming language)\nBritish countesses\nBritish women computer scientists\nBritish women mathematicians\nBurials in Nottinghamshire\nAda\nComputer designers\nDaughters of barons\nDeaths from cancer in England\nDeaths from uterine cancer\nEnglish computer programmers\nEnglish people of Scottish descent\nEnglish women poets\nGodwin family\nLord Byron\nMathematicians from London\nWomen of the Victorian era\nBurials at the Church of St Mary Magdalene, Hucknall"
    ],
    [
        "Adder (disambiguation)\nVipera berus, the common European adder, is a snake found in Europe and northern Asia.\n\nAdder may also refer to:\n\n AA-12 Adder, a Russian air-to-air missile\n Adder (electronics), an electronic circuit designed to do addition\n Adder Technology, a manufacturing company\n Armstrong Siddeley Adder, a late 1940s British turbojet engine\n Blackadder, a series of BBC sitcoms\n Golden Axe: The Revenge of Death Adder, a video game\n HMS Adder, any of seven ships of the Royal Navy\n Any of several groups of venomous snakes\n USS Adder, a US submarine\n\nSee also \n Addition, a mathematical operation"
    ],
    [
        "Adobe Inc.\nAdobe Inc. ( ), formerly Adobe Systems Incorporated, is an American multinational computer software company incorporated in Delaware",
        "and headquartered in San Jose, California. It has historically specialized in software for the creation and publication of a wide range of content, including graphics, photography, illustration, animation, multimedia/video, motion pictures, and print. Its flagship products include Adobe Photoshop image editing software; Adobe Illustrator vector-based illustration software; Adobe Acrobat Reader and the Portable Document Format (PDF); and a host of tools primarily for audio-visual content creation, editing and publishing. Adobe offered a bundled solution of its products named Adobe Creative Suite, which evolved into a subscription software as a service (SaaS) offering named Adobe Creative Cloud. The company also expanded into digital marketing software and in 2021 was considered one of the",
        "which evolved into a subscription software as a service (SaaS) offering named Adobe Creative Cloud. The company also expanded into digital marketing software and in 2021 was considered one of the top global leaders in Customer Experience Management (CXM).",
        "Adobe was founded in December 1982 by John Warnock and Charles Geschke, who established the company after leaving Xerox PARC to develop and sell the PostScript page description language. In 1985, Apple Computer licensed PostScript for use in its LaserWriter printers, which helped spark the desktop publishing revolution. Adobe later developed animation and multimedia through its acquisition of Macromedia, from which it acquired Macromedia Flash; video editing and compositing software with Adobe Premiere, later known as Adobe Premiere Pro; low-code web development with Adobe Muse; and a suite of software for digital marketing management.",
        "As of 2022, Adobe has more than 26,000 employees worldwide. Adobe also has major development operations in the United States in Newton, New York City, Arden Hills, Lehi, Seattle, Austin and San Francisco. It also has major development operations in Noida and Bangalore in India.\n\nHistory",
        "History\n\nThe company was started in John Warnock's garage. The name of the company, Adobe, comes from Adobe Creek in Los Altos, California, a stream which ran behind Warnock's house. That creek is so named because of the type of clay found there (Adobe being a Spanish word for Mudbrick), which alludes to the creative nature of the company's software. Adobe's corporate logo features a stylized \"A\" and was designed by graphic designer Marva Warnock, John Warnock's wife. In 2020, the company updated its visual identity, including updating its logo to a single color, an all-red logo.",
        "Steve Jobs attempted to buy the company for $5 million in 1982, but Warnock and Geschke refused. Their investors urged them to work something out with Jobs, so they agreed to sell him shares worth 19 percent of the company. Jobs paid a five-times multiple of their company's valuation at the time, plus a five-year license fee for PostScript, in advance. The purchase and advance made Adobe the first company in the history of Silicon Valley to become profitable in its first year.\n\nWarnock and Geschke considered various business options including a copy-service business and a turnkey system for office printing. Then they chose to focus on developing specialized printing software and created the Adobe PostScript page description language.",
        "PostScript was the first truly international standard for computer printing as it included algorithms describing the letter-forms of many languages. Adobe added kanji printer products in 1988. Warnock and Geschke were also able to bolster the credibility of PostScript by connecting with a typesetting manufacturer. They weren't able to work with Compugraphic, but then worked with Linotype to license the Helvetica and Times Roman fonts (through the Linotron 100). By 1987, PostScript had become the industry-standard printer language with more than 400 third-party software programs and licensing agreements with 19 printer companies.\n\nWarnock described the language as \"extensible\" in its ability to apply graphic arts standards to office printing.",
        "Warnock described the language as \"extensible\" in its ability to apply graphic arts standards to office printing.\n\nAdobe's first products after PostScript were digital fonts which they released in a proprietary format called Type 1, worked on by Bill Paxton after he left Stanford. Apple subsequently developed a competing standard, TrueType, which provided full scalability and precise control of the pixel pattern created by the font's outlines, and licensed it to Microsoft.\n\nIn the mid-1980s, Adobe entered the consumer software market with Illustrator, a vector-based drawing program for the Apple Macintosh. Illustrator, which grew out of the firm's in-house font-development software, helped popularize PostScript-enabled laser printers.",
        "Adobe entered the NASDAQ Composite index in August 1986. Its revenue has grown from roughly $1 billion in 1999 to $4 billion in 2012. Adobe's fiscal years run from December to November. For example, the 2020 fiscal year ended on November 27, 2020.\n\nIn 1989, Adobe introduced what was to become its flagship product, a graphics editing program for the Macintosh called Photoshop. Stable and full-featured, Photoshop 1.0 was ably marketed by Adobe and soon dominated the market.\n\nIn 1993, Adobe introduced PDF, the Portable Document Format, and its Adobe Acrobat and Reader software. PDF is now an International Standard: ISO 32000-1:2008.",
        "In 1993, Adobe introduced PDF, the Portable Document Format, and its Adobe Acrobat and Reader software. PDF is now an International Standard: ISO 32000-1:2008.\n\nIn December 1991, Adobe released Adobe Premiere, which Adobe rebranded as Adobe Premiere Pro in 2003. In 1992, Adobe acquired OCR Systems, Inc. In 1994, Adobe acquired the Aldus Corporation and added PageMaker and After Effects to its product line later in the year; it also controls the TIFF file format. In the same year, Adobe acquired LaserTools Corp and Compution Inc. In 1995, Adobe added FrameMaker, the long-document DTP application, to its product line after Adobe acquired Frame Technology Corp. In 1996, Adobe acquired Ares Software Corp. In 2002, Adobe acquired Canadian company Accelio (also known as JetForm).",
        "In May 2003, Adobe purchased audio editing and multitrack recording software Cool Edit Pro from Syntrillium Software for $16.5 million, as well as a large loop library called \"Loopology\". Adobe then renamed Cool Edit Pro to \"Adobe Audition\" and included it in the Creative Suite.\n\nOn December 3, 2005, Adobe acquired its main rival, Macromedia, in a stock swap valued at about $3.4 billion, adding ColdFusion, Contribute, Captivate, Breeze (rebranded as Adobe Connect), Director, Dreamweaver, Fireworks, Flash, FlashPaper, Flex, FreeHand, HomeSite, JRun, Presenter, and Authorware to Adobe's product line.",
        "Adobe released Adobe Media Player in April 2008. On April 27, Adobe discontinued the development and sales of its older HTML/web development software, GoLive, in favor of Dreamweaver. Adobe offered a discount on Dreamweaver for GoLive users and supports those who still use GoLive with online tutorials and migration assistance. On June 1, Adobe launched Acrobat.com, a series of web applications geared for collaborative work. Creative Suite 4, which includes Design, Web, Production Premium, and Master Collection came out in October 2008 in six configurations at prices from about US$1,700 to $2,500 or by individual application. The Windows version of Photoshop includes 64-bit processing. On December 3, 2008, Adobe laid off 600 of its employees (8% of the worldwide staff) citing the weak",
        "$2,500 or by individual application. The Windows version of Photoshop includes 64-bit processing. On December 3, 2008, Adobe laid off 600 of its employees (8% of the worldwide staff) citing the weak economic environment.",
        "On September 15, 2009, Adobe Systems announced that it would acquire online marketing and web analytics company Omniture for $1.8 billion. The deal was completed on October 23, 2009. Former Omniture products were integrated into the Adobe Marketing Cloud.\n\n On November 10, 2009, the company laid off a further 680 employees.",
        "On November 10, 2009, the company laid off a further 680 employees.\n\nAdobe's 2010 was marked by continuing front-and-back arguments with Apple over the latter's non-support for Adobe Flash on its iPhone, iPad and other products. Former Apple CEO Steve Jobs claimed that Flash was not reliable or secure enough, while Adobe executives have argued that Apple wishes to maintain control over the iOS platform. In April 2010, Steve Jobs published a post titled Thoughts on Flash where he outlined his thoughts on Flash and the rise of HTML 5.\nIn July 2010, Adobe bought Day Software integrating their line of CQ Products: WCM, DAM, SOCO, and Mobile",
        "In January 2011, Adobe acquired DemDex, Inc. with the intent of adding DemDex's audience-optimization software to its online marketing suite. At Photoshop World 2011, Adobe unveiled a new mobile photo service. Carousel is a new application for iPhone, iPad, and Mac that uses Photoshop Lightroom technology to allow users to adjust and fine-tune images on all platforms. Carousel will also allow users to automatically sync, share and browse photos. The service was later renamed to \"Adobe Revel\".\n\nIn October 2011, Adobe acquired Nitobi Software, the maker of the mobile application development framework PhoneGap. As part of the acquisition, the source code of PhoneGap was submitted to the Apache Foundation, where it became Apache Cordova.",
        "In November 2011, Adobe announced that they would cease development of Flash for mobile devices following version 11.1. Instead, it would focus on HTML 5 for mobile devices. In December 2011, Adobe announced that it had entered into a definitive agreement to acquire privately held Efficient Frontier.\n\nIn December 2012, Adobe opened a new  corporate campus in Lehi, Utah.\n\nIn 2013, Adobe endured a major security breach. Vast portions of the source code for the company's software were stolen and posted online and over 150 million records of Adobe's customers have been made readily available for download. In 2012, about 40 million sets of payment card information were compromised by a hack at Adobe.",
        "A class-action lawsuit alleging that the company suppressed employee compensation was filed against Adobe, and three other Silicon Valley-based companies in a California federal district court in 2013. In May 2014, it was revealed the four companies, Adobe, Apple, Google, and Intel had reached an agreement with the plaintiffs, 64,000 employees of the four companies, to pay a sum of $324.5 million to settle the suit.",
        "In March 2018, at Adobe Summit, the company and Nvidia publicized a key association to quickly upgrade their industry-driving AI and profound learning innovations. Expanding on years of coordinated effort, the organizations will work to streamline the Adobe Sensei AI and machine learning structure for Nvidia GPUs. The joint effort will speed up time to showcase and enhance the execution of new Sensei-powered services for Adobe Creative Cloud and Experience Cloud clients and engineers.",
        "Adobe and Nvidia have co-operated for over 10 years on empowering GPU quickening for a wide arrangement of Adobe's creative and computerized encounter items. This incorporates Sensei-powered features, for example, auto lip-sync in Adobe Character Animator CC and face-aware editing in Photoshop CC, and also cloud-based AI/ML items and features, for example, picture investigation for Adobe Stock and Lightroom CC and auto-labeling in Adobe Experience Supervisor.\n\nIn May 2018, Adobe stated they would buy e-commerce services provider Magento Commerce from private equity firm Permira for $1.68 billion. This deal will help bolster its Experience Cloud business, which provides services including analytics, advertising, and marketing. The deal is closed on June 19, 2018.",
        "In September 2018, Adobe announced its acquisition of marketing automation software company Marketo.\n\nIn October 2018, Adobe officially changed its name from Adobe Systems Incorporated to Adobe Inc.\n\nIn January 2019, Adobe announced its acquisition of 3D texturing company Allegorithmic.\n\nIn 2020, the annual Adobe Summit was canceled due to the COVID-19 pandemic. The event took place online and saw over 21 million total video views and over 2.2 million visits to the event website.\n\nThe software giant has imposed a ban on the political ads features on its digital advert sales platform as the United States presidential elections approach.",
        "The software giant has imposed a ban on the political ads features on its digital advert sales platform as the United States presidential elections approach.\n\nOn November 9, 2020, Adobe announced it would spend US$1.5 billion to acquire Workfront, a provider of marketing collaboration software. The acquisition was completed in early December 2020.\n\nOn August 19, 2021, Adobe announced it had entered into a definitive agreement to acquire Frame.io, a leading cloud-based video collaboration platform. The transaction is valued at $1.275 billion and closed during the fourth quarter of Adobe's 2021 fiscal year.",
        "On September 15, 2021, Adobe Inc. formally announced that it will add payment services to its e-commerce platform this year, allowing merchants on their platform a method to accept payments including credit cards and PayPal.\n\nIn September 2022, Adobe announced that it had agreed to buy the software design start-up Figma for $20billion. The cloud-based design software from Figma directly competes with Adobe XD. The deal faces regulatory scrutiny. In February 2023, it was announced the European Commission would review the acquisition under European Union merger regulation (EUMR).\n\nFinances\n\nProducts\n\nAdobe's currently supported roster of software, online services and file formats comprises the following ():",
        "Digital Marketing Management Software\n Adobe Experience Cloud, Adobe Experience Manager (AEM 6.2), XML Documentation add-on (for AEM), Mixamo\n Formats\n Portable Document Format (PDF), PDF's predecessor PostScript, ActionScript, Shockwave Flash (SWF), Flash Video (FLV), and Filmstrip (.flm)\n Web-hosted services\n Adobe Color, Photoshop Express, Acrobat.com, Behance and Adobe Express.\n Adobe Renderer\n Adobe Media Encoder\n Adobe Stock",
        "Web-hosted services\n Adobe Color, Photoshop Express, Acrobat.com, Behance and Adobe Express.\n Adobe Renderer\n Adobe Media Encoder\n Adobe Stock\n A microstock agency that presently provides over 57 million high-resolution, royalty-free images and videos available to license (via subscription or credit purchase methods). In 2015, Adobe acquired Fotolia, a stock content marketplace founded in 2005 by Thibaud Elziere, Oleg Tscheltzoff, and Patrick Chassany which operated in 23 countries. It is run as a stand-alone website.\n Adobe Experience Platform\n A family of content, development, and customer relationship management products, with what Adobe calls the \"next generation\" of its Sensei artificial intelligence and machine learning framework, introduced in March 2019.",
        "Reception\nSince 2000, Fortune has recognized Adobe as one of the 100 Best Companies to Work For. In 2021, Adobe was ranked 16th. Glassdoor recognized Adobe as a Best Place to Work. In October 2021, Fast Company included Adobe on their Brands That Matter list. In October 2008, Adobe Systems Canada Inc. was named one of \"Canada's Top 100 Employers\" by Mediacorp Canada Inc. and was featured in Maclean's newsmagazine.\n\nAdobe received a five-star rating from the Electronic Frontier Foundation with regard to its handling of government data requests in 2017.\n\nIn 2022, Adobe was listed as one of the Best Places to Work for Disability Inclusion by the Disability Equality Index (DEI).\n\nCriticisms",
        "In 2022, Adobe was listed as one of the Best Places to Work for Disability Inclusion by the Disability Equality Index (DEI).\n\nCriticisms\n\nPricing\nAdobe has been criticized for its pricing practices, with retail prices being up to twice as much in non-US countries. For example, it is significantly cheaper to pay for a return airfare ticket to the United States and purchase one particular collection of Adobe's software there than to buy it locally in Australia.",
        "After Adobe revealed the pricing for the Creative Suite 3 Master Collection, which was £1,000 higher for European customers, a petition to protest over \"unfair pricing\" was published and signed by 10,000 users. In June 2009, Adobe further increased its prices in the UK by 10% in spite of weakening of the pound against the dollar, and UK users were not allowed to buy from the US store.\n\nAdobe's Reader and Flash programs were listed on \"The 10 most hated programs of all time\" article by TechRadar.",
        "Adobe's Reader and Flash programs were listed on \"The 10 most hated programs of all time\" article by TechRadar.\n\nSecurity \nHackers have exploited vulnerabilities in Adobe programs, such as Adobe Reader, to gain unauthorized access to computers. Adobe's Flash Player has also been criticized for, among other things, suffering from performance, memory usage and security problems (see criticism of Flash Player). A report by security researchers from Kaspersky Lab criticized Adobe for producing the products having top 10 security vulnerabilities.",
        "Observers noted that Adobe was spying on its customers by including spyware in the Creative Suite 3 software and quietly sending user data to a firm named Omniture. When users became aware, Adobe explained what the suspicious software did and admitted that they: \"could and should do a better job taking security concerns into account\". When a security flaw was later discovered in Photoshop CS5, Adobe sparked outrage by saying it would leave the flaw unpatched, so anyone who wanted to use the software securely would have to pay for an upgrade. Following a fierce backlash Adobe decided to provide the software patch.",
        "Adobe has been criticized for pushing unwanted software including third-party browser toolbars and free virus scanners, usually as part of the Flash update process, and for pushing a third-party scareware program designed to scare users into paying for unneeded system repairs.",
        "Customer data breach\nOn October 3, 2013, the company initially revealed that 2.9 million customers' sensitive and personal data was stolen in a security breach which included encrypted credit card information. Adobe later admitted that 38 million active users have been affected and the attackers obtained access to their IDs and encrypted passwords, as well as to many inactive Adobe accounts. The company did not make it clear if all the personal information was encrypted, such as email addresses and physical addresses, though data privacy laws in 44 states require this information to be encrypted.",
        "A 3.8 GB file stolen from Adobe and containing 152 million usernames, reversibly encrypted passwords and unencrypted password hints was posted on AnonNews.org. LastPass, a password security firm, said that Adobe failed to use best practices for securing the passwords and has not salted them. Another security firm, Sophos, showed that Adobe used a weak encryption method permitting the recovery of a lot of information with very little effort. According to IT expert Simon Bain, Adobe has failed its customers and 'should hang their heads in shame'.",
        "Many of the credit cards were tied to the Creative Cloud software-by-subscription service. Adobe offered its affected US customers a free membership in a credit monitoring service, but no similar arrangements have been made for non-US customers. When a data breach occurs in the US, penalties depend on the state where the victim resides, not where the company is based.",
        "After stealing the customers' data, cyber-thieves also accessed Adobe's source code repository, likely in mid-August 2013. Because hackers acquired copies of the source code of Adobe proprietary products, they could find and exploit any potential weaknesses in its security, computer experts warned. Security researcher Alex Holden, chief information security officer of Hold Security, characterized this Adobe breach, which affected Acrobat, ColdFusion and numerous other applications, as \"one of the worst in US history\". Adobe also announced that hackers stole parts of the source code of Photoshop, which according to commentators could allow programmers to copy its engineering techniques and would make it easier to pirate Adobe's expensive products.",
        "Published on a server of a Russian-speaking hacker group, the \"disclosure of encryption algorithms, other security schemes, and software vulnerabilities can be used to bypass protections for individual and corporate data\" and may have opened the gateway to new generation zero-day attacks. Hackers already used ColdFusion exploits to make off with usernames and encrypted passwords of PR Newswire's customers, which has been tied to the Adobe security breach. They also used a ColdFusion exploit to breach Washington state court and expose up to 200,000 Social Security numbers.",
        "Anti-competitive practices\nIn 1994, Adobe acquired Aldus Corp., a software vendor that sold FreeHand, a competing product. FreeHand was direct competition to Adobe Illustrator, Adobe's flagship vector-graphics editor. The Federal Trade Commission intervened and forced Adobe to sell FreeHand back to Altsys, and also banned Adobe from buying back FreeHand or any similar program for the next 10 years (1994–2004). Altsys was then bought by Macromedia, which released versions 5 to 11. When Adobe acquired Macromedia in December 2005, it stalled development of FreeHand in 2007, effectively rendering it obsolete. With FreeHand and Illustrator, Adobe controlled the only two products that compete in the professional illustration program market for Macintosh operating systems.",
        "In 2011, a group of 5,000 FreeHand graphic designers convened under the banner Free FreeHand, and filed a civil antitrust complaint in the US District Court for the Northern District of California against Adobe. The suit alleged that Adobe has violated federal and state antitrust laws by abusing its dominant position in the professional vector graphic illustration software market and that Adobe has engaged in a series of exclusionary and anti-competitive acts and strategies designed to kill FreeHand, the dominant competitor to Adobe's Illustrator software product, instead of competing on the basis of product merit according to the principals of free market capitalism. Adobe had no response to the claims and the lawsuit was eventually settled. The FreeHand community believes Adobe should",
        "basis of product merit according to the principals of free market capitalism. Adobe had no response to the claims and the lawsuit was eventually settled. The FreeHand community believes Adobe should release the product to an open-source community if it cannot update it internally.",
        ", on its FreeHand product page, Adobe stated, \"While we recognize FreeHand has a loyal customer base, we encourage users to migrate to the new Adobe Illustrator CS4 software which supports both PowerPC and Intel-based Macs and Microsoft Windows XP and Windows Vista.\" , the FreeHand page no longer exists; instead, it simply redirects to the Illustrator page. Adobe's software FTP server still contains a directory for FreeHand, but it is empty.",
        "Cancellation fees \nIn April 2021, Adobe received criticism from Twitter users for the company's cancellation fees after a customer shared a tweet showing they had been charged a $291.45 cancellation fee for their Adobe Creative Cloud subscription. Many also showed their cancellation fees for Adobe Creative Cloud, with this leading to many encouraging piracy of Adobe products and/or purchase of alternatives with lower prices or using free and open-source software instead. Furthermore, there have been reports that with changing subscriptions it is possible to avoid paying this fee.\n\nChief executive officers\nJohn Warnock (1982–2000)\nBruce Chizen (2000–2007)\nShantanu Narayen (2007–present)\n\nSee also",
        "Chief executive officers\nJohn Warnock (1982–2000)\nBruce Chizen (2000–2007)\nShantanu Narayen (2007–present)\n\nSee also\n\n Adobe MAX\n Digital rights management (DRM)\n List of acquisitions by Adobe\n United States v. Elcom Ltd.\n\nReferences\n\nExternal links\n\n \n\n \n\n \n1982 establishments in California\nCompanies based in San Jose, California\nCompanies listed on the Nasdaq\nMultinational companies headquartered in the United States\nSoftware companies based in the San Francisco Bay Area\nSoftware companies established in 1982\nType foundries\nAmerican companies established in 1982\n1980s initial public offerings\nSoftware companies of the United States"
    ],
    [
        "Adrian Lamo\nAdrián Alfonso Lamo Atwood (February 20, 1981 – March 14, 2018) was an American threat analyst and hacker. Lamo first gained media attention for breaking into several high-profile computer networks, including those of The New York Times, Yahoo!, and Microsoft, culminating in his 2003 arrest.\n\nLamo was best known for reporting U.S. soldier Chelsea Manning to Army criminal investigators in 2010 for leaking hundreds of thousands of sensitive U.S. government documents to WikiLeaks. Lamo died on March 14, 2018, at the age of 37.",
        "Early life and education\nAdrian Lamo was born in Malden, Massachusetts His father, Mario Ricardo Lamo, was Colombian. Adrian Lamo attended high schools in Bogotá and San Francisco, from which he did not graduate, but received a GED and was court-ordered to take courses at American River College, a community college in Sacramento County, California. Lamo began his hacking efforts by hacking games on the Commodore 64 and through phone phreaking.\n\nActivities and legal issues\nLamo first became known for operating AOL watchdog site Inside-AOL.com.",
        "Security compromise\nLamo was a grey hat hacker who viewed the rise of the World Wide Web with a mixture of excitement and alarm. He felt that others failed to see the importance of internet security in the early days of the World Wide Web. Lamo would break into corporate computer systems, but he never caused damage to the systems involved. Instead, he would offer to fix the security flaws free of charge, and if the flaw was not fixed, he would alert the media. Lamo hoped to be hired by a corporation to attempt to break into systems and test their security, a practice that came to be known as red teaming. However, by the time this practice was common, his felony conviction prevented him from being hired.",
        "In December 2001, Lamo was praised by Worldcom for helping to fortify their corporate security. In February 2002, he broke into the internal computer network of The New York Times, added his name to the internal database of expert sources, and used the paper's LexisNexis account to conduct research on high-profile subjects. The New York Times filed a complaint, and a warrant for Lamo's arrest was issued in August 2003 following a 15-month investigation by federal prosecutors in New York. At 10:15 a.m. on September 9, after spending a few days in hiding, he surrendered to the US Marshals in Sacramento, California. He re-surrendered to the FBI in New York City on September 11, and pleaded guilty to one felony count of computer crimes against Microsoft, LexisNexis, and The New York Times on",
        "California. He re-surrendered to the FBI in New York City on September 11, and pleaded guilty to one felony count of computer crimes against Microsoft, LexisNexis, and The New York Times on January 8, 2004.",
        "In July 2004, Lamo was sentenced to two years' probation, with six months to be served in home detention, and ordered to pay $65,000 in restitution. He was convicted of compromising security at The New York Times, Microsoft, Yahoo!, and WorldCom.\n\nWhen challenged for a response to allegations that he was glamorizing crime for the sake of publicity, his response was: \"Anything I could say about my person or my actions would only cheapen what they have to say for themselves\". When approached for comment during his criminal case, Lamo frustrated reporters with non-sequiturs, such as \"Faith manages\" and \"It's a beautiful day.\"",
        "At his sentencing, Lamo expressed remorse for harm he had caused by his intrusions. The court record quotes him as adding: \"I want to answer for what I have done and do better with my life.\"\n\nHe subsequently declared on the question-and-answer site Quora that: \"We all own our actions in fullness, not just the pleasant aspects of them.\" Lamo accepted that he had made mistakes.\n\nDNA controversy",
        "DNA controversy\n\nOn May 9, 2006, while 18 months into a two-year probation sentence, Lamo refused to give the United States government a blood sample, which they had demanded to record his DNA in their CODIS system. According to his attorney at the time Lamo had a religious objection to giving blood but was willing to give his DNA in another form. On June 15, 2006, lawyers for Lamo filed a motion citing the Book of Genesis as one basis for Lamo's religious opposition to the giving of blood.\n\nOn June 20, 2007, Lamo's legal counsel reached a settlement agreement with the U.S. Department of Justice whereby Lamo would submit a cheek swab in place of the blood sample.\n\nWikiLeaks and Chelsea Manning",
        "WikiLeaks and Chelsea Manning\n\nIn February 2009, a partial list of the anonymous donors to the WikiLeaks website was leaked and published on the WikiLeaks website. Some media sources indicated at the time that Lamo was among the donors on the list. Lamo commented on his Twitter page, \"Thanks WikiLeaks, for leaking your donor list... That's dedication.\"\n\nIn May 2010, Lamo informed U.S. Army authorities that Chelsea Manning had claimed to have leaked a large body of classified documents, including 260,000 classified United States diplomatic cables. Lamo stated that Manning also \"took credit for leaking\" the video footage of the July 12, 2007, Baghdad airstrike, which has since come to be known as the \"Collateral Murder\" video.",
        "Lamo stated that he would not have turned Manning in \"if lives weren't in danger\". He characterized her as \"in a war zone and basically trying to vacuum up as much classified information as [she] could, and just throwing it up into the air.\" WikiLeaks responded by denouncing Lamo and the author of the article as \"notorious felons, informers & manipulators\", and said: \"journalists should take care.\"",
        "Lamo was a volunteer \"adversary characterization\" analyst for Project Vigilant, a Florida-based government contractor, which encouraged him to inform the government about the alleged WikiLeaks source. The head of Project Vigilant, Chet Uber, claimed, \"I'm the one who called the U.S. government... All the people who say that Adrian is a narc, he did a patriotic thing. He sees all kinds of hacks, and he was seriously worried about people dying.\"",
        "The Taliban insurgency later announced its intention to execute Afghan nationals named in the leaks as having cooperated with the U.S.-led coalition in Afghanistan. By that time, the United States had received months of advance warning that their names were among the leaks. Manning was arrested and incarcerated in the U.S. military justice system and later sentenced to 35 years in confinement. President Barack Obama commuted the sentence to a total of seven years, including time served. Lamo responded to the commutation with a post on Medium and an interview with U.S. News & World Report.",
        "Lamo characterized his decision to work with the government as morally ambiguous, but objectively necessary, writing that \"there were no right choices that day, only less wrong ones. It was cold, it was needful, and it was no one's to make except mine.\" Lamo was criticized by fellow hackers, such as those at the Hackers on Planet Earth conference in 2010, who labeled him a \"snitch.\" Another commented to Lamo, following his speech during a panel discussion, that: \"from my perspective, I see what you have done as treason.\"",
        "Greenwald, Lamo, and Wired magazine\nLamo's role in the Manning case drew criticism from Glenn Greenwald, who suggested that Lamo lied to Manning by turning her in, and then lied after the fact to cover up the circumstances of her confessions. In an article about the Manning case, Greenwald mentioned Wired reporter Kevin Poulsen's 1994 felony conviction for computer hacking and suggested that \"over the years, Poulsen has served more or less as Lamo's personal media voice.\" In an article entitled \"The Worsening Journalistic Disgrace at Wired\", Greenwald wrote that Wired was \"actively conceal[ing] from the public, for months on end, the key evidence [the full Lamo–Manning chat logs] in a political story that has generated headlines around the world.\"",
        "This drew a response from Wired: \"At his most reasonable, Greenwald impugns our motives, attacks the character of our staff and carefully selects his facts and sources to misrepresent the truth and generate outrage in his readership.\"\n\nOn July 13, 2011, Wired published the Lamo–Manning chat logs in full, stating: \"The most significant of the unpublished details have now been publicly established with sufficient authority that we no longer believe any purpose is served by withholding the logs.\" Greenwald wrote that in his opinion the newly released logs validated his claim that Wired had concealed important evidence.",
        "Film and television\nOn August 22, 2002, Lamo was removed from a segment of NBC Nightly News when, after being asked to demonstrate his skills for the camera, he gained access to NBC's internal network. NBC was concerned that they broke the law by taping Lamo while he possibly broke the law. Lamo was a guest on The Screen Savers five times beginning in 2002.",
        "Hackers Wanted, a documentary film focusing on Lamo's life as a hacker, was produced by Trigger Street Productions, and narrated by Kevin Spacey. Focusing on the 2003 hacking scene, the film features interviews with Kevin Rose and Steve Wozniak. The film has not been conventionally released. In May 2009, a video purporting to be a trailer for Hackers Wanted was allegedly leaked onto the Internet film site Eye Crave Network. In May 2010, an early cut of the film was leaked via BitTorrent. According to an insider, what was leaked on the Internet was very different from the newer version, which includes additional footage. On June 12, 2010, a director's cut version of the film was also leaked onto torrent sites.",
        "Lamo also appeared on Good Morning America, Fox News, Democracy Now!, Frontline, and repeatedly on KCRA-TV News as an expert on netcentric crime and incidents. He was interviewed for the documentaries We Steal Secrets: The Story of WikiLeaks and True Stories: WikiLeaks – Secrets and Lies. Lamo reconnected with Leo Laporte in 2015 as a result of a Quora article on the \"dark web\" for an episode of The New Screen Savers.\n\nLamo wrote the book Ask Adrian, a collection of his best Q&A drawn from over 500 pages of Quora answers.",
        "Lamo wrote the book Ask Adrian, a collection of his best Q&A drawn from over 500 pages of Quora answers.\n\nPersonal life and death\nLamo was known as the \"Homeless Hacker\" for his reportedly transient lifestyle, claiming that he spent much of his travels couch-surfing, squatting in abandoned buildings, and traveling to Internet cafés, libraries, and universities to investigate networks, sometimes exploiting security holes. He usually preferred sleeping on couches, and when he did sleep on beds, he did not sleep under covers. He would also often wander through homes and offices in the middle of the night, by the light of a flashlight.",
        "Lamo was bisexual and volunteered for the gay and lesbian media firm PlanetOut Inc. in the mid-1990s. In 1998, Lamo was appointed to the Lesbian, Gay, Bisexual, Transgender, Queer and Questioning Youth Task Force by the San Francisco Board of Supervisors.\n\nLamo used a wide variety of supplements and drugs throughout his life. His wife, Lauren Fisher, called his drug use \"body hacking\". One of Lamo's preferred supplements was 'kratom' (Mitragyna speciosa), which he used as a less-dangerous alternative to opioids. In 2001, he overdosed on prescription amphetamines. After he turned in Manning, his drug use escalated, but he later claimed that he was in recovery.",
        "In a 2004 interview with Wired, an ex-girlfriend of Lamo's described him as \"very controlling\", alleging \"he carried a stun gun, which he used on me\". The same article claimed a court had issued a restraining order against Lamo; he disputed the claim, writing: \"I have never been subject to a restraining order in my life\".\n\nLamo said in a Wired article that, in May 2010, after he reported the theft of his backpack, an investigating officer noted unusual behavior and placed him under a 72-hour involuntary psychiatric hold, which was extended to a nine-day hold. Lamo said he was diagnosed with Asperger syndrome at the psychiatric ward.\n\nFor a period of time in March 2011, Lamo was allegedly \"in hiding\", claiming that his \"life was under threat\" after turning in Manning.",
        "For a period of time in March 2011, Lamo was allegedly \"in hiding\", claiming that his \"life was under threat\" after turning in Manning.\n\nLamo died on March 14, 2018, in Wichita, Kansas, at the age of 37. Nearly three months later, the Sedgwick County Regional Forensic Science Center reported that \"Despite a complete autopsy and supplemental testing, no definitive cause of death was identified.\" However, many bottles of pills were found in his home. Several of the pills found there were known to cause severe health problems when combined with kratom. As a result, evidence points to an accidental death due to drug abuse.\n\nSee also\nList of unsolved deaths\n\nReferences\n\nExternal links",
        "See also\nList of unsolved deaths\n\nReferences\n\nExternal links\n\n \n \n\n1981 births\n2018 deaths\nAmerican computer criminals\nAmerican people of Colombian descent\nAmerican River College alumni\nAmerican bisexual people\nBisexual men\nLGBT Hispanic and Latino American people\nLGBT people from Massachusetts\nMicrosoft people\nThe New York Times people\nHackers\nPeople from Boston\nAmerican people with disabilities\nPeople with Asperger syndrome\n20th-century squatters\nUnsolved deaths in the United States\nPeople associated with WikiLeaks\nYahoo! people\n21st-century American LGBT people"
    ],
    [
        "Advanced Encryption Standard\nThe Advanced Encryption Standard (AES), also known by its original name Rijndael (), is a specification for the encryption of electronic data established by the U.S. National Institute of Standards and Technology (NIST) in 2001.\n\nAES is a variant of the Rijndael block cipher developed by two Belgian cryptographers, Joan Daemen and Vincent Rijmen, who submitted a proposal to NIST during the AES selection process. Rijndael is a family of ciphers with different key and block sizes. For AES, NIST selected three members of the Rijndael family, each with a block size of 128 bits, but three different key lengths: 128, 192 and 256 bits.",
        "AES has been adopted by the U.S. government. It supersedes the Data Encryption Standard (DES), which was published in 1977. The algorithm described by AES is a symmetric-key algorithm, meaning the same key is used for both encrypting and decrypting the data.\n\nIn the United States, AES was announced by the NIST as U.S. FIPS PUB 197 (FIPS 197) on November 26, 2001. This announcement followed a five-year standardization process in which fifteen competing designs were presented and evaluated, before the Rijndael cipher was selected as the most suitable.",
        "AES is included in the ISO/IEC 18033-3 standard.  AES became effective as a U.S. federal government standard on May 26, 2002, after approval by U.S. Secretary of Commerce Donald Evans. AES is available in many different encryption packages, and is the first (and only) publicly accessible cipher approved by the U.S. National Security Agency (NSA) for top secret information when used in an NSA approved cryptographic module.\n\nDefinitive standards \n\nThe Advanced Encryption Standard (AES) is defined in each of:\n\n FIPS PUB 197: Advanced Encryption Standard (AES)\n ISO/IEC 18033-3: Block ciphers",
        "Definitive standards \n\nThe Advanced Encryption Standard (AES) is defined in each of:\n\n FIPS PUB 197: Advanced Encryption Standard (AES)\n ISO/IEC 18033-3: Block ciphers\n\nDescription of the ciphers \nAES is based on a design principle known as a substitution–permutation network, and is efficient in both software and hardware. Unlike its predecessor DES, AES does not use a Feistel network. AES is a variant of Rijndael, with a fixed block size of 128 bits, and a key size of 128, 192, or 256 bits. By contrast, Rijndael per se is specified with block and key sizes that may be any multiple of 32 bits, with a minimum of 128 and a maximum of 256 bits. Most AES calculations are done in a particular finite field.\n\nAES operates on a 4 × 4 column-major order array of 16 bytes  termed the state:",
        "AES operates on a 4 × 4 column-major order array of 16 bytes  termed the state:\n\nThe key size used for an AES cipher specifies the number of transformation rounds that convert the input, called the plaintext, into the final output, called the ciphertext. The number of rounds are as follows:\n\n 10 rounds for 128-bit keys.\n 12 rounds for 192-bit keys.\n 14 rounds for 256-bit keys.\n\nEach round consists of several processing steps, including one that depends on the encryption key itself. A set of reverse rounds are applied to transform ciphertext back into the original plaintext using the same encryption key.\n\nHigh-level description of the algorithm",
        "High-level description of the algorithm \n\n round keys are derived from the cipher key using the AES key schedule. AES requires a separate 128-bit round key block for each round plus one more.\n Initial round key addition:\n each byte of the state is combined with a byte of the round key using bitwise xor.\n 9, 11 or 13 rounds:\n a non-linear substitution step where each byte is replaced with another according to a lookup table.\n a transposition step where the last three rows of the state are shifted cyclically a certain number of steps.\n a linear mixing operation which operates on the columns of the state, combining the four bytes in each column.\n \n Final round (making 10, 12 or 14 rounds in total):\n\nThe  step",
        "In the  step, each byte  in the state array is replaced with a   using an 8-bit substitution box. Note that before round 0, the state array is simply the plaintext/input. This operation provides the non-linearity in the cipher. The S-box used is derived from the multiplicative inverse over , known to have good non-linearity properties. To avoid attacks based on simple algebraic properties, the S-box is constructed by combining the inverse function with an invertible affine transformation. The S-box is also chosen to avoid any fixed points (and so is a derangement), i.e., , and also any opposite fixed points, i.e., .",
        "While performing the decryption, the  step (the inverse of ) is used, which requires first taking the inverse of the affine transformation and then finding the multiplicative inverse.",
        "The  step \n\nThe  step operates on the rows of the state; it cyclically shifts the bytes in each row by a certain offset. For AES, the first row is left unchanged. Each byte of the second row is shifted one to the left. Similarly, the third and fourth rows are shifted by offsets of two and three respectively. In this way, each column of the output state of the  step is composed of bytes from each column of the input state. The importance of this step is to avoid the columns being encrypted independently, in which case AES would degenerate into four independent block ciphers.\n\nThe  step",
        "The  step \n\nIn the  step, the four bytes of each column of the state are combined using an invertible linear transformation. The  function takes four bytes as input and outputs four bytes, where each input byte affects all four output bytes. Together with ,  provides diffusion in the cipher.\n\nDuring this operation, each column is transformed using a fixed matrix (matrix left-multiplied by column gives new value of column in the state):",
        "During this operation, each column is transformed using a fixed matrix (matrix left-multiplied by column gives new value of column in the state):\n\nMatrix multiplication is composed of multiplication and addition of the entries. Entries are bytes treated as coefficients of polynomial of order . Addition is simply XOR. Multiplication is modulo irreducible polynomial . If processed bit by bit, then, after shifting, a conditional XOR with 1B16 should be performed if the shifted value is larger than FF16 (overflow must be corrected by subtraction of generating polynomial). These are special cases of the usual multiplication in .",
        "In more general sense, each column is treated as a polynomial over  and is then multiplied modulo  with a fixed polynomial . The coefficients are displayed in their hexadecimal equivalent of the binary representation of bit polynomials from . The  step can also be viewed as a multiplication by the shown particular MDS matrix in the finite field . This process is described further in the article Rijndael MixColumns.\n\nThe   \n\nIn the  step, the subkey is combined with the state. For each round, a subkey is derived from the main key using Rijndael's key schedule; each subkey is the same size as the state. The subkey is added by combining of the state with the corresponding byte of the subkey using bitwise XOR.",
        "Optimization of the cipher \nOn systems with 32-bit or larger words, it is possible to speed up execution of this cipher by combining the  and  steps with the  step by transforming them into a sequence of table lookups. This requires four 256-entry 32-bit tables (together occupying 4096 bytes).  A round can then be performed with 16 table lookup operations and 12 32-bit exclusive-or operations, followed by four 32-bit exclusive-or operations in the  step.  Alternatively, the table lookup operation can be performed with a single 256-entry 32-bit table (occupying 1024 bytes) followed by circular rotation operations.\n\nUsing a byte-oriented approach, it is possible to combine the , , and  steps into a single round operation.",
        "Using a byte-oriented approach, it is possible to combine the , , and  steps into a single round operation.\n\nSecurity \nThe National Security Agency (NSA) reviewed all the AES finalists, including Rijndael, and stated that all of them were secure enough for U.S. Government non-classified data. In June 2003, the U.S. Government announced that AES could be used to protect classified information:",
        "The design and strength of all key lengths of the AES algorithm (i.e., 128, 192 and 256) are sufficient to protect classified information up to the SECRET level. TOP SECRET information will require use of either the 192 or 256 key lengths. The implementation of AES in products intended to protect national security systems and/or information must be reviewed and certified by NSA prior to their acquisition and use.\n\nAES has 10 rounds for 128-bit keys, 12 rounds for 192-bit keys, and 14 rounds for 256-bit keys.\n\nBy 2006, the best known attacks were on 7 rounds for 128-bit keys, 8 rounds for 192-bit keys, and 9 rounds for 256-bit keys.",
        "By 2006, the best known attacks were on 7 rounds for 128-bit keys, 8 rounds for 192-bit keys, and 9 rounds for 256-bit keys.\n\nKnown attacks \nFor cryptographers, a cryptographic \"break\" is anything faster than a brute-force attack – i.e., performing one trial decryption for each possible key in sequence. A break can thus include results that are infeasible with current technology. Despite being impractical, theoretical breaks can sometimes provide insight into vulnerability patterns. The largest successful publicly known brute-force attack against a widely implemented block-cipher encryption algorithm was against a 64-bit RC5 key by distributed.net in 2006.",
        "The key space increases by a factor of 2 for each additional bit of key length, and if every possible value of the key is equiprobable, this translates into a doubling of the average brute-force key search time. This implies that the effort of a brute-force search increases exponentially with key length. Key length in itself does not imply security against attacks, since there are ciphers with very long keys that have been found to be vulnerable.",
        "AES has a fairly simple algebraic framework. In 2002, a theoretical attack, named the \"XSL attack\", was announced by Nicolas Courtois and Josef Pieprzyk, purporting to show a weakness in the AES algorithm, partially due to the low complexity of its nonlinear components. Since then, other papers have shown that the attack, as originally presented, is unworkable; see XSL attack on block ciphers.",
        "During the AES selection process, developers of competing algorithms wrote of Rijndael's algorithm \"we are concerned about [its] use ... in security-critical applications.\" In October 2000, however, at the end of the AES selection process, Bruce Schneier, a developer of the competing algorithm Twofish, wrote that while he thought successful academic attacks on Rijndael would be developed someday, he \"did not believe that anyone will ever discover an attack that will allow someone to read Rijndael traffic.\"",
        "Until May 2009, the only successful published attacks against the full AES were side-channel attacks on some specific implementations. In 2009, a new related-key attack was discovered that exploits the simplicity of AES's key schedule and has a complexity of 2119. In December 2009 it was improved to 299.5. This is a follow-up to an attack discovered earlier in 2009 by Alex Biryukov, Dmitry Khovratovich, and Ivica Nikolić, with a complexity of 296 for one out of every 235 keys. However, related-key attacks are not of concern in any properly designed cryptographic protocol, as a properly designed protocol (i.e., implementational software) will take care not to allow related keys, essentially by constraining an attacker's means of selecting keys for relatedness.",
        "Another attack was blogged by Bruce Schneier\non July 30, 2009, and released as a preprint\non August 3, 2009. This new attack, by Alex Biryukov, Orr Dunkelman, Nathan Keller, Dmitry Khovratovich, and Adi Shamir, is against AES-256 that uses only two related keys and 239 time to recover the complete 256-bit key of a 9-round version, or 245 time for a 10-round version with a stronger type of related subkey attack, or 270 time for an 11-round version. 256-bit AES uses 14 rounds, so these attacks are not effective against full AES.\n\nThe practicality of these attacks with stronger related keys has been criticized, for instance, by the paper on chosen-key-relations-in-the-middle attacks on AES-128 authored by Vincent Rijmen in 2010.",
        "In November 2009, the first known-key distinguishing attack against a reduced 8-round version of AES-128 was released as a preprint.\nThis known-key distinguishing attack is an improvement of the rebound, or the start-from-the-middle attack, against AES-like permutations, which view two consecutive rounds of permutation as the application of a so-called Super-S-box. It works on the 8-round version of AES-128, with a time complexity of 248, and a memory complexity of 232.  128-bit AES uses 10 rounds, so this attack is not effective against full AES-128.",
        "The first key-recovery attacks on full AES were by Andrey Bogdanov, Dmitry Khovratovich, and Christian Rechberger, and were published in 2011. The attack is a biclique attack and is faster than brute force by a factor of about four. It requires 2126.2 operations to recover an AES-128 key. For AES-192 and AES-256, 2190.2 and 2254.6 operations are needed, respectively. This result has been further improved to 2126.0 for AES-128, 2189.9 for AES-192 and 2254.3 for AES-256, which are the current best results in key recovery attack against AES.",
        "This is a very small gain, as a 126-bit key (instead of 128-bits) would still take billions of years to brute force on current and foreseeable hardware. Also, the authors calculate the best attack using their technique on AES with a 128-bit key requires storing 288 bits of data. That works out to about 38 trillion terabytes of data, which is more than all the data stored on all the computers on the planet in 2016. As such, there are no practical implications on AES security. The space complexity has later been improved to 256 bits, which is 9007 terabytes (while still keeping a time complexity of 2126.2).\n\nAccording to the Snowden documents, the NSA is doing research on whether a cryptographic attack based on tau statistic may help to break AES.",
        "According to the Snowden documents, the NSA is doing research on whether a cryptographic attack based on tau statistic may help to break AES.\n\nAt present, there is no known practical attack that would allow someone without knowledge of the key to read data encrypted by AES when correctly implemented.\n\nSide-channel attacks \n\nSide-channel attacks do not attack the cipher as a black box, and thus are not related to cipher security as defined in the classical context, but are important in practice. They attack implementations of the cipher on hardware or software systems that inadvertently leak data. There are several such known attacks on various implementations of AES.",
        "In April 2005, D. J. Bernstein announced a cache-timing attack that he used to break a custom server that used OpenSSL's AES encryption. The attack required over 200 million chosen plaintexts. The custom server was designed to give out as much timing information as possible (the server reports back the number of machine cycles taken by the encryption operation). However, as Bernstein pointed out, \"reducing the precision of the server's timestamps, or eliminating them from the server's responses, does not stop the attack: the client simply uses round-trip timings based on its local clock, and compensates for the increased noise by averaging over a larger number of samples.\"",
        "In October 2005, Dag Arne Osvik, Adi Shamir and Eran Tromer presented a paper demonstrating several cache-timing attacks against the implementations in AES found in OpenSSL and Linux's dm-crypt partition encryption function. One attack was able to obtain an entire AES key after only 800 operations triggering encryptions, in a total of 65 milliseconds. This attack requires the attacker to be able to run programs on the same system or platform that is performing AES.\n\nIn December 2009 an attack on some hardware implementations was published that used differential fault analysis and allows recovery of a key with a complexity of 232.",
        "In December 2009 an attack on some hardware implementations was published that used differential fault analysis and allows recovery of a key with a complexity of 232.\n\nIn November 2010 Endre Bangerter, David Gullasch and Stephan Krenn published a paper which described a practical approach to a \"near real time\" recovery of secret keys from AES-128 without the need for either cipher text or plaintext. The approach also works on AES-128 implementations that use compression tables, such as OpenSSL. Like some earlier attacks, this one requires the ability to run unprivileged code on the system performing the AES encryption, which may be achieved by malware infection far more easily than commandeering the root account.",
        "In March 2016, Ashokkumar C., Ravi Prakash Giri and Bernard Menezes presented a side-channel attack on AES implementations that can recover the complete 128-bit AES key in just 6–7 blocks of plaintext/ciphertext, which is a substantial improvement over previous works that require between 100 and a million encryptions. The proposed attack requires standard user privilege and key-retrieval algorithms run under a minute.\n\nMany modern CPUs have built-in hardware instructions for AES, which protect against timing-related side-channel attacks.",
        "Many modern CPUs have built-in hardware instructions for AES, which protect against timing-related side-channel attacks.\n\nQuantum attacks \nAES-256 is considered to be quantum resistant, as it has similar quantum resistance to AES-128's resistance against traditional, non-quantum, attacks. Whilst AES-192 and AES-128 are not considered quantum resistant due to their smaller key sizes. AES-192 has a strength of 96-bits against quantum attacks and AES-128 has 64-bits of strength against quantum attacks, making them both insecure.",
        "NIST/CSEC validation \nThe Cryptographic Module Validation Program (CMVP) is operated jointly by the United States Government's National Institute of Standards and Technology (NIST) Computer Security Division and the Communications Security Establishment (CSE) of the Government of Canada. The use of cryptographic modules validated to NIST FIPS 140-2 is required by the United States Government for encryption of all data that has a classification of Sensitive but Unclassified (SBU) or above. From NSTISSP #11, National Policy Governing the Acquisition of Information Assurance: “Encryption products for protecting classified information will be certified by NSA, and encryption products intended for protecting sensitive information will be certified in accordance with NIST FIPS 140-2.”",
        "The Government of Canada also recommends the use of FIPS 140 validated cryptographic modules in unclassified applications of its departments.\n\nAlthough NIST publication 197 (“FIPS 197”) is the unique document that covers the AES algorithm, vendors typically approach the CMVP under FIPS 140 and ask to have several algorithms (such as Triple DES or SHA1) validated at the same time. Therefore, it is rare to find cryptographic modules that are uniquely FIPS 197 validated and NIST itself does not generally take the time to list FIPS 197 validated modules separately on its public web site. Instead, FIPS 197 validation is typically just listed as an \"FIPS approved: AES\" notation (with a specific FIPS 197 certificate number) in the current list of FIPS 140 validated cryptographic modules.",
        "The Cryptographic Algorithm Validation Program (CAVP) allows for independent validation of the correct implementation of the AES algorithm. Successful validation results in being listed on the NIST validations page. This testing is a pre-requisite for the FIPS 140-2 module validation. However, successful CAVP validation in no way implies that the cryptographic module implementing the algorithm is secure. A cryptographic module lacking FIPS 140-2 validation or specific approval by the NSA is not deemed secure by the US Government and cannot be used to protect government data.",
        "FIPS 140-2 validation is challenging to achieve both technically and fiscally. There is a standardized battery of tests as well as an element of source code review that must be passed over a period of a few weeks. The cost to perform these tests through an approved laboratory can be significant (e.g., well over $30,000 US) and does not include the time it takes to write, test, document and prepare a module for validation. After validation, modules must be re-submitted and re-evaluated if they are changed in any way. This can vary from simple paperwork updates if the security functionality did not change to a more substantial set of re-testing if the security functionality was impacted by the change.",
        "Test vectors \nTest vectors are a set of known ciphers for a given input and key. NIST distributes the reference of AES test vectors as AES Known Answer Test (KAT) Vectors.\n\nPerformance \nHigh speed and low RAM requirements were some of the criteria of the AES selection process. As the chosen algorithm, AES performed well on a wide variety of hardware, from 8-bit smart cards to high-performance computers.\n\nOn a Pentium Pro, AES encryption requires 18 clock cycles per byte, equivalent to a throughput of about 11 MiB/s for a 200 MHz processor.\n\nOn Intel Core and AMD Ryzen CPUs supporting AES-NI instruction set extensions, throughput can be multiple GiB/s (even over 15 GiB/s on an i7-12700k).\n\nImplementations",
        "On Intel Core and AMD Ryzen CPUs supporting AES-NI instruction set extensions, throughput can be multiple GiB/s (even over 15 GiB/s on an i7-12700k).\n\nImplementations\n\nSee also \nAES modes of operation\nDisk encryption\nEncryption\nWhirlpool – hash function created by Vincent Rijmen and Paulo S. L. M. Barreto\nList of free and open-source software packages\n\nNotes\n\nReferences \n\n \n \n  alternate link (companion web site contains online lectures on AES)\n\nExternal links \n\n AES algorithm archive information – (old, unmaintained)",
        "Notes\n\nReferences \n\n \n \n  alternate link (companion web site contains online lectures on AES)\n\nExternal links \n\n AES algorithm archive information – (old, unmaintained)\n\n Animation of Rijndael – AES deeply explained and animated using Flash (by Enrique Zabala / University ORT / Montevideo / Uruguay). This animation (in English, Spanish, and German) is also part of CrypTool 1 (menu Indiv. Procedures → Visualization of Algorithms → AES).\n HTML5 Animation of Rijndael – Same Animation as above made in HTML5.\n\nAdvanced Encryption Standard\nCryptography"
    ],
    [
        "Advanced Mobile Phone System",
        "Advanced Mobile Phone System (AMPS) was an analog mobile phone system standard originally developed by Bell Labs and later modified in a cooperative effort between Bell Labs and Motorola. It was officially introduced in the Americas on October 13, 1983, and was deployed in many other countries too, including Israel in 1986, Australia in 1987, Singapore in 1988, and Pakistan in 1990. It was the primary analog mobile phone system in North America (and other locales) through the 1980s and into the 2000s. As of February 18, 2008, carriers in the United States were no longer required to support AMPS and companies such as AT&T and Verizon Communications have discontinued this service permanently. AMPS was discontinued in Australia in September 2000, in Pakistan by October 2004, in Israel by",
        "AMPS and companies such as AT&T and Verizon Communications have discontinued this service permanently. AMPS was discontinued in Australia in September 2000, in Pakistan by October 2004, in Israel by January 2010, and Brazil by 2010.",
        "History \n\nThe first cellular network efforts began at Bell Labs and with research conducted at Motorola. \nIn 1960, John F. Mitchell\nbecame Motorola's chief engineer for its mobile-communication products, and oversaw the development and marketing of the first pager to use transistors.\n\nMotorola had long produced mobile telephones for automobiles, but these large and heavy models consumed too much power to allow their use without the automobile's engine running. Mitchell's team, which included the gifted Dr. Martin Cooper, developed portable cellular telephony. Cooper and Mitchell were among the Motorola employees granted a patent for this work in 1973.  The first call on the prototype connected, reportedly, to a wrong number.",
        "While Motorola was developing a cellular phone, from 1968 to 1983 Bell Labs worked out a system called Advanced Mobile Phone System (AMPS), which became the first cellular network standard in the United States.  The first system was successfully deployed in Chicago, Illinois, in 1979.  Motorola and others designed and built the cellular phones for this and other cellular systems.\n\nMartin Cooper, a former general manager for the systems division at Motorola, led a team that produced the first cellular handset in 1973 and made the first phone call from it. In 1983 Motorola introduced the DynaTAC 8000x, the first commercially available cellular phone small enough to be easily carried. He later introduced the so-called Bag Phone.",
        "In 1992, the first smartphone, called IBM Simon, used AMPS.  Frank Canova led its design at IBM and it was demonstrated that year at the COMDEX computer-industry trade-show.  A refined version of the product was marketed to consumers in 1994 by BellSouth under the name Simon Personal Communicator.  The Simon was the first device that can be properly referred to as a \"smartphone\", even though that term was not yet coined.",
        "Technology \nAMPS is a first-generation cellular technology that uses separate frequencies, or \"channels\", for each conversation. It therefore required considerable bandwidth for a large number of users. In general terms, AMPS was very similar to the older \"0G\" Improved Mobile Telephone Service it replaced, but used considerably more computing power to select frequencies, hand off conversations to land lines, and handle billing and call setup.",
        "What really separated AMPS from older systems is the \"back end\" call setup functionality. In AMPS, the cell centers could flexibly assign channels to handsets based on signal strength, allowing the same frequency to be re-used, without interference, if locations were separated enough. The channels were grouped so a specific set was different of the one used on the cell nearby. This allowed a larger number of phones to be supported over a geographical area. AMPS pioneers coined the term \"cellular\" because of its use of small hexagonal \"cells\" within a system.",
        "AMPS suffered from many weaknesses compared to today's digital technologies. As an analog standard, it was susceptible to static and noise, and there was no protection from 'eavesdropping' using a scanner or an older TV set that could tune into channels 70-83.\n\nCloning",
        "Cloning\n\nIn the 1990s, an epidemic of \"cloning\" cost the cellular carriers millions of dollars. An eavesdropper with specialized equipment could intercept a handset's ESN (Electronic Serial Number) and MDN or CTN (Mobile Directory Number or Cellular Telephone Number). The Electronic Serial Number, a 12-digit number sent by the handset to the cellular system for billing purposes, uniquely identified that phone on the network. The system then allowed or disallowed calls and/or features based on its customer file. A person intercepting an ESN/MDN pair could clone the combination onto a different phone and use it in other areas for making calls without paying.\n\nCellular phone cloning became possible with off-the-shelf technology in the 1990s. Would-be cloners required three key items :",
        "Cellular phone cloning became possible with off-the-shelf technology in the 1990s. Would-be cloners required three key items :\n\n A radio receiver, such as the Icom PCR-1000, that could tune into the Reverse Channel (the frequency on which AMPS phones transmit data to the tower)\n A PC with a sound card and a software program called Banpaia\n A phone that could easily be used for cloning, such as the Oki 900",
        "The radio, when tuned to the proper frequency, would receive the signal transmitted by the cell phone to be cloned, containing the phone's ESN/MDN pair. This signal would feed into the sound-card audio-input of the PC, and Banpaia would decode the ESN/MDN pair from this signal and display it on the screen. The hacker could then copy that data into the Oki 900 phone and reboot it, after which the phone network could not distinguish the Oki from the original phone whose signal had been received. This gave the cloner, through the Oki phone, the ability to use the mobile-phone service of the legitimate subscriber whose phone was cloned – just as if that phone had been physically stolen, except that the subscriber retained his or her phone, unaware that the phone had been cloned—at least until",
        "subscriber whose phone was cloned – just as if that phone had been physically stolen, except that the subscriber retained his or her phone, unaware that the phone had been cloned—at least until that subscriber received his or her next bill.",
        "The problem became so large that some carriers required the use of a PIN before making calls. Eventually, the cellular companies initiated a system called RF Fingerprinting, whereby it could determine subtle differences in the signal of one phone from another and shut down some cloned phones. Some legitimate customers had problems with this though if they made certain changes to their own phone, such as replacing the battery and/or antenna.\n\nThe Oki 900 could listen in to AMPS phone-calls right out-of-the-box with no hardware modifications.",
        "The Oki 900 could listen in to AMPS phone-calls right out-of-the-box with no hardware modifications.\n\nStandards\nAMPS was originally standardized by American National Standards Institute (ANSI) as EIA/TIA/IS-3. EIA/TIA/IS-3 was superseded by EIA/TIA-553 and TIA interim standard with digital technologies, the cost of wireless service is so low that the problem of cloning  has virtually disappeared.",
        "Frequency bands\nAMPS cellular service operated in the 850 MHz Cellular band. For each market area, the United States Federal Communications Commission (FCC) allowed two licensees (networks) known as \"A\" and \"B\" carriers. Each carrier within a market used a specified \"block\" of frequencies consisting of 21 control channels and 395 voice channels. Originally, the B (wireline) side license was usually owned by the local phone company, and the A (non-wireline) license was given to wireless telephone providers.",
        "At the inception of cellular in 1983, the FCC had granted each carrier within a market 333 channel pairs (666 channels total). By the late 1980s, the cellular industry's subscriber base had grown into the millions across America and it became necessary to add channels for additional capacity. In 1989, the FCC granted carriers an expansion from the previous 666 channels to the final 832 (416 pairs per carrier). The additional frequencies were from the band held in reserve for future (inevitable) expansion. These frequencies were immediately adjacent to the existing cellular band. These bands had previously been allocated to UHF TV channels 70–83.",
        "Each duplex channel was composed of 2 frequencies. 416 of these were in the 824–849 MHz range for transmissions from mobile stations to the base stations, paired with 416 frequencies in the 869–894 MHz range for transmissions from base stations to the mobile stations. Each cell site used a different subset of these channels than its neighbors to avoid interference. This significantly reduced the number of channels available at each site in real-world systems. Each AMPS channel had a one way bandwidth of 30 kHz, for a total of 60 kHz for each duplex channel.",
        "Laws were passed in the US which prohibited the FCC type acceptance and sale of any receiver which could tune the frequency ranges occupied by analog AMPS cellular services. Though the service is no longer offered, these laws remain in force (although they may no longer be enforced).\n\nNarrowband AMPS\nIn 1991, Motorola proposed an AMPS enhancement known as narrowband AMPS (NAMPS or N-AMPS).\n\nDigital AMPS",
        "Narrowband AMPS\nIn 1991, Motorola proposed an AMPS enhancement known as narrowband AMPS (NAMPS or N-AMPS).\n\nDigital AMPS\n\nLater, many AMPS networks were partially converted to D-AMPS, often referred to as TDMA (though TDMA is a generic term that applies to many 2G cellular systems).  D-AMPS, commercially deployed since 1993, was a digital, 2G standard used mainly by AT&T Mobility and U.S. Cellular in the United States, Rogers Wireless in Canada, Telcel in Mexico, Telecom Italia Mobile (TIM) in Brazil, VimpelCom in Russia, Movilnet in Venezuela, and Cellcom in Israel.  In most areas, D-AMPS is no longer offered and has been replaced by more advanced digital wireless networks.",
        "Successor technologies\nAMPS and D-AMPS have now been phased out in favor of either CDMA2000 or GSM, which allow for higher capacity data transfers for services such as WAP, Multimedia Messaging System (MMS), and wireless Internet access. There are some phones capable of supporting AMPS, D-AMPS and GSM all in one phone (using the GAIT standard).\n\nAnalog AMPS being replaced by digital\nIn 2002, the FCC decided to no longer require A and B carriers to support AMPS service as of February 18, 2008. All AMPS carriers have converted to a digital standard such as CDMA2000 or GSM. Digital technologies such as GSM and CDMA2000 support multiple voice calls on the same channel and offer enhanced features such as two-way text messaging and data services.",
        "Unlike in the United States, the Canadian Radio-television and Telecommunications Commission (CRTC) and Industry Canada have not set any requirement for maintaining AMPS service in Canada. Rogers Wireless has dismantled their AMPS (along with IS-136) network; the networks were shut down May 31, 2007. Bell Mobility and Telus Mobility, who operated AMPS networks in Canada, announced that they would observe the same timetable as outlined by the FCC in the United States, and as a result would not begin to dismantle their AMPS networks until after February 2008.",
        "OnStar relied heavily on North American AMPS service for its subscribers because, when the system was developed, AMPS offered the most comprehensive wireless coverage in the US. In 2006, ADT asked the FCC to extend the AMPS deadline due to many of their alarm systems still using analog technology to communicate with the control centers. Cellular companies who own an A or B license (such as Verizon and Alltel) were required to provide analog service until February 18, 2008. After that point, however, most cellular companies were eager to shut down AMPS and use the remaining channels for digital services. OnStar transitioned to digital service with the help of data transport technology developed by Airbiquity, but warned customers who could not be upgraded to digital service that their",
        "services. OnStar transitioned to digital service with the help of data transport technology developed by Airbiquity, but warned customers who could not be upgraded to digital service that their service would permanently expire on January 1, 2008.",
        "Commercial deployments of AMPS by country\n\nSee also\nHistory of mobile phones\n\nCitations\n\nReferences\nInterview of Joel Engel\n\nHistory of mobile phones\nMobile radio telephone systems\nTelecommunications-related introductions in 1983"
    ],
    [
        "Adware",
        "Adware, often called advertising-supported software by its developers, is software that generates revenue for its developer by automatically generating online advertisements in the user interface of the software or on a screen presented to the user during the installation process. The software may generate two types of revenue: one is for the display of the advertisement and another on a \"pay-per-click\" basis, if the user clicks on the advertisement. Some advertisements also act as spyware, collecting and reporting data about the user, to be sold or used for targeted advertising or user profiling. The software may implement advertisements in a variety of ways, including a static box display, a banner display, a full screen, a video, a pop-up ad or in some other form. All forms of",
        "profiling. The software may implement advertisements in a variety of ways, including a static box display, a banner display, a full screen, a video, a pop-up ad or in some other form. All forms of advertising carry health, ethical, privacy and security risks for users.",
        "The 2003 Microsoft Encyclopedia of Security and some other sources use the term \"adware\" differently: \"any software that installs itself on your system without your knowledge and displays advertisements when the user browses the Internet\", i.e., a form of malware.\n\nSome software developers offer their software free of charge, and rely on revenue from advertising to recoup their expenses and generate income. Some also offer a version of the software at a fee without advertising.",
        "Advertising-supported software",
        "In legitimate software, the advertising functions are integrated into or bundled with the program. Adware is usually seen by the developer as a way to recover development costs, and generate revenue. In some cases, the developer may provide the software to the user free of charge or at a reduced price. The income derived from presenting advertisements to the user may allow or motivate the developer to continue to develop, maintain and upgrade the software product. The use of advertising-supported software in business is becoming increasingly popular, with a third of IT and business executives in a 2007 survey by McKinsey & Company planning to be using ad-funded software within the following two years. Advertisement-funded software is also one of the business models for open-source",
        "in a 2007 survey by McKinsey & Company planning to be using ad-funded software within the following two years. Advertisement-funded software is also one of the business models for open-source software.",
        "Application software \nSome software is offered in both an advertising-supported mode and a paid, advertisement-free mode. The latter is usually available by an online purchase of a license or registration code for the software that unlocks the mode, or the purchase and download of a separate version of the software.\n\nSome software authors offer advertising-supported versions of their software as an alternative option to business organizations seeking to avoid paying large sums for software licenses, funding the development of the software with higher fees for advertisers.",
        "Examples of advertising-supported software include Adblock Plus (\"Acceptable Ads\"), the Windows version of the Internet telephony application Skype, and the Amazon Kindle 3 family of e-book readers, which has versions called \"Kindle with Special Offers\" that display advertisements on the home page and in sleep mode in exchange for substantially lower pricing.\n\nIn 2012, Microsoft and its advertising division, Microsoft Advertising, announced that Windows 8, the major release of the Microsoft Windows operating system, would provide built-in methods for software authors to use advertising support as a business model. The idea had been considered since as early as 2005. Most editions of Windows 10 include adware by default.",
        "Software as a service \nSupport by advertising is a popular business model of software as a service (SaaS) on the Web. Notable examples include the email service Gmail and other Google Workspace products (previously called Google Apps and G Suite), and the social network Facebook. Microsoft has also adopted the advertising-supported model for many of its social software SaaS offerings. The Microsoft Office Live service was also available in an advertising-supported mode.",
        "Definition of Spyware, Consent, and Ethics \nIn the view of Federal Trade Commission staff, there appears to be general agreement that software should be considered \"spyware\" only if it is downloaded or installed on a computer without the user's knowledge and consent. However, unresolved issues remain concerning how, what, and when consumers need to be told about software installed on their computers. For instance, distributors often disclose in an end-user license agreement that there is additional software bundled with primary software, but some participants did not view such disclosure as sufficient to infer consent.",
        "Much of the discussion on the topic involves the idea of informed consent, the assumption being that this standard eliminates any ethical issues with any given software's behavior. However, if a majority of important software, websites and devices were to adopt similar behavior and only the standard of informed consent is used, then logically a user's only recourse against that behavior would become not using a computer. The contract would become an ultimatum - agree or be ostracized from the modern world. This is a form of psychological coercion and presents an ethical problem with using implied or inferred consent as a standard. There are notable similarities between this situation and binding arbitration clauses which have become inevitable in contracts in the United States.",
        "Furthermore, certain forms and strategies of advertising have been shown to lead to psychological harm, especially in children. One example is childhood eating disorders - several studies have reported a positive association between exposure to beauty and fashion magazines and an increased level of weight concerns or eating disorder symptoms in girls.\n\nMalware \nThe term adware is frequently used to describe a form of malware (malicious software) which presents unwanted advertisements to the user of a computer. The advertisements produced by adware are sometimes in the form of a pop-up, sometimes in an \"unclosable window\", and sometimes injected into web pages.",
        "When the term is used in this way, the severity of its implication varies. While some sources rate adware only as an \"irritant\", others classify it as an \"online threat\" or even rate it as seriously as computer viruses and trojans. The precise definition of the term in this context also varies. Adware that observes the computer user's activities without their consent and reports it to the software's author is called spyware. Adwares may collect the personal information of the user, causing privacy concerns. However, most adware operates legally and some adware manufacturers have even sued antivirus companies for blocking adware.",
        "Programs have been developed to detect, quarantine, and remove advertisement-displaying malware, including Ad-Aware, Malwarebytes' Anti-Malware, Spyware Doctor and Spybot – Search & Destroy. In addition, almost all commercial antivirus software currently detect adware and spyware, or offer a separate detection module.\n\nA new wrinkle is adware (using stolen certificates) that disables anti-malware and virus protection; technical remedies are available.",
        "A new wrinkle is adware (using stolen certificates) that disables anti-malware and virus protection; technical remedies are available.\n\nAdware has also been discovered in certain low-cost Android devices, particularly those made by small Chinese firms running on Allwinner systems-on-chip. There are even cases where adware code is embedded deep into files stored on the system and boot partitions, to which removal involves extensive (and complex) modifications to the firmware.\n\nIn recent years, machine-learning based systems have been implemented to detect malicious adware on Android devices by examining features in the flow of network traffic.\n\nSee also \n Malvertising\nOnline advertising\n Typhoid adware\n\nNotes\n\nReferences \n\n \nOnline advertising\nTypes of malware"
    ],
    [
        "AI-complete\nIn the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems, assuming intelligence is computational, is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI.  To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm.  \n\nAI-complete problems are hypothesised to include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.",
        "AI-complete problems are hypothesised to include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.\n\nCurrently, AI-complete problems cannot be solved with modern computer technology alone, but would also require human computation.  This property could be useful, for example, to test for the presence of humans as CAPTCHAs aim to do, and for computer security to circumvent brute-force attacks.\n\nHistory\nThe term was coined by Fanya Montalvo by analogy with NP-complete and NP-hard in complexity theory, which formally describes the most famous class of difficult problems. Early uses of the term are in Erik Mueller's 1987 PhD dissertation and in Eric Raymond's 1991 Jargon File.",
        "AI-complete problems\nAI-complete problems are hypothesized to include:\n\n AI peer review (composite natural language understanding, automated reasoning, automated theorem proving, formalized logic expert system)\n Bongard problems\n Computer vision (and subproblems such as object recognition)\n Natural language understanding (and subproblems such as text mining, machine translation, and word-sense disambiguation)\n Autonomous driving\n Dealing with unexpected circumstances while solving any real world problem, whether it's navigation or planning or even the kind of reasoning done by expert systems.\n\nMachine translation",
        "To translate accurately, a machine must be able to understand the text. It must be able to follow the author's argument, so it must have some ability to reason. It must have extensive world knowledge so that it knows what is being discussed — it must at least be familiar with all the same commonsense facts that the average human translator knows. Some of this knowledge is in the form of facts that can be explicitly represented, but some knowledge is unconscious and closely tied to the human body: for example, the machine may need to understand how an ocean makes one feel to accurately translate a specific metaphor in the text. It must also model the authors' goals, intentions, and emotional states to accurately reproduce them in a new language. In short, the machine is required to have",
        "a specific metaphor in the text. It must also model the authors' goals, intentions, and emotional states to accurately reproduce them in a new language. In short, the machine is required to have wide variety of human intellectual skills, including reason, commonsense knowledge and the intuitions that underlie motion and manipulation, perception, and social intelligence. Machine translation, therefore, is believed to be AI-complete: it may require strong AI to be done as well as humans can do it.",
        "Software brittleness",
        "Current AI systems can solve very simple and/or restricted versions of AI-complete problems, but never in their full generality. When AI researchers attempt to \"scale up\" their systems to handle more complicated, real-world situations, the programs tend to become excessively brittle without commonsense knowledge or a rudimentary understanding of the situation: they fail as unexpected circumstances outside of its original problem context begin to appear.  When human beings are dealing with new situations in the world, they are helped immensely by the fact that they know what to expect: they know what all things around them are, why they are there, what they are likely to do and so on. They can recognize unusual situations and adjust accordingly. A machine without strong AI has no other",
        "know what all things around them are, why they are there, what they are likely to do and so on. They can recognize unusual situations and adjust accordingly. A machine without strong AI has no other skills to fall back on.",
        "DeepMind published a work in May 2022 in which they trained a single model to do several things at the same time. The model, named Gato, can \"play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.\"\n\nFormalization\nComputational complexity theory deals with the relative computational difficulty of computable functions.  By definition, it does not cover problems whose solution is unknown or has not been characterised formally.  Since many AI problems have no formalisation yet, conventional complexity theory does not allow the definition of AI-completeness.",
        "To address this problem, a complexity theory for AI has been proposed. It is based on a model of computation that splits the computational burden between a computer and a human: one part is solved by computer and the other part solved by human.  This is formalised by a human-assisted Turing machine.  The formalisation defines algorithm complexity, problem complexity and reducibility which in turn allows equivalence classes to be defined.\n\nThe complexity of executing an algorithm with a human-assisted Turing machine is given by a pair , where the first element represents the complexity of the human's part and the second element is the complexity of the machine's part.\n\nResults\nThe complexity of solving the following problems with a human-assisted Turing machine is:",
        "Results\nThe complexity of solving the following problems with a human-assisted Turing machine is:\n\n Optical character recognition for printed text: \n Turing test:\n for an -sentence conversation where the oracle remembers the conversation history (persistent oracle): \n for an -sentence conversation where the conversation history must be retransmitted: \n for an -sentence conversation where the conversation history must be retransmitted and the person takes linear time to read the query: \n ESP game: \n Image labelling (based on the Arthur–Merlin protocol): \n Image classification: human only: , and with less reliance on the human: .\n\nSee also\n ASR-complete\n List of unsolved problems in computer science\n Synthetic intelligence\n Practopoiesis\n\nReferences",
        "See also\n ASR-complete\n List of unsolved problems in computer science\n Synthetic intelligence\n Practopoiesis\n\nReferences\n\nArtificial intelligence\nComputational problems"
    ],
    [
        "Ai\nAI is artificial intelligence, intellectual ability in machines and robots.\n\nAi, AI or A.I. may also refer to:\n\nAnimals\n Ai (chimpanzee), an individual experimental subject in Japan\n Ai (sloth) or the pale-throated sloth, northern Amazonian mammal species\n\nArts, entertainment and media\n\nWorks\n Ai (album), a 2004 release by Seraphim\n A.I. Artificial Intelligence, a 2001 American film\n A.I. Rising, a 2018 Serbian film\n AI: The Somnium Files, a 2019 video game\n American Idol, televised singing contest\n The American Interest, a bimonthly magazine (2005–2020)\n I (2015 film), an Indian Tamil film (initial title: Ai)\n\nOther uses in arts and media",
        "Other uses in arts and media\n\n A.i. (band), a Californian rock–electroclash group\n All in (poker), wagering one's entire stake\n Appreciation Index, a British measure of broadcast programme approval\n The Art Institutes, a chain of American art schools\n Non-player character, in gaming (colloquially, an AI)\n\nBusiness\n\n , a phrase in job titles\n Appreciative inquiry, an organizational development method\n All-inclusive, a full service at a vacation resort including meals and drinks",
        "Business\n\n , a phrase in job titles\n Appreciative inquiry, an organizational development method\n All-inclusive, a full service at a vacation resort including meals and drinks\n\nOrganizations and businesses\n Accuracy International, a firearms manufacturer\n Adventure International, a video game publisher\n Air India, the flag carrier airline of India, based in Delhi\n Alitalia, the former flag carrier airline of Italy\n Astra International, an Indonesian automotive company\n Alexis I. duPont High School, Delaware, U.S.\n Amnesty International, a human rights organisation\n Appraisal Institute, an association of real estate appraisers\n The Art Institutes, a chain of art schools",
        "People\n Ai (surname), a Chinese surname\n Ai (given name), a given name and list of people and characters with the name\n King Ai of Zhou (died 441 BC)\n Emperor Ai of Han (27–1 BC)\n Emperor Ai of Jin (341–365)\n Emperor Ai of Tang (892–908)\n Ai (poet) (1947–2010), American poet\n Ai (singer) (born 1981), Japanese-American singer and songwriter\n Allen Iverson (born 1975), American retired professional basketball player (\"A.I.\")\n Andre Iguodala (born 1984), American professional basketball player (\"A.I. 2.0\")\n\nPlaces\n\nAreas\n Anguilla, a Caribbean territory (by ISO 3166-1 code)\n Appenzell Innerrhoden, a Swiss canton\n\nCities\n Ai (Canaan), Biblical city\n\nUnited States \n Ai, Alabama\n Ai, Georgia\n Ai, North Carolina\n Ai, Ohio\n\nLandforms",
        "Cities\n Ai (Canaan), Biblical city\n\nUnited States \n Ai, Alabama\n Ai, Georgia\n Ai, North Carolina\n Ai, Ohio\n\nLandforms\n\nReligion, philosophy and mythology\n Ái, a Norse god\n Ai (Canaan), Biblical city\n Ai (), Sinic concepts of love from Confucianism and Buddhism\n , colloquially , a Greek word for 'saint'\n Ai Toyon, the Yakut god of light\n\nScience and technology\n\nAgricultural science\n Active ingredient, part of a pesticide\n Artificial insemination of livestock and pets, in animal breeding\n\nAir force and aviation\n Airborne Internet, a proposed air-to-air data network\n Airborne Interception radar, a Royal Air Force air-to-air system\n Air interdiction, an aerial military capability\n Attitude indicator, a flight instrument on aircraft\n\nThe Internet\n .ai, a top-level domain",
        "The Internet\n .ai, a top-level domain\n\nMedical conditions\n Accommodative insufficiency of the eye\n Amelogenesis imperfecta of teeth\n Aortic insufficiency of the heart\n\nMedicines and healthcare\n Active ingredient, part of a drug\n Adequate intake, a Dietary Reference Intake nutritional parameter\n Aromatase inhibitor, a class of breast-cancer drug\n Articulation Index, a method of measuring hearing loss\n Artificial insemination, a method of fertilisation\n Automated immunoassay, automated clinical analyses of blood\n\nVisual arts technologies\n Adobe Illustrator, a vector graphics editor\n .ai, Adobe Illustrator Artwork filename extension\n Automatic Maximum-Aperture Indexing, aperture coupling on Nikon camera lenses",
        "See also\n Artificial intelligence (disambiguation)\n Art Institute (disambiguation)\n All In (disambiguation)\n A1 (disambiguation)\n AL (disambiguation)\n AY (disambiguation)\n Aye (disambiguation)\n Eye (disambiguation)"
    ],
    [
        "AIM (software)\nAIM (AOL Instant Messenger) was an instant messaging and presence computer program created by AOL, which used the proprietary OSCAR instant messaging protocol and the TOC protocol to allow registered users to communicate in real time.",
        "AIM was popular by the late 1990s, in United States and other countries, and was the leading instant messaging application in that region into the following decade. Teens and college students were known to use the messenger's away message feature to keep in touch with friends, often frequently changing their away message throughout a day or leaving a message up with one's computer left on to inform buddies of their ongoings, location, parties, thoughts, or jokes. AIM's popularity declined as AOL subscribers started decreasing and steeply towards the 2010s, as Gmail's Google Talk, SMS, and Internet social networks, like Facebook gained popularity. Its fall has often been compared with other once-popular Internet services, such as Myspace.",
        "In June 2015, AOL was acquired by Verizon Communications. In June 2017, Verizon combined AOL and Yahoo into its subsidiary Oath Inc. (now called Yahoo). The company discontinued AIM as a service on December 15, 2017.\n\nHistory\nIn May 1997, AIM was released unceremoniously as a stand-alone download for Microsoft Windows. AIM was an outgrowth of \"online messages\" in the original platform written in PL/1 on a Stratus computer by Dave Brown. At one time, the software had the largest share of the instant messaging market in North America, especially in the United States (with 52% of the total reported ). This does not include other instant messaging software related to or developed by AOL, such as ICQ and iChat.",
        "During its heyday, its main competitors were ICQ (which AOL acquired in 1998), Yahoo! Messenger and MSN Messenger. AOL particularly had a rivalry or \"chat war\" with PowWow and Microsoft, starting in 1999. There were several attempts from Microsoft to simultaneously log into their own and AIM's protocol servers. AOL was unhappy about this and started blocking MSN Messenger from being able to access AIM. This led to efforts by many companies to challenge the AOL and Time Warner merger on the grounds of antitrust behaviour, leading to the formation of the OpenNet Coalition.",
        "Official mobile versions of AIM appeared as early as 2001 on Palm OS through the AOL application. Third-party applications allowed it to be used in 2002 for the Sidekick. A version for Symbian OS was announced in 2003 as were others for BlackBerry and Windows Mobile\n\nAfter 2012, stand-alone official AIM client software included advertisements and was available for Microsoft Windows, Windows Mobile, Classic Mac OS, macOS, Android, iOS, and BlackBerry OS.",
        "Usage decline and product sunset\nAround 2011, AIM started to lose popularity rapidly, partly due to the quick rise of Gmail and its built-in real-time Google Chat instant messenger integration in 2011 and because many people migrated to SMS or iMessages text messaging and later, social networking websites and apps for instant messaging, in particular, Facebook Messenger, which was released as a standalone application the same year. AOL made a partnership to integrate AIM messaging in Google Talk, and had a feature for AIM users to send SMS messages directly from AIM to any number, as well as for SMS users to send an IM to any AIM user.",
        "As of June 2011, one source reported AOL Instant Messenger market share had collapsed to 0.73%. However, this number only reflected installed IM applications, and not active users. The engineers responsible for AIM claimed that they were unable to convince AOL management that free was the future.\n\nOn March 3, 2012, AOL ended employment of AIM's development staff while leaving it active and with help support still provided. On October 6, 2017, it was announced that the AIM service would be discontinued on December 15; however, a non-profit development team known as Wildman Productions started up a server for older versions of AOL Instant Messenger, known as AIM Phoenix.\n\nThe \"AIM Man\"",
        "The \"AIM Man\"\n\nThe AIM mascot was designed by JoRoan Lazaro and was implemented in the first release in 1997. This was a yellow stickman-like figure, often called the \"Running Man\". The mascot appeared on all AIM logos and most wordmarks, and always appeared at the top of the buddy list. AIM's popularity in the late 1990s and the 2000s led to the \"Running Man\" becoming a familiar brand on the Internet. After over 14 years, the iconic logo disappeared as part of the AIM rebranding in 2011. However, in August 2013, the \"Running Man\" returned. It was used for other AOL services like AOL Top Speed.",
        "In 2014, a Complex editor called it a \"symbol of America\". In April 2015, the Running Man was officially featured in the Virgin London Marathon, dressed by a person for the AOL-partnered Free The Children charity.\n\nProtocol\nThe standard protocol that AIM clients used to communicate is called Open System for CommunicAtion in Realtime (OSCAR). Most AOL-produced versions of AIM and popular third party AIM clients use this protocol. However, AOL also created a simpler protocol called TOC that lacks many of OSCAR's features, but was sometimes used for clients that only require basic chat functionality. The TOC/TOC2 protocol specifications were made available by AOL, while OSCAR is a closed protocol that third parties had to reverse-engineer.",
        "In January 2008, AOL introduced experimental Extensible Messaging and Presence Protocol (XMPP) support for AIM, allowing AIM users to communicate using the standardized, open-source XMPP. However, in March 2008, this service was discontinued. In May 2011, AOL started offering limited XMPP support. On March 1, 2017, AOL announced (via XMPP-login-time messages) that the AOL XMPP gateway would be desupported, effective March 28, 2017.\n\nPrivacy\nFor privacy regulations, AIM had strict age restrictions. AIM accounts are available only for people over the age of 13; children younger than that were not permitted access to AIM.\n\nUnder the AIM Privacy Policy, AOL had no rights to read or monitor any private communications between users. The profile of the user had no privacy.",
        "Under the AIM Privacy Policy, AOL had no rights to read or monitor any private communications between users. The profile of the user had no privacy.\n\nIn November 2002, AOL targeted the corporate industry with Enterprise AIM Services (EAS), a higher security version of AIM.\n\nIf public content was accessed, it could be used for online, print or broadcast advertising, etc. This was outlined in the policy and terms of service: \"... you grant AOL, its parent, affiliates, subsidiaries, assigns, agents and licensees the irrevocable, perpetual, worldwide right to reproduce, display, perform, distribute, adapt and promote this Content in any medium\". This allowed anything users posted to be used without a separate request for permission.",
        "AIM's security was called into question. AOL stated that it had taken great pains to ensure that personal information will not be accessed by unauthorized members, but that it cannot guarantee that it will not happen.",
        "AIM was different from other clients, such as Yahoo! Messenger, in that it did not require approval from users to be added to other users' buddy lists. As a result, it was possible for users to keep other unsuspecting users on their buddy list to see when they were online, read their status and away messages, and read their profiles. There was also a Web API to display one's status and away message as a widget on one's webpage. Though one could block a user from communicating with them and seeing their status, this did not prevent that user from creating a new account that would not automatically be blocked and therefore able to track their status. A more conservative privacy option was to select a menu feature that only allowed communication with users on one's buddy list; however, this",
        "be blocked and therefore able to track their status. A more conservative privacy option was to select a menu feature that only allowed communication with users on one's buddy list; however, this option also created the side-effect of blocking all users who were not on one's buddy list. Users could also choose to be invisible to all.",
        "Chat robots\nAOL and various other companies supplied robots (bots) on AIM that could receive messages and send a response based on the bot's purpose. For example, bots could help with studying, like StudyBuddy. Some were made to relate to children and teenagers, like Spleak.\n\nOthers gave advice. The more useful chat bots had features like the ability to play games, get sport scores, weather forecasts or financial stock information. Users were able to talk to automated chat bots that could respond to natural human language. They were primarily put into place as a marketing strategy and for unique advertising options. It was used by advertisers to market products or build better consumer relations.",
        "Before the inclusions of such bots, the other bots DoorManBot and AIMOffline provided features that were provided by AOL for those who needed it. ZolaOnAOL and ZoeOnAOL were short-lived bots that ultimately retired their features in favor of SmarterChild.",
        "URI scheme\nAOL Instant Messenger's installation process automatically installed an extra URI scheme (\"protocol\") handler into some Web browsers, so URIs beginning \"aim:\" could open a new AIM window with specified parameters. This was similar in function to the mailto: URI scheme, which created a new e-mail message using the system's default mail program. For instance, a webpage might have included a link like the following in its HTML source to open a window for sending a message to the AIM user notarealuser:\n <a href=\"aim:goim?screenname=notarealuser\">Send Message</a>\n\nTo specify a message body, the message parameter was used, so the link location would have looked like this:\n aim:goim?screenname=notarealuser&message=This+is+my+message",
        "To specify a message body, the message parameter was used, so the link location would have looked like this:\n aim:goim?screenname=notarealuser&message=This+is+my+message\n\nTo specify an away message, the message parameter was used, so the link location would have looked like this:\n aim:goaway?message=Hello,+my+name+is+Bill\nWhen placing this inside a URL link, an AIM user could click on the URL link and the away message \"Hello, my name is Bill\" would instantly become their away message.\n\nTo add a buddy, the addbuddy message was used, with the \"screenname\" parameter\n aim:addbuddy?screenname=notarealuser\nThis type of link was commonly found on forum profiles to easily add contacts.",
        "Vulnerabilities\nAIM had security weaknesses that have enabled exploits to be created that used third-party software to perform malicious acts on users' computers. Although most were relatively harmless, such as being kicked off the AIM service, others performed potentially dangerous actions, such as sending viruses. Some of these exploits relied on social engineering to spread by automatically sending instant messages that contained a Uniform Resource Locator (URL) accompanied by text suggesting the receiving user click on it, an action which leads to infection, i.e., a trojan horse. These messages could easily be mistaken as coming from a friend and contain a link to a Web address that installed software on the user's computer to restart the cycle.",
        "Users also have reported sudden additions of toolbars and advertisements from third parties in the newer version of AIM. Multiple complaints about the lack of control of third party involvement have caused many users to stop using the service.\n\nExtra features",
        "Extra features\n\niPhone application\nOn March 6, 2008, during Apple Inc.'s iPhone SDK event, AOL announced that they would be releasing an AIM application for iPhone and iPod Touch users. The application was available for free from the App Store, but the company also provides a paid version, which displays no advertisements. Both were available from the App Store. The AIM client for iPhone and iPod Touch supported standard AIM accounts, as well as MobileMe accounts. There was also an express version of AIM accessible through the Safari browser on the iPhone and iPod Touch.",
        "In 2011, AOL launched an overhaul of their Instant Messaging service. Included in the update was a brand new iOS application for iPhone and iPod Touch that incorporated all the latest features. A brand new icon was used for the application, featuring the new cursive logo for AIM. The user-interface was entirely redone for the features including: a new buddy list, group messaging, in-line photos and videos, as well as improved file-sharing.\n\nVersion 5.0.5, updated in March 2012, it supported more social stream features, much like Facebook and Twitter, as well as the ability to send voice messages up to 60 seconds long.",
        "Version 5.0.5, updated in March 2012, it supported more social stream features, much like Facebook and Twitter, as well as the ability to send voice messages up to 60 seconds long.\n\niPad application\nOn April 3, 2010, Apple released the first generation iPad. Along with this newly released device AOL released the AIM application for iPad. It was built entirely from scratch for the new version iOS with a specialized user-interface for the device. It supports geo location, Facebook status updates and chat, Myspace, Twitter, YouTube, Foursquare and many social networking platforms.",
        "AIM Express\nAIM Express ran in a pop-up browser window. It was intended for use by people who are unwilling or unable to install a standalone application or those at computers that lack the AIM application. AIM Express supported many of the standard features included in the stand-alone client, but did not provide advanced features like file transfer, audio chat, video conferencing, or buddy info. It was implemented in Adobe Flash. It was an upgrade to the prior AOL Quick Buddy, which was later available for older systems that cannot handle Express before being discontinued. Express and Quick Buddy were similar to MSN Web Messenger and Yahoo! Web Messenger. This web version evolved into AIM.com's web-based messenger.",
        "AIM Pages\nAIM Pages was a free website released in May 2006 by AOL in replacement of AIMSpace. Anyone who had an AIM user name and was at least 16 years of age could create their own web page (to display an online, dynamic profile) and share it with buddies from their AIM Buddy list.\n\nLayout \nAIM Pages included links to the email and Instant Message of the owner, along with a section listing the owners \"buddies\", which included AIM user names. It was possible to create modules in a Module T microformat. Video hosting sites like Netflix and YouTube could be added to ones AIM Page, as well as other sites like Amazon.com. It was also possible to insert HTML code.",
        "The main focus of AIM Pages was the integration of external modules, like those listed above, into the AOL Instant Messenger experience.\n\nDiscontinuation \nBy late 2007, AIM Pages had been discontinued. After AIM Pages shutdown, links to AIM Pages were redirected to AOL Lifestream, AOL's new site aimed at collecting external modules in one place, independent of AIM buddies. AOL Lifestream was shut down February 24, 2017.\n\nAIM for Mac\nAOL released an all-new AIM for the Mac on September 29, 2008, and the final build on December 15, 2008. The redesigned AIM for Mac is a full universal binary Cocoa API application that supports both Tiger and Leopard — Mac OS X 10.4.8 (and above) or Mac OS X 10.5.3 (and above). On October 1, 2009, AOL released AIM 2.0 for Mac.",
        "AIM real-time IM\nThis feature is available for AIM 7 and allows for a user to see what the other is typing as it is being done. It was developed and built with assistance from Trace Research and Development Centre at University of Wisconsin–Madison and Gallaudet University. The application provides visually impaired users the ability to convert messages from text (words) to speech. For the application to work users must have AIM 6.8 or higher, as it is not compatible with older versions of AIM software, AIM for Mac or iChat.\n\nAIM to mobile (messaging to phone numbers)\nThis feature allows text messaging to a phone number (text messaging is less functional than instant messaging).\n\nDiscontinued features",
        "AIM to mobile (messaging to phone numbers)\nThis feature allows text messaging to a phone number (text messaging is less functional than instant messaging).\n\nDiscontinued features\n\nAIM Phoneline\nAIM Phoneline was a Voice over IP PC-PC, PC-Phone and Phone-to-PC service provided via the AIM application. It was also known to work with Apple's iChat Client. The service was officially closed to its customers on January 13, 2009. The closing of the free service caused the number associated with the service to be disabled and not transferable for a different service. AIM Phoneline website was recommending users switch to a new service named AIM Call Out, also discontinued now.",
        "Launched on May 16, 2006, AIM Phoneline provided users the ability to have several local numbers, allowing AIM users to receive free incoming calls. The service allowed users to make calls to landlines and mobile devices through the use of a computer. The service, however, was only free for receiving and AOL charged users $14.95 a month for an unlimited calling plan. In order to use AIM Phoneline users had to install the latest free version of AIM Triton software and needed a good set of headphones with a boom microphone. It could take several days after a user signed up before it started working.",
        "AIM Call Out\nAIM Call Out is a discontinued Voice over IP PC-PC, PC-Phone and Phone-to-PC service provided by AOL via its AIM application that replaced the defunct AIM Phoneline service in November 2007. It did not depend on the AIM client and could be used with only an AIM screenname via the WebConnect feature or a dedicated SIP device. The AIM Call Out service was shut down on March 25, 2009.",
        "Security",
        "On November 4, 2014, AIM scored one out of seven points on the Electronic Frontier Foundation's secure messaging scorecard. AIM received a point for encryption during transit, but lost points because communications are not encrypted with a key to which the provider has no access, i.e., the communications are not end-to-end encrypted, users can't verify contacts' identities, past messages are not secure if the encryption keys are stolen, (i.e., the service does not provide forward secrecy), the code is not open to independent review, i.e., the code is not open-source), the security design is not properly documented, and there has not been a recent independent security audit. BlackBerry Messenger, Ebuddy XMS, Hushmail, Kik Messenger, Skype, Viber, and Yahoo! Messenger also scored one out of",
        "properly documented, and there has not been a recent independent security audit. BlackBerry Messenger, Ebuddy XMS, Hushmail, Kik Messenger, Skype, Viber, and Yahoo! Messenger also scored one out of seven points.",
        "See also\n Comparison of cross-platform instant messaging clients\n List of defunct instant messaging platforms\n\nReferences\n\nExternal links\n\n \n\n1997 software\n\nAndroid (operating system) software\nInstant Messenger\nBlackBerry software\nClassic Mac OS instant messaging clients\nComputer-related introductions in 1997\nCross-platform software\nDefunct instant messaging clients\nInstant messaging clients\nInternet properties disestablished in 2017\nIOS software\nMacOS instant messaging clients\nOnline chat\nSymbian software\nUnix instant messaging clients\nVideotelephony\nWindows instant messaging clients"
    ],
    [
        "Alan Kay\nAlan Curtis Kay (born May 17, 1940) is an American computer scientist best known for his pioneering work on object-oriented programming and windowing graphical user interface (GUI) design. At Xerox PARC he led the design and development of the first modern windowed computer desktop interface.  There he also led the development of the influential object-oriented programming language Smalltalk, both personally designing most of the early versions of the language and coining the term \"object-oriented.\"\nHe has been elected a Fellow of the American Academy of Arts and Sciences, the National Academy of Engineering, and the Royal Society of Arts.  He received the Turing award in 2003.",
        "Kay is also a former professional jazz guitarist, composer, and theatrical designer. He also is an amateur classical pipe organist.\n\nEarly life and work \n\nIn an interview on education in America with the Davis Group Ltd., Kay said:\n\nOriginally from Springfield, Massachusetts, Kay's family relocated several times due to his father's career in physiology before ultimately settling in the New York metropolitan area.\n\nHe attended Brooklyn Technical High School. Having accumulated enough credits to graduate, he then attended Bethany College in Bethany, West Virginia, where he majored in biology and minored in mathematics.",
        "Kay then taught guitar in Denver, Colorado for a year. He was drafted in the United States Army, then qualified for officer training in the United States Air Force, where he became a computer programmer after passing an aptitude test.\n\nAfter his discharge, he enrolled at the University of Colorado Boulder and earned a Bachelor of Science (B.S.) in mathematics and molecular biology in 1966.",
        "In the autumn of 1966, he began graduate school at the University of Utah College of Engineering. He earned a Master of Science in electrical engineering in 1968, then a Doctor of Philosophy in computer science in 1969. His doctoral dissertation, FLEX: A Flexible Extendable Language, described the invention of a computer language named FLEX. While there, he worked with \"fathers of computer graphics\" David C. Evans (who had recently been recruited from the University of California, Berkeley to start Utah's computer science department) and Ivan Sutherland (best known for writing such pioneering programs as Sketchpad). Kay credits Sutherland's 1963 thesis for influencing his views on objects and computer programming. As he grew busier with research for the Defense Advanced Research Projects",
        "programs as Sketchpad). Kay credits Sutherland's 1963 thesis for influencing his views on objects and computer programming. As he grew busier with research for the Defense Advanced Research Projects Agency (DARPA), he ended his musical career.",
        "In 1968, he met Seymour Papert and learned of the programming language Logo, a dialect of Lisp optimized for educational purposes. This led him to learn of the work of Jean Piaget, Jerome Bruner, Lev Vygotsky, and of constructionist learning, further influencing his professional orientation.\n\nIn 1969, Kay became a visiting researcher at the Stanford Artificial Intelligence Laboratory in anticipation of accepting a professorship at Carnegie Mellon University. Instead, in 1970, he joined the Xerox PARC research staff in Palo Alto, California. Through the decade, he developed prototypes of networked workstations using the programming language Smalltalk.",
        "Along with some colleagues at PARC, Kay is one of the fathers of the idea of object-oriented programming (OOP), which he named. Some original object-oriented concepts, including the use of the words 'object' and 'class', had been developed for Simula 67 at the Norwegian Computing Center. Kay said:\n\nI'm sorry that I long ago coined the term \"objects\" for this topic because it gets many people to focus on the lesser idea. The big idea is \"messaging\".",
        "I'm sorry that I long ago coined the term \"objects\" for this topic because it gets many people to focus on the lesser idea. The big idea is \"messaging\".\n\nWhile at PARC, Kay conceived the Dynabook concept, a key progenitor of laptop and tablet computers and the e-book. He is also the architect of the modern overlapping windowing graphical user interface (GUI). Because the Dynabook was conceived as an educational platform, he is considered one of the first researchers into mobile learning; many features of the Dynabook concept have been adopted in the design of the One Laptop Per Child educational platform, with which Kay is actively involved.",
        "Subsequent work \nFrom 1981 to 1984, Kay was Chief Scientist at Atari. In 1984, he became an Apple Fellow. After the closure of the Apple Advanced Technology Group in 1997, he was recruited by his friend Bran Ferren, head of research and development at Disney, to join Walt Disney Imagineering as a Disney Fellow. He remained there until Ferren left to start Applied Minds Inc with Imagineer Danny Hillis, leading to the cessation of the Fellows program.",
        "In 2001, Kay founded Viewpoints Research Institute, a nonprofit organization dedicated to children, learning, and advanced software development. For their first ten years, Kay and his Viewpoints group were based at Applied Minds in Glendale, California, where he and Ferren worked on various projects. Kay served as president of the Institute until its closure in 2018.\n\nIn 2002 Kay joined HP Labs as a senior fellow, departing when HP disbanded the Advanced Software Research Team on July 20, 2005.\nHe has been an adjunct professor of computer science at the University of California, Los Angeles, a visiting professor at Kyoto University, and an adjunct professor at the Massachusetts Institute of Technology (MIT). Kay served on the advisory board of TTI/Vanguard.",
        "Squeak, Etoys, and Croquet \nIn December 1995, while still at Apple, Kay collaborated with many others to start the open source Squeak version of Smalltalk. As part of this effort, in November 1996, his team began research on what became the Etoys system. More recently he started, with David A. Smith, David P. Reed, Andreas Raab, Rick McGeer, Julian Lombardi, and Mark McCahill, the Croquet Project, an open-source networked 2D and 3D environment for collaborative work.",
        "Tweak \nIn 2001, it became clear that the Etoy architecture in Squeak had reached its limits in what the Morphic interface infrastructure could do. Andreas Raab, a researcher in Kay's group then at Hewlett-Packard, proposed defining a \"script process\" and providing a default scheduling mechanism that avoided several more general problems. The result was a new user interface, proposed to replace the Squeak Morphic user interface. Tweak added mechanisms of islands, asynchronous messaging, players and costumes, language extensions, projects, and tile scripting. Its underlying object system is class-based, but to users (during programming) it acts as if it were prototype-based. Tweak objects are created and run in Tweak project windows.",
        "The Children's Machine \nIn November 2005, at the World Summit on the Information Society, the MIT research laboratories unveiled a new laptop computer for educational use around the world. It has many names, including the $100 Laptop, the One Laptop per Child program, the Children's Machine, and the XO-1. The program was founded and is sustained by Kay's friend Nicholas Negroponte, and is based on Kay's Dynabook ideal. Kay is a prominent co-developer of the computer, focusing on its educational software using Squeak and Etoys.",
        "Reinventing programming \nKay has lectured extensively on the idea that the computer revolution is very new, and all of the good ideas have not been universally implemented. His lectures at the OOPSLA 1997 conference, and his ACM Turing Award talk, \"The Computer Revolution Hasn't Happened Yet\", were informed by his experiences with Sketchpad, Simula, Smalltalk, and the bloated code of commercial software.",
        "On August 31, 2006, Kay's proposal to the United States National Science Foundation (NSF) was granted, funding Viewpoints Research Institute for several years. The proposal title was \"STEPS Toward the Reinvention of Programming: A compact and Practical Model of Personal Computing as a Self-exploratorium\". A sense of what Kay is trying to do comes from this quote, from the abstract of a seminar at Intel Research Labs, Berkeley: \"The conglomeration of commercial and most open source software consumes in the neighborhood of several hundreds of millions of lines of code these days. We wonder: how small could be an understandable practical 'Model T' design that covers this functionality? 1M lines of code? 200K LOC? 100K LOC? 20K LOC?\"\n\nAwards and honors",
        "Kay has received many awards and honors, including:\n UdK 01-Award in Berlin, Germany for pioneering the GUI; J-D Warnier Prix D'Informatique; NEC C&C Prize (2001)\n Telluride Tech Festival Award of Technology in Telluride, Colorado (2002)\n ACM Turing Award \"For pioneering many of the ideas at the root of contemporary object-oriented programming languages, leading the team that developed Smalltalk, and for fundamental contributions to personal computing\" (2003)\n Kyoto Prize; Charles Stark Draper Prize with Butler W. Lampson, Robert W. Taylor and Charles P. Thacker (2004)\n UPE Abacus Award, for individuals who have provided extensive support and leadership for student-related activities in the computing and information disciplines (2012)\n Honorary doctorates:",
        "UPE Abacus Award, for individuals who have provided extensive support and leadership for student-related activities in the computing and information disciplines (2012)\n Honorary doctorates:\n– Kungliga Tekniska Högskolan (Royal Institute of Technology) in Stockholm (2002)\n– Georgia Institute of Technology (2005)\n– Columbia College Chicago awarded Doctor of Humane Letters, Honoris Causa (2005)\n– Laurea Honoris Causa in Informatica, Università di Pisa, Italy (2007)\n– University of Waterloo (2008)\n– Kyoto University (2009)\n– Universidad de Murcia (2010)\n– University of Edinburgh (2017)\n  Honorary Professor, Berlin University of the Arts\n Elected fellow of:\n– American Academy of Arts and Sciences",
        "– Universidad de Murcia (2010)\n– University of Edinburgh (2017)\n  Honorary Professor, Berlin University of the Arts\n Elected fellow of:\n– American Academy of Arts and Sciences\n– National Academy of Engineering for inventing the concept of portable personal computing. (1997)\n– Royal Society of Arts\n– Computer History Museum \"for his fundamental contributions to personal computing and human-computer interface development.\" (1999)\n– Association for Computing Machinery \"For fundamental contributions to personal computing and object-oriented programming.\" (2008)\n– Hasso Plattner Institute (2011)",
        "His other honors include the J-D Warnier Prix d'Informatique, the ACM Systems Software Award, the NEC Computers & Communication Foundation Prize, the Funai Foundation Prize, the Lewis Branscomb Technology Award, and the ACM SIGCSE Award for Outstanding Contributions to Computer Science Education.\n\nSee also \n List of pioneers in computer science\n\nReferences\n\nExternal links \n Viewpoints Research Institute\n \n \"There is no information content in Alan Kay\" 2012\n  Programming a problem-oriented language, an unpublished book, by Charles H. Moore, June 1970",
        "1940 births\nAmerican computer programmers\nAmerican computer scientists\nApple Inc. employees\nApple Fellows\nAtari people\nComputer science educators\nDraper Prize winners\nFellows of the American Association for the Advancement of Science\nFellows of the Association for Computing Machinery\nHewlett-Packard people\nHuman–computer interaction researchers\nLiving people\nMassachusetts Institute of Technology faculty\nOpen source advocates\nPeople from Springfield, Massachusetts\nProgramming language designers\nScientists at PARC (company)\nTuring Award laureates\nUniversity of California, Los Angeles faculty\nUniversity of Colorado Boulder alumni\nUniversity of Utah alumni\nKyoto laureates in Advanced Technology\nAcademic staff of the Berlin University of the Arts"
    ],
    [
        "Alan Turing\nAlan Mathison Turing  (; 23 June 1912 – 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. He is widely considered to be the father of theoretical computer science and artificial intelligence.",
        "Born in Maida Vale, London, Turing was raised in southern England. He graduated at King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at Cambridge, he published a proof demonstrating that some purely mathematical yes–no questions can never be answered by computation. He defined a Turing machine and proved that the halting problem for Turing machines is undecidable. In 1938, he obtained his PhD from the Department of Mathematics at Princeton University. During the Second World War, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding",
        "codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Axis powers in many crucial engagements, including the Battle of the Atlantic.",
        "After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis and predicted oscillating chemical reactions such as the Belousov–Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, Turing was never fully recognised in Britain during his lifetime because much of his work was covered by the Official Secrets Act.",
        "Turing was prosecuted in 1952 for homosexual acts. He accepted hormone treatment with DES, a procedure commonly referred to as chemical castration, as an alternative to prison. Turing died on 7 June 1954, 16 days before his 42nd birthday, from cyanide poisoning. An inquest determined his death as a suicide, but it has been noted that the known evidence is also consistent with accidental poisoning.",
        "Following a public campaign in 2009, British prime minister Gordon Brown made an official public apology on behalf of the government for \"the appalling way [Turing] was treated\". Queen Elizabeth II granted a posthumous pardon in 2013. The term \"Alan Turing law\" is now used informally to refer to a 2017 law in the United Kingdom that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.",
        "Turing has an extensive legacy with statues of him and many things named after him, including an annual award for computer science innovations. He appears on the current Bank of England £50 note, which was released on 23 June 2021 to coincide with his birthday. A 2019 BBC series, as voted by the audience, named him the greatest person of the 20th century.\n\nEarly life and education\n\nFamily",
        "Turing was born in Maida Vale, London, while his father, Julius Mathison Turing was on leave from his position with the Indian Civil Service (ICS) of the British Raj government at Chatrapur, then in the Madras Presidency and presently in Odisha state, in India. Turing's father was the son of a clergyman, the Rev. John Robert Turing, from a Scottish family of merchants that had been based in the Netherlands and included a baronet. Turing's mother, Julius's wife, was Ethel Sara Turing (), daughter of Edward Waller Stoney, chief engineer of the Madras Railways. The Stoneys were a Protestant Anglo-Irish gentry family from both County Tipperary and County Longford, while Ethel herself had spent much of her childhood in County Clare. Julius and Ethel married on 1 October 1907 at Bartholomew's",
        "gentry family from both County Tipperary and County Longford, while Ethel herself had spent much of her childhood in County Clare. Julius and Ethel married on 1 October 1907 at Bartholomew's church on Clyde Road, in Dublin.",
        "Julius's work with the ICS brought the family to British India, where his grandfather had been a general in the Bengal Army. However, both Julius and Ethel wanted their children to be brought up in Britain, so they moved to Maida Vale, London, where Alan Turing was born on 23 June 1912, as recorded by a blue plaque on the outside of the house of his birth, later the Colonnade Hotel. Turing had an elder brother, John Ferrier Turing, father of Sir John Dermot Turing, 12th Baronet of the Turing baronets.",
        "Turing's father's civil service commission was still active during Turing's childhood years, and his parents travelled between Hastings in the United Kingdom and India, leaving their two sons to stay with a retired Army couple. At Hastings, Turing stayed at Baston Lodge, Upper Maze Hill, St Leonards-on-Sea, now marked with a blue plaque. The plaque was unveiled on 23 June 2012, the centenary of Turing's birth.\n\nVery early in life, Turing showed signs of the genius that he was later to display prominently. His parents purchased a house in Guildford in 1927, and Turing lived there during school holidays. The location is also marked with a blue plaque.",
        "School\nTuring's parents enrolled him at St Michael's, a primary school at 20 Charles Road, St Leonards-on-Sea, from the age of six to nine. The headmistress recognised his talent, noting that she has \"...had clever boys and hardworking boys, but Alan is a genius\".",
        "Between January 1922 and 1926, Turing was educated at Hazelhurst Preparatory School, an independent school in the village of Frant in Sussex (now East Sussex). In 1926, at the age of 13, he went on to Sherborne School, an independent boarding school in the market town of Sherborne in Dorset, where he boarded at Westcott House. The first day of term coincided with the 1926 General Strike, in Britain, but Turing was so determined to attend that he rode his bicycle unaccompanied  from Southampton to Sherborne, stopping overnight at an inn.",
        "Turing's natural inclination towards mathematics and science did not earn him respect from some of the teachers at Sherborne, whose definition of education placed more emphasis on the classics. His headmaster wrote to his parents: \"I hope he will not fall between two stools. If he is to stay at public school, he must aim at becoming educated. If he is to be solely a Scientific Specialist, he is wasting his time at a public school\". Despite this, Turing continued to show remarkable ability in the studies he loved, solving advanced problems in 1927 without having studied even elementary calculus. In 1928, aged 16, Turing encountered Albert Einstein's work; not only did he grasp it, but it is possible that he managed to deduce Einstein's questioning of Newton's laws of motion from a text in",
        "In 1928, aged 16, Turing encountered Albert Einstein's work; not only did he grasp it, but it is possible that he managed to deduce Einstein's questioning of Newton's laws of motion from a text in which this was never made explicit.",
        "Christopher Morcom\nAt Sherborne, Turing formed a significant friendship with fellow pupil Christopher Collan Morcom (13 July 1911 – 13 February 1930), who has been described as Turing's first love. Their relationship provided inspiration in Turing's future endeavours, but it was cut short by Morcom's death, in February 1930, from complications of bovine tuberculosis, contracted after drinking infected cow's milk some years previously.\n\nThe event caused Turing great sorrow. He coped with his grief by working that much harder on the topics of science and mathematics that he had shared with Morcom. In a letter to Morcom's mother, Frances Isobel Morcom (née Swan), Turing wrote:",
        "Turing's relationship with Morcom's mother continued long after Morcom's death, with her sending gifts to Turing, and him sending letters, typically on Morcom's birthday. A day before the third anniversary of Morcom's death (13 February 1933), he wrote to Mrs. Morcom: \n\nSome have speculated that Morcom's death was the cause of Turing's atheism and materialism. Apparently, at this point in his life he still believed in such concepts as a spirit, independent of the body and surviving death. In a later letter, also written to Morcom's mother, Turing wrote:",
        "University and work on computability",
        "After graduating from Sherborne, Turing studied the undergraduate course in Schedule B (that is, a three-year Parts I and II, of the Mathematical Tripos, with extra courses at the end of the third year, as Part III only emerged as a separate degree in 1934) from February 1931 to November 1934 at King's College, Cambridge, where he was awarded first-class honours in mathematics. His dissertation, On the Gaussian error function, written during his senior year and delivered in November 1934 (with a deadline date of 6 December) proved a version of the central limit theorem. It was finally accepted on 16 March 1935. By spring of that same year, Turing started his master's course (Part III)—which he completed in 1937—and, at the same time, he published his first paper, a one-page article called",
        "on 16 March 1935. By spring of that same year, Turing started his master's course (Part III)—which he completed in 1937—and, at the same time, he published his first paper, a one-page article called Equivalence of left and right almost periodicity (sent on 23 April), featured in the tenth volume of the Journal of the London Mathematical Society. Later that year, Turing was elected a Fellow of King's College on the strength of his dissertation. However, and, unknown to Turing, this version of the theorem he proved in his paper, had already been proven, in 1922, by Jarl Waldemar Lindeberg. Despite this, the committee found Turing's methods original and so regarded the work worthy of consideration for the fellowship. Abram Besicovitch's report for the committee went so far as to say that if",
        "this, the committee found Turing's methods original and so regarded the work worthy of consideration for the fellowship. Abram Besicovitch's report for the committee went so far as to say that if Turing's work had been published before Lindeberg's, it would have been \"an important event in the mathematical literature of that year\".",
        "Between the springs of 1935 and 1936, at the same time as Church, Turing worked on the decidability of problems, starting from Godel's incompleteness theorems. In mid-April 1936, Turing sent Max Newman the first draft typescript of his investigations. That same month, Alonzo Church published his An Unsolvable Problem of Elementary Number Theory, with similar conclusions to Turing's then-yet unpublished work. Finally, on 28 May of that year, he finished and delivered his 36-page paper for publication called \"On Computable Numbers, with an Application to the Entscheidungsproblem\". It was published in the Proceedings of the London Mathematical Society journal in two parts, the first on 30 November and the second on 23 December. In this paper, Turing reformulated Kurt Gödel's 1931 results on",
        "in the Proceedings of the London Mathematical Society journal in two parts, the first on 30 November and the second on 23 December. In this paper, Turing reformulated Kurt Gödel's 1931 results on the limits of proof and computation, replacing Gödel's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. The Entscheidungsproblem (decision problem) was originally posed by German mathematician David Hilbert in 1928. Turing proved that his \"universal computing machine\" would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the decision problem by first showing that the halting problem for Turing machines is",
        "computation if it were representable as an algorithm. He went on to prove that there was no solution to the decision problem by first showing that the halting problem for Turing machines is undecidable: it is not possible to decide algorithmically whether a Turing machine will ever halt. This paper has been called \"easily the most influential math paper in history\".",
        "Although Turing's proof was published shortly after Alonzo Church's equivalent proof using his lambda calculus, Turing's approach is considerably more accessible and intuitive than Church's. It also included a notion of a 'Universal Machine' (now known as a universal Turing machine), with the idea that such a machine could perform the tasks of any other computation machine (as indeed could Church's lambda calculus). According to the Church–Turing thesis, Turing machines and the lambda calculus are capable of computing anything that is computable. John von Neumann acknowledged that the central concept of the modern computer was due to Turing's paper. To this day, Turing machines are a central object of study in theory of computation.",
        "From September 1936 to July 1938, Turing spent most of his time studying under Church at Princeton University, in the second year as a Jane Eliza Procter Visiting Fellow. In addition to his purely mathematical work, he studied cryptology and also built three of four stages of an electro-mechanical binary multiplier. In June 1938, he obtained his PhD from the Department of Mathematics at Princeton; his dissertation, Systems of Logic Based on Ordinals, introduced the concept of ordinal logic and the notion of relative computing, in which Turing machines are augmented with so-called oracles, allowing the study of problems that cannot be solved by Turing machines. John von Neumann wanted to hire him as his postdoctoral assistant, but he went back to the United Kingdom.",
        "Career and research\nWhen Turing returned to Cambridge, he attended lectures given in 1939 by Ludwig Wittgenstein about the foundations of mathematics. The lectures have been reconstructed verbatim, including interjections from Turing and other students, from students' notes. Turing and Wittgenstein argued and disagreed, with Turing defending formalism and Wittgenstein propounding his view that mathematics does not discover any absolute truths, but rather invents them.\n\nCryptanalysis\nDuring the Second World War, Turing was a leading participant in the breaking of German ciphers at Bletchley Park. The historian and wartime codebreaker Asa Briggs has said, \"You needed exceptional talent, you needed genius at Bletchley and Turing's was that genius.\"",
        "From September 1938, Turing worked part-time with the Government Code and Cypher School (GC&CS), the British codebreaking organisation. He concentrated on cryptanalysis of the Enigma cipher machine used by Nazi Germany, together with Dilly Knox, a senior GC&CS codebreaker. Soon after the July 1939 meeting near Warsaw at which the Polish Cipher Bureau gave the British and French details of the wiring of Enigma machine's rotors and their method of decrypting Enigma machine's messages, Turing and Knox developed a broader solution. The Polish method relied on an insecure indicator procedure that the Germans were likely to change, which they in fact did in May 1940. Turing's approach was more general, using crib-based decryption for which he produced the functional specification of the bombe",
        "the Germans were likely to change, which they in fact did in May 1940. Turing's approach was more general, using crib-based decryption for which he produced the functional specification of the bombe (an improvement on the Polish Bomba).",
        "On 4 September 1939, the day after the UK declared war on Germany, Turing reported to Bletchley Park, the wartime station of GC&CS. Like all others who came to Bletchley, he was required to sign the Official Secrets Act, in which he agreed not to disclose anything about his work at Bletchley, with severe legal penalties for violating the Act.",
        "Specifying the bombe was the first of five major cryptanalytical advances that Turing made during the war. The others were: deducing the indicator procedure used by the German navy; developing a statistical procedure dubbed Banburismus for making much more efficient use of the bombes; developing a procedure dubbed Turingery for working out the cam settings of the wheels of the Lorenz SZ 40/42 (Tunny) cipher machine and, towards the end of the war, the development of a portable secure voice scrambler at Hanslope Park that was codenamed Delilah.",
        "By using statistical techniques to optimise the trial of different possibilities in the code breaking process, Turing made an innovative contribution to the subject. He wrote two papers discussing mathematical approaches, titled The Applications of Probability to Cryptography and Paper on Statistics of Repetitions, which were of such value to GC&CS and its successor GCHQ that they were not released to the UK National Archives until April 2012, shortly before the centenary of his birth. A GCHQ mathematician, \"who identified himself only as Richard,\" said at the time that the fact that the contents had been restricted under the Official Secrets Act for some 70 years demonstrated their importance, and their relevance to post-war cryptanalysis:",
        "Turing had a reputation for eccentricity at Bletchley Park. He was known to his colleagues as \"Prof\" and his treatise on Enigma was known as the \"Prof's Book\". According to historian Ronald Lewin, Jack Good, a cryptanalyst who worked with Turing, said of his colleague:\n\nPeter Hilton recounted his experience working with Turing in Hut 8 in his \"Reminiscences of Bletchley Park\" from A Century of Mathematics in America:\n\nHilton echoed similar thoughts in the Nova PBS documentary Decoding Nazi Secrets.",
        "Hilton echoed similar thoughts in the Nova PBS documentary Decoding Nazi Secrets.\n\nWhile working at Bletchley, Turing, who was a talented long-distance runner, occasionally ran the  to London when he was needed for meetings, and he was capable of world-class marathon standards. Turing tried out for the 1948 British Olympic team, but he was hampered by an injury. His tryout time for the marathon was only 11 minutes slower than British silver medallist Thomas Richards' Olympic race time of 2 hours 35 minutes. He was Walton Athletic Club's best runner, a fact discovered when he passed the group while running alone. When asked why he ran so hard in training he replied:",
        "Due to the problems of counterfactual history, it is hard to estimate the precise effect Ultra intelligence had on the war. However, official war historian Harry Hinsley estimated that this work shortened the war in Europe by more than two years and saved over 14 million lives.\n\nAt the end of the war, a memo was sent to all those who had worked at Bletchley Park, reminding them that the code of silence dictated by the Official Secrets Act did not end with the war but would continue indefinitely. Thus, even though Turing was appointed an Officer of the Order of the British Empire (OBE) in 1946 by King George VI for his wartime services, his work remained secret for many years.\n\nBombe",
        "Bombe\n\nWithin weeks of arriving at Bletchley Park, Turing had specified an electromechanical machine called the bombe, which could break Enigma more effectively than the Polish bomba kryptologiczna, from which its name was derived. The bombe, with an enhancement suggested by mathematician Gordon Welchman, became one of the primary tools, and the major automated one, used to attack Enigma-enciphered messages.",
        "The bombe searched for possible correct settings used for an Enigma message (i.e., rotor order, rotor settings and plugboard settings) using a suitable crib: a fragment of probable plaintext. For each possible setting of the rotors (which had on the order of 1019 states, or 1022 states for the four-rotor U-boat variant), the bombe performed a chain of logical deductions based on the crib, implemented electromechanically.",
        "The bombe detected when a contradiction had occurred and ruled out that setting, moving on to the next. Most of the possible settings would cause contradictions and be discarded, leaving only a few to be investigated in detail. A contradiction would occur when an enciphered letter would be turned back into the same plaintext letter, which was impossible with the Enigma. The first bombe was installed on 18 March 1940.\n\nAction This Day",
        "Action This Day \n\nBy late 1941, Turing and his fellow cryptanalysts Gordon Welchman, Hugh Alexander and Stuart Milner-Barry were frustrated. Building on the work of the Poles, they had set up a good working system for decrypting Enigma signals, but their limited staff and bombes meant they could not translate all the signals. In the summer, they had considerable success, and shipping losses had fallen to under 100,000 tons a month; however, they badly needed more resources to keep abreast of German adjustments. They had tried to get more people and fund more bombes through the proper channels, but had failed.",
        "On 28 October they wrote directly to Winston Churchill explaining their difficulties, with Turing as the first named. They emphasised how small their need was compared with the vast expenditure of men and money by the forces and compared with the level of assistance they could offer to the forces. As Andrew Hodges, biographer of Turing, later wrote, \"This letter had an electric effect.\" Churchill wrote a memo to General Ismay, which read: \"ACTION THIS DAY. Make sure they have all they want on extreme priority and report to me that this has been done.\" On 18 November, the chief of the secret service reported that every possible measure was being taken. The cryptographers at Bletchley Park did not know of the Prime Minister's response, but as Milner-Barry recalled, \"All that we did notice",
        "service reported that every possible measure was being taken. The cryptographers at Bletchley Park did not know of the Prime Minister's response, but as Milner-Barry recalled, \"All that we did notice was that almost from that day the rough ways began miraculously to be made smooth.\" More than two hundred bombes were in operation by the end of the war.",
        "Hut 8 and the naval Enigma\nTuring decided to tackle the particularly difficult problem of German naval Enigma \"because no one else was doing anything about it and I could have it to myself\". In December 1939, Turing solved the essential part of the naval indicator system, which was more complex than the indicator systems used by the other services.",
        "That same night, he also conceived of the idea of Banburismus, a sequential statistical technique (what Abraham Wald later called sequential analysis) to assist in breaking the naval Enigma, \"though I was not sure that it would work in practice, and was not, in fact, sure until some days had actually broken\". For this, he invented a measure of weight of evidence that he called the ban. Banburismus could rule out certain sequences of the Enigma rotors, substantially reducing the time needed to test settings on the bombes. Later this sequential process of accumulating sufficient weight of evidence using decibans (one tenth of a ban) was used in Cryptanalysis of the Lorenz cipher.",
        "Turing travelled to the United States in November 1942 and worked with US Navy cryptanalysts on the naval Enigma and bombe construction in Washington. He also visited their Computing Machine Laboratory in Dayton, Ohio.\n\nTuring's reaction to the American bombe design was far from enthusiastic:\n\nDuring this trip, he also assisted at Bell Labs with the development of secure speech devices. He returned to Bletchley Park in March 1943. During his absence, Hugh Alexander had officially assumed the position of head of Hut 8, although Alexander had been de facto head for some time (Turing having little interest in the day-to-day running of the section). Turing became a general consultant for cryptanalysis at Bletchley Park.\n\nAlexander wrote of Turing's contribution:\n\nTuringery",
        "In July 1942, Turing devised a technique termed Turingery (or jokingly Turingismus) for use against the Lorenz cipher messages produced by the Germans' new Geheimschreiber (secret writer) machine. This was a teleprinter rotor cipher attachment codenamed Tunny at Bletchley Park. Turingery was a method of wheel-breaking, i.e., a procedure for working out the cam settings of Tunny's wheels. He also introduced the Tunny team to Tommy Flowers who, under the guidance of Max Newman, went on to build the Colossus computer, the world's first programmable digital electronic computer, which replaced a simpler prior machine (the Heath Robinson), and whose superior speed allowed the statistical decryption techniques to be applied usefully to the messages. Some have mistakenly said that Turing was a",
        "simpler prior machine (the Heath Robinson), and whose superior speed allowed the statistical decryption techniques to be applied usefully to the messages. Some have mistakenly said that Turing was a key figure in the design of the Colossus computer. Turingery and the statistical approach of Banburismus undoubtedly fed into the thinking about cryptanalysis of the Lorenz cipher, but he was not directly involved in the Colossus development.",
        "Delilah",
        "Following his work at Bell Labs in the US, Turing pursued the idea of electronic enciphering of speech in the telephone system. In the latter part of the war, he moved to work for the Secret Service's Radio Security Service (later HMGCC) at Hanslope Park. At the park, he further developed his knowledge of electronics with the assistance of REME officer Donald Bayley. Together they undertook the design and construction of a portable secure voice communications machine codenamed Delilah. The machine was intended for different applications, but it lacked the capability for use with long-distance radio transmissions. In any case, Delilah was completed too late to be used during the war. Though the system worked fully, with Turing demonstrating it to officials by encrypting and decrypting a",
        "radio transmissions. In any case, Delilah was completed too late to be used during the war. Though the system worked fully, with Turing demonstrating it to officials by encrypting and decrypting a recording of a Winston Churchill speech, Delilah was not adopted for use. Turing also consulted with Bell Labs on the development of SIGSALY, a secure voice system that was used in the later years of the war.",
        "Early computers and the Turing test\n\nBetween 1945 and 1947, Turing lived in Hampton, London, while he worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory (NPL). He presented a paper on 19 February 1946, which was the first detailed design of a stored-program computer. Von Neumann's incomplete First Draft of a Report on the EDVAC had predated Turing's paper, but it was much less detailed and, according to John R. Womersley, Superintendent of the NPL Mathematics Division, it \"contains a number of ideas which are Dr. Turing's own\".",
        "Although ACE was a feasible design, the effect of the Official Secrets Act surrounding the wartime work at Bletchley Park made it impossible for Turing to explain the basis of his analysis of how a computer installation involving human operators would work. This led to delays in starting the project and he became disillusioned. In late 1947 he returned to Cambridge for a sabbatical year during which he produced a seminal work on Intelligent Machinery that was not published in his lifetime. While he was at Cambridge, the Pilot ACE was being built in his absence. It executed its first program on 10 May 1950, and a number of later computers around the world owe much to it, including the English Electric DEUCE and the American Bendix G-15. The full version of Turing's ACE was not built until",
        "10 May 1950, and a number of later computers around the world owe much to it, including the English Electric DEUCE and the American Bendix G-15. The full version of Turing's ACE was not built until after his death.",
        "According to the memoirs of the German computer pioneer Heinz Billing from the Max Planck Institute for Physics, published by Genscher, Düsseldorf, there was a meeting between Turing and Konrad Zuse. It took place in Göttingen in 1947. The interrogation had the form of a colloquium. Participants were Womersley, Turing, Porter from England and a few German researchers like Zuse, Walther, and Billing (for more details see Herbert Bruderer, Konrad Zuse und die Schweiz).",
        "In 1948, Turing was appointed reader in the Mathematics Department at the Victoria University of Manchester. A year later, he became deputy director of the Computing Machine Laboratory, where he worked on software for one of the earliest stored-program computers—the Manchester Mark 1. Turing wrote the first version of the Programmer's Manual for this machine, and was recruited by Ferranti as a consultant in the development of their commercialised machine, the Ferranti Mark 1. He continued to be paid consultancy fees by Ferranti until his death. During this time, he continued to do more abstract work in mathematics, and in \"Computing Machinery and Intelligence\" (Mind, October 1950), Turing addressed the problem of artificial intelligence, and proposed an experiment that became known as the",
        "work in mathematics, and in \"Computing Machinery and Intelligence\" (Mind, October 1950), Turing addressed the problem of artificial intelligence, and proposed an experiment that became known as the Turing test, an attempt to define a standard for a machine to be called \"intelligent\". The idea was that a computer could be said to \"think\" if a human interrogator could not tell it apart, through conversation, from a human being. In the paper, Turing suggested that rather than building a program to simulate the adult mind, it would be better to produce a simpler one to simulate a child's mind and then to subject it to a course of education. A reversed form of the Turing test is widely used on the Internet; the CAPTCHA test is intended to determine whether the user is a human or a computer.",
        "In 1948, Turing, working with his former undergraduate colleague, D.G. Champernowne, began writing a chess program for a computer that did not yet exist. By 1950, the program was completed and dubbed the Turochamp. In 1952, he tried to implement it on a Ferranti Mark 1, but lacking enough power, the computer was unable to execute the program. Instead, Turing \"ran\" the program by flipping through the pages of the algorithm and carrying out its instructions on a chessboard, taking about half an hour per move. The game was recorded. According to Garry Kasparov, Turing's program \"played a recognizable game of chess\". The program lost to Turing's colleague Alick Glennie, although it is said that it won a game against Champernowne's wife, Isabel.",
        "His Turing test was a significant, characteristically provocative, and lasting contribution to the debate regarding artificial intelligence, which continues after more than half a century.\n\nPattern formation and mathematical biology",
        "When Turing was 39 years old in 1951, he turned to mathematical biology, finally publishing his masterpiece \"The Chemical Basis of Morphogenesis\" in January 1952. He was interested in morphogenesis, the development of patterns and shapes in biological organisms. He suggested that a system of chemicals reacting with each other and diffusing across space, termed a reaction–diffusion system, could account for \"the main phenomena of morphogenesis\". He used systems of partial differential equations to model catalytic chemical reactions. For example, if a catalyst A is required for a certain chemical reaction to take place, and if the reaction produced more of the catalyst A, then we say that the reaction is autocatalytic, and there is positive feedback that can be modelled by nonlinear",
        "chemical reaction to take place, and if the reaction produced more of the catalyst A, then we say that the reaction is autocatalytic, and there is positive feedback that can be modelled by nonlinear differential equations. Turing discovered that patterns could be created if the chemical reaction not only produced catalyst A, but also produced an inhibitor B that slowed down the production of A. If A and B then diffused through the container at different rates, then you could have some regions where A dominated and some where B did. To calculate the extent of this, Turing would have needed a powerful computer, but these were not so freely available in 1951, so he had to use linear approximations to solve the equations by hand. These calculations gave the right qualitative results, and",
        "a powerful computer, but these were not so freely available in 1951, so he had to use linear approximations to solve the equations by hand. These calculations gave the right qualitative results, and produced, for example, a uniform mixture that oddly enough had regularly spaced fixed red spots. The Russian biochemist Boris Belousov had performed experiments with similar results, but could not get his papers published because of the contemporary prejudice that any such thing violated the second law of thermodynamics. Belousov was not aware of Turing's paper in the Philosophical Transactions of the Royal Society.",
        "Although published before the structure and role of DNA was understood, Turing's work on morphogenesis remains relevant today and is considered a seminal piece of work in mathematical biology. One of the early applications of Turing's paper was the work by James Murray explaining spots and stripes on the fur of cats, large and small. Further research in the area suggests that Turing's work can partially explain the growth of \"feathers, hair follicles, the branching pattern of lungs, and even the left-right asymmetry that puts the heart on the left side of the chest\". In 2012, Sheth, et al. found that in mice, removal of Hox genes causes an increase in the number of digits without an increase in the overall size of the limb, suggesting that Hox genes control digit formation by tuning the",
        "that in mice, removal of Hox genes causes an increase in the number of digits without an increase in the overall size of the limb, suggesting that Hox genes control digit formation by tuning the wavelength of a Turing-type mechanism. Later papers were not available until Collected Works of A. M. Turing was published in 1992.",
        "A study conducted in 2023 confirmed Turing's mathematical model hypothesis. Presented by the American Physical Society, the experiment involved growing chia seeds in even layers within trays, later adjusting the available moisture. Researchers experimentally tweaked the factors which appear in the Turing equations, and, as a result, patterns resembling those seen in natural environments emerged. This is believed to be the first time that experiments with living vegetation have verified Turing's mathematical insight.\n\nPersonal life",
        "Personal life\n\nTreasure\nIn the 1940s, Turing became worried about losing his savings in the event of a German invasion. In order to protect it, he bought two silver bars weighing  and worth £250 (in 2022, £8,000 adjusted for inflation, £48,000 at spot price) and buried them in a wood near Bletchley Park. Upon returning to dig them up, Turing found that he was unable to break his own code describing where exactly he had hidden them. This, along with the fact that the area had been renovated, meant that he never regained the silver.",
        "Engagement\nIn 1941, Turing proposed marriage to Hut 8 colleague Joan Clarke, a fellow mathematician and cryptanalyst, but their engagement was short-lived. After admitting his homosexuality to his fiancée, who was reportedly \"unfazed\" by the revelation, Turing decided that he could not go through with the marriage.",
        "Homosexuality and indecency conviction",
        "In January 1952, Turing was 39 when he started a relationship with Arnold Murray, a 19-year-old unemployed man. Just before Christmas, Turing was walking along Manchester's Oxford Road when he met Murray just outside the Regal Cinema and invited him to lunch. On 23 January, Turing's house was burgled. Murray told Turing that he and the burglar were acquainted, and Turing reported the crime to the police. During the investigation, he acknowledged a sexual relationship with Murray. Homosexual acts were criminal offences in the United Kingdom at that time, and both men were charged with \"gross indecency\" under Section 11 of the Criminal Law Amendment Act 1885. Initial committal proceedings for the trial were held on 27 February during which Turing's solicitor \"reserved his defence\", i.e.,",
        "indecency\" under Section 11 of the Criminal Law Amendment Act 1885. Initial committal proceedings for the trial were held on 27 February during which Turing's solicitor \"reserved his defence\", i.e., did not argue or provide evidence against the allegations. The proceedings were held at the Sessions House in Knutsford.",
        "Turing was later convinced by the advice of his brother and his own solicitor, and he entered a plea of guilty. The case, Regina v. Turing and Murray, was brought to trial on 31 March 1952. Turing was convicted and given a choice between imprisonment and probation. His probation would be conditional on his agreement to undergo hormonal physical changes designed to reduce libido, known as \"chemical castration\". He accepted the option of injections of what was then called stilboestrol (now known as diethylstilbestrol or DES), a synthetic oestrogen; this feminization of his body was continued for the course of one year. The treatment rendered Turing impotent and caused breast tissue to form, fulfilling in the literal sense Turing's prediction that \"no doubt I shall emerge from it all a",
        "for the course of one year. The treatment rendered Turing impotent and caused breast tissue to form, fulfilling in the literal sense Turing's prediction that \"no doubt I shall emerge from it all a different man, but quite who I've not found out\". Murray was given a conditional discharge.",
        "Turing's conviction led to the removal of his security clearance and barred him from continuing with his cryptographic consultancy for the Government Communications Headquarters (GCHQ), the British signals intelligence agency that had evolved from GC&CS in 1946, though he kept his academic job. His trial took place only months after the defection to the Soviet Union of Guy Burgess and Donald Maclean in summer 1951 after which the Foreign Office started to consider anyone known to be homosexual as a potential security risk.",
        "Turing was denied entry into the United States after his conviction in 1952, but was free to visit other European countries. In the summer of 1952 he visited Norway which was more tolerant of homosexuals. Among the various men he met there was one named Kjell Carlson. Kjell intended to visit Turing in the UK but the authorities intercepted Kjell's postcard detailing his travel arrangements and were able to intercept and deport him before the two could meet. It was also during this time that Turing started consulting a psychiatrist, Dr Franz Greenbaum, with whom he got on well and subsequently becoming a family friend.\n\nDeath",
        "Death\n\nOn 8 June 1954, at his house at 43 Adlington Road, Wilmslow, Turing's housekeeper found him dead. A post mortem was held that evening which determined that he had died the previous day at the age of 41 with Cyanide poisoning cited as the cause of death. When his body was discovered, an apple lay half-eaten beside his bed, and although the apple was not tested for cyanide, it was speculated that this was the means by which Turing had consumed a fatal dose.",
        "Turing's brother John identified the body the following day and took the advice given by Dr. Greenbaum to accept the verdict of the inquest as there was little prospect of establishing that the death was accidental. The inquest was held the following day which determined the cause of death to be suicide. Turing's remains were cremated at Woking Crematorium just two days later on 12 June 1954 with just three people attending and his ashes were scattered in the gardens of the crematorium, just as his father's had been. Turing's mother was on holiday in Italy at the time of his death and returned home after the inquest. She never accepted the verdict of suicide.",
        "Andrew Hodges and another biographer, David Leavitt, have both speculated that Turing was re-enacting a scene from the Walt Disney film Snow White and the Seven Dwarfs (1937), his favourite fairy tale. Both men noted that (in Leavitt's words) he took \"an especially keen pleasure in the scene where the Wicked Queen immerses her apple in the poisonous brew\".",
        "Philosopher Jack Copeland has questioned various aspects of the coroner's historical verdict. He suggested an alternative explanation for the cause of Turing's death: the accidental inhalation of cyanide fumes from an apparatus used to electroplate gold onto spoons. The potassium cyanide was used to dissolve the gold. Turing had such an apparatus set up in his tiny spare room. Copeland noted that the autopsy findings were more consistent with inhalation than with ingestion of the poison. Turing also habitually ate an apple before going to bed, and it was not unusual for the apple to be discarded half-eaten. Furthermore, Turing had reportedly borne his legal setbacks and hormone treatment (which had been discontinued a year previously) \"with good humour\" and had shown no sign of",
        "be discarded half-eaten. Furthermore, Turing had reportedly borne his legal setbacks and hormone treatment (which had been discontinued a year previously) \"with good humour\" and had shown no sign of despondency before his death. He even set down a list of tasks that he intended to complete upon returning to his office after the holiday weekend. Turing's mother believed that the ingestion was accidental, resulting from her son's careless storage of laboratory chemicals. Biographer Andrew Hodges theorised that Turing deliberately left the nature of his death ambiguous in order to shield his mother from the knowledge that he had killed himself.",
        "It has been suggested that Turing's belief in fortune-telling may have caused his depressed mood. As a youth, Turing had been told by a fortune-teller that he would be a genius. In mid-May 1954, shortly before his death, Turing again decided to consult a fortune-teller during a day-trip to St Annes-on-Sea with the Greenbaum family. According to the Greenbaums' daughter, Barbara:\n\nGovernment apology and pardon\n\nIn August 2009, British programmer John Graham-Cumming started a petition urging the British government to apologise for Turing's prosecution as a homosexual. The petition received more than 30,000 signatures. The prime minister, Gordon Brown, acknowledged the petition, releasing a statement on 10 September 2009 apologising and describing the treatment of Turing as \"appalling\":",
        "In December 2011, William Jones and his member of Parliament, John Leech, created an e-petition requesting that the British government pardon Turing for his conviction of \"gross indecency\":\n\nThe petition gathered over 37,000 signatures, and was submitted to Parliament by the Manchester MP John Leech but the request was discouraged by Justice Minister Lord McNally, who said:",
        "John Leech, the MP for Manchester Withington (2005–15), submitted several bills to Parliament and led a high-profile campaign to secure the pardon. Leech made the case in the House of Commons that Turing's contribution to the war made him a national hero and that it was \"ultimately just embarrassing\" that the conviction still stood. Leech continued to take the bill through Parliament and campaigned for several years, gaining the public support of numerous leading scientists, including Stephen Hawking. At the British premiere of a film based on Turing's life, The Imitation Game, the producers thanked Leech for bringing the topic to public attention and securing Turing's pardon. Leech is now regularly described as the \"architect\" of Turing's pardon and subsequently the Alan Turing Law which",
        "Leech for bringing the topic to public attention and securing Turing's pardon. Leech is now regularly described as the \"architect\" of Turing's pardon and subsequently the Alan Turing Law which went on to secure pardons for 75,000 other men and women convicted of similar crimes.",
        "On 26 July 2012, a bill was introduced in the House of Lords to grant a statutory pardon to Turing for offences under section 11 of the Criminal Law Amendment Act 1885, of which he was convicted on 31 March 1952. Late in the year in a letter to The Daily Telegraph, the physicist Stephen Hawking and 10 other signatories including the Astronomer Royal Lord Rees, President of the Royal Society Sir Paul Nurse, Lady Trumpington (who worked for Turing during the war) and Lord Sharkey (the bill's sponsor) called on Prime Minister David Cameron to act on the pardon request. The government indicated it would support the bill, and it passed its third reading in the House of Lords in October.",
        "At the bill's second reading in the House of Commons on 29 November 2013, Conservative MP Christopher Chope objected to the bill, delaying its passage. The bill was due to return to the House of Commons on 28 February 2014, but before the bill could be debated in the House of Commons, the government elected to proceed under the royal prerogative of mercy. On 24 December 2013, Queen Elizabeth II signed a pardon for Turing's conviction for \"gross indecency\", with immediate effect. Announcing the pardon, Lord Chancellor Chris Grayling said Turing deserved to be \"remembered and recognised for his fantastic contribution to the war effort\" and not for his later criminal conviction. The Queen officially pronounced Turing pardoned in August 2014. The Queen's action is only the fourth royal pardon",
        "fantastic contribution to the war effort\" and not for his later criminal conviction. The Queen officially pronounced Turing pardoned in August 2014. The Queen's action is only the fourth royal pardon granted since the conclusion of the Second World War. Pardons are normally granted only when the person is technically innocent, and a request has been made by the family or other interested party; neither condition was met in regard to Turing's conviction.",
        "In September 2016, the government announced its intention to expand this retroactive exoneration to other men convicted of similar historical indecency offences, in what was described as an \"Alan Turing law\". The Alan Turing law is now an informal term for the law in the United Kingdom, contained in the Policing and Crime Act 2017, which serves as an amnesty law to retroactively pardon men who were cautioned or convicted under historical legislation that outlawed homosexual acts. The law applies in England and Wales.",
        "On 19 July 2023, following an apology to LGBT veterans from the UK Government, Defence Secretary Ben Wallace suggested Turing should be honoured with a permanent statue on the fourth plinth of Trafalgar Square, describing Dr Turing as \"probably the greatest war hero, in my book, of the Second World War, [whose] achievements shortened the war, saved thousands of lives, helped defeat the Nazis. And his story is a sad story of a society and how it treated him.\"\n\nSee also \n Legacy of Alan Turing\n List of things named after Alan Turing\n\nNotes and references\n\nNotes\n\nReferences\n\nSources",
        "in \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  and \n  Turing's mother, who survived him by many years, wrote this 157-page biography of her son, glorifying his life. It was published in 1959, and so could not cover his war work. Scarcely 300 copies were sold (Sara Turing to Lyn Newman, 1967, Library of St John's College, Cambridge). The six-page foreword by Lyn Irvine includes reminiscences and is more frequently quoted. It was re-published by Cambridge University Press in 2012, to honour the centenary of his birth, and included a new foreword by Martin Davis, as well as a never-before-published memoir by Turing's older brother John F. Turing.",
        "This 1986 Hugh Whitemore play tells the story of Turing's life and death. In the original West End and Broadway runs, Derek Jacobi played Turing and he recreated the role in a 1997 television film based on the play made jointly by the BBC and WGBH, Boston. The play is published by Amber Lane Press, Oxford, ASIN: B000B7TM0Q",
        "Further reading\n\nArticles\n\nBooks\n \n \n \n \n  (originally published in 1983); basis of the film The Imitation Game\n  (originally published in 1959 by W. Heffer & Sons, Ltd)\n\nExternal links",
        "Oral history interview with Nicholas C. Metropolis, Charles Babbage Institute, University of Minnesota. Metropolis was the first director of computing services at Los Alamos National Laboratory; topics include the relationship between Turing and John von Neumann\n How Alan Turing Cracked The Enigma Code Imperial War Museums\n Alan Turing Year \n CiE 2012: Turing Centenary Conference\n Science in the Making Alan Turing's papers in the Royal Society's archives\n Alan Turing site maintained by Andrew Hodges including a short biography\n AlanTuring.net – Turing Archive for the History of Computing by Jack Copeland\n The Turing Archive – contains scans of some unpublished documents and material from the King's College, Cambridge archive",
        "The Turing Archive – contains scans of some unpublished documents and material from the King's College, Cambridge archive\n Alan Turing Papers – University of Manchester Library, Manchester\n \n Sherborne School Archives – holds papers relating to Turing's time at Sherborne School\n Alan Turing plaques recorded on openplaques.org\n Alan Turing archive on New Scientist",
        "1912 births\n1954 suicides\n1954 deaths\n20th-century English mathematicians\n20th-century atheists\n20th-century English philosophers\nAcademics of the University of Manchester\nAcademics of the University of Manchester Institute of Science and Technology\nAlumni of King's College, Cambridge\nArtificial intelligence researchers\nBayesian statisticians\nBletchley Park people\nBritish anti-fascists\nBritish cryptographers\nBritish people of World War II\nComputability theorists\nComputer designers\nEnglish atheists\nEnglish computer scientists\nEnglish inventors\nEnglish logicians\nEnglish male long-distance runners\nEnglish people of Irish descent\nEnglish people of Scottish descent\nFellows of King's College, Cambridge\nFellows of the Royal Society\nFormer Protestants\nForeign Office personnel of World War II",
        "English people of Irish descent\nEnglish people of Scottish descent\nFellows of King's College, Cambridge\nFellows of the Royal Society\nFormer Protestants\nForeign Office personnel of World War II\nGay academics\nGay scientists\nGay sportsmen\nGCHQ people\nHistory of computing in the United Kingdom\nLGBT-related suicides\nLGBT mathematicians\nLGBT philosophers\nBritish LGBT scientists\nEnglish LGBT sportspeople\nLGBT track and field athletes\nOfficers of the Order of the British Empire\nPeople educated at Sherborne School\nPeople from Maida Vale\nPeople from Wilmslow\nPeople convicted for homosexuality in the United Kingdom\nPeople who have received posthumous pardons\nPrinceton University alumni\nRecipients of British royal pardons\nTheoretical biologists\nTheoretical computer scientists",
        "People who have received posthumous pardons\nPrinceton University alumni\nRecipients of British royal pardons\nTheoretical biologists\nTheoretical computer scientists\nSuicides by cyanide poisoning\nSuicides in England\nCastrated people\nScientists of the National Physical Laboratory (United Kingdom)\nComputer chess people\nEnigma machine"
    ],
    [
        "Alexey Pajitnov\nAlexey Leonidovich Pajitnov (born April 16, 1955) is a Russian computer engineer and video game designer who is best known for creating, designing, and developing Tetris in 1985 while working at the Dorodnitsyn Computing Centre under the Academy of Sciences of the Soviet Union (now the Russian Academy of Sciences).\n\nIn 1991, he moved to the United States and later became a U.S. citizen. In 1996, Pajitnov founded The Tetris Company alongside Dutch video game designer Henk Rogers. Pajitnov did not receive royalties from Tetris prior to this time, despite the game's high popularity.",
        "Early life\nPajitnov was born to parents who were both writers. His father was an art critic. His mother was a journalist who wrote for both newspapers and a film magazine. It was through his parents that Pajitnov gained exposure to the arts, eventually developing a passion for cinema. He accompanied his mother to many film screenings, including the Moscow Film Festival. Pajitnov was also mathematically inclined, enjoying puzzles and problem solving.\n\nIn 1967, when he was 11 years old, Pajitnov's parents divorced. For several years, he lived with his mother in a one-bedroom apartment owned by the state. The two were eventually able to move into a private apartment at 49 Gertsen Street, when Pajitnov was 17. He later went on to study applied mathematics at the Moscow Aviation Institute.",
        "Career \nIn 1977, Pajitnov worked as a summer intern at the Soviet Academy of Sciences. Once he graduated in 1979, he accepted a job there working on speech recognition at the Academy's Dorodnitsyn Computing Centre. When the Computing Centre received new equipment, its researchers would write a small program for it in order to test its computing capabilities. According to Pajitnov, this \"became [his] excuse for making games\". Computer games were fascinating to him because they offered a way to bridge the gap between logic and emotion, and Pajitnov held interests in both mathematics and puzzles, as well as the psychology of computing.",
        "Searching for inspiration, Pajitnov recalled his childhood memories of playing pentominoes, a game in which the user creates pictures using its shapes. Remembering the difficulty he had in putting the pieces back into their box, Pajitnov felt inspired to create a game based on that concept. Using an Electronika 60 in the Computing Centre, he began working on what would become the first version of Tetris. Building the first prototype in two weeks, Pajitnov spent longer playtesting and adding to the game, completing it on June 6, 1985. This primitive version did not have levels or a scoring system, but Pajitnov knew he had a potentially great game, since he could not stop playing it at work.",
        "The game attracted the interest of coworkers like fellow programmer Dmitri Pevlovsky, who helped Pajitnov connect with Vadim Gerasimov, a 16-year-old intern at the Soviet Academy. Pajitnov wanted to make a color version of Tetris for the IBM Personal Computer, and enlisted the intern to help. Gerasimov created the PC version in less than three weeks, and with contributions from Pevlovsky, spent an additional month adding new features like scorekeeping and sound effects. The game, first available in the Soviet Union, appeared in the West in 1986.",
        "Pajitnov created a sequel to Tetris, entitled Welltris, which has the same principle, but in a three-dimensional environment where the player sees the playing area from above. Tetris was licensed and managed by Soviet company ELORG, which had a monopoly on the import and export of computer hardware and software in the Soviet Union, and advertised with the slogan \"From Russia with Love\" (on NES: \"From Russia with Fun!\"). Because he was employed by the Soviet government, Pajitnov did not receive royalties.",
        "Pajitnov, together with Vladimir Pokhilko, moved to the United States following the collapse of the Soviet Union in 1991, and in 1996 founded The Tetris Company with Henk Rogers, which finally allowed him to collect royalties from his game. He helped design the puzzles in the Super NES versions of Yoshi's Cookie and designed the game Pandora's Box, which incorporates more traditional jigsaw-style puzzles. Pajitnov and Pokhilko founded the 3D software technology company AnimaTek, which developed the game / screensaver El-Fish.",
        "He was employed by Microsoft from October 1996 until 2005. While there, he worked on the Microsoft Entertainment Pack: The Puzzle Collection, MSN Mind Aerobics and MSN Games groups. Pajitnov's new, enhanced version of Hexic, Hexic HD, was included with every new Xbox 360 Premium package.\n\nIn August 2005, WildSnake Software announced that Pajitnov would be collaborating with them to release a new line of puzzle games.\n\nPersonal life \nPajitnov moved to the United States in 1991, was naturalized as a U.S. citizen and now lives in Clyde Hill, Washington. He has a wife, Nina, with whom he had two sons named Artem and Dmitri.",
        "Political views \nAfter the Russian invasion of Ukraine in 2022, Pajitnov issued a statement condemning the war and stating that he was \"sure that Putin and his hateful regime will fall down and the normal peaceful way of living will be restored in Ukraine and, hopefully, in Russia.\"\n\nGames\n\nAwards and recognition\nIn 1996, GameSpot named him as the fourth most influential computer game developer of all time.\n\nIn March 2007, he received the Game Developers Choice Awards First Penguin Award. The award was given for pioneering the casual games market.\n\nIn June 2009, he received the honorary award at the LARA - Der Deutsche Games Award in Cologne, Germany.",
        "In June 2009, he received the honorary award at the LARA - Der Deutsche Games Award in Cologne, Germany.\n\nIn 2012, IGN included Pajitnov on their list of 5 Memorable Video Game Industry One-Hit Wonders, calling him \"the ultimate video game one-hit wonder.\"\n\nIn 2015, Pajitnov won the Bizkaia Award at the Fun & Serious Game Festival.\n\nPajitnov was portrayed by Russian actor Nikita Yefremov in the 2023 movie Tetris, a dramatised retelling of the licensing bidding war for Tetris in the late 1980s.\n\nSee also\nBreakThru!, video game endorsed by Pajitnov\nClockWerx, video game endorsed by Pajitnov\n\nNotes\n\nReferences\n\nExternal links\n\nAlexey L. Pajitnov profile at MobyGames\nTetris Creator Claims Free and Open Source Software Destroys the Market.\nTetris — From Russia with Love, BBC documentary",
        "Notes\n\nReferences\n\nExternal links\n\nAlexey L. Pajitnov profile at MobyGames\nTetris Creator Claims Free and Open Source Software Destroys the Market.\nTetris — From Russia with Love, BBC documentary\n\n1955 births\nLiving people\nGame Developers Conference Pioneer Award recipients\nMoscow Aviation Institute alumni\nMicrosoft employees\nRussian activists against the Russian invasion of Ukraine\nRussian computer programmers\nRussian emigrants to the United States\nRussian inventors\nRussian video game designers\nScientists from Moscow\nSoviet computer scientists\nTetris\nRussian scientists"
    ],
    [
        "ALGOL\nALGOL (; short for \"Algorithmic Language\") is a family of imperative computer programming languages originally developed in 1958. ALGOL heavily influenced many other languages and was the standard method for algorithm description used by the Association for Computing Machinery (ACM) in textbooks and academic sources for more than thirty years.\n\nIn the sense that the syntax of most modern languages is \"Algol-like\", it was arguably more influential than three other high-level programming languages among which it was roughly contemporary: FORTRAN, Lisp, and COBOL. It was designed to avoid some of the perceived problems with FORTRAN and eventually gave rise to many other programming languages, including PL/I, Simula, BCPL, B, Pascal, and C.",
        "ALGOL introduced code blocks and the begin...end pairs for delimiting them. It was also the first language implementing nested function definitions with lexical scope. Moreover, it was the first programming language which gave detailed attention to formal language definition and through the Algol 60 Report introduced Backus–Naur form, a principal formal grammar notation for language design.\n\nThere were three major specifications, named after the years they were first published:\n ALGOL 58 – originally proposed to be called IAL, for International Algebraic Language.\n ALGOL 60 – first implemented as X1 ALGOL 60 in 1961. Revised 1963.\n ALGOL 68 – introduced new elements including flexible arrays, slices, parallelism, operator identification. Revised 1973.",
        "ALGOL 68 is substantially different from ALGOL 60 and was not well received, so in general \"Algol\" means ALGOL 60 and its dialects.\n\nHistory\n\nALGOL was developed jointly by a committee of European and American computer scientists in a meeting in 1958 at the Swiss Federal Institute of Technology in Zurich (cf. ALGOL 58). It specified three different syntaxes: a reference syntax, a publication syntax, and an implementation syntax. The different syntaxes permitted it to use different keyword names and conventions for decimal points (commas vs periods) for different languages.",
        "ALGOL was used mostly by research computer scientists in the United States and in Europe. Its use in commercial applications was hindered by the absence of standard input/output facilities in its description and the lack of interest in the language by large computer vendors other than Burroughs Corporation. ALGOL 60 did however become the standard for the publication of algorithms and had a profound effect on future language development.\n\nJohn Backus developed the Backus normal form method of describing programming languages specifically for ALGOL 58. It was revised and expanded by Peter Naur for ALGOL 60, and at Donald Knuth's suggestion renamed Backus–Naur form.",
        "Peter Naur: \"As editor of the ALGOL Bulletin I was drawn into the international discussions of the language and was selected to be member of the European language design group in November 1959. In this capacity I was the editor of the ALGOL 60 report, produced as the result of the ALGOL 60 meeting in Paris in January 1960.\"",
        "The following people attended the meeting in Paris (from 1 to 16 January):\n Friedrich Ludwig Bauer, Peter Naur, Heinz Rutishauser, Klaus Samelson, Bernard Vauquois, Adriaan van Wijngaarden, and Michael Woodger (from Europe)\n John Warner Backus, Julien Green, Charles Katz, John McCarthy, Alan Jay Perlis, and Joseph Henry Wegstein (from the US).\nAlan Perlis gave a vivid description of the meeting: \"The meetings were exhausting, interminable, and exhilarating. One became aggravated when one's good ideas were discarded along with the bad ones of others. Nevertheless, diligence persisted during the entire period. The chemistry of the 13 was excellent.\"",
        "ALGOL 60 inspired many languages that followed it. Tony Hoare remarked: \"Here is a language so far ahead of its time that it was not only an improvement on its predecessors but also on nearly all its successors.\" The Scheme programming language, a variant of Lisp that adopted the block structure and lexical scope of ALGOL, also adopted the wording \"Revised Report on the Algorithmic Language Scheme\" for its standards documents in homage to ALGOL.",
        "ALGOL and programming language research\nAs Peter Landin noted, ALGOL was the first language to combine seamlessly imperative effects with the (call-by-name) lambda calculus. Perhaps the most elegant formulation of the language is due to John C. Reynolds, and it best exhibits its syntactic and semantic purity. Reynolds's idealized ALGOL also made a convincing methodologic argument regarding the suitability of local effects in the context of call-by-name languages, in contrast with the global effects used by call-by-value languages such as ML. The conceptual integrity of the language made it one of the main objects of semantic research, along with Programming Computable Functions (PCF) and ML.",
        "IAL implementations timeline\nTo date there have been at least 70 augmentations, extensions, derivations and sublanguages of Algol 60.\n\nThe Burroughs dialects included special Bootstrapping dialects such as ESPOL and NEWP.  The latter is still used for Unisys MCP system software.\n\nProperties\nALGOL 60 as officially defined had no I/O facilities; implementations defined their own in ways that were rarely compatible with each other. In contrast, ALGOL 68 offered an extensive library of transput (input/output) facilities.",
        "ALGOL 60 allowed for two evaluation strategies for parameter passing: the common call-by-value, and call-by-name. Call-by-name has certain effects in contrast to call-by-reference. For example, without specifying the parameters as value or reference, it is impossible to develop a procedure that will swap the values of two parameters if the actual parameters that are passed in are an integer variable and an array that is indexed by that same integer variable. Think of passing a pointer to swap(i, A[i]) in to a function. Now that every time swap is referenced, it is reevaluated. Say i := 1 and A[i] := 2, so every time swap is referenced it will return the other combination of the values ([1,2], [2,1], [1,2] and so on). A similar situation occurs with a random function passed as actual",
        "and A[i] := 2, so every time swap is referenced it will return the other combination of the values ([1,2], [2,1], [1,2] and so on). A similar situation occurs with a random function passed as actual argument.",
        "Call-by-name is known by many compiler designers for the interesting \"thunks\" that are used to implement it. Donald Knuth devised the \"man or boy test\" to separate compilers that correctly implemented \"recursion and non-local references.\" This test contains an example of call-by-name.",
        "ALGOL 68 was defined using a two-level grammar formalism invented by Adriaan van Wijngaarden and which bears his name. Van Wijngaarden grammars use a context-free grammar to generate an infinite set of productions that will recognize a particular ALGOL 68 program; notably, they are able to express the kind of requirements that in many other programming language standards are labelled \"semantics\" and have to be expressed in ambiguity-prone natural language prose, and then implemented in compilers as ad hoc code attached to the formal language parser.\n\nExamples and portability issues\n\nCode sample comparisons\n\nALGOL 60\n(The way the bold text has to be written depends on the implementation, e.g. 'INTEGER'—quotation marks included—for integer. This is known as stropping.)",
        "Code sample comparisons\n\nALGOL 60\n(The way the bold text has to be written depends on the implementation, e.g. 'INTEGER'—quotation marks included—for integer. This is known as stropping.)\n\n procedure Absmax(a) Size:(n, m) Result:(y) Subscripts:(i, k);\n     value n, m; array a; integer n, m, i, k; real y;\n comment The absolute greatest element of the matrix a, of size n by m,\n     is copied to y, and the subscripts of this element to i and k;\n begin\n     integer p, q;\n     y := 0; i := k := 1;\n     for p := 1 step 1 until n do\n         for q := 1 step 1 until m do\n             if abs(a[p, q]) > y then\n                 begin y := abs(a[p, q]);\n                     i := p; k := q\n                 end\n end Absmax\n\nHere is an example of how to produce a table using Elliott 803 ALGOL.",
        "Here is an example of how to produce a table using Elliott 803 ALGOL.\n\n  FLOATING POINT ALGOL TEST'\n  BEGIN REAL A,B,C,D'\n  READ D'\n  FOR A:= 0.0 STEP D UNTIL 6.3 DO\n  BEGIN\n    PRINT PUNCH(3),££L??'\n    B := SIN(A)'\n    C := COS(A)'\n    PRINT PUNCH(3),SAMELINE,ALIGNED(1,6),A,B,C'\n  END\n  END'\n\nPUNCH(3) sends output to the teleprinter rather than the tape punch.\nSAMELINE suppresses the carriage return + line feed normally printed between arguments.\nALIGNED(1,6) controls the format of the output with one digit before and six after the decimal point.\n\nALGOL 68\nThe following code samples are ALGOL 68 versions of the above ALGOL 60 code samples.",
        "ALGOL 68\nThe following code samples are ALGOL 68 versions of the above ALGOL 60 code samples.\n\nALGOL 68 implementations used ALGOL 60's approaches to stropping. In ALGOL 68's case tokens with the bold typeface are reserved words, types (modes) or operators.\n\n proc abs max = ([,]real a, ref real y, ref int i, k)real:\n comment The absolute greatest element of the matrix a, of size ⌈a by 2⌈a\n is transferred to y, and the subscripts of this element to i and k; comment\n begin\n    real y := 0; i := ⌊a; k := 2⌊a;\n    for p from ⌊a to ⌈a do\n      for q from 2⌊a to 2⌈a do\n        if abs a[p, q] > y then\n            y := abs a[p, q];\n            i := p; k := q\n        fi\n      od\n    od;\n    y\n end # abs max #",
        "Note: lower (⌊) and upper (⌈) bounds of an array, and array slicing, are directly available to the programmer.\n\n floating point algol68 test:\n (\n   real a,b,c,d;\n    \n   # printf – sends output to the file stand out. #\n   # printf($p$); – selects a new page #\n   printf(($pg$,\"Enter d:\"));  \n   read(d);\n    \n   for step from 0 while a:=step*d; a <= 2*pi do\n     printf($l$);  # $l$ - selects a new line. #\n     b := sin(a);\n     c := cos(a);\n     printf(($z-d.6d$,a,b,c))  # formats output with 1 digit before and 6 after the decimal point. #\n   od\n )\n\nTimeline: Hello world\nThe variations and lack of portability of the programs from one implementation to another is easily demonstrated by the classic hello world program.\n\nALGOL 58 (IAL)\n\nALGOL 58 had no I/O facilities.\n\nALGOL 60 family",
        "ALGOL 58 (IAL)\n\nALGOL 58 had no I/O facilities.\n\nALGOL 60 family\n\nSince ALGOL 60 had no I/O facilities, there is no portable hello world program in ALGOL.\nThe next three examples are in Burroughs Extended Algol. The first two direct output at the interactive terminal they are run on. The first uses a character array, similar to C. The language allows the array identifier to be used as a pointer to the array, and hence in a REPLACE statement.\n\nA simpler program using an inline format:\n\nAn even simpler program using the Display statement. Note that its output would end up at the system console ('SPO'):\n\nAn alternative example, using Elliott Algol I/O is as follows.  Elliott Algol used different characters for \"open-string-quote\" and \"close-string-quote\", represented here by  and .",
        "An alternative example, using Elliott Algol I/O is as follows.  Elliott Algol used different characters for \"open-string-quote\" and \"close-string-quote\", represented here by  and .\n\nBelow is a version from Elliott 803 Algol (A104). The standard Elliott 803 used five-hole paper tape and thus only had upper case.  The code lacked any quote characters so £ (UK Pound Sign) was used for open quote and ? (Question Mark) for close quote.  Special sequences were placed in double quotes (e.g. ££L?? produced a new line on the teleprinter).\n\n   HIFOLKS'\n   BEGIN\n      PRINT £HELLO WORLD£L??'\n   END'",
        "HIFOLKS'\n   BEGIN\n      PRINT £HELLO WORLD£L??'\n   END'\n\nThe ICT 1900 series Algol I/O version allowed input from paper tape or punched card.  Paper tape 'full' mode allowed lower case.  Output was to a line printer. The open and close quote characters were represented using '(' and ')' and spaces by %.\n   'BEGIN'\n      WRITE TEXT('('HELLO%WORLD')');\n   'END'\n\nALGOL 68\n\nALGOL 68 code was published with reserved words typically in lowercase, but bolded or underlined.\n begin\n   printf(($gl$,\"Hello, world!\"))\n end\nIn the language of the \"Algol 68 Report\" the input/output facilities were collectively called the \"Transput\".\n\nTimeline of ALGOL special characters",
        "Timeline of ALGOL special characters\n\nThe ALGOLs were conceived at a time when character sets were diverse and evolving rapidly; also, the ALGOLs were defined so that only uppercase letters were required.\n\n1960: IFIP – The Algol 60 language and report included several mathematical symbols which are available on modern computers and operating systems, but, unfortunately, were unsupported on most computing systems at the time. For instance: ×, ÷, ≤, ≥, ≠, ¬, ∨, ∧, ⊂, ≡, ␣ and ⏨.\n\n1961 September: ASCII – The ASCII character set, then in an early stage of development, had the \\ (Back slash) character added to it in order to support ALGOL's boolean operators /\\ and \\/.",
        "1961 September: ASCII – The ASCII character set, then in an early stage of development, had the \\ (Back slash) character added to it in order to support ALGOL's boolean operators /\\ and \\/.\n\n1962: ALCOR – This character set included the unusual \"᛭\" runic cross character for multiplication and the \"⏨\" Decimal Exponent Symbol for floating point notation.\n\n1964: GOST – The 1964 Soviet standard GOST 10859 allowed the encoding of 4-bit, 5-bit, 6-bit and 7-bit characters in ALGOL.",
        "1964: GOST – The 1964 Soviet standard GOST 10859 allowed the encoding of 4-bit, 5-bit, 6-bit and 7-bit characters in ALGOL.\n\n1968: The \"Algol 68 Report\" – used extant ALGOL characters, and further adopted →, ↓, ↑, □, ⌊, ⌈, ⎩, ⎧, ○, ⊥, and ¢ characters which can be found on the IBM 2741 keyboard with typeball (or golf ball) print heads inserted (such as the APL golf ball). These became available in the mid-1960s while ALGOL 68 was being drafted.  The report was translated into Russian, German, French, and Bulgarian, and allowed programming in languages with larger character sets, e.g., Cyrillic alphabet of the Soviet BESM-4. All ALGOL's characters are also part of the Unicode standard and most of them are available in several popular fonts.",
        "2009 October: Unicode – The ⏨ (Decimal Exponent Symbol) for floating point notation was added to Unicode 5.2 for backward compatibility with historic Buran programme ALGOL software.\n\nSee also\n\nReferences\n\nFurther reading\n \n Brian Randell and L. J. Russell, ALGOL 60 Implementation: The Translation and Use of ALGOL 60 Programs on a Computer. Academic Press, 1964. The design of the Whetstone Compiler. One of the early published descriptions of implementing a compiler. See the related papers: Whetstone Algol Revisited, and The Whetstone KDF9 Algol Translator by Brian Randell\n \n \n Revised Report on the Algorithmic Language Algol 60 by Peter Naur, et al. ALGOL definition\n \"The European Side of the Last Phase of the Development of ALGOL 60\" by Peter Naur",
        "External links\n History of ALGOL at the Computer History Museum\n Web enabled ALGOL-F compiler for small experiments\n An online ALGOL compiler\n\nALGOL 60 dialect\n \nArticles with example ALGOL 60 code\nComputer-related introductions in 1958\nProcedural programming languages\nProgramming languages created in 1958\nStructured programming languages\nSystems programming languages"
    ],
    [
        "Algorithm\nIn mathematics and computer science, an algorithm () is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".",
        "In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.",
        "As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\nHistory",
        "History\n\nAncient algorithms \nSince antiquity, step-by-step procedures for solving mathematical problems have been attested. This includes Babylonian mathematics (around 2500 BC), Egyptian mathematics (around 1550 BC), Indian mathematics (around 800 BC and later; e.g. Shulba Sutras, Kerala School, and Brāhmasphuṭasiddhānta), The Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC, e.g. sieve of Eratosthenes and Euclidean algorithm), and Arabic mathematics (9th century, e.g. cryptographic algorithms for code-breaking based on frequency analysis).",
        "Al-Khwārizmī and the term algorithm \nAround 825, Muḥammad ibn Mūsā al-Khwārizmī wrote kitāb al-ḥisāb al-hindī (\"Book of Indian computation\") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī (\"Addition and subtraction in Indian arithmetic\"). Both of these texts are lost in the original Arabic at this time. (However, his other book on algebra remains.)\n\nIn the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu–Arabic numeral system and arithmetic appeared: Liber Alghoarismi de practica arismetrice (attributed to John of Seville) and Liber Algorismi de numero Indorum (attributed to Adelard of Bath). Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi (\"Thus spoke Al-Khwarizmi\").",
        "In 1240, Alexander of Villedieu writes a Latin text titled Carmen de Algorismo. It begins with:\n\nwhich translates to:\n\nThe poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.\n\nEnglish evolution of the word \nAround 1230, the English word algorism is attested and then by Chaucer in 1391. English adopted the French term.\n\nIn the 15th century, under the influence of the Greek word ἀριθμός (arithmos, \"number\"; cf. \"arithmetic\"), the Latin word was altered to algorithmus.\n\nIn 1656, in the English dictionary Glossographia, it says:\n\nAlgorism ([Latin] algorismus) the Art or use of Cyphers, or of numbering by Cyphers; skill in accounting.\n\nAugrime ([Latin] algorithmus) skil in accounting or numbring.",
        "Algorism ([Latin] algorismus) the Art or use of Cyphers, or of numbering by Cyphers; skill in accounting.\n\nAugrime ([Latin] algorithmus) skil in accounting or numbring.\n\nIn 1658, in the first edition of The New World of English Words, it says:\n\nAlgorithme, (a word compounded of Arabick and Spanish,) the art of reckoning by Cyphers.\n\nIn 1706, in the sixth edition of The New World of English Words, it says:\n\nAlgorithm, the Art of computing or reckoning by numbers, which contains the five principle Rules of Arithmetick, viz. Numeration, Addition, Subtraction, Multiplication and Division; to which may be added Extraction of Roots: It is also call'd Logistica Numeralis.",
        "Algorism, the practical Operation in the several Parts of Specious Arithmetick or Algebra; sometimes it is taken for the Practice of Common Arithmetick by the ten Numeral Figures.\n\nIn 1751, in the Young Algebraist's Companion, Daniel Fenning contrasts the terms algorism and algorithm as follows:\n\nAlgorithm signifies the first Principles, and Algorism the practical Part, or knowing how to put the Algorithm in Practice.\n\n, the term algorithm is attested to mean a \"step-by-step procedure\" in English.\n\nIn 1842, in the Dictionary of Science, Literature and Art, it says:\n\nALGORITHM, signifies the art of computing in reference to some particular subject, or in some particular way; as the algorithm of numbers; the algorithm of the differential calculus.\n\nMachine usage",
        "ALGORITHM, signifies the art of computing in reference to some particular subject, or in some particular way; as the algorithm of numbers; the algorithm of the differential calculus.\n\nMachine usage \n\nIn 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.\n\nInformal definition",
        "Informal definition \n\nOne informal definition is \"a set of rules that precisely defines a sequence of operations\", which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure\nor cook-book recipe.\n\nIn general, a program is an algorithm only if it stops eventually—even though infinite loops may sometimes prove desirable.\n\nA prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.\n\n offer an informal meaning of the word \"algorithm\" in the following quotation:",
        "An \"enumerably infinite set\" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that \"creates\" output integers from an arbitrary \"input\" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary \"input variables\" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):",
        "Precise instructions (in a language understood by \"the computer\") for a fast, efficient, \"good\" process that specifies the \"moves\" of \"the computer\" (machine or human, equipped with the necessary internally contained information and capabilities) to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = ... and \"effectively\" produce, in a \"reasonable\" time, output-integer y at a specified place and in a specified format.",
        "The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.",
        "Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\n\nFormalization",
        "Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987), and Gurevich (2000):",
        "Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem.",
        "Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.\n\nFor some of these computational processes, the algorithm must be rigorously defined: and specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).",
        "Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting \"from the top\" and going \"down to the bottom\"—an idea that is described more formally by flow of control.\n\nSo far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one that attempts to describe a task in discrete, \"mechanical\" means. Associated with this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of \"memory\" as a scratchpad. An example of such an assignment can be found below.",
        "For some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.",
        "Expressing algorithms\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.",
        "There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state-transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see Turing machine for more).",
        "Representations of algorithms can be classed into three accepted levels of Turing machine description, as follows:\n 1 High-level description\n \"...prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head.\"\n 2 Implementation description\n \"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function.\"\n 3 Formal description\n Most detailed, \"lowest level\", gives the Turing machine's \"state table\".\n\nFor an example of the simple algorithm \"Add m+n\" described in all three levels, see Examples.\n\nDesign",
        "For an example of the simple algorithm \"Add m+n\" described in all three levels, see Examples.\n\nDesign \n\nAlgorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.\n\nOne of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size of its input increases.",
        "Typical steps in the development of algorithms:\n Problem definition\n Development of a model\n Specification of the algorithm\n Designing an algorithm\n Checking the correctness of the algorithm\n Analysis of algorithm\n Implementation of algorithm\n Program testing\n Documentation preparation\n\nComputer algorithms \n\n\"Elegant\" (compact) programs, \"good\" (fast) programs : The notion of \"simplicity and elegance\" appears informally in Knuth and precisely in Chaitin:\nKnuth: \" ... we want good algorithms in some loosely defined aesthetic sense. One criterion ... is the length of time taken to perform the algorithm .... Other criteria are adaptability of the algorithm to computers, its simplicity, and elegance, etc.\"",
        "Chaitin: \" ... a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does\"\n\nChaitin prefaces his definition with: \"I'll show you can't prove that a program is 'elegant—such a proof would solve the Halting problem (ibid).\n\nAlgorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that \"It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms\".",
        "Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.\n\nComputers (and computors), models of computation: A computer (or human \"computer\") is a restricted type of machine, a \"discrete deterministic mechanical device\" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.",
        "Minsky describes a more congenial variation of Lambek's \"abacus\" model in his \"Very Simple Bases for Computability\". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write \"code\" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO,",
        "with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT. However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is convenient; it can be constructed by initializing a dedicated location to zero e.g. the instruction \" Z ← 0 \"; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.",
        "Simulation of an algorithm: computer (computor) language: Knuth advises the reader that \"the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example\". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computer must know how to take a square root. If they do not, then the algorithm, to be effective, must provide a set of rules for extracting a square root.\n\nThis means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).",
        "This means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).\n\nBut what model should be used for the simulation? Van Emde Boas observes \"even if we base complexity theory on abstract instead of concrete machines, the arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters\". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a \"modulus\" instruction available rather than just subtraction (or worse: just Minsky's \"decrement\").",
        "Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a",
        "in a structured language\". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.",
        "Canonical flowchart symbols: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are shown in the diagram.\n\nExamples",
        "Examples\n\nAlgorithm example \nOne of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:\n\nHigh-level description:\n If there are no numbers in the set, then there is no highest number.\n Assume the first number in the set is the largest number in the set.\n For each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.\n When there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.",
        "(Quasi-)formal description:\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n\n Input: A list of numbers L.\n Output: The largest number in the list L.\n\n if L.size = 0 return null\n largest ← L[0]\n for each item in L, do\n     if item > largest, then\n         largest ← item\n return largest\n\nEuclid's algorithm",
        "if L.size = 0 return null\n largest ← L[0]\n for each item in L, do\n     if item > largest, then\n         largest ← item\n return largest\n\nEuclid's algorithm \n\nIn mathematics, the Euclidean algorithm or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), the largest number that divides them both without a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his Elements (). It is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.",
        "Euclid poses the problem thus: \"Given two numbers not prime to one another, to find their greatest common measure\". He defines \"A number [to be] a multitude composed of units\": a counting number, a positive integer not including zero. To \"measure\" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s. In modern words, remainder r = l − q×s, q being the quotient, or remainder r is the \"modulus\", the integer-fractional part left over after the division.",
        "For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be \"proper\"; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).\n\nEuclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest. While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number \"1\" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.",
        "Computer language for Euclid's algorithm \nOnly a few instruction types are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.\n A location is symbolized by upper case letter(s), e.g. S, A, etc.\n The varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might contain the number l = 3009.\n\nAn inelegant program for Euclid's algorithm",
        "An inelegant program for Euclid's algorithm \n\nThe following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:\n\nINPUT:\n  [Into two locations L and S put the numbers l and s that represent the two lengths]:\n INPUT L, S\n  [Initialize R: make the remaining length r equal to the starting/initial/input length l]:\n R ← L",
        "E0: [Ensure r ≥ s.]\n  [Ensure the smaller of the two numbers is in S and the larger in R]:\n IF R > S THEN\n the contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:\n GOTO step 7\n ELSE\n swap the contents of R and S.\n  L ← R (this first step is redundant, but is useful for later discussion).\n  R ← S\n  S ← L\n\nE1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.\n  IF S > R THEN\n done measuring so\n GOTO 10\n ELSE\n measure again,\n  R ← R − S\n  [Remainder-loop]:\n GOTO 7.",
        "E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.\n  IF R = 0 THEN\n done so\n GOTO step 15\n ELSE\n CONTINUE TO step 11,\n\nE3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.\n  L ← R\n  R ← S\n  S ← L\n  [Repeat the measuring process]:\n GOTO 7\n\nOUTPUT:\n\n  [Done. S contains the greatest common divisor]:\n PRINT S\n\nDONE:\n  HALT, END, STOP.",
        "An elegant program for Euclid's algorithm \n The flowchart of \"Elegant\" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by ←.\n5 REM Euclid's algorithm for greatest common divisor\n6 PRINT \"Type two integers greater than 0\"\n10 INPUT A,B\n20 IF B=0 THEN GOTO 80\n30 IF A > B THEN GOTO 60\n40 LET B=B-A\n50 GOTO 20\n60 LET A=A-B\n70 GOTO 20\n80 PRINT A\n90 END",
        "6 PRINT \"Type two integers greater than 0\"\n10 INPUT A,B\n20 IF B=0 THEN GOTO 80\n30 IF A > B THEN GOTO 60\n40 LET B=B-A\n50 GOTO 20\n60 LET A=A-B\n70 GOTO 20\n80 PRINT A\n90 END\nHow \"Elegant\" works: In place of an outer \"Euclid loop\", \"Elegant\" shifts back and forth between two \"co-loops\", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S (Difference = Minuend − Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the \"sense\" of the subtraction reverses.",
        "The following version can be used with programming languages from the C-family:\n// Euclid's algorithm for greatest common divisor\nint euclidAlgorithm (int A, int B) {\n     A = abs(A);\n     B = abs(B);\n     while (B != 0) {\n          while (A > B) {\n               A = A-B;\n          }\n          B = B-A;\n     }\n     return A;\n}\n\nTesting the Euclid algorithms \nDoes an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.",
        "But \"exceptional cases\" must be identified and tested. Will \"Inelegant\" perform properly when R > S, S > R, R = S? Ditto for \"Elegant\": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? (\"Inelegant\" computes forever in all cases; \"Elegant\" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).",
        "Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an \"extended\" version of Euclid's algorithm, and he proposes \"a general method applicable to proving the validity of any algorithm\". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.",
        "Measuring and improving the Euclid algorithms \nElegance (compactness) versus goodness (speed): With only six core instructions, \"Elegant\" is the clear winner, compared to \"Inelegant\" at thirteen instructions. However, \"Inelegant\" is faster (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: \"Elegant\" does two conditional tests in every subtraction loop, whereas \"Inelegant\" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a \"B = 0?\" test that is needed only after the remainder is computed.\n\nCan the algorithms be improved?: Once the programmer judges a program \"fit\" and \"effective\"—that is, it computes the function intended by its author—then the question becomes, can it be improved?",
        "The compactness of \"Inelegant\" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with \"Elegant\" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it \"more elegant\" than \"Elegant\", at nine steps.",
        "The speed of \"Elegant\" can be improved by moving the \"B=0?\" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now \"Elegant\" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.\n\nAlgorithmic analysis",
        "Algorithmic analysis \n\nIt is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of , using big O notation. At all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. Therefore, it is said to have a space requirement of , if the space required to store the input numbers is not counted, or  if it is counted.",
        "Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost ) outperforms a sequential search (cost  ) when used for table lookups on sorted lists or arrays.\n\nFormal versus empirical",
        "The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely",
        "is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.",
        "Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.\n\nExecution efficiency",
        "Execution efficiency \n\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n\nClassification \nThere are various ways to classify algorithms, each with its own merits.\n\nBy implementation \nOne way to classify algorithms is by implementation means.",
        "Recursion\n A recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\n Serial, parallel or distributed",
        "Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms are algorithms that take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms are algorithms that use multiple machines connected with a computer network. Parallel and distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. For example, a CPU would be an example of a parallel algorithm. The resource consumption in such",
        "the problem into more symmetrical or asymmetrical subproblems and collect the results back together. For example, a CPU would be an example of a parallel algorithm. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.",
        "Deterministic or non-deterministic\n Deterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.\n Exact or approximate",
        "Exact or approximate\n While many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. The approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. Its goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.\n Quantum algorithm",
        "Quantum algorithm\n They run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.",
        "By design paradigm \nAnother way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:",
        "Brute-force or exhaustive search\n Brute force is a method of problem-solving that involves systematically trying every possible option until the optimal solution is found. This approach can be very time consuming, as it requires going through every possible combination of variables. However, it is often used when other methods are not available or too complex. Brute force can be used to solve a variety of problems, including finding the shortest path between two points and cracking passwords.\n Divide and conquer",
        "A divide-and-conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease-and-conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and",
        "solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.",
        "Search and enumeration\n Many problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.\nRandomized algorithm",
        "Randomized algorithm\n Such algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:\n Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.",
        "Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.\n Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.\n Reduction of complexity",
        "Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.\n Reduction of complexity\n This technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.\n Back tracking",
        "Back tracking\n In this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.",
        "Optimization problems \nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:",
        "Linear programming",
        "When searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that",
        "if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.",
        "Dynamic programming",
        "When a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The",
        "The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.",
        "The greedy method\n A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.\nThe heuristic method",
        "In optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation",
        "algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.",
        "By field of study \n\nEvery field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.\n\nFields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.\n\nBy complexity",
        "By complexity \n\nAlgorithms can be classified by the amount of time they need to complete compared to their input size:\n Constant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.\n Logarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.\n Linear time: if the time is proportional to the input size. E.g. the traverse of a list.\n Polynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.\n Exponential time: if the time is an exponential function of the input size. E.g. Brute-force search.",
        "Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.",
        "Continuous algorithms \nThe adjective \"continuous\" when applied to the word \"algorithm\" can mean:\n An algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis; or\n An algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.\n\nAlgorithm = Logic + Control \nIn  logic programming, algorithms are viewed as having both \"a logic component, which specifies the knowledge to be  used in solving problems, and a control component, which determines the problem-solving strategies by means of which that knowledge is used.\"",
        "The Euclidean algorithm illustrates this view of an algorithm.  Here is a logic programming representation, using :- to represent \"if\", and the relation gcd(A, B, C) to represent the function gcd(A, B) = C: \ngcd(A, A, A).\ngcd(A, B, C) :- A > B, gcd(A-B, B, C).\ngcd(A, B, C) :- B > A, gcd(A, B-A, C).\n\nIn the logic programming language  Ciao the gcd relation can be represented directly in functional notation:\ngcd(A, A) := A.\ngcd(A, B) := gcd(A-B, B) :- A > B.\ngcd(A, B) := gcd(A, B-A) :- B > A.\n\nThe Ciao implementation translates the functional notation into a relational representation in   Prolog, extracting the embedded subtractions, A-B and B-A, as separate conditions:\ngcd(A, A, A).\ngcd(A, B, C) :- A > B, A' is A-B, gcd(A', B, C).\ngcd(A, B, C) :- B > A, B' is B-A, gcd(A, B, C).",
        "The resulting program has a purely  logical (and  \"declarative\") reading, as a recursive (or inductive) definition, which is independent of how the logic is used to solve problems:\nThe gcd of A and A is A.\nThe gcd of A and B is C, if A > B and A' is A-B and the gcd of A' and B is C.\nThe gcd of A and B is C, if B > A and B' is B-A and the gcd of A and B' is C.",
        "Different problem-solving strategies turn the logic into different algorithms. In theory, given a pair of integers A and B, forward (or \"bottom-up\") reasoning could be used to generate all instances of the gcd relation, terminating when the desired gcd of A and B is generated. Of course, forward reasoning is entirely useless in this case. But in other cases, such as the definition of the Fibonacci sequence and Datalog, forward reasoning can be an efficient problem solving strategy. (See for example the logic program for computing fibonacci numbers in  Algorithm = Logic + Control).",
        "In contrast with the inefficiency of forward reasoning in this example,  backward (or \"top-down\") reasoning using SLD resolution turns the logic into the Euclidean algorithm:\nTo find the gcd C of two given numbers A and B:\nIf A = B, then C = A.\nIf A > B, then let A' = A-B and find the gcd of A' and B, which is C.\nIf B > A, then let B' = B-A and find the gcd of A and B', which is C.\n\nOne of the advantages of the logic programming representation of the algorithm is that its purely logical reading makes it easier to verify that the algorithm is correct relative to the standard non-recursive definition of gcd. Here is the standard definition written in Prolog:",
        "gcd(A, B, C) :- divides(C, A), divides(C, B),\n    forall((divides(D, A), divides(D, B)), D =< C).\n           \n divides(C, Number) :- \n   between(1, Number, C), 0 is Number mod C.",
        "This definition, which is the specification of the Euclidean algorithm, is also executable in Prolog: Backward reasoning treats the specification as the brute-force algorithm that iterates through all of the integers C between 1 and A, checking whether C divides both A and B, and then for each such C iterates again through all of the integers D between 1 and A, until it finds a C such that C is greater than or equal to all of the D that also divide both A and B. Although this algorithm is hopelessly inefficient, it shows that formal specifications can often be written in logic programming form, and they can be executed by Prolog, to check that they correctly represent informal requirements.\n\nLegal issues",
        "Legal issues \n\nAlgorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial, and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent.\n\nAdditionally, some cryptographic algorithms have export restrictions (see export of cryptography).",
        "Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).\n\nHistory: Development of the notion of \"algorithm\"\n\nAncient Near East \nThe earliest evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to  described the earliest division algorithm. During the Hammurabi dynasty , Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.",
        "Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus . Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus, and the Euclidean algorithm, which was first described in Euclid's Elements ().",
        "Discrete and distinguishable symbols \nTally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.",
        "Manipulation of symbols as \"place holders\" for numbers: algebra \nMuhammad ibn Mūsā al-Khwārizmī, a Persian mathematician, wrote the Al-jabr in the 9th century. The terms \"algorism\" and \"algorithm\" are derived from the name al-Khwārizmī, while the term \"algebra\" is derived from the book Al-jabr. In Europe, the word \"algorithm\" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz's notion of the calculus ratiocinator ():",
        "Cryptographic algorithms \nThe first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.",
        "Mechanical contrivances with discrete states",
        "The clock: Bolter credits the invention of the weight-driven clock as \"The key invention [of Europe in the Middle Ages]\", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called \"history's first programmer\" as a result, though a full implementation of Babbage's",
        "the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called \"history's first programmer\" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.",
        "Logical machines 1870 – Stanley Jevons' \"logical abacus\" and \"logical machine\": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple \"abacus\" of \"slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine\" His machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc.] ...\". With this machine he could analyze a \"syllogism or any other simple logical",
        "machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc.] ...\". With this machine he could analyze a \"syllogism or any other simple logical argument\".",
        "This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: \"I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines\"; see more at Algorithm characterizations. But not to be outdone he too presented \"a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can",
        "to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".",
        "Jacquard loom, Hollerith punch cards, telegraphy and telephony – the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. By the late 19th century the ticker tape () was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter () with its punched-paper use of Baudot code on tape.",
        "Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the \"burdensome' use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".\n\nThe mathematician Martin Davis observes the particular importance of the electromechanical relay (with its two \"binary states\" open and closed):\n It was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned.\"",
        "Mathematics during the 19th century up to the mid-20th century \nSymbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was \"the first attempt at an axiomatization of mathematics in a symbolic language\".",
        "But Heijenoort gives Frege (1879) this kudos: Frege's is \"perhaps the most important single work ever written in logic. ... in which we see a formula language', that is a lingua characterica, a language written with special symbols, \"for pure thought\", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules\". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).",
        "The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.",
        "Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an \"effective method\" or \"effective calculation\" or \"effective calculability\" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus a finely honed definition of \"general recursion\" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of",
        "that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his \"a- [automatic-] machine\"—in effect almost identical to Post's \"formulation\", J. Barkley Rosser's definition of \"effective method\" in terms of \"a machine\". Kleene's proposal of a precursor to \"Church thesis\" that he called \"Thesis I\", and a few years later Kleene's renaming his Thesis \"Church's Thesis\" and proposing \"Turing's Thesis\".",
        "Emil Post (1936) and Alan Turing (1936–37, 1939) \nEmil Post (1936) described the actions of a \"computer\" (human being) as follows:\n\"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.\n\nHis symbol space would be\n\"a two-way infinite sequence of spaces or boxes ... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time. ... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.",
        "\"One box is to be singled out and called the starting point. ... a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise, the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes...\n\n\"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]\". See more at Post–Turing machine",
        "Alan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: \"Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical. Given the prevalence at the time of Morse code, telegraphy, ticker tape machines, and teletypewriters, it is quite possible that all were influences on Turing during his youth.",
        "Turing—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and \"states of mind\". But he continues a step further and creates a machine as a model of computation of numbers.\n\n\"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite...",
        "\"The behavior of the computer at any moment is determined by the symbols which he is observing, and his \"state of mind\" at that moment. We may suppose that there is a bound B to the number of symbols or squares that the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite...\n\n\"Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided.\"",
        "Turing's reduction yields the following:\n\"The simple operations must therefore include:\n\"(a) Changes of the symbol on one of the observed squares\n\"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.\n\"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:\n\"(A) A possible change (a) of symbol together with a possible change of state of mind.\n\"(B) A possible change (b) of observed squares, together with a possible change of state of mind\"\n\n\"We may now construct a machine to do the work of this computer.\"",
        "A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:",
        "\"A function is said to be \"effectively calculable\" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition ... [he discusses the history of the definition pretty much as presented above with respect to Gödel, Herbrand, Kleene, Church, Turing, and Post] ... We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability †",
        "in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability † with effective calculability...",
        "\"† We shall use the expression \"computable function\" to mean a function calculable by a machine, and we let \"effectively calculable\" refer to the intuitive idea without particular identification with any one of these definitions\".",
        "J. B. Rosser (1939) and S. C. Kleene (1943) \nJ. Barkley Rosser defined an \"effective [mathematical] method\" in the following manner (italicization added):",
        "Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the",
        "and (later) reading the answer. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one.\" (Rosser 1939:225–226)",
        "Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular, Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and Gödel and their use of recursion, in particular, Gödel's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.",
        "Stephen C. Kleene defined as his now-famous \"Thesis I\" known as the Church–Turing thesis. But he did this in the following context (boldface in original):\n\"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, \"yes\" or \"no,\" to the question, \"is the predicate value true?\"\" (Kleene 1943:273)",
        "History after 1950 \nA number of efforts have been directed toward further refinement of the definition of \"algorithm\", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.\n\nSee also",
        "See also \n\n Abstract machine\n ALGOL\n Algorithm engineering\n Algorithm characterizations\n Algorithmic bias\n Algorithmic composition\n Algorithmic entities\n Algorithmic synthesis\n Algorithmic technique\n Algorithmic topology\n Garbage in, garbage out\n Introduction to Algorithms (textbook)\n Government by algorithm\n List of algorithms\n List of algorithm general topics\n Regulation of algorithms\n Theory of computation\n Computability theory\n Computational complexity theory\n Computational mathematics\n\nNotes\n\nBibliography",
        "Bell, C. Gordon and Newell, Allen (1971), Computer Structures: Readings and Examples, McGraw–Hill Book Company, New York. .\n  Includes a bibliography of 56 references.\n , \n : cf. Chapter 3 Turing machines where they discuss \"certain enumerable sets not effectively (mechanically) enumerable\".\n \n Campagnolo, M.L., Moore, C., and Costa, J.F. (2000) An analog characterization of the subrecursive functions. In Proc. of the 4th Conference on Real Numbers and Computers, Odense University, pp. 91–109\n  Reprinted in The Undecidable, p. 89ff. The first expression of \"Church's Thesis\". See in particular page 100 (The Undecidable) where he defines the notion of \"effective calculability\" in terms of \"an algorithm\", and he uses the word \"terminates\", etc.",
        "Reprinted in The Undecidable, p. 110ff. Church shows that the Entscheidungsproblem is unsolvable in about 3 pages of text and 3 pages of footnotes.\n \n  Davis gives commentary before each article. Papers of Gödel, Alonzo Church, Turing, Rosser, Kleene, and Emil Post are included; those cited in the article are listed here by author's name.\n  Davis offers concise biographies of Leibniz, Boole, Frege, Cantor, Hilbert, Gödel and Turing with von Neumann as the show-stealing villain. Very brief bios of Joseph-Marie Jacquard, Babbage, Ada Lovelace, Claude Shannon, Howard Aiken, etc.\n \n \n \n , \n Yuri Gurevich, Sequential Abstract State Machines Capture Sequential Algorithms, ACM Transactions on Computational Logic, Vol 1, no 1 (July 2000), pp. 77–111. Includes bibliography of 33 sources.",
        "Yuri Gurevich, Sequential Abstract State Machines Capture Sequential Algorithms, ACM Transactions on Computational Logic, Vol 1, no 1 (July 2000), pp. 77–111. Includes bibliography of 33 sources.\n , 3rd edition 1976[?],  (pbk.)\n , . Cf. Chapter \"The Spirit of Truth\" for a history leading to, and a discussion of, his proof.\n  Presented to the American Mathematical Society, September 1935. Reprinted in The Undecidable, p. 237ff. Kleene's definition of \"general recursion\" (known now as mu-recursion) was used by Church in his 1935 paper An Unsolvable Problem of Elementary Number Theory that proved the \"decision problem\" to be \"undecidable\" (i.e., a negative result).",
        "Reprinted in The Undecidable, p. 255ff. Kleene refined his definition of \"general recursion\" and proceeded in his chapter \"12. Algorithmic theories\" to posit \"Thesis I\" (p. 274); he would later repeat this thesis (in Kleene 1952:300) and name it \"Church's Thesis\"(Kleene 1952:317) (i.e., the Church thesis).\n \n \n \n Kosovsky, N.K. Elements of Mathematical Logic and its Application to the theory of Subrecursive Algorithms, LSU Publ., Leningrad, 1981",
        "Kosovsky, N.K. Elements of Mathematical Logic and its Application to the theory of Subrecursive Algorithms, LSU Publ., Leningrad, 1981\n \n A.A. Markov (1954) Theory of algorithms. [Translated by Jacques J. Schorr-Kon and PST staff] Imprint Moscow, Academy of Sciences of the USSR, 1954 [i.e., Jerusalem, Israel Program for Scientific Translations, 1961; available from the Office of Technical Services, U.S. Dept. of Commerce, Washington] Description 444 p. 28 cm. Added t.p. in Russian Translation of Works of the Mathematical Institute, Academy of Sciences of the USSR, v. 42. Original title: Teoriya algerifmov. [QA248.M2943 Dartmouth College library. U.S. Dept. of Commerce, Office of Technical Services, number OTS .]",
        "Minsky expands his \"...idea of an algorithm – an effective procedure...\" in chapter 5.1 Computability, Effective Procedures and Algorithms. Infinite machines.\n  Reprinted in The Undecidable, pp. 289ff. Post defines a simple algorithmic-like process of a man writing marks or erasing marks and going from box to box and eventually halting, as he follows a list of simple instructions. This is cited by Kleene as one source of his \"Thesis I\", the so-called Church–Turing thesis.",
        "Reprinted in The Undecidable, p. 223ff. Herein is Rosser's famous definition of \"effective method\": \"...a method each step of which is precisely predetermined and which is certain to produce the answer in a finite number of steps... a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer\" (p. 225–226, The Undecidable)\n \n \n \n \n  Cf. in particular the first chapter titled: Algorithms, Turing Machines, and Programs. His succinct informal definition: \"...any sequence of instructions that can be obeyed by a robot, is called an algorithm\" (p. 4).",
        ". Corrections, ibid, vol. 43(1937) pp. 544–546. Reprinted in The Undecidable, p. 116ff. Turing's famous paper completed as a Master's dissertation while at King's College Cambridge UK.\n  Reprinted in The Undecidable, pp. 155ff. Turing's paper that defined \"the oracle\" was his PhD thesis while at Princeton.\n United States Patent and Trademark Office (2006), 2106.02 **>Mathematical Algorithms: 2100 Patentability, Manual of Patent Examining Procedure (MPEP). Latest revision August 2006",
        "Zaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76–99. https://doi.org/10.2307/3027363\n\nFurther reading\n\n \n \n \n \n \n \n Knuth, Donald E. (2000). Selected Papers on Analysis of Algorithms . Stanford, California: Center for the Study of Language and Information.\n Knuth, Donald E. (2010). Selected Papers on Design of Algorithms . Stanford, California: Center for the Study of Language and Information.\n\nExternal links",
        "External links\n\n \n \n \n Dictionary of Algorithms and Data Structures – National Institute of Standards and Technology\n Algorithm repositories\n The Stony Brook Algorithm Repository – State University of New York at Stony Brook\n Collected Algorithms of the ACM – Associations for Computing Machinery\n The Stanford GraphBase  – Stanford University\n\n \nArticles with example pseudocode\nMathematical logic\nTheoretical computer science"
    ],
    [
        "Algorithms for calculating variance\nAlgorithms for calculating variance play a major role in computational statistics. A key difficulty in the design of good algorithms for this problem is that formulas for the variance may involve sums of squares, which can lead to numerical instability as well as to arithmetic overflow when dealing with large values.\n\nNaïve algorithm\nA formula for calculating the variance of an entire population of size N is:\n\nUsing Bessel's correction to calculate an unbiased estimate of the population variance from a finite sample of n observations, the formula is:\n\nTherefore, a naïve algorithm to calculate the estimated variance is given by the following:\n\n Let \n For each datum :",
        "Therefore, a naïve algorithm to calculate the estimated variance is given by the following:\n\n Let \n For each datum :\n \n \n \n \n\nThis algorithm can easily be adapted to compute the variance of a finite population: simply divide by n instead of n − 1 on the last line.\n\nBecause  and  can be very similar numbers, cancellation can lead to the precision of the result to be much less than the inherent precision of the floating-point arithmetic used to perform the computation.  Thus this algorithm should not be used in practice, and several alternate, numerically stable, algorithms have been proposed. This is particularly bad if the standard deviation is small relative to the mean.\n\nComputing shifted data",
        "Computing shifted data\n\nThe variance is invariant with respect to changes in a location parameter, a property which can be used to avoid the catastrophic cancellation in this formula.\n\nwith  any constant, which leads to the new formula\n\nthe closer  is to the mean value the more accurate the result will be, but just choosing a value inside the\nsamples range will guarantee the desired stability. If the values  are small then there are no problems with the sum of its squares, on the contrary, if they are large it necessarily means that the variance is large as well. In any case the second term in the formula is always smaller than the first one therefore no cancellation may occur.\n\nIf just the first sample is taken as  the algorithm can be written in Python programming language as",
        "If just the first sample is taken as  the algorithm can be written in Python programming language as\n\ndef shifted_data_variance(data):\n    if len(data) < 2:\n        return 0.0\n    K = data[0]\n    n = Ex = Ex2 = 0.0\n    for x in data:\n        n += 1\n        Ex += x - K\n        Ex2 += (x - K) ** 2\n    variance = (Ex2 - Ex**2 / n) / (n - 1)\n    # use n instead of (n-1) if want to compute the exact variance of the given data\n    # use (n-1) if data are samples of a larger population\n    return variance\n\nThis formula also facilitates the incremental computation that can be expressed as\nK = Ex = Ex2 = 0.0\nn = 0\n\ndef add_variable(x):\n    global K, n, Ex, Ex2\n    if n == 0:\n        K = x\n    n += 1\n    Ex += x - K\n    Ex2 += (x - K) ** 2",
        "def add_variable(x):\n    global K, n, Ex, Ex2\n    if n == 0:\n        K = x\n    n += 1\n    Ex += x - K\n    Ex2 += (x - K) ** 2\n\ndef remove_variable(x):\n    global K, n, Ex, Ex2\n    n -= 1\n    Ex -= x - K\n    Ex2 -= (x - K) ** 2\n\ndef get_mean():\n    global K, n, Ex\n    return K + Ex / n\n\ndef get_variance():\n    global n, Ex, Ex2\n    return (Ex2 - Ex**2 / n) / (n - 1)\n\nTwo-pass algorithm\nAn alternative approach, using a different formula for the variance, first computes the sample mean,\n\nand then computes the sum of the squares of the differences from the mean,\n\nwhere s is the standard deviation.  This is given by the following code:\n\ndef two_pass_variance(data):\n    n = len(data)\n    mean = sum(data) / n\n    variance = sum([(x - mean) ** 2 for x in data]) / (n - 1)\n    return variance",
        "def two_pass_variance(data):\n    n = len(data)\n    mean = sum(data) / n\n    variance = sum([(x - mean) ** 2 for x in data]) / (n - 1)\n    return variance\n\nThis algorithm is numerically stable if n is small. However, the results of both of these simple algorithms (\"naïve\" and \"two-pass\") can depend inordinately on the ordering of the data and can give poor results for very large data sets due to repeated roundoff error in the accumulation of the sums. Techniques such as compensated summation can be used to combat this error to a degree.",
        "Welford's online algorithm\nIt is often useful to be able to compute the variance in a single pass, inspecting each value  only once; for example, when the data is being collected without enough storage to keep all the values, or when costs of memory access dominate those of computation.  For such an online algorithm, a recurrence relation is required between quantities from which the required statistics can be calculated in a numerically stable fashion.\n\nThe following formulas can be used to update the mean and (estimated) variance of the sequence, for an additional element xn. Here,  denotes the sample mean of the first n samples ,  their biased sample variance, and  their unbiased sample variance.",
        "These formulas suffer from numerical instability , as they repeatedly subtract a small number from a big number which scales with n. A better quantity for updating is the sum of squares of differences from the current mean, , here denoted :\n\n \n\nThis algorithm was found by Welford, and it has been thoroughly analyzed. It is also common to denote  and .\n\nAn example Python implementation for Welford's algorithm is given below.",
        "This algorithm was found by Welford, and it has been thoroughly analyzed. It is also common to denote  and .\n\nAn example Python implementation for Welford's algorithm is given below.\n\n# For a new value new_value, compute the new count, new mean, the new M2.\n# mean accumulates the mean of the entire dataset\n# M2 aggregates the squared distance from the mean\n# count aggregates the number of samples seen so far\ndef update(existing_aggregate, new_value):\n    (count, mean, M2) = existing_aggregate\n    count += 1\n    delta = new_value - mean\n    mean += delta / count\n    delta2 = new_value - mean\n    M2 += delta * delta2\n    return (count, mean, M2)",
        "# Retrieve the mean, variance and sample variance from an aggregate\ndef finalize(existing_aggregate):\n    (count, mean, M2) = existing_aggregate\n    if count < 2:\n        return float(\"nan\")\n    else:\n        (mean, variance, sample_variance) = (mean, M2 / count, M2 / (count - 1))\n        return (mean, variance, sample_variance)\n\nThis algorithm is much less prone to loss of precision due to catastrophic cancellation, but might not be as efficient because of the division operation inside the loop.  For a particularly robust two-pass algorithm for computing the variance, one can first compute and subtract an estimate of the mean, and then use this algorithm on the residuals.\n\nThe parallel algorithm below illustrates how to merge multiple sets of statistics calculated online.",
        "The parallel algorithm below illustrates how to merge multiple sets of statistics calculated online.\n\nWeighted incremental algorithm\nThe algorithm can be extended to handle unequal sample weights, replacing the simple counter n with the sum of weights seen so far.  West (1979) suggests this incremental algorithm:\n\ndef weighted_incremental_variance(data_weight_pairs):\n    w_sum = w_sum2 = mean = S = 0\n\n    for x, w in data_weight_pairs:\n        w_sum = w_sum + w\n        w_sum2 = w_sum2 + w**2\n        mean_old = mean\n        mean = mean_old + (w / w_sum) * (x - mean_old)\n        S = S + w * (x - mean_old) * (x - mean)",
        "population_variance = S / w_sum\n    # Bessel's correction for weighted samples\n    # Frequency weights\n    sample_frequency_variance = S / (w_sum - 1)\n    # Reliability weights\n    sample_reliability_variance = S / (w_sum - w_sum2 / w_sum)\n\nParallel algorithm\nChan et al. note that Welford's online algorithm detailed above is a special case of an algorithm that works for combining arbitrary sets  and :\n.\nThis may be useful when, for example, multiple processing units may be assigned to discrete parts of the input.",
        "Chan's method for estimating the mean is numerically unstable when  and both are large, because the numerical error in  is not scaled down in the way that it is in the  case. In such cases, prefer .\ndef parallel_variance(n_a, avg_a, M2_a, n_b, avg_b, M2_b):\n    n = n_a + n_b\n    delta = avg_b - avg_a\n    M2 = M2_a + M2_b + delta**2 * n_a * n_b / n\n    var_ab = M2 / (n - 1)\n    return var_ab\nThis can be generalized to allow parallelization with AVX, with GPUs, and computer clusters, and to covariance.",
        "Example\nAssume that all floating point operations use standard IEEE 754 double-precision arithmetic. Consider the sample (4, 7, 13, 16) from an infinite population. Based on this sample, the estimated population mean is 10, and the unbiased estimate of population variance is 30.  Both the naïve algorithm and two-pass algorithm compute these values correctly.\n\nNext consider the sample (, , , ), which gives rise to the same estimated variance as the first sample.  The two-pass algorithm computes this variance estimate correctly, but the naïve algorithm returns 29.333333333333332 instead of 30.",
        "While this loss of precision may be tolerable and viewed as a minor flaw of the naïve algorithm, further increasing the offset makes the error catastrophic.  Consider the sample (, , , ).  Again the estimated population variance of 30 is computed correctly by the two-pass algorithm, but the naïve algorithm now computes it as −170.66666666666666.  This is a serious problem with naïve algorithm and is due to catastrophic cancellation in the subtraction of two similar numbers at the final stage of the algorithm.\n\nHigher-order statistics\nTerriberry extends Chan's formulae to calculating the third and fourth central moments, needed for example when estimating skewness and kurtosis:\n\nHere the  are again the sums of powers of differences from the mean , giving",
        "Here the  are again the sums of powers of differences from the mean , giving\n \n\nFor the incremental case (i.e., ), this simplifies to:\n \n\nBy preserving the value , only one division operation is needed and the higher-order statistics can thus be calculated for little incremental cost.\n\nAn example of the online algorithm for kurtosis implemented as described is:\ndef online_kurtosis(data):\n    n = mean = M2 = M3 = M4 = 0",
        "An example of the online algorithm for kurtosis implemented as described is:\ndef online_kurtosis(data):\n    n = mean = M2 = M3 = M4 = 0\n\n    for x in data:\n        n1 = n\n        n = n + 1\n        delta = x - mean\n        delta_n = delta / n\n        delta_n2 = delta_n**2\n        term1 = delta * delta_n * n1\n        mean = mean + delta_n\n        M4 = M4 + term1 * delta_n2 * (n**2 - 3*n + 3) + 6 * delta_n2 * M2 - 4 * delta_n * M3\n        M3 = M3 + term1 * delta_n * (n - 2) - 3 * delta_n * M2\n        M2 = M2 + term1\n\n    # Note, you may also calculate variance using M2, and skewness using M3\n    # Caution: If all the inputs are the same, M2 will be 0, resulting in a division by 0.\n    kurtosis = (n * M4) / (M2**2) - 3\n    return kurtosis",
        "Pébaÿ\nfurther extends these results to arbitrary-order central moments, for the incremental and the pairwise cases, and subsequently Pébaÿ et al.\nfor weighted and compound moments. One can also find there similar formulas for covariance.",
        "Choi and Sweetman",
        "offer two alternative methods to compute the skewness and kurtosis, each of which can save substantial computer memory requirements and CPU time in certain applications. The first approach is to compute the statistical moments by separating the data into bins and then computing the moments from the geometry of the resulting histogram, which effectively becomes a one-pass algorithm for higher moments. One benefit is that the statistical moment calculations can be carried out to arbitrary accuracy such that the computations can be tuned to the precision of, e.g., the data storage format or the original measurement hardware.  A relative histogram of a random variable can be constructed in the conventional way: the range of potential values is divided into bins and the number of occurrences",
        "original measurement hardware.  A relative histogram of a random variable can be constructed in the conventional way: the range of potential values is divided into bins and the number of occurrences within each bin are counted and plotted such that the area of each rectangle equals the portion of the sample values within that bin:",
        "where  and  represent the frequency and the relative frequency at bin  and  is the total area of the histogram. After this normalization, the  raw moments and central moments of  can be calculated from the relative histogram:\n\n \n\n \n\nwhere the superscript  indicates the moments are calculated from the histogram. For constant bin width  these two expressions can be simplified using :",
        "where the superscript  indicates the moments are calculated from the histogram. For constant bin width  these two expressions can be simplified using :\n\n \n\n \n\nThe second approach from Choi and Sweetman is an analytical methodology to combine statistical moments from individual segments of a time-history such that the resulting overall moments are those of the complete time-history. This methodology could be used for parallel computation of statistical moments with subsequent combination of those moments, or for combination of statistical moments computed at sequential times.\n\nIf  sets of statistical moments are known:\n for , then each  can\nbe expressed in terms of the equivalent  raw moments:",
        "If  sets of statistical moments are known:\n for , then each  can\nbe expressed in terms of the equivalent  raw moments:\n\n \n\nwhere  is generally taken to be the duration of the  time-history, or the number of points if  is constant.\n\nThe benefit of expressing the statistical moments in terms of  is that the  sets can be combined by addition, and there is no upper limit on the value of .\n\n \n\nwhere the subscript  represents the concatenated time-history or combined . These combined values of  can then be inversely transformed into raw moments representing the complete concatenated time-history",
        "Known relationships between the raw moments () and the central moments ()\nare then used to compute the central moments of the concatenated time-history.  Finally, the statistical moments of the concatenated history are computed from the central moments:\n\nCovariance\nVery similar algorithms can be used to compute the covariance.\n\nNaïve algorithm\nThe naïve algorithm is\n\nFor the algorithm above, one could use the following Python code:\ndef naive_covariance(data1, data2):\n    n = len(data1)\n    sum1 = sum(data1)\n    sum2 = sum(data2)\n    sum12 = sum([i1 * i2 for i1, i2 in zip(data1, data2)])\n\n    covariance = (sum12 - sum1 * sum2 / n) / n\n    return covariance",
        "covariance = (sum12 - sum1 * sum2 / n) / n\n    return covariance\n\nWith estimate of the mean\nAs for the variance, the covariance of two random variables is also shift-invariant, so given any two constant values  and  it can be written:\n\nand again choosing a value inside the range of values will stabilize the formula against catastrophic cancellation as well as make it more robust against big sums. Taking the first value of each data set, the algorithm can be written as:\n\ndef shifted_data_covariance(data_x, data_y):\n    n = len(data_x)\n    if n < 2:\n        return 0\n    kx = data_x[0]\n    ky = data_y[0]\n    Ex = Ey = Exy = 0\n    for ix, iy in zip(data_x, data_y):\n        Ex += ix - kx\n        Ey += iy - ky\n        Exy += (ix - kx) * (iy - ky)\n    return (Exy - Ex * Ey / n) / n",
        "Two-pass\nThe two-pass algorithm first computes the sample means, and then the covariance:\n\nThe two-pass algorithm may be written as:\ndef two_pass_covariance(data1, data2):\n    n = len(data1)\n    mean1 = sum(data1) / n\n    mean2 = sum(data2) / n\n\n    covariance = 0\n    for i1, i2 in zip(data1, data2):\n        a = i1 - mean1\n        b = i2 - mean2\n        covariance += a * b / n\n    return covariance\n\nA slightly more accurate compensated version performs the full naive algorithm on the residuals.  The final sums  and  should be zero, but the second pass compensates for any small error.\n\nOnline\n\nA stable one-pass algorithm exists, similar to the online algorithm for computing the variance, that computes co-moment :",
        "Online\n\nA stable one-pass algorithm exists, similar to the online algorithm for computing the variance, that computes co-moment :\n\nThe apparent asymmetry in that last equation is due to the fact that , so both update terms are equal to .  Even greater accuracy can be achieved by first computing the means, then using the stable one-pass algorithm on the residuals.\n\nThus the covariance can be computed as\n\ndef online_covariance(data1, data2):\n    meanx = meany = C = n = 0\n    for x, y in zip(data1, data2):\n        n += 1\n        dx = x - meanx\n        meanx += dx / n\n        meany += (y - meany) / n\n        C += dx * (y - meany)\n\n    population_covar = C / n\n    # Bessel's correction for sample variance\n    sample_covar = C / (n - 1)",
        "population_covar = C / n\n    # Bessel's correction for sample variance\n    sample_covar = C / (n - 1)\n\nA small modification can also be made to compute the weighted covariance:\n\ndef online_weighted_covariance(data1, data2, data3):\n    meanx = meany = 0\n    wsum = wsum2 = 0\n    C = 0\n    for x, y, w in zip(data1, data2, data3):\n        wsum += w\n        wsum2 += w * w\n        dx = x - meanx\n        meanx += (w / wsum) * dx\n        meany += (w / wsum) * (y - meany)\n        C += w * dx * (y - meany)\n\n    population_covar = C / wsum\n    # Bessel's correction for sample variance\n    # Frequency weights\n    sample_frequency_covar = C / (wsum - 1)\n    # Reliability weights\n    sample_reliability_covar = C / (wsum - wsum2 / wsum)",
        "Likewise, there is a formula for combining the covariances of two sets that can be used to parallelize the computation:\n\nWeighted batched version\n\nA version of the weighted online algorithm that does batched updated also exists: let  denote the weights, and write\n\nThe covariance can then be computed as\n\nSee also\nKahan summation algorithm\nSquared deviations from the mean\nYamartino method\n\nReferences\n\nExternal links\n \n\nStatistical algorithms\nStatistical deviation and dispersion\nArticles with example pseudocode\nArticles with example Python (programming language) code"
    ],
    [
        "Ality\nAlityPC is a Hong Kong technology company that designs and develops technology products for the home. Their products include digital photo frames and high-tech lifestyle gadgets.\n\nThe Pixxa model of photo frame features a clock and calendar, a mirrored surface when photos are not being displayed, and a touch-screen user interface.\n\nAlityPC also developed the first patented Wi-Fi digital photo frame with a peer-to-peer communication protocol. Most Wi-Fi photo frames available in the market now are one-way downstream. With AlityPC's peer-to-peer integration, it allows the product to be more interactive. Users can send photo messages to the photo frame via the Internet, similar to most IM application in the market. It can also synchronize with Google calendar and download live news.",
        "Ality Online is an online service provided by AlityPC, it's the central portal for friends and family to send photo messages and greeting cards to Wireless Pixxa owners. Users can invite their friends and family to join Ality Online with no service charge. Photo Frame owners can also customize their online photo frame settings, for example, Google Calendar, Flickr, Picasa, Stock and live news.\n\nAlityPC also developed the slimmest LCD photo frame in the world, Pixxa Slim.\n\nReferences\n\nExternal links \nAlity Global Website\n Ality Online Web Service\n\nTechnology companies of Hong Kong\nTechnology companies established in 2005"
    ],
    [
        "Alpha compositing\nIn computer graphics, alpha compositing or alpha blending is the process of combining one image with a background to create the appearance of partial or full transparency. It is often useful to render picture elements (pixels) in separate passes or layers and then combine the resulting 2D images into a single, final image called the composite. Compositing is used extensively in film when combining computer-rendered image elements with live footage. Alpha blending is also used in 2D computer graphics to put rasterized foreground elements over a background.",
        "In order to combine the picture elements of the images correctly, it is necessary to keep an associated matte for each element in addition to its color. This matte layer contains the coverage information—the shape of the geometry being drawn—making it possible to distinguish between parts of the image where something was drawn and parts that are empty.\n\nAlthough the most basic operation of combining two images is to put one over the other, there are many operations, or blend modes, that are used.",
        "Although the most basic operation of combining two images is to put one over the other, there are many operations, or blend modes, that are used.\n\nHistory \nThe concept of an alpha channel was introduced by Alvy Ray Smith and  in the late 1970s at the New York Institute of Technology Computer Graphics Lab. Bruce A. Wallace derived the same straight over operator based on a physical reflectance/transmittance model in 1981. A 1984 paper by Thomas Porter and Tom Duff introduced premultiplied alpha using a geometrical approach.",
        "The use of the term alpha is explained by Smith as follows: \"We called it that because of the classic linear interpolation formula  that uses the Greek letter  (alpha) to control the amount of interpolation between, in this case, two images A and B\". That is, when compositing image A atop image B, the value of  in the formula is taken directly from A's alpha channel.",
        "Description\nIn a 2D image a color combination is stored for each picture element (pixel), often a combination of red, green and blue (RGB). When alpha compositing is in use, each pixel has an additional numeric value stored in its alpha channel, with a value ranging from 0 to 1. A value of 0 means that the pixel is fully transparent and the color in the pixel beneath will show through. A value of 1 means that the pixel is fully opaque.",
        "With the existence of an alpha channel, it is possible to express compositing image operations using a compositing algebra. For example, given two images A and B, the most common compositing operation is to combine the images so that A appears in the foreground and B appears in the background. This can be expressed as A over B. In addition to over, Porter and Duff defined the compositing operators in, held out by (the phrase refers to holdout matting and is usually abbreviated out), atop, and xor (and the reverse operators rover, rin, rout, and ratop) from a consideration of choices in blending the colors of two pixels when their coverage is, conceptually, overlaid orthogonally:\n\nAs an example, the over operator can be accomplished by applying the following formula to each pixel:",
        "As an example, the over operator can be accomplished by applying the following formula to each pixel:\n\nHere ,  and  stand for the color components of the pixels in the result, image A and image B respectively, applied to each color channel (red/green/blue) individually, whereas ,  and  are the alpha values of the respective pixels.\n\nThe over operator is, in effect, the normal painting operation (see Painter's algorithm). The in and out operators are the alpha compositing equivalent of clipping. The two use only the alpha channel of the second image and ignore the color components. In addition, plus defines additive blending.\n\nStraight versus premultiplied",
        "Straight versus premultiplied \n\nIf an alpha channel is used in an image, there are two common representations that are available: straight (unassociated) alpha and premultiplied (associated) alpha.\n With straight alpha, the RGB components represent the color of the object or pixel, disregarding its opacity. This is the method implied by the over operator in the previous section.\n With premultiplied alpha, the RGB components represent the emission of the object or pixel, and the alpha represents the occlusion. The over operator then becomes:\n\nComparison",
        "Comparison \n\nThe most significant advantage of premultiplied alpha is that it allows for correct blending, interpolation, and filtering. Ordinary interpolation without premultiplied alpha leads to RGB information leaking out of fully transparent (A=0) regions, even though this RGB information is ideally invisible. When interpolating or filtering images with abrupt borders between transparent and opaque regions, this can result in borders of colors that were not visible in the original image. Errors also occur in areas of semitransparency because the RGB components are not correctly weighted, giving incorrectly high weighting to the color of the more transparent (lower alpha) pixels.",
        "Premultiplied alpha may also be used to allow regions of regular alpha blending (e.g. smoke) and regions with additive blending mode (e.g. flame and glitter effects) to be encoded within the same image. This is represented by an RGBA triplet that express emission with no occlusion, such as (0.4, 0.3, 0.2, 0.0).\n\nAnother advantage of premultiplied alpha is performance; in certain situations, it can reduce the number of multiplication operations (e.g. if the image is used many times during later compositing). The Porter–Duff operations have a simple form only in premultiplied alpha. Some rendering pipelines expose a \"straight alpha\" API surface, but converts them into premultiplied alpha for performance.",
        "One disadvantage of premultiplied alpha is that it can reduce the available relative precision in the RGB values when using integer or fixed-point representation for the color components. This may cause a noticeable loss of quality if the color information is later brightened or if the alpha channel is removed. In practice, this is not usually noticeable because during typical composition operations, such as OVER, the influence of the low-precision color information in low-alpha areas on the final output image (after composition) is correspondingly reduced. This loss of precision also makes premultiplied images easier to compress using certain compression schemes, as they do not record the color variations hidden inside transparent regions, and can allocate fewer bits to encode low-alpha",
        "images easier to compress using certain compression schemes, as they do not record the color variations hidden inside transparent regions, and can allocate fewer bits to encode low-alpha areas. The same “limitations” of lower quantisation bit depths such as 8 bit per channel are also present in imagery without alpha, and this argument is problematic as a result.",
        "Examples \nAssuming that the pixel color is expressed using straight (non-premultiplied) RGBA tuples, a pixel value of (0, 0.7, 0, 0.5) implies a pixel that has 70% of the maximum green intensity and 50% opacity. If the color were fully green, its RGBA would be (0, 1, 0, 0.5). However, if this pixel uses premultiplied alpha, all of the RGB values (0, 0.7, 0) are multiplied, or scaled for occlusion, by the alpha value 0.5, which is appended to yield (0, 0.35, 0, 0.5). In this case, the 0.35 value for the G channel actually indicates 70% green emission intensity (with 50% occlusion). A pure green emission would be encoded as (0, 0.5, 0, 0.5). Knowing whether a file uses straight or premultiplied alpha is essential to correctly process or composite it, as a different calculation is required.",
        "Emission with no occlusion cannot be represented in straight alpha. No conversion is available in this case.\n\nImage formats supporting alpha channels \n\nThe most popular image formats that support the alpha channel are PNG and TIFF. GIF supports alpha channels, but is considered to be inefficient when it comes to file size. Support for alpha channels is present in some video codecs, such as Animation and Apple ProRes 4444 of the QuickTime format, or in the Techsmith multi-format codec.",
        "The file format BMP generally does not support this channel; however, in different formats such as 32-bit (888-8) or 16-bit (444-4) it is possible to save the alpha channel, although not all systems or programs are able to read it: it is exploited mainly in some video games or particular applications; specific programs have also been created for the creation of these BMPs.\n\nGamma correction \n\nThe RGB values of typical digital images do not directly correspond to the physical light intensities, but are rather compressed by a gamma correction function:\n\n \n\nThis transformation better utilizes the limited number of bits in the encoded image by choosing  that better matches the non-linear human perception of luminance.",
        "This transformation better utilizes the limited number of bits in the encoded image by choosing  that better matches the non-linear human perception of luminance.\n\nAccordingly, computer programs that deal with such images must decode the RGB values into a linear space (by undoing the gamma-compression), blend the linear light intensities, and re-apply the gamma compression to the result:\n\nWhen combined with premultiplied alpha, pre-multiplication is done in linear space, prior to gamma compression. This results in the following formula:\n\nNote that the alpha channel may or may not undergo gamma-correction, even when the color channels do.\n\nOther transparency methods",
        "Note that the alpha channel may or may not undergo gamma-correction, even when the color channels do.\n\nOther transparency methods\n\nAlthough used for similar purposes, transparent colors and image masks do not permit the smooth blending of the superimposed image pixels with those of the background (only whole image pixels or whole background pixels allowed).\n\nA similar effect can be achieved with a 1-bit alpha channel, as found in the 16-bit RGBA high color mode of the Truevision TGA image file format and related TARGA and AT-Vista/NU-Vista display adapters' high color graphic mode. This mode devotes 5 bits for every primary RGB color (15-bit RGB) plus a remaining bit as the \"alpha channel\".\n\nDithering can be used to simulate partial occlusion where only 1-bit alpha is available.",
        "Dithering can be used to simulate partial occlusion where only 1-bit alpha is available.\n\nFor some applications, a single alpha channel is not sufficient: a stained-glass window, for instance, requires a separate transparency channel for each RGB channel to model the red, green and blue transparency separately. More alpha channels can be added for accurate spectral color filtration applications.\n\nSome order-independent transparency methods replace the over operator with a commutative approximation.\n\nSee also\n\nReferences\n\nExternal links\n Compositing Digital Images - Thomas Porter and Tom Duff (Original Paper)\n Image Compositing Fundamentals\n Understand Compositing and Color extensions in SVG 1.2 in 30 minutes!\n Alpha Matting and Premultiplication\n\nComputer graphics algorithms"
    ],
    [
        "Alvin Toffler\nAlvin Eugene Toffler (October 4, 1928 – June 27, 2016) was an American writer, futurist, and businessman known for his works discussing modern technologies, including the digital revolution and the communication revolution, with emphasis on their effects on cultures worldwide. He is regarded as one of the world's outstanding futurists.\n\nToffler was an associate editor of Fortune magazine. In his early works he focused on technology and its impact, which he termed \"information overload.\" In 1970, his first major book about the future, Future Shock, became a worldwide best-seller and has sold over 6 million copies.",
        "He and his wife Heidi Toffler, who collaborated with him for most of his writings, moved on to examining the reaction to changes in society with another best-selling book, The Third Wave, in 1980. In it, he foresaw such technological advances as cloning, personal computers, the Internet, cable television and mobile communication. His later focus, via their other best-seller, Powershift, (1990), was on the increasing power of 21st-century military hardware and the proliferation of new technologies.",
        "He founded Toffler Associates, a management consulting company, and was a visiting scholar at the Russell Sage Foundation, visiting professor at Cornell University, faculty member of the New School for Social Research, a White House correspondent, and a business consultant. Toffler's ideas and writings were a significant influence on the thinking of business and government leaders worldwide, including China's Zhao Ziyang, and AOL founder Steve Case.",
        "Early life\nAlvin Toffler was born on October 4, 1928, in New York City, and raised in Brooklyn. He was the son of Rose (Albaum) and Sam Toffler, a furrier, both Polish Jews who had migrated to America. He had one younger sister. He was inspired to become a writer at the age of 7 by his aunt and uncle, who lived with the Tofflers. \"They were Depression-era literary intellectuals,\" Toffler said, \"and they always talked about exciting ideas.\"",
        "Toffler graduated from New York University in 1950 as an English major, though by his own account he was more focused on political activism than grades. He met his future wife, Adelaide Elizabeth Farrell (nicknamed \"Heidi\"), when she was starting a graduate course in linguistics. Being radical students, they decided against further graduate work and moved to Cleveland, Ohio, where they married on April 29, 1950.",
        "Career\nSeeking experiences to write about, Alvin and Heidi Toffler spent the next five years as blue collar workers on assembly lines while studying industrial mass production in their daily work. He compared his own desire for experience to other writers, such as Jack London, who in his quest for subjects to write about sailed the seas, and John Steinbeck, who went to pick grapes with migrant workers. In their first factory jobs, Heidi became a union shop steward in the aluminum foundry where she worked. Alvin became a millwright and welder. In the evenings Alvin would write poetry and fiction, but discovered he was proficient at neither.",
        "His hands-on practical labor experience helped Alvin Toffler land a position at a union-backed newspaper, a transfer to its Washington bureau in 1957, then three years as a White House correspondent, covering Congress and the White House for a Pennsylvania daily newspaper.",
        "They returned to New York City in 1959 when Fortune magazine invited Alvin to become its labor columnist, later having him write about business and management. After leaving Fortune magazine in 1962, Toffler began a freelance career, writing long form articles for scholarly journals and magazines. His 1964 Playboy interviews with Russian novelist Vladimir Nabokov and Ayn Rand were considered among the magazine's best. His interview with Rand was the first time the magazine had given such a platform to a female intellectual, which as one commentator said, \"the real bird of paradise Toffler captured for Playboy in 1964 was Ayn Rand.\"",
        "Toffler was hired by IBM to conduct research and write a paper on the social and organizational impact of computers, leading to his contact with the earliest computer \"gurus\" and artificial intelligence researchers and proponents. Xerox invited him to write about its research laboratory and AT&T consulted him for strategic advice. This AT&T work led to a study of telecommunications, which advised the company's top management to break up the company more than a decade before the government forced AT&T to break up.",
        "In the mid-1960s, the Tofflers began five years of research on what would become Future Shock, published in 1970. It has sold over 6 million copies worldwide, according to the New York Times, or over 15 million copies according to the Tofflers' Web site. Toffler coined the term \"future shock\" to refer to what happens to a society when change happens too fast, which results in social confusion and normal decision-making processes breaking down. The book has never been out of print and has been translated into dozens of languages.",
        "He continued the theme in The Third Wave in 1980. While he describes the first and second waves as the agricultural and industrial revolutions, the \"third wave,\" a phrase he coined, represents the current information, computer-based revolution. He forecast the spread of the Internet and email, interactive media, cable television, cloning, and other digital advancements. He claimed that one of the side effects of the digital age has been \"information overload,\" another term he coined. In 1990, he wrote Powershift, also with the help of his wife, Heidi.",
        "In 1996, with American business consultant Tom Johnson, they co-founded Toffler Associates, an advisory firm designed to implement many of the ideas the Tofflers had written on. The firm worked with businesses, NGOs, and governments in the United States, South Korea, Mexico, Brazil, Singapore, Australia, and other countries. During this period in his career, Toffler lectured worldwide, taught at several schools and met world leaders, such as Mikhail Gorbachev, along with key executives and military officials.\n\nIdeas and opinions",
        "Ideas and opinions\n\nToffler stated many of his ideas during an interview with the Australian Broadcasting Corporation in 1998. \"Society needs people who take care of the elderly and who know how to be compassionate and honest,\" he said. \"Society needs people who work in hospitals. Society needs all kinds of skills that are not just cognitive; they're emotional, they're affectional. You can't run the society on data and computers alone.\"\n\nHis opinions about the future of education, many of which were in Future Shock, have often been quoted. An often misattributed quote, however, is that of psychologist Herbert Gerjuoy: \"Tomorrow's illiterate will not be the man who can't read; he will be the man who has not learned how to learn.\"",
        "Early in his career, after traveling to other countries, he became aware of the new and myriad inputs that visitors received from these other cultures. He explained during an interview that some visitors would become \"truly disoriented and upset\" by the strange environment, which he described as a reaction to culture shock. From that issue, he foresaw another problem for the future, when a culturally \"new environment comes to you ... and comes to you rapidly.\" That kind of sudden cultural change within one's own country, which he felt many would not understand, would lead to a similar reaction, one of \"future shock\", which he wrote about in his book by that title. Toffler writes:",
        "In The Third Wave, Toffler describes three types of societies, based on the concept of \"waves\"—each wave pushes the older societies and cultures aside. He describes the \"First Wave\" as the society after agrarian revolution and replaced the first hunter-gatherer cultures. The \"Second Wave,\" he labels society during the Industrial Revolution (ca. late 17th century through the mid-20th century). That period saw the increase of urban industrial populations which had undermined the traditional nuclear family, and initiated a factory-like education system, and the growth of the corporation. Toffler said:",
        "The \"Third Wave\" was a term he coined to describe the post-industrial society, which began in the late 1950s. His description of this period dovetails with other futurist writers, who also wrote about the Information Age, Space Age, Electronic Era, Global Village, terms which highlighted a scientific-technological revolution. The Tofflers claimed to have predicted a number of geopolitical events, such as the collapse of the Soviet Union, the fall of the Berlin Wall and the future economic growth in the Asia-Pacific region.\n\nInfluences and popular culture",
        "Toffler often visited with dignitaries in Asia, including China's Zhao Ziyang, Singapore's Lee Kuan Yew and South Korea's Kim Dae Jung, all of whom were influenced by his views as Asia's emerging markets increased in global significance during the 1980s and 1990s. Although they had originally censored some of his books and ideas, China's government cited him along with Franklin Roosevelt and Bill Gates as being among the Westerners who had most influenced their country. The Third Wave along with a video documentary based on it became best-sellers in China and were widely distributed to schools. The video's success inspired the marketing of videos on related themes in the late 1990s by Infowars, whose name is derived from the term coined by Toffler in the book. Toffler's influence on Asian",
        "The video's success inspired the marketing of videos on related themes in the late 1990s by Infowars, whose name is derived from the term coined by Toffler in the book. Toffler's influence on Asian thinkers was summed up in an article in Daedalus, published by the American Academy of Arts & Sciences:",
        "U.S. House Speaker Newt Gingrich publicly lauded his ideas about the future, and urged members of Congress to read Toffler's book, Creating a New Civilization (1995). Others, such as AOL founder Steve Case, cited Toffler's The Third Wave as a formative influence on his thinking, which inspired him to write The Third Wave: An Entrepreneur's Vision of the Future in 2016. Case said that Toffler was a \"real pioneer in helping people, companies and even countries lean into the future.\"",
        "In 1980, Ted Turner founded CNN, which he said was inspired by Toffler's forecasting the end of the dominance of the three main television networks. Turner's company, Turner Broadcasting, published Toffler's Creating a New Civilization in 1995. Shortly after the book was released, the former Soviet president Mikhail Gorbachev hosted the Global Governance Conference in San Francisco with the theme, Toward a New Civilization, which was attended by dozens of world figures, including the Tofflers, George H. W. Bush, Margaret Thatcher, Carl Sagan, Abba Eban and Turner with his then-wife, actress Jane Fonda.\n\nMexican billionaire Carlos Slim was influenced by his works, and became a friend of the writer. Global marketer J.D. Power also said he was inspired by Toffler's works.",
        "Mexican billionaire Carlos Slim was influenced by his works, and became a friend of the writer. Global marketer J.D. Power also said he was inspired by Toffler's works.\n\nSince the 1960s, people had tried to make sense out of the effect of new technologies and social change, a problem which made Toffler's writings widely influential beyond the confines of scientific, economic, and public policy. His works and ideas have been subject to various criticisms, usually with the same argumentation used against futurology: that foreseeing the future is nigh impossible.\n\nTechno music pioneer Juan Atkins cites Toffler's phrase \"techno rebels\" in The Third Wave as inspiring him to use the word \"techno\" to describe the musical style he helped to create",
        "Techno music pioneer Juan Atkins cites Toffler's phrase \"techno rebels\" in The Third Wave as inspiring him to use the word \"techno\" to describe the musical style he helped to create\n\nMusician Curtis Mayfield released a disco song called \"Future Shock,\" later covered in an electro version by Herbie Hancock. Science fiction author John Brunner wrote \"The Shockwave Rider,\" from the concept of \"future shock.\"\n\nThe nightclub Toffler, in Rotterdam, is named after him.\n\nIn the song \"Victoria\" by The Exponents, the protagonist's daily routine and cultural interests are described: \"She's up in time to watch the soap operas, reads Cosmopolitan and Alvin Toffler\".",
        "Critical assessment \nAccenture, the management consultancy firm, identified Toffler in 2002 as being among the most influential voices in business leaders, along with Bill Gates and Peter Drucker. Toffler has also been described in a Financial Times interview as the \"world's most famous futurologist\". In 2006, the People's Daily classed him among the 50 foreigners who shaped modern China, which one U.S. newspaper notes made him a \"guru of sorts to world statesmen.\" Chinese Premier and General Secretary Zhao Ziyang was greatly influenced by Toffler. He convened conferences to discuss The Third Wave in the early 1980s, and in 1985 the book was the No. 2 best seller in China.\n\nAuthor Mark Satin characterizes Toffler as an important early influence on radical centrist political thought.",
        "Author Mark Satin characterizes Toffler as an important early influence on radical centrist political thought.\n\nNewt Gingrich became close to the Tofflers in the 1970s and said The Third Wave had immensely influenced his own thinking and was \"one of the great seminal works of our time.\"\n\nSelected awards\nToffler has received several prestigious prizes and awards, including the McKinsey Foundation Book Award for Contributions to Management Literature, Officier de L'Ordre des Arts et Lettres, and appointments, including Fellow of the American Association for the Advancement of Science and the International Institute for Strategic Studies.\n\nIn 2006, Alvin and Heidi Toffler were recipients of Brown University's Independent Award.",
        "In 2006, Alvin and Heidi Toffler were recipients of Brown University's Independent Award.\n\nPersonal life\nToffler was married to Heidi Toffler (born Adelaide Elizabeth Farrell), also a writer and futurist. They lived in the Bel Air section of Los Angeles, California, and previously lived in Redding, Connecticut.\n\nThe couple's only child, Karen Toffler (1954–2000), died at age 46 after more than a decade suffering from Guillain–Barré syndrome.\n\nAlvin Toffler died in his sleep on June 27, 2016, at his home in Los Angeles. No cause of death was given. He is buried at Westwood Memorial Park.",
        "Bibliography\nAlvin Toffler co-wrote his books with his wife Heidi.\n The Culture Consumers (1964) St. Martin's Press, \n The Schoolhouse in the City (1968) Praeger (editors), \n Future Shock (1970) Bantam Books, \n The Futurists (1972) Random House (editors), \n Learning for Tomorrow (1974) Random House (editors), \n The Eco-Spasm Report (1975) Bantam Books, \n The Third Wave (1980) Bantam Books, \n Previews & Premises (1983) William Morrow & Co, \n The Adaptive Corporation (1985) McGraw-Hill, \n Powershift: Knowledge, Wealth and Violence at the Edge of the 21st Century (1990) Bantam Books, \n War and Anti-War (1993) Warner Books, \n Creating a New Civilization (1995) Turner Pub, \n Revolutionary Wealth (2006) Knopf,\n\nSee also\n Daniel Bell\n Norman Swan\n Human nature\n John Naisbitt\n\nReferences",
        "See also\n Daniel Bell\n Norman Swan\n Human nature\n John Naisbitt\n\nReferences\n\nExternal links\n\n  – official Alvin Toffler site\n Toffler Associates\n Interview with Alvin Toffler by the World Affairs Council\n \n Discuss Alvin Toffler's Future Shock with other readers, BookTalk.org\n \n \nFuture Shock Forum 2018\nFinding aid to the Alvin and Heidi Toffler papers at Columbia University. Rare Book & Manuscript Library",
        "1928 births\n2016 deaths\nAmerican people of Polish-Jewish descent\nAmerican technology writers\nAmerican futurologists\nBurials at Westwood Village Memorial Park Cemetery\nJewish American writers\nPeople from Ridgefield, Connecticut\nWriters from Connecticut\nWriters from Brooklyn\n20th-century American non-fiction writers\n21st-century American non-fiction writers\nAmerican transhumanists\nNew York University alumni\nSingularitarians\nPeople from Redding, Connecticut\n20th-century American male writers\nAmerican male non-fiction writers\nJewish American journalists\nPeople from Bel Air, Los Angeles\n21st-century American male writers\n21st-century American Jews"
    ],
    [
        "American Registry for Internet Numbers\nThe American Registry for Internet Numbers (ARIN) is the regional Internet registry  for the United States, Canada, and many Caribbean and North Atlantic islands. ARIN manages the distribution of Internet number resources, including IPv4 and IPv6 address space and AS numbers. ARIN opened for business on December 22, 1997 after incorporating on April 18, 1997. ARIN is a nonprofit corporation with headquarters in Chantilly, Virginia, United States.<ref>\"Chantilly CDP, Virginia .\" U.S. Census Bureau. Retrieved on September 16, 2009.</ref>\n\nARIN is one of five regional Internet registries in the world. Like the other regional Internet registries, ARIN:",
        "ARIN is one of five regional Internet registries in the world. Like the other regional Internet registries, ARIN:\n\nProvides services related to the technical coordination and management of Internet number resources\nFacilitates policy development by its members and stakeholders\nParticipates in the international Internet community\nIs a nonprofit, community-based organization\nIs governed by an executive board elected by its membership",
        "Services\nARIN provides services related to the technical coordination and management of Internet number resources. The nature of these services is described in ARIN's mission statement:Applying the principles of stewardship, ARIN, a nonprofit corporation, allocates Internet Protocol resources; develops consensus-based policies; and facilitates the advancement of the Internet through information and educational outreach.These services are grouped in three areas: Registration, Organization, and Policy Development.\n\nRegistration services\nRegistration services pertain to the technical coordination and inventory management of Internet number resources. Services include:",
        "Registration services\nRegistration services pertain to the technical coordination and inventory management of Internet number resources. Services include:\n\nIPv4 address allocation and assignment\nIPv6 address allocation and assignment\nAS number assignment\nDirectory services including:\nRegistration transaction information (WHOIS)\nRouting information (Internet routing registry)\nDNS (Reverse)\n\nFor information on requesting Internet number resources from ARIN, see https://www.arin.net/resources/index.html. This section includes the request templates, specific distribution policies, and guidelines for requesting and managing Internet number resources.\n\nOrganization services\nOrganization services pertain to interaction between stakeholders, ARIN members, and ARIN. Services include:",
        "Organization services\nOrganization services pertain to interaction between stakeholders, ARIN members, and ARIN. Services include:\n\nElections\nMembers meetings\nInformation publication and dissemination\nEducation and training\n\nPolicy development services\nPolicy development services facilitate the development of policy for the technical coordination and management of Internet number resources.\n\nAll ARIN policies are set by the community. Everyone is encouraged to participate in the policy development process at public policy meetings and on the Public Policy Mailing List. The ARIN Board of Trustees ratifies policies only after:\n\ndiscussion on mailing lists and at meetings;\nARIN Advisory Council recommendation;\ncommunity consensus in favor of the policy; and\nfull legal and fiscal review.",
        "discussion on mailing lists and at meetings;\nARIN Advisory Council recommendation;\ncommunity consensus in favor of the policy; and\nfull legal and fiscal review.\n\nMembership is not required to participate in ARIN's policy development process or to apply for Internet number resources.\n\nServices include:\n\nMaintaining discussion e-mail lists\nConducting public policy meetings\nPublishing policy documents\n\nOrganizational structure\nARIN consists of the Internet community within its region, its members, a 7-member Board of Trustees, a 15-member Advisory Council, and a professional staff of about 50. The board of trustees and Advisory Council are elected by ARIN members for three-year terms.",
        "Board of trustees\nThe ARIN membership elects the Board of Trustees (BoT), which has ultimate responsibility for the business affairs and financial health of ARIN, and manages ARIN's operations in a manner consistent with the guidance received from the Advisory Council and the goals set by the registry's members. The BoT is responsible for determining the disposition of all revenues received to ensure all services are provided in an equitable manner. The BoT ratifies proposals generated from the membership and submitted through the Advisory Council. Executive decisions are carried out following approval by the BoT. The BoT consists of 7 members consisting of a President and CEO, a chairman, a Treasurer, and others.",
        "Advisory Council\nIn addition to the BoT, ARIN has an advisory council that advises ARIN and the BoT on IP address allocation policy and related matters. Adhering to the procedures in the Internet Resource Policy Evaluation Process, the advisory council forwards consensus-based policy proposals to the BoT for ratification. The advisory council consists of 15 elected members consisting of a Chair, Vice Chair, and others.",
        "History\nThe organization was formed in December 1997 to \"provide IP registration services as an independent, nonprofit corporation.\" Until this time, IP address registration (outside of RIPE and APNIC regions) was done in accordance with policies set by the IETF by Network Solutions corporation as part of the InterNIC project.  The National Science Foundation approved the plan for the creation of the not-for-profit organization to \"give the users of IP numbers (mostly Internet service providers, corporations and other large institutions) a voice in the policies by which they are managed and allocated within the North American region.\"''.  As part of the transition, Network Solutions corporation transitioned these tasks as well as initial staff and computer infrastructure to ARIN.",
        "The initial Board of Trustees consisted of  Scott Bradner, John Curran, Kim Hubbard, Don Telage, Randy Bush, Raymundo Vega Aguilar, and Jon Postel (IANA) as an ex-officio member.\n\nThe first president of ARIN was Kim Hubbard, from 1997 until 2000. Kim was succeeded by Raymond \"Ray\" Plzak until the end of 2008. Trustee John Curran was acting president until July 1 of 2009 when he assumed the CEO role permanently.\n\nUntil late 2002 it served Mexico, Central America, South America and all of the Caribbean. LACNIC now handles parts of the Caribbean, Mexico, Central America, and South America. Also, Sub-Saharan Africa was part of its region until April 2005, when AfriNIC was officially recognized by ICANN as the fifth regional Internet registry.",
        "On 24 September 2015 ARIN has declared exhaustion of the ARIN IPv4 addresses pool.\n\nService region\nThe countries in the ARIN service region are:\n\nFormer service regions\nARIN formerly covered Angola, Botswana, Burundi, Republic of Congo, Democratic Republic of Congo, Eswatini, Lesotho, Malawi, Mozambique, Namibia, Rwanda, South Africa, Tanzania, Zambia, and Zimbabwe until AfriNIC was formed.\n\nARIN formerly covered Argentina, Aruba, Belize, Bolivia, Brazil, Chile, Colombia, Costa Rica, Cuba, Dominican Republic, Dutch West Indies, Ecuador, El Salvador, Falkland Islands (UK), French Guiana, Guatemala, Guyana, Haiti, Honduras, Mexico, Nicaragua, Panama, Paraguay, Peru, South Georgia and the South Sandwich Islands, Suriname, Trinidad and Tobago, Uruguay, and Venezuela until LACNIC was formed.",
        "References\n\nExternal links\nARIN Home Page\n\nRegional Internet registries\nInternet in Canada\nInternet in the United States\nInternet exchange points in North America\nOrganizations based in Virginia"
    ],
    [
        "Amiga 1000\nThe Amiga 1000, also known as the A1000, is the first personal computer released by Commodore International in the Amiga line. It combines the 16/32-bit Motorola 68000 CPU which was powerful by 1985 standards with one of the most advanced graphics and sound systems in its class. It runs a preemptive multitasking operating system that fits into  of read-only memory and was shipped with 256 KB of RAM.  The primary memory can be expanded internally with a manufacturer-supplied 256 KB module for a total of 512 KB of RAM. Using the external slot the primary memory can be expanded up to",
        "Design \nThe A1000 has a number of characteristics that distinguish it from later Amiga models: It is the only model to feature the short-lived Amiga check-mark logo on its case, the majority of the case is elevated slightly to give a storage area for the keyboard when not in use (a \"keyboard garage\"), and the inside of the case is engraved with the signatures of the Amiga designers (similar to the Macintosh); including Jay Miner and the paw print of his dog Mitchy. The A1000's case was designed by Howard Stolz.  As Senior Industrial Designer at Commodore, Stolz was the mechanical lead and primary interface with Sanyo in Japan, the contract manufacturer for the A1000 casing.",
        "The Amiga 1000 was manufactured in two variations:  One uses the NTSC television standard and the other uses the PAL television standard. The NTSC variant was the initial model manufactured and sold in North America. The later PAL model was manufactured in Germany and sold in countries using the PAL television standard. The first NTSC systems lack the EHB video mode which is present in all later Amiga models.",
        "Because AmigaOS was rather buggy at the time of the A1000's release, the OS was not placed in ROM then. Instead, the A1000 includes a daughterboard with 256 KB of RAM, dubbed the \"writable control store\" (WCS), into which the core of the operating system is loaded from floppy disk (this portion of the operating system is known as the \"Kickstart\"). The WCS is write-protected after loading, and system resets do not require a reload of the WCS. In Europe, the WCS was often referred to as WOM (Write Once Memory), a play on the more conventional term \"ROM\" (read-only memory).",
        "Technical information \nThe preproduction Amiga (which was codenamed \"Velvet\") released to developers in early 1985 contained  of RAM with an option to expand it to  Commodore later increased the system memory to  due to objections by the Amiga development team. The names of the custom chips were different; Denise and Paula were called Daphne and Portia respectively. The casing of the preproduction Amiga was almost identical to the production version: the main difference being an embossed Commodore logo in the top left corner. It did not have the developer signatures.",
        "The Amiga 1000 has a Motorola 68000 CPU running at 7.15909 MHz on NTSC systems or 7.09379 MHz on PAL systems, precisely double the video color carrier frequency for NTSC or 1.6 times the color carrier frequency for PAL. The system clock timings are derived from the video frequency, which simplifies glue logic and allows the Amiga 1000 to make do with a single crystal. In keeping with its video game heritage, the chipset was designed to synchronize CPU memory access and chipset DMA so the hardware runs in real time without wait-state delays.",
        "Though most units were sold with an analog RGB monitor, the A1000 also has a built-in composite video output which allows the computer to be connected directly to some monitors other than their standard RGB monitor. The A1000 also has a \"TV MOD\" output, into which an RF Modulator can be plugged, allowing connection to older televisions that did not have a composite video input.",
        "The original 68000 CPU can be directly replaced with a Motorola 68010, which can execute instructions slightly faster than the 68000 but also introduces a small degree of software incompatibility. Third-party CPU upgrades, which mostly fit in the CPU socket, use faster 68020 or 68030 microprocessors and integrated memory, as well as provide support for a 68881 or 68882 FPU. Such upgrades often have the option to revert to 68000 mode for full compatibility. Some boards have a socket to seat the original 68000, whereas the 68030 cards typically come with an on-board 68000.",
        "The original Amiga 1000 is the only model to have 256 KB of Amiga Chip RAM, which can be expanded to 512 KB with the addition of a daughterboard under a cover in the center front of the machine. RAM may also be upgraded via official and third-party upgrades, with a practical upper limit of about 9 MB of \"fast RAM\" due to the 68000's 24-bit address bus. This memory is accessible only by the CPU permitting faster code execution as DMA cycles are not shared with the chipset.",
        "The Amiga 1000 features an 86-pin expansion port (electrically identical to the later Amiga 500 expansion port, though the A500's connector is inverted). This port is used by third-party expansions such as memory upgrades and SCSI adapters. These resources are handled by the Amiga Autoconfig standard. Other expansion options are available including a bus expander which provides two Zorro-II slots.\n\nSpecifications",
        "Specifications\n\nRetail \nIntroduced on July 23, 1985, during a star-studded gala featuring Andy Warhol and Debbie Harry held at the Vivian Beaumont Theater at Lincoln Center in New York City, machines began shipping in September with a base configuration of 256 KB of RAM at the retail price of . A  analog RGB monitor was available for around , bringing the price of a complete Amiga system to US$1,595 (). Before the release of the Amiga 500 and Amiga 2000 models in 1987, the A1000 was marketed as simply the Amiga, although the model number was there from the beginning, as the original box indicates.\n\nIn the US, the A1000 was marketed as The Amiga from Commodore, with the Commodore logo omitted from the case. The Commodore branding was retained for the international versions.",
        "In the US, the A1000 was marketed as The Amiga from Commodore, with the Commodore logo omitted from the case. The Commodore branding was retained for the international versions.  \n\nAdditionally, the Amiga 1000 was sold exclusively in computer stores in the US rather than the various non computer-dedicated department and toy stores through which the VIC-20 and Commodore 64 were retailed. These measures were an effort to avoid Commodore's \"toy-store\" computer image created during the Tramiel era. \n\nAlong with the operating system, the machine came bundled with a version of AmigaBASIC developed by Microsoft and a speech synthesis library developed by Softvoice, Inc.",
        "Along with the operating system, the machine came bundled with a version of AmigaBASIC developed by Microsoft and a speech synthesis library developed by Softvoice, Inc.\n\nAftermarket upgrades \nMany A1000 owners remained attached to their machines long after newer models rendered the units technically obsolete, and it attracted numerous aftermarket upgrades. Many CPU upgrades that plugged into the Motorola 68000 socket functioned in the A1000. Additionally, a line of products called the Rejuvenator series allowed the use of newer chipsets in the A1000, and an Australian-designed replacement A1000 motherboard called The Phoenix utilized the same chipset as the A3000 and added an A2000-compatible video slot and on-board SCSI controller.",
        "Reception and impact \nIn its product preview, Byte magazine was impressed by the computer's multitasking capabilities and the quality of its graphics and sound systems. It also praised its text-to-speech library for voice output, and predicted that the Amiga would be successful enough to influence the personal computer industry.",
        "The Amiga 1000 was released to positive reviews. Compute! lauded it as an inexpensive, truly general-purpose computer that might break preconceptions dividing the microcomputer marketplace. In this case, it was capable of outperforming most business, as well as arcade game machines and delivering sampled sound, making it suitable for offices, gamers, and digital artists. Computer Gaming World praised the machine's versatility without any obvious hardware shortcomings and stressed that it was ideal for game designers demanding fewer system constraints. Creative Computing magazine had only minor criticisms for what they otherwise called a \"dream machine.\" These criticisms were directed toward its case quality, the disk drives slowing during certain operations, and not finding an AUTOEXEC",
        "criticisms for what they otherwise called a \"dream machine.\" These criticisms were directed toward its case quality, the disk drives slowing during certain operations, and not finding an AUTOEXEC command in AmigaDOS, though the marketing vice president of Commodore, Clive Smith, assured the magazine that later production units would address most of its complaints. Months after the Amiga 1000 was released, InfoWorld offered a mixed review. It praised Intuition and the customizability of Workbench, but took issue with the operating system's bugs such as memory overflow and screen flickering of single lines as a result of their being interleaved when displayed in high resolution mode. It also criticized the sparseness of the software library preventing the publication from fully realizing",
        "single lines as a result of their being interleaved when displayed in high resolution mode. It also criticized the sparseness of the software library preventing the publication from fully realizing the computer's potential.",
        "In 1994, as Commodore filed for bankruptcy, Byte magazine called the Amiga 1000 \"the first multimedia computer... so far ahead of its time that almost nobody—including Commodore's marketing department—could fully articulate what it was all about\". In 2006, PC World rated the Amiga 1000 as the 7th greatest PC of all time. In 2007, it was rated by the same magazine as the 37th best tech product of all time. Also that year, IDG Sweden ranked it the 10th best computer of all time.",
        "Joe Pillow \n\"Joe Pillow\" was the name given on the ticket for the extra airline seat purchased to hold the first Amiga prototype while on the way to the January 1984 Consumer Electronics Show. The airlines required a name for the airline ticket and Joe Pillow was born. The engineers (RJ Mical and Dale Luck) who flew with the Amiga prototype (codenamed Lorraine) drew a happy face on the front of the pillowcase and even added a tie.\n\nJoe Pillow extended his fifteen minutes of fame when the Amiga went to production. All fifty-three Amiga team members who worked on the project signed the Amiga case. This included Joe Pillow and Jay Miner's dog Michy who each got to \"sign\" the case in their own unique way.\n\nSee also",
        "See also \n\n Amiga models and variants\n Amiga Sidecarfor using MS-DOS with Intel 8088 @ 4.77 MHz with 256 KB RAM\n\nReferences\n\nExternal links \n\n The Commodore Amiga A1000 at OLD-COMPUTERS.COM\n Who was Joe Pillow?\n\nAmiga computers\n68000-based home computers\nComputer-related introductions in 1985"
    ],
    [
        "Amiga 500\nThe Amiga 500, also known as the A500, was the first popular version of the Amiga home computer, \"redefining the home computer market and making so-called luxury features such as multitasking and colour a standard long before Microsoft or Apple sold these to the masses\". It contains the same Motorola 68000 as the Amiga 1000, as well as the same graphics and sound coprocessors, but is in a smaller case similar to that of the Commodore 128.",
        "Commodore announced the Amiga 500 at the January 1987 winter Consumer Electronics Showat the same time as the high-end Amiga 2000. It was initially available in the Netherlands in April 1987, then the rest of Europe in May. In North America and the UK it was released in October 1987 with a  list price. It competed directly against models in the Atari ST line.",
        "The Amiga 500 was sold in the same retail outlets as the Commodore 64, as opposed to the computer store-only Amiga 1000. It proved to be Commodore's best-selling model, particularly in Europe. Although popular with hobbyists, arguably its most widespread use was as a gaming machine, where its graphics and sound were of significant benefit. It was followed by a revised version of the computer, the Amiga 500 Plus, and the 500 series was discontinued in 1992.",
        "Releases \nIn mid-1988, the Amiga 500 dropped its price from £499 to £399 (https://amr.abime.net/issue_535_pages page 7), and it was later bundled with the Batman Pack in the United Kingdom (from October 1989 to September 1990) which included the games Batman, F/A-18 Interceptor, The New Zealand Story and the bitmap graphics editor Deluxe Paint 2. Also included was the Amiga video connector which allows the A500 to be used with a conventional CRT television.",
        "In November 1991, the enhanced Amiga 500 Plus replaced the 500 in some markets. It was bundled with the Cartoon Classics pack in the United Kingdom at £399, although many stores still advertised it as an 'A500'. The Amiga 500 Plus was virtually identical except for its new operating system, different 'trap-door' expansion slot and slightly different keyboard, and in mid-1992, the two were discontinued and effectively replaced by the Amiga 600. In late 1992, Commodore released the Amiga 1200, a machine closer in concept to the original Amiga 500, but with significant technical improvements. Despite this, neither the A1200 nor the A600 replicated the commercial success of its predecessor. By this time, the home market was strongly shifting to IBM PC compatibles with VGA graphics and the",
        "Despite this, neither the A1200 nor the A600 replicated the commercial success of its predecessor. By this time, the home market was strongly shifting to IBM PC compatibles with VGA graphics and the \"low-cost\" Macintosh Classic, LC, and IIsi models.",
        "Description",
        "Outwardly resembling the Commodore 128 and codenamed \"Rock Lobster\" during development, the Amiga 500's base houses a keyboard and a CPU in one shell, unlike the Amiga 1000. The keyboard for Amiga 500s sold in the United States contains 94 keys, including ten function keys, four cursor keys, and a number pad. All European versions the keyboard have an additional two keys, except for the British variety, which still uses 94 keys. It uses a Motorola 68000 microprocessor running at  in NTSC regions and  in PAL regions. The CPU implements a 32-bit model and has 32-bit registers, but it has a 16-bit main ALU and uses a 16-bit external data bus and a 23-bit address bus, providing a maximum of 16 MB of address space. Also built in to the base of the computer is a -inch floppy disk drive. The",
        "main ALU and uses a 16-bit external data bus and a 23-bit address bus, providing a maximum of 16 MB of address space. Also built in to the base of the computer is a -inch floppy disk drive. The user can also install up to three external floppy drives, either - or -inch, via the disk drive port. The second and third additional drives are installed by daisy-chaining them. Supported by these drives are double-sided disks with a capacity of 901,120 bytes, as well as 360- and 720-KB disks formatted for IBM PC compatibles.",
        "The earliest Amiga 500 models use nearly the same Original Amiga chipset as the Amiga 1000. So graphics can be displayed in multiple resolutions and color depths, even on the same screen. Resolutions vary from 320×200 (up to 32 colors) to 640×400 (up to 16 colors) for NTSC (704×484 overscan) and 320×256 to 640×512 for PAL (704×576 overscan.) The system uses planar graphics, with up to five bitplanes (four in high resolution) allowing 2-, 4-, 8-, 16-, and 32-color screens, from a palette of 4096 colors. Two special graphics modes are also available: Extra HalfBrite, which uses a 6th bitplane as a mask to cut the brightness of any pixel in half (resulting in 32 arbitrary colors plus 32 more colors set at half the value of the first 32), and Hold-And-Modify (HAM) which allows all 4096 colors",
        "a mask to cut the brightness of any pixel in half (resulting in 32 arbitrary colors plus 32 more colors set at half the value of the first 32), and Hold-And-Modify (HAM) which allows all 4096 colors to be used on screen simultaneously. Later revisions of the chipset are PAL/NTSC switchable in software.",
        "The sound chip produces four hardware-mixed channels, two to the left and two to the right, of 8-bit PCM at a sampling frequency up to .  Each hardware channel has its own independent volume level and sampling rate, and can be designated to another channel where it can modulate both volume and frequency using its own output.  With DMA disabled it's possible to output with a sampling frequency up to . There is a common trick to output sound with 14-bit precision that can be combined to output 14-bit  sound.\n\nThe stock system comes with AmigaOS version 1.2 or 1.3 and  of chip RAM (150 ns access time), one built-in double-density standard floppy disk drive that is completely programmable and can read  IBM PC disks,  standard Amiga disks, and up to  using custom-formatting drivers.",
        "Despite the lack of Amiga 2000-compatible internal expansion slots, there are many ports and expansion options. There are two DE9M Atari joystick ports for joysticks or mice, and stereo audio RCA connectors (1 V p-p). There is a floppy drive port for daisy-chaining up to three extra floppy disk drives via a DB23F connector. The then-standard RS-232 serial port (DB25M) and Centronics parallel port (DB25F) are also included. The power supply is (, ).",
        "The system displays video in analog RGB  PAL or  NTSC through a proprietary DB23M connector and in NTSC mode the line frequency is  HSync for standard video modes, which is compatible with NTSC television and CVBS/RGB video, but out of range for most VGA-compatible monitors, while a multisync monitor is required for some of the higher resolutions. This connection can also be genlocked to an external video signal. The system was bundled with an RF adapter to provide output on televisions with a coaxial RF input, while monochrome video is available via an RCA connector (also coaxial). On the left side, behind a plastic cover, there is a Zorro (Zorro I) bus expansion external edge connector with 86 pins. Peripherals such as a hard disk drive can be added via the expansion slot and are",
        "the left side, behind a plastic cover, there is a Zorro (Zorro I) bus expansion external edge connector with 86 pins. Peripherals such as a hard disk drive can be added via the expansion slot and are configured automatically by the Amiga's AutoConfig standard, so that multiple devices do not conflict with each other. Up to  of so-called \"fast RAM\" (memory that can be accessed by the CPU only) can be added using the side expansion slot. This connector is electronically identical with the Amiga 1000's, but swapped on the other side.",
        "The Amiga 500 has a \"trap-door\" slot on the underside for a RAM upgrade (typically ). This extra RAM is classified as \"fast\" RAM, but is sometimes referred to as \"slow\" RAM: due to the design of the expansion bus, it is actually on the chipset bus. Such upgrades usually include a battery-backed real-time clock. All versions of the A500 can have the additional RAM configured as chip RAM by a simple hardware modification, which involves fitting a later model (8372A) Agnus chip. Likewise, all versions of the A500 can be upgraded to  chip RAM by fitting the  chip and adding additional memory.",
        "The Amiga 500 also sports an unusual feature for a budget machine, socketed chips, which allow easy replacement of defective chips. The CPU can be directly upgraded on the motherboard to a 68010; or to a 68020, 68030, or 68040 via the side expansion slot; or by removing the CPU and plugging a CPU expansion card into the CPU socket (this requires opening the computer and thus voided any remaining warranty).  In fact, all the custom chips can be upgraded to the Amiga Enhanced Chip Set (ECS) versions.",
        "The plastic case is made of acrylonitrile butadiene styrene, or ABS. ABS degrades with time due to exposure to oxygen, causing a yellowing of the case. Other factors contributing to the degradation and yellowing include heat, shear, and ultraviolet light. The yellowing can be reversed by using an optical brightener, though without stabilizing agents or antioxidants to block oxygen, the yellowing will return.\n\nTechnical specifications",
        "OCS (1.2 & 1.3 models) or ECS (1.3 and 500+ 2.04 models) chipset. ECS revisions of the chipset made PAL/NTSC mode switchable in software.\n Sound: 4 hardware-mixed channels of 8-bit sound at up to . The hardware channels have independent volumes (65 levels) and sampling rates, and are mixed down to two fully left and fully right stereo outputs. A software controllable low-pass audio filter is also included.\n 512 KB of chip RAM (150 ns access time)\n AmigaOS 1.2 or 1.3 (upgradeable up to 3.1.4 if 2 MB of RAM are installed)\n One 3.5\" double-density floppy disk drive is built in, which is completely programmable and thus can read  IBM PC disks,  standard Amiga disks, and up to  with custom formatting (such as Klaus Deppich's diskspare.device). Uses  (5 rotations/second) and .",
        "Built-in keyboard\n A two-button mouse is included.",
        "Graphics \n PAL mode: 768×580 maximum (overscan interlaced if viewed on composite monitor/TV). Typical resolutions: 320×256, 640×256 or 640×512 (all displayed with borders). \n NTSC mode: 768×484 maximum (overscan interlaced if viewed on composite monitor/TV). Typical resolutions: 320×200, 640×200 or 640×400 (all displayed with borders). \n Graphics can be of arbitrary dimensions, resolution and colour depth, even on the same screen. The Amiga can show multiple resolution modes at the same time, splitting the screen vertically.\n Planar graphics are used, with up to 5 bitplanes (4 in hires); this allowed 2, 4, 8, 16 and 32 colour screens, from a palette of 4096 colours. \n Two special graphics modes are also included:",
        "Planar graphics are used, with up to 5 bitplanes (4 in hires); this allowed 2, 4, 8, 16 and 32 colour screens, from a palette of 4096 colours. \n Two special graphics modes are also included: \n Extra Half Brite (EHB), which uses a 6th bitplane as a mask that halved the brightness of any colour seen\n Hold-And-Modify (HAM), which allows all 4096 colours on screen at once. HAM makes it possible to use  over a  wide span. This works by letting each pixel position use the previous RGB value and modify one of the red, green or blue values to a new 4-bit value. This will cause some negligible colour artifacts however.",
        "Memory \nUsing various expansion techniques, the A500's total RAM can reach up to 138 MB – 2 MB Chip RAM, 8 MB 16-bit Fast RAM, and 128 MB 32-bit Fast RAM.\n\nChip RAM \nThe stock 512 KB Chip RAM can be complemented by 512 KB using a \"trapdoor\" expansion (Commodore A501 or compatible). While that expansion memory is connected to the chip bus, hardware limitations of early stock Agnus chip revisions prevent its use as Chip RAM, only the CPU can access it. Suffering from the same contention limitations as Chip RAM, that memory is known as \"Slow RAM\" or \"Pseudo-fast RAM\". Agnus revisions shipped with late A500 are ECS and allow use of trapdoor RAM as real Chip RAM for a total 1 MB.",
        "Additionally, several third-party expansions exist with up to 2 MB on the trapdoor board. Using a Gary adapter, that memory will be mapped as either split on Chip RAM and Slow RAM or fully as Slow RAM, depending on configuration.\n\nFurthermore, using an A3000 Agnus on an adapter board, it is possible to expand the Chip RAM to 2 MB, matching the A500+.\n\nFast RAM \n\"Fast\" RAM is located on the CPU-side bus. Its access is exclusive to the CPU and not slowed by any chipset access. The side expansion port allows for up to 8 MB of Zorro-style expansion RAM. Alternatively, a CPU adapter allows for internal expansion.",
        "Accelerator RAM \nInternal or external CPU accelerators often include their own expansion memory. 16-bit CPUs are limited by the 24-bit address space but they can repurpose otherwise unused memory space for their included RAM. 32-bit CPU accelerators aren't limited by 24-bit addressing and can include up to 128 MB of Fast RAM (and potentially more).\n\nMemory map\n\nConnectors",
        "Two Atari joystick ports for joysticks or mice\n Stereo audio RCA connectors ( p-p)\n A floppy drive port (DB23F), for daisy-chaining up to 3 extra floppy disk drives via a DB23F connector\n A standard RS-232 serial port (DB25M)\n A parallel port (DB25F)\n Power inlet (, )",
        "A floppy drive port (DB23F), for daisy-chaining up to 3 extra floppy disk drives via a DB23F connector\n A standard RS-232 serial port (DB25M)\n A parallel port (DB25F)\n Power inlet (, )\n Analogue RGB  PAL and  NTSC video output, provided on an Amiga-specific DB23M video connector. Can drive video with  HSync for standard Amiga video modes. This is not compatible with most VGA monitors. A Multisync monitor is required for some higher resolutions. This connection can also be genlocked to an external video signal. An RF adapter (A520) was frequently bundled with the machine to provide output on regular televisions or on composite monitors. A digital 16 colour Red-Green-Blue-Intensity signal is available too on the same connector.\n Monochrome video via an RCA connector",
        "Monochrome video via an RCA connector\n Zorro II bus expansion on the left side behind a plastic cover\n Trapdoor slot under the machine, for RAM expansion and real-time clock",
        "Expansions \n Expansion ports are limited to a side expansion port and a trapdoor expansion on the underside of the machine. The casing can also be opened up (voiding the warranty), all larger chips are socketed rather than being TH/SMD soldered directly to the motherboard, so they can be replaced by hand.\n The CPU can be upgraded to a Motorola 68010 directly or to a 68020, 68030 or 68040 via the side expansion slot or a CPU socket adapter board.\n The chip RAM can be upgraded to  directly on the motherboard, provided a Fat Agnus chip is also installed to support it.\n Likewise, all the custom chips can be upgraded to the ECS chipset.",
        "The chip RAM can be upgraded to  directly on the motherboard, provided a Fat Agnus chip is also installed to support it.\n Likewise, all the custom chips can be upgraded to the ECS chipset.\n The A500+ model instead allowed upgrading by  trapdoor chip RAM without clock, but there was no visible means on board to map any of this as FAST, causing incompatibility with some stubbornly coded programs.\n There were modification instructions available for the A500 to solder or socket another  RAM on the board, then run extra address lines to the trapdoor slot to accommodate an additional  of fast or chip RAM depending on the installed chipset.",
        "Up to  of \"fast RAM\" can be added via the side expansion slot, even more if an accelerator with a non-EC (without reduced data/address bus) processor and 32-bit RAM is used.\n Hard drive and other peripherals can be added via the side expansion slot.\n Several companies provided combined CPU, memory and hard drive upgradesor provided chainable expansions that extended the bus as they were addedas there is only one side expansion slot.\n Expansions are configured automatically by AutoConfig software, so multiple pieces of hardware did not conflict with each other.",
        "Diagnostics\nWhen the computer is powered on a self-diagnostic test is run that will indicates failure with a specific colour:\n\n Medium green means no chip RAM found or is damaged.\n Red means bad kickstart-ROM.\n Yellow means the CPU has crashed (no trap routine or trying to run bad code) or a bad Zorro expansion card.\n Blue means a custom chip problem (Denise, Paula, or Agnus).\n Light green means CIA problem.\n Light gray means that the CIA might be defective.\n mean there is a ROM or CIA problem.\n Black-only (no video) means there is no video output.\n\nThe keyboard LED uses blink codes:\n\nOne blink means the keyboard ROM has a checksum error.\nTwo blinks means RAM failure.\nThree blinks means watchdog timer failure.\nMeasurements",
        "The keyboard LED uses blink codes:\n\nOne blink means the keyboard ROM has a checksum error.\nTwo blinks means RAM failure.\nThree blinks means watchdog timer failure.\nMeasurements\n\nOverall (base): 6.2 cm x 47.4 cm x 33 cm; 2 7/16 in x 18 11/16 in x 13 in.\n\nTrap-door expansion 501 \n\nA popular expansion for the Amiga 500 was the Amiga 501 circuit board that can be installed underneath the computer behind a plastic cover. The expansion contains  RAM configured by default as \"Slow RAM\" or \"trap-door RAM\" and a battery-backed real-time clock (RTC).\n\nThe 512 KB trap-door RAM and 512 KB of original chip RAM will result in 1 MB of total memory. The added memory is known as \"Slow RAM\", as its access is impacted by chip-bus bandwidth contention, while the chipset is not actually able to address it.",
        "Later revisions of the motherboard provide solder-jumpers to relocate the trap-door RAM to the chip memory pool, given the Agnus chip is the newer ECS version, shipped in later A500 motherboards. Newest (rev 8) A500s would share motherboard with A500+, and configure the expansion memory as CHIP by default.\n\nSoftware \n\nEach time the Amiga 500 is booted, it executes code from the Kickstart ROM. The Amiga 500 initially came shipped with AmigaOS 1.2, but units since October 1988 had version 1.3 installed.",
        "Software \n\nEach time the Amiga 500 is booted, it executes code from the Kickstart ROM. The Amiga 500 initially came shipped with AmigaOS 1.2, but units since October 1988 had version 1.3 installed.\n\nReception and sales \nThe Amiga 500 was the best-selling model in the Amiga family of computers. The German computer magazine Chip awarded the model the annual \"Home Computer of the Year\" title three consecutive times. At the European Computer Trade Show 1991, it also won the Leisure Award for the similar \"Home Computer of the Year\" title.",
        "Owing to the inexpensive cost of the Amiga 500 in then price-sensitive Europe, sales of the Amiga family of computers were strongest there, constituting 85 percent of Commodore's total sales in the fourth quarter of 1990. The Amiga 500 was widely perceived as a gaming machine and the Amiga 2000 a computer for artists and hobbyists. \n\nIt has been claimed that Commodore sold as many as six million units worldwide. However, Commodore UK refuted that figure and said that the entire Amiga line sold between four and five million computers. Indeed, Ars Technica provides a year-by-year graph of the sales of all Amiga computers.\n\nThe machine is reported to have sold 1,160,500 units in Germany (including Amiga 500 Plus sales).\n\nAmiga 500 Plus",
        "The machine is reported to have sold 1,160,500 units in Germany (including Amiga 500 Plus sales).\n\nAmiga 500 Plus \n\nThe Amiga 500 Plus (often A500 Plus or simply A500+) is a revised version of the original Amiga 500 computer. The A500+ featured minor changes to the motherboard to make it cheaper to produce than the original A500. It was notable for introducing new versions of Kickstart and Workbench, and for some minor improvements in the custom chips, known as the Enhanced Chip Set (or ECS).",
        "Although officially introduced in 1992, some Amiga 500 units sold in late 1991 actually featured the revised motherboard used in the A500+. Although the Amiga 500+ was an improvement to the Amiga 500, it was minor. It was discontinued and replaced by the Amiga 600 in summer 1992, making it the shortest-lived Amiga model.\n\nCompatibility problems",
        "Due to the new Kickstart v2.04, quite a few popular games (such as Treasure Island Dizzy, Lotus Esprit Turbo Challenge, and SWIV) failed to work on the Amiga 500+, and some people took them back to dealers demanding an original Kickstart 1.3 Amiga 500. This problem was largely solved by third parties who produced Kickstart ROM switching boards, that could allow the Amiga 500+ to be downgraded to Kickstart 1.2 or 1.3. It also encouraged game developers to use better programming habits, which was important since Commodore already had plans for the introduction of the next-generation Amiga 1200 computer. A program, Relokick, was also released (and included with an issue of CU Amiga) which loaded a Kickstart 1.3 ROM image into memory and booted the machine into Kickstart 1.3, allowing most",
        "computer. A program, Relokick, was also released (and included with an issue of CU Amiga) which loaded a Kickstart 1.3 ROM image into memory and booted the machine into Kickstart 1.3, allowing most incompatible software to run (the software did take up 512 KB of system memory, meaning that some 1 MB only games would now  fail for lack of available memory). In some cases, updated compatible versions of games were later released, such as budget versions of Lotus 1 and SWIV, and an update to Bubble Bobble. Double Dragon 2 by Binary Design received an update for ECS machines with the \"Amiga phase-alternated linescan version 4.01/ECS\". This solved compatibility issues with the graphics which appeared garbled on ECS machines, and it also slashed the in-game loading times from around 20 seconds",
        "linescan version 4.01/ECS\". This solved compatibility issues with the graphics which appeared garbled on ECS machines, and it also slashed the in-game loading times from around 20 seconds to just over 6.",
        "Technical specifications \n Motorola 68000 CPU running at  (PAL) /  (NTSC), like its predecessor\n 1 MB of Chip RAM (very early versions came with 512 KB)\n Kickstart 2.04 (v37.175)\n Workbench 37.67 (release 2.04)\n Built-in battery backed RTC (Real Time Clock)\n Full ECS chipset including new version of the Agnus chip and Denise chip\n\nSee also \n\n Amiga models and variants\n Minimigan open-source hardware FPGA implementation\n\nNotes\n\nReferences\n\nFurther reading\n\nExternal links\n\n Amiga 500 Buyers guide\n\nAmiga\nProducts introduced in 1987\n68000-based home computers"
    ],
    [
        "Amiga\nAmiga is a family of personal computers introduced by Commodore in 1985. The original model is one of a number of mid-1980s computers with 16- or 16/32-bit processors, 256 KB or more of RAM, mouse-based GUIs, and significantly improved graphics and audio compared to previous 8-bit systems. These systems include the Atari ST—released earlier the same year—as well as the Macintosh and Acorn Archimedes. Based on the Motorola 68000 microprocessor, the Amiga differs from its contemporaries through the inclusion of custom hardware to accelerate graphics and sound, including sprites and a blitter, and a pre-emptive multitasking operating system called AmigaOS.",
        "The Amiga 1000 was released in July 1985, but production problems kept it from becoming widely available until early 1986. The best-selling model, the Amiga 500, was introduced in 1987 along with the more expandable Amiga 2000. The Amiga 3000 was introduced in 1990, followed by the Amiga 500 Plus, and Amiga 600 in March 1992. Finally, the Amiga 1200 and Amiga 4000 were released in late 1992. The Amiga line sold an estimated 4.85 million units.",
        "Although early advertisements cast the computer as an all-purpose business machine, especially when outfitted with the Sidecar IBM PC compatibility add-on, the Amiga was most commercially successful as a home computer, with a wide range of games and creative software. The Video Toaster hardware and software suite helped Amiga find a prominent role in desktop video and video production. The Amiga's audio hardware made it a popular platform for music tracker software. The processor and memory capacity enabled 3D rendering packages, including LightWave 3D, Imagine, and Traces, a predecessor to Blender.",
        "Poor marketing and the failure of later models to repeat the technological advances of the first systems resulted in Commodore quickly losing market share to the rapidly dropping prices of IBM PC compatibles, which gained 256 color graphics in 1987, as well as the fourth generation of video game consoles.",
        "Commodore ultimately went bankrupt in April 1994 after a version of the Amiga packaged as a game console, the Amiga CD32, failed in the marketplace. Since the demise of Commodore, various groups have marketed successors to the original Amiga line, including Genesi, Eyetech, ACube Systems Srl and A-EON Technology. AmigaOS has influenced replacements, clones, and compatible systems such as MorphOS and AROS. Currently Belgian company Hyperion Entertainment maintains and develops AmigaOS 4, which is an official and direct descendant of AmigaOS 3.1 – the last system made by Commodore for the original Amiga Computers.\n\nHistory",
        "History\n\nConcept and early development \nJay Miner joined Atari, Inc. in the 1970s to develop custom integrated circuits, and led development of the Atari Video Computer System's TIA. When complete, the team began developing a much more sophisticated set of chips, CTIA, ANTIC and POKEY, that formed the basis of the Atari 8-bit family.",
        "With the 8-bit line's launch in 1979, the team once again started looking at a next generation chipset. Nolan Bushnell had sold the company to Warner Communications in 1978, and the new management was much more interested in the existing lines than development of new products that might cut into their sales. Miner wanted to start work with the new Motorola 68000, but management was only interested in another 6502 based system. Miner left the company, and, for a time, the industry.",
        "In 1979, Larry Kaplan left Atari and founded Activision. In 1982, Kaplan was approached by a number of investors who wanted to develop a new game platform. Kaplan hired Miner to run the hardware side of the newly formed company, \"Hi-Toro\". The system was code-named \"Lorraine\" in keeping with Miner's policy of giving systems female names, in this case the company president's wife, Lorraine Morse. When Kaplan left the company late in 1982, Miner was promoted to head engineer and the company relaunched as Amiga Corporation.",
        "The Amiga hardware was designed by Miner, RJ Mical, and Dale Luck. A breadboard prototype for testing and development was largely completed by late 1983, and shown at the January 1984 Consumer Electronics Show (CES). At the time, the operating system was not ready, so the machine was demonstrated with the \"Boing Ball\" demo, a real-time animation showing a red-and-white spinning ball bouncing and casting a shadow; this bouncing ball later became the official logo of Escom subsidiary Amiga Technologies. CES attendees had trouble believing the computer being demonstrated had the power to display such a demo and searched in vain for the \"real\" computer behind it.",
        "A further developed version of the system was demonstrated at the June 1984 CES and shown to many companies in hopes of garnering further funding, but found little interest in a market that was in the final stages of the video game crash of 1983.\n\nIn March, Atari expressed a tepid interest in Lorraine for its potential use in a games console or home computer tentatively known as the . The talks were progressing slowly, and Amiga was running out of money. A temporary arrangement in June led to a $500,000 loan from Atari to Amiga to keep the company going. The terms required the loan to be repaid at the end of the month, otherwise Amiga would forfeit the Lorraine design to Atari.",
        "Commodore \nDuring 1983, Atari lost over  a week, due to the combined effects of the crash and the ongoing price war in the home computer market. By the end of the year, Warner was desperate to sell the company. In January 1984, Jack Tramiel resigned from Commodore due to internal battles over the future direction of the company. A number of Commodore employees followed him to his new company, Tramel Technology. This included a number of the senior technical staff, where they began development of a 68000-based machine of their own. In June, Tramiel arranged a no-cash deal to take over Atari, reforming Tramel Technology as Atari Corporation.",
        "As many Commodore technical staff had moved to Atari, Commodore was left with no workable path to design their own next-generation computer. The company approached Amiga offering to fund development as a home computer system. They quickly arranged to repay the Atari loan, ending that threat. The two companies were initially arranging a  license agreement before Commodore offered  to purchase Amiga outright.",
        "By late 1984, the prototype breadboard chipset had successfully been turned into integrated circuits, and the system hardware was being readied for production. At this time the operating system (OS) was not as ready, and led to a deal to port an OS known as TRIPOS to the platform. TRIPOS was a multitasking system that had been written in BCPL during the 1970s for the PDP-11 minicomputer, but later experimentally ported to the 68000. This early version was known as AmigaDOS and the GUI as Workbench. The BCPL parts were later rewritten in the C language, and the entire system became AmigaOS.",
        "The system was enclosed in a pizza box form factor case; a late change was the introduction of vertical supports on either side of the case to provide a \"garage\" under the main section of the system where the keyboard could be stored.",
        "Launch\nThe first model was announced in 1985 as simply \"The Amiga from Commodore\", later to be retroactively dubbed the Amiga 1000. They were first offered for sale in August, but by October only 50 had been built, all of which were used by Commodore. Machines only began to arrive in quantity in mid-November, meaning they missed the Christmas buying rush. By the end of the year, they had sold 35,000 machines, and severe cashflow problems made the company pull out of the January 1986 CES. Bad or entirely missing marketing, forcing the development team to move to the east coast, notorious stability problems and other blunders limited sales in early 1986 to between 10,000 and 15,000 units a month.",
        "Later models \nIn late 1985, Thomas Rattigan was promoted to COO of Commodore, and then to CEO in February 1986. He immediately implemented an ambitious plan that covered almost all of the company's operations. Among these was the long-overdue cancellation of the now outdated PET and VIC-20 lines, as well as a variety of poorly selling Commodore 64 offshoots and the Commodore 900 workstation effort.\n\nAnother one of the changes was to split the Amiga into two products, a new high-end version of the Amiga aimed at the creative market, and a cost-reduced version that would take over for the Commodore 64 in the low-end market. These new designs were released in 1987 as the Amiga 2000 and Amiga 500, the latter of which went on to widespread success and became their best selling model.",
        "Similar high-end/low-end models would make up the Amiga line for the rest of its history; follow-on designs included the Amiga 3000/Amiga 500 Plus/Amiga 600, and the Amiga 4000/Amiga 1200. These models incorporated a series of technical upgrades known as the ECS and AGA, which added higher resolution displays among many other improvements and simplifications.",
        "The Amiga line sold an estimated 4,850,000 machines over its lifetime. The machines were most popular in the UK and Germany, with about 1.5 million sold in each country, and sales in the high hundreds of thousands in other European nations. The machine was less popular in North America, where an estimated 700,000 were sold. In the United States, the Amiga found a niche with enthusiasts and in vertical markets for video processing and editing. In Europe, it was more broadly popular as a home computer and often used for video games. Beginning in 1988 it overlapped with the 16-bit Mega Drive, then the Super Nintendo Entertainment System in the early 1990s. Commodore UK's Kelly Sumner did not see Sega or Nintendo as competitors, but instead credited their marketing campaigns which spent over",
        "the Super Nintendo Entertainment System in the early 1990s. Commodore UK's Kelly Sumner did not see Sega or Nintendo as competitors, but instead credited their marketing campaigns which spent over  or  for promoting video games as a whole and thus helping to boost Amiga sales.",
        "Bankruptcy \nIn spite of his successes in making the company profitable and bringing the Amiga line to market, Rattigan was soon forced out in a power struggle with majority shareholder, Irving Gould. This is widely regarded as the turning point, as further improvements to the Amiga were eroded by rapid improvements in other platforms.",
        "Commodore shut down the Amiga division on April 26, 1994, and filed for bankruptcy three days later. Commodore's assets were purchased by Escom, a German PC manufacturer, who created the subsidiary company Amiga Technologies. They re-released the A1200 and A4000T, and introduced a new 68060 version of the A4000T. In 1996, it was reported that Escom had sold the Amiga intellectual property to VIScorp for $40m (). Amiga Technologies researched and developed the Amiga Walker prototype. They presented the machine publicly at CeBit, but this deal fell through, and Escom, in turn, went bankrupt in 1997.",
        "A U.S. Wintel PC manufacturer, Gateway 2000, then purchased the Amiga branch and technology. In 2000, Gateway sold the Amiga brand to Amiga, Inc., without having released any products. Amiga, Inc. licensed the rights to sell hardware using the AmigaOne brand to Eyetech Group and Hyperion Entertainment. In 2019, Amiga, Inc. sold its intellectual property to Amiga Corporation.\n\nHardware \n\nThe Amiga has a custom chipset consisting of several coprocessors which handle audio, video, and direct memory access independently of the Central Processing Unit (CPU). This architecture gave the Amiga a performance edge over its competitors, particularly for graphics-intensive applications and games.",
        "The architecture uses two distinct bus subsystems: the chipset bus and the CPU bus. The chipset bus allows the coprocessors and CPU to address \"Chip RAM\". The CPU bus provides addressing to conventional RAM, ROM and the Zorro II or Zorro III expansion subsystems. This enables independent operation of the subsystems. The CPU bus can be much faster than the chipset bus. CPU expansion boards may provide additional custom buses. Additionally, \"busboards\" or \"bridgeboards\" may provide ISA or PCI buses.",
        "Central processing unit \nThe most popular models from Commodore, including the Amiga 1000, Amiga 500, and Amiga 2000, use the Motorola 68000 as the CPU. From a developer's point of view, the 68000 provides a full suite of 32-bit operations, but the chip can address only 16 MB of physical memory and is implemented using a 16-bit arithmetic logic unit and has a 16-bit external data bus, so 32-bit computations are transparently handled as multiple 16-bit values at a performance cost. The later Amiga 2500 and the Amiga 3000 models use fully 32-bit, 68000 compatible, processors from Motorola improved performance and larger addressing capability.",
        "CPU upgrades were offered by both Commodore and third-party manufacturers. Most Amiga models can be upgraded either by direct CPU replacement or through expansion boards. Such boards often included faster and higher capacity memory interfaces and hard disk controllers.\n\nTowards the end of Commodore's time in charge of Amiga development, there were suggestions that Commodore intended to move away from the 68000 series to higher performance RISC processors, such as the PA-RISC. Those ideas were never developed before Commodore filed for bankruptcy. Despite this, third-party manufacturers designed upgrades featuring a combination of 68000 series and PowerPC processors along with a PowerPC native microkernel and software. Later Amiga clones featured PowerPC processors only.",
        "Custom chipset \nThe custom chipset at the core of the Amiga design appeared in three distinct generations, with a large degree of backward-compatibility. The Original Chip Set (OCS) appeared with the launch of the A1000 in 1985. OCS was eventually followed by the modestly improved Enhanced Chip Set (ECS) in 1990 and finally by the partly 32-bit Advanced Graphics Architecture (AGA) in 1992. Each chipset consists of several coprocessors that handle graphics acceleration, digital audio, direct memory access and communication between various peripherals (e.g., CPU, memory and floppy disks). In addition, some models featured auxiliary custom chips that performed tasks such as SCSI control and display de-interlacing.\n\nGraphics",
        "Graphics \n\nAll Amiga systems can display full-screen animated planar graphics with 2, 4, 8, 16, 32, 64 (EHB Mode), or 4096 colors (HAM Mode). Models with the AGA chipset (A1200 and A4000) also have non-EHB 64, 128, 256, and 262144 (HAM8 Mode) color modes and a palette expanded from 4096 to 16.8 million colors.",
        "The Amiga chipset can genlock, which is the ability to adjust its own screen refresh timing to match an incoming NTSC or PAL video signal. When combined with setting transparency, this allows an Amiga to overlay an external video source with graphics. This ability made the Amiga popular for many applications, and provides the ability to do character generation and CGI effects far more cheaply than earlier systems. This ability has been frequently utilized by wedding videographers, TV stations and their weather forecasting divisions (for weather graphics and radar), advertising channels, music video production, and desktop videographers. The NewTek Video Toaster was made possible by the genlock ability of the Amiga.",
        "In 1988, the release of the Amiga A2024 fixed-frequency monochrome monitor with built-in framebuffer and flicker fixer hardware provided the Amiga with a choice of high-resolution graphic modes (1024×800 for NTSC and 1024×1024 for PAL).\n\nReTargetable Graphics \n\nReTargetable Graphics is an API for device drivers mainly used by 3rd party graphics hardware to interface with AmigaOS via a set of libraries. The software libraries may include software tools to adjust resolution, screen colors, pointers and screenmodes. The standard Intuition interface is limited to display depths of 8 bits, while RTG makes it possible to handle higher depths like 24-bits.",
        "Sound",
        "The sound chip, named Paula, supports four PCM sound channels  (two for the left speaker and two for the right) with 8-bit resolution for each channel and a 6-bit volume control per channel. The analog output is connected to a low-pass filter, which filters out high-frequency aliasing when the Amiga is using a lower sampling rate (see Nyquist frequency). The brightness of the Amiga's power LED is used to indicate the status of the Amiga's low-pass filter. The filter is active when the LED is at normal brightness, and deactivated when dimmed (or off on older A500 Amigas). On Amiga 1000 (and first Amiga 500 and Amiga 2000 model), the power LED had no relation to the filter's status, and a wire needed to be manually soldered between pins on the sound chip to disable the filter. Paula can",
        "first Amiga 500 and Amiga 2000 model), the power LED had no relation to the filter's status, and a wire needed to be manually soldered between pins on the sound chip to disable the filter. Paula can read arbitrary waveforms at arbitrary rates and amplitudes directly from the system's RAM, using direct memory access (DMA), making sound playback without CPU intervention possible.",
        "Although the hardware is limited to four separate sound channels, software such as OctaMED uses software mixing to allow eight or more virtual channels, and it was possible for software to mix two hardware channels to achieve a single 14-bit resolution channel by playing with the volumes of the channels in such a way that one of the source channels contributes the most significant bits and the other the least.",
        "The quality of the Amiga's sound output, and the fact that the hardware is ubiquitous and easily addressed by software, were standout features of Amiga hardware unavailable on PC platforms for years. Third-party sound cards exist that provide DSP functions, multi-track direct-to-disk recording, multiple hardware sound channels and 16-bit and beyond resolutions. A retargetable sound API called AHI was developed allowing these cards to be used transparently by the OS and software.\n\nKickstart firmware",
        "Kickstart firmware \n\nKickstart is the firmware upon which AmigaOS is bootstrapped. Its purpose is to initialize the Amiga hardware and core components of AmigaOS and then attempt to boot from a bootable volume, such as a floppy disk or hard disk drive. Most models (excluding the Amiga 1000) come equipped with Kickstart on an embedded ROM-chip.\n\nKeyboard and mouse",
        "The keyboard on Amiga computers is similar to that found on a mid-80s IBM PC: Ten function keys, a numeric keypad, and four separate directional arrow keys. Caps Lock and Control share space to the left of A. Absent are Home, End, Page Up, and Page Down keys: These functions are accomplished on Amigas by pressing shift and the appropriate arrow key. The Amiga keyboard adds a Help key, which a function key usually acts as on PCs (usually F1). In addition to the Control and Alt modifier keys, the Amiga has 2 \"Amiga\" keys, rendered as \"Open Amiga\" and \"Closed Amiga\" similar to the Open/Closed Apple logo keys on Apple II keyboards. The left is used to manipulate the operating system (moving screens and the like) and the right delivers commands to the application. The absence of Num lock frees",
        "logo keys on Apple II keyboards. The left is used to manipulate the operating system (moving screens and the like) and the right delivers commands to the application. The absence of Num lock frees space for more mathematical symbols around the numeric pad.",
        "Like IBM-compatible computers, the mouse has two buttons, but in AmigaOS, pressing and holding the right button replaces the system status line at the top of the screen with a Maclike menu bar. As with Apple's Mac OS prior to Mac OS 8, menu options are selected by releasing the button over that option, not by left clicking. Menu items that have a boolean toggle state can be left clicked whilst the menu is kept open with the right button, which allows the user – for example – to set some selected text to bold, underline and italics in one visit to the menus.",
        "The mouse plugs into one of two Atari joystick ports used for joysticks, game paddles, and graphics tablets. Although compatible with analog joysticks, Atari-style digital joysticks became standard. Unusually, two independent mice can be connected to the joystick ports; some games, such as Lemmings, were designed to take advantage of this.\n\nOther peripherals and expansions \n\nThe Amiga was one of the first computers for which inexpensive sound sampling and video digitization accessories were available. As a result of this and the Amiga's audio and video capabilities, the Amiga became a popular system for editing and producing both music and video.",
        "Many expansion boards were produced for Amiga computers to improve the performance and capability of the hardware, such as memory expansions, SCSI controllers, CPU boards, and graphics boards. Other upgrades include genlocks, network cards for Ethernet, modems, sound cards and samplers, video digitizers, extra serial ports, and IDE controllers. Additions after the demise of Commodore company are USB cards. The most popular upgrades were memory, SCSI controllers and CPU accelerator cards. These were sometimes combined into one device.",
        "Early CPU accelerator cards used the full 32-bit CPUs of the 68000 family such as the Motorola 68020 and Motorola 68030, almost always with 32-bit memory and usually with FPUs and MMUs or the facility to add them. Later designs feature the Motorola 68040 or Motorola 68060. Both CPUs feature integrated FPUs and MMUs. Many CPU accelerator cards also had integrated SCSI controllers.",
        "Phase5 designed the PowerUP boards (Blizzard PPC and CyberStorm PPC) featuring both a 68k (a 68040 or 68060) and a PowerPC (603 or 604) CPU, which are able to run the two CPUs at the same time and share the system memory. The PowerPC CPU on PowerUP boards is usually used as a coprocessor for heavy computations; a powerful CPU is needed to run MAME for example, but even decoding JPEG pictures and MP3 audio was considered heavy computation at the time. It is also possible to ignore the 68k CPU and run Linux on the PPC via project Linux APUS, but a PowerPC-native AmigaOS promised by Amiga Technologies GmbH was not available when the PowerUP boards first appeared.",
        "24-bit graphics cards and video cards were also available. Graphics cards were designed primarily for 2D artwork production, workstation use, and later, gaming. Video cards are designed for inputting and outputting video signals, and processing and manipulating video.",
        "In the North American market, the NewTek Video Toaster was a video effects board that turned the Amiga into an affordable video processing computer that found its way into many professional video environments. One well-known use was to create the special effects in early series of Babylon 5. Due to its NTSC-only design, it did not find a market in countries that used the PAL standard, such as in Europe. In those countries, the OpalVision card was popular, although less featured and supported than the Video Toaster. Low-cost time base correctors (TBC) specifically designed to work with the Toaster quickly came to market, most of which were designed as standard Amiga bus cards.",
        "Various manufacturers started producing PCI busboards for the A1200, A3000 and A4000, allowing standard Amiga computers to use PCI cards such as graphics cards, Sound Blaster sound cards, 10/100 Ethernet cards, USB cards, and television tuner cards. Other manufacturers produced hybrid boards that contained an Intel x86 series chip, allowing the Amiga to emulate a PC.\n\nPowerPC upgrades with Wide SCSI controllers, PCI busboards with Ethernet, sound and 3D graphics cards, and tower cases allowed the A1200 and A4000 to survive well into the late nineties.",
        "PowerPC upgrades with Wide SCSI controllers, PCI busboards with Ethernet, sound and 3D graphics cards, and tower cases allowed the A1200 and A4000 to survive well into the late nineties.\n\nExpansion boards were made by Richmond Sound Design that allow their show control and sound design software to communicate with their custom hardware frames either by ribbon cable or fiber optic cable for long distances, allowing the Amiga to control up to eight million digitally controlled external audio, lighting, automation, relay and voltage control channels spread around a large theme park, for example. See Amiga software for more information on these applications.",
        "Other devices included the following:\n Amiga 501 with 512 KB RAM and real-time clock\n Trumpcard 500 Zorro-II SCSI interface\n GVP A530 Turbo, accelerator, RAM expansion, PC emulator\n A2091 / A590 SCSI hard disk controller + 2 MB RAM expansion\n A3070 SCSI tape backup unit with a capacity of , OEM Archive Viper 1/4-inch\n A2065 Ethernet Zorro-II interface – the first Ethernet interface for Amiga; uses the AMD Am7990 chip The same interface chip is used in DECstation as well.\n Ariadne Zorro-II Ethernet interface using the AMD Am7990\n A4066 Zorro II Ethernet interface using the SMC 91C90QF\n X-Surf from Individual Computers using the Realtek 8019AS\n A2060 Arcnet",
        "Ariadne Zorro-II Ethernet interface using the AMD Am7990\n A4066 Zorro II Ethernet interface using the SMC 91C90QF\n X-Surf from Individual Computers using the Realtek 8019AS\n A2060 Arcnet\n A1010 floppy disk drive consisting of a 3.5-inch double density (DD), ,  drive unit connected via DB-23 connector; track-to-track delay is on the order of . The default capacity is . Many clone drives were available, and products such as the Catweasel and KryoFlux make it possible to read and write Amiga and other special disc formats on standard x86 PCs.\n NE2000-compatible PCMCIA Ethernet cards for Amiga 600 and Amiga 1200",
        "Serial ports \nThe Commodore A2232 board provides seven RS-232C serial ports in addition to the Amiga's built-in serial port. Each port can be driven independently at speeds of 50 to . There is, however, a driver available on Aminet that allows two of the serial ports to be driven at . The serial card used the 65CE02 CPU clocked at . This CPU was also part of the CSG 4510 CPU core that was used in the Commodore 65 computer.",
        "Networking \nAmiga has three networking interface APIs:\n AS225: the official Commodore TCP/IP stack API with hard-coded drivers in revision 1 (AS225r1) for the A2065 Ethernet and the A2060 Arcnet interfaces. In revision 2, (AS225r2) the SANA-II interface was used.\n SANA-II: a standardized API for hardware of network interfaces. It uses an inefficient buffer handling scheme, and lacks proper support for promiscuous and multicast modes.\n Miami Network Interface (MNI): an API that doesn't have the problems that SANA-II suffers from. It requires AmigaOS v2.04 or higher.\n\nDifferent network media were used:\n\nModels and variants",
        "Different network media were used:\n\nModels and variants \n\nThe original Amiga models were produced from 1985 to 1996. They are, in order of production: 1000, 2000, 500, 1500, 2500, 3000, 3000UX, 3000T, CDTV, 500+, 600, 4000, 1200, CD32, and 4000T. The PowerPC-based AmigaOne computers were later marketed beginning in 2002. Several companies and private persons have also released Amiga clones and still do so today.\n\nCommodore Amiga \n\nThe first Amiga model, the Amiga 1000, was launched in 1985. In 2006, PC World rated the Amiga 1000 as the seventh greatest PC of all time, stating \"Years ahead of its time, the Amiga was the world's first multimedia, multitasking personal computer\".",
        "Commodore updated the desktop line of Amiga computers with the Amiga 2000 in 1987, the Amiga 3000 in 1990, and the Amiga 4000 in 1992, each offering improved capabilities and expansion options. The best-selling models were the budget models, however, particularly the highly successful Amiga 500 (1987) and the Amiga 1200 (1992). The Amiga 500+ (1991) was the shortest-lived model, replacing the Amiga 500 and lasting only six months until it was phased out and replaced with the Amiga 600 (1992), which in turn was also quickly replaced by the Amiga 1200.\n\nThe CDTV, launched in 1991, was a CD-ROM-based game console and multimedia appliance several years before CD-ROM drives were common. The system never achieved any real success.",
        "The CDTV, launched in 1991, was a CD-ROM-based game console and multimedia appliance several years before CD-ROM drives were common. The system never achieved any real success.\n\nCommodore's last Amiga offering before filing for bankruptcy was the Amiga CD32 (1993), a 32-bit CD-ROM games console. Although discontinued after Commodore's demise it met with moderate commercial success in Europe. The CD32 was a next-generation CDTV, and it was designed to save Commodore by entering the growing video game console market.",
        "Following purchase of Commodore's assets by Escom in 1995, the A1200 and A4000T continued to be sold in small quantities until 1996, though the ground lost since the initial launch and the prohibitive expense of these units meant that the Amiga line never regained any real popularity.\n\nSeveral Amiga models contained references to songs by the rock band The B-52's. Early A500 units had the words \"B52/ROCK LOBSTER\" silk-screen printed onto their printed circuit board, a reference to the song \"Rock Lobster\" The Amiga 600 referenced \"JUNE BUG\" (after the song \"Junebug\") and the Amiga 1200 had \"CHANNEL Z\" (after \"Channel Z\"), and the CD-32 had \"Spellbound.\"\n\nAmigaOS 4 systems",
        "AmigaOS 4 systems \n\nAmigaOS 4 is designed for PowerPC Amiga systems. It is mainly based on AmigaOS 3.1 source code, with some parts of version 3.9. Currently runs on both Amigas equipped with CyberstormPPC or BlizzardPPC accelerator boards, on the Teron series based AmigaOne computers built by Eyetech under license by Amiga, Inc., on the Pegasos II from Genesi/bPlan GmbH, on the ACube Systems Srl Sam440ep / Sam460ex / AmigaOne 500 systems and on the A-EON AmigaOne X1000.",
        "AmigaOS 4.0 had been available only in developer pre-releases for numerous years until it was officially released in December 2006. Due to the nature of some provisions of the contract between Amiga Inc. and Hyperion Entertainment (the Belgian company that is developing the OS), the commercial AmigaOS 4 had been available only to licensed buyers of AmigaOne motherboards.",
        "AmigaOS 4.0 for Amigas equipped with PowerUP accelerator boards was released in November 2007. Version 4.1 was released in August 2008 for AmigaOne systems, and in May 2011 for Amigas equipped with PowerUP accelerator boards. The most recent release of AmigaOS for all supported platforms is 4.1 update 5. Starting with release 4.1 update 4 there is an Emulation drawer containing official AmigaOS 3.x ROMs (all classic Amiga models including CD32) and relative Workbench files.",
        "Acube Systems entered an agreement with Hyperion under which it has ported AmigaOS 4 to its Sam440ep and Sam460ex line of PowerPC-based motherboards. In 2009 a version for Pegasos II was released in co-operation with Acube Systems. In 2012, A-EON Technology Ltd manufactured and released the AmigaOne X1000 to consumers through their partner, Amiga Kit who provided end-user support, assembly and worldwide distribution of the new system.",
        "Amiga hardware clones",
        "Long-time Amiga developer MacroSystem entered the Amiga-clone market with their DraCo non-linear video editing system. It appears in two versions, initially a tower model and later a cube. DraCo expanded upon and combined a number of earlier expansion cards developed for Amiga (VLabMotion, Toccata, WarpEngine, RetinaIII) into a true Amiga-clone powered by the Motorola 68060 processor. The DraCo can run AmigaOS 3.1 up through AmigaOS 3.9. It is the only Amiga-based system to support FireWire for video I/O. DraCo also offers an Amiga-compatible Zorro-II expansion bus and introduced a faster custom DraCoBus, capable of  transfer rates (faster than Commodore's Zorro-III). The technology was later used in the Casablanca system, a set-top-box also designed for non-linear video editing.",
        "In 1998, Index Information released the Access, an Amiga-clone similar to the Amiga 1200, but on a motherboard that could fit into a standard -inch drive bay. It features either a 68020 or 68030 CPU, with a AGA chipset, and runs AmigaOS 3.1.\n\nIn 1998, former Amiga employees (John Smith, Peter Kittel, Dave Haynie and Andy Finkel to mention few) formed a new company called PIOS. Their hardware platform, PIOS One, was aimed at Amiga, Atari and Macintosh users. The company was renamed to Met@box in 1999 until it folded.",
        "The NatAmi (short for Native Amiga) hardware project began in 2005 with the aim of designing and building an Amiga clone motherboard that is enhanced with modern features. The NatAmi motherboard is a standard Mini-ITX-compatible form factor computer motherboard, powered by a Motorola/Freescale 68060 and its chipset. It is compatible with the original Amiga chipset, which has been inscribed on a programmable FPGA Altera chip on the board. The NatAmi is the second Amiga clone project after the Minimig motherboard, and its history is very similar to that of the C-One mainboard developed by Jeri Ellsworth and Jens Schönfeld. From a commercial point of view, Natami's circuitry and design are currently closed source. One goal of the NatAmi project is to design an Amiga-compatible motherboard",
        "Ellsworth and Jens Schönfeld. From a commercial point of view, Natami's circuitry and design are currently closed source. One goal of the NatAmi project is to design an Amiga-compatible motherboard that includes up-to-date features but that does not rely on emulation (as in WinUAE), modern PC Intel components, or a modern PowerPC mainboard. As such, NatAmi is not intended to become another evolutionary heir to classic Amigas, such as with AmigaOne or Pegasos computers. This \"purist\" philosophy essentially limits the resulting processor speed but puts the focus on bandwidth and low latencies. The developers also recreated the entire Amiga chipset, freeing it from legacy Amiga limitations such as two megabytes of audio and video graphics RAM as in the AGA chipset, and rebuilt this new",
        "The developers also recreated the entire Amiga chipset, freeing it from legacy Amiga limitations such as two megabytes of audio and video graphics RAM as in the AGA chipset, and rebuilt this new chipset by programming a modern FPGA Altera Cyclone IV chip. Later, the developers decided to create from scratch a new software-form processor chip, codenamed \"N68050\" that resides in the physical Altera FPGA programmable chip.",
        "In 2006, two new Amiga clones were announced, both using FPGA based hardware synthesis to replace the Amiga OCS custom chipset. The first, the Minimig, is a personal project of Dutch engineer Dennis van Weeren. Referred to as \"new Amiga hardware\", the original model was built on a Xilinx Spartan-3 development board, but soon a dedicated board was developed. The minimig uses the FPGA to reproduce the custom Denise, Agnus, Paula and Gary chips as well as both 8520 CIAs and implements a simple version of Amber. The rest of the chips are an actual 68000 CPU, ram chips, and a PIC microcontroller for BIOS control. The design for Minimig was released as open-source on July 25, 2007. In February 2008, an Italian company Acube Systems began selling Minimig boards. A third party upgrade replaces",
        "for BIOS control. The design for Minimig was released as open-source on July 25, 2007. In February 2008, an Italian company Acube Systems began selling Minimig boards. A third party upgrade replaces the PIC microcontroller with a more powerful ARM processor, providing more functionality such as write access and support for hard disk images. The Minimig core has been ported to the FPGArcade \"Replay\" board. The Replay uses an FPGA with about three times more capacity and that does support the AGA chipset and a 68020 soft core with 68030 capabilities. The Replay board is designed to implement many older computers and classic arcade machines.",
        "The second is the Clone-A system announced by Individual Computers. As of mid-2007 it has been shown in its development form, with FPGA-based boards replacing the Amiga chipset and mounted on an Amiga 500 motherboard.\n\nOperating systems\n\nAmigaOS \n\nAmigaOS is a single-user multitasking operating system. It was one of the first commercially available consumer operating systems for personal computers to implement preemptive multitasking. It was developed first by Commodore International and initially introduced in 1985 with the Amiga 1000. John C. Dvorak wrote in PC Magazine in 1996:",
        "AmigaOS combines a command-line interface and graphical user interface. AmigaDOS is the disk operating system and command line portion of the OS and Workbench the native graphical windowing, graphical environment for file management and launching applications. AmigaDOS allows long filenames (up to 107 characters) with whitespace and does not require filename extensions. The windowing system and user interface engine that handles all input events is called Intuition.\n\nThe multi-tasking kernel is called Exec. It acts as a scheduler for tasks running on the system, providing pre-emptive multitasking with prioritised round-robin scheduling. It enabled true pre-emptive multitasking in as little as 256 KB of free memory.",
        "AmigaOS does not implement memory protection; the 68000 CPU does not include a memory management unit. Although this speeds and eases inter-process communication because programs can communicate by simply passing a pointer back and forth, the lack of memory protection made the AmigaOS more vulnerable to crashes from badly behaving programs than other multitasking systems that did implement memory protection, and Amiga OS is fundamentally incapable of enforcing any form of security model since any program had full access to the system. A co-operational memory protection feature was implemented in AmigaOS 4 and could be retrofitted to old AmigaOS systems using Enforcer or CyberGuard tools.",
        "The problem was somewhat exacerbated by Commodore's initial decision to release documentation relating not only to the OS's underlying software routines, but also to the hardware itself, enabling intrepid programmers who had developed their skills on the Commodore 64 to POKE the hardware directly, as was done on the older platform. While the decision to release the documentation was a popular one and allowed the creation of fast, sophisticated sound and graphics routines in games and demos, it also contributed to system instabilityas some programmers lacked the expertise to program at this level. For this reason, when the new AGA chipset was released, Commodore declined to release low-level documentation in an attempt to force developers into using the approved software routines.",
        "Influence on other operating systems",
        "AmigaOS directly or indirectly inspired the development of various operating systems. MorphOS and AROS clearly inherit heavily from the structure of AmigaOS as explained directly in articles regarding these two operating systems. AmigaOS also influenced BeOS, which featured a centralized system of Datatypes, similar to that present in AmigaOS. Likewise, DragonFly BSD was also inspired by AmigaOS as stated by Dragonfly developer Matthew Dillon who is a former Amiga developer. WindowLab and amiwm are among several window managers for the X Window System seek to mimic the Workbench interface. IBM licensed the Amiga GUI from Commodore in exchange for the REXX language license. This allowed OS/2 to have the WPS (Workplace Shell) GUI shell for OS/2 2.0, a 32-bit operating system.",
        "Unix and Unix-like systems \nCommodore-Amiga produced Amiga Unix, informally known as Amix, based on AT&T SVR4. It supports the Amiga 2500 and Amiga 3000 and is included with the Amiga 3000UX. Among other unusual features of Amix is a hardware-accelerated windowing system that can scroll windows without copying data. Amix is not supported on the later Amiga systems based on 68040 or 68060 processors.\n\nOther, still maintained, operating systems are available for the classic Amiga platform, including Linux and NetBSD. Both require a CPU with MMU such as the 68020 with 68851 or full versions of the 68030, 68040 or 68060. There is also a version of Linux for Amigas with PowerPC accelerator cards. Debian and Yellow Dog Linux can run on the AmigaOne.",
        "There is an official, older version of OpenBSD. The last Amiga release is 3.2. MINIX 1.5.10 also runs on Amiga.\n\nEmulating other systems \n\nThe Amiga Sidecar is a complete IBM PC XT compatible computer contained in an expansion card. It was released by Commodore in 1986 and promoted as a way to run business software on the Amiga 1000.\n\nAmiga software",
        "Amiga software \n\nIn the late 1980s and early 1990s the platform became particularly popular for gaming, demoscene activities and creative software uses. During this time commercial developers marketed a wide range of games and creative software, often developing titles simultaneously for the Atari ST due to the similar hardware architecture. Popular creative software included 3D rendering (ray-tracing) packages, bitmap graphics editors, desktop video software, software development packages and \"tracker\" music editors.",
        "Until the late 1990s the Amiga remained a popular platform for non-commercial software, often developed by enthusiasts, and much of which was freely redistributable. An on-line archive, Aminet, was created in 1991 and until the late-1990s was the largest public archive of software, art and documents for any platform.\n\nMarketing \n\nThe name Amiga was chosen by the developers from the Spanish word for a female friend, because they knew Spanish, and because it occurred before Apple and Atari alphabetically. It also conveyed the message that the Amiga computer line was \"user friendly\" as a pun or play on words.",
        "The first official Amiga logo was a rainbow-colored double check mark. In later marketing material Commodore largely dropped the checkmark and used logos styled with various typefaces. Although it was never adopted as a trademark by Commodore, the \"Boing Ball\" has been synonymous with Amiga since its launch. It became an unofficial and enduring theme after a visually impressive animated demonstration at the 1984 Winter Consumer Electronics Show in January 1984 showing a checkered ball bouncing and rotating. Following Escom's purchase of Commodore in 1996, the Boing Ball theme was incorporated into a new logo.",
        "Early Commodore advertisements attempted to cast the computer as an all-purpose business machine, though the Amiga was most commercially successful as a home computer. Throughout the 1980s and early 1990s Commodore primarily placed advertising in computer magazines and occasionally in national newspapers and on television.\n\nLegacy \nSince the demise of Commodore, various groups have marketed successors to the original Amiga line:",
        "Genesi sold PowerPC based hardware under the Pegasos brand running AmigaOS and MorphOS;\n Eyetech sold PowerPC based hardware under the AmigaOne brand from 2002 to 2005 running AmigaOS 4;\n Amiga Kit distributes and sells PowerPC based hardware under the AmigaOne brand from 2010 to present day running AmigaOS 4;\n ACube Systems sells the AmigaOS 3 compatible Minimig system with a Freescale MC68SEC000 CPU (Motorola 68000 compatible) and AmigaOS 4 compatible Sam440 / Sam460 / AmigaOne 500 systems with PowerPC processors;\n A-EON Technology Ltd sells the AmigaOS 4 compatible AmigaOne X1000 system with P.A. Semi PWRficient PA6T-1682M processor.\n Amiga Kit, Vesalia Computer and AMIGAstore.eu sell numerous items from aftermarket components to refurbished classic systems.",
        "AmigaOS and MorphOS are commercial proprietary operating systems. AmigaOS 4, based on AmigaOS 3.1 source code with some parts of version 3.9, is developed by Hyperion Entertainment and runs on PowerPC based hardware. MorphOS, based on some parts of AROS source code, is developed by MorphOS Team and is continued on Apple and other PowerPC based hardware.\n\nThere is also AROS, a free and open source operating system (re-implementation of the AmigaOS 3.1 APIs), for Amiga 68k, x86 and ARM hardware (one version runs Linux-hosted on the Raspberry Pi). In particular, AROS for Amiga 68k hardware aims to create an open source Kickstart ROM replacement for emulation purpose and/or for use on real \"classic\" hardware.",
        "Magazines \nAmiga Format continued publication until 2000. Amiga Active was launched in 1999 and was published until 2001.\n\nSeveral magazines are in publication today: Amiga Future, which is available in both English and German; Bitplane.it, a bimonthly magazine in Italian; and AmigaPower, a long-running French magazine. Print magazine Amiga Addict started publication in 2020.\n\nTrade Shows \nThe Amiga continues to be popular enough that fans to support conferences such as Amiga37 which had over 50 vendors.",
        "Trade Shows \nThe Amiga continues to be popular enough that fans to support conferences such as Amiga37 which had over 50 vendors.\n\nUses \nThe Amiga series of computers found a place in early computer graphic design and television presentation. Season 1 and part of season 2 of the television series Babylon 5 were rendered in LightWave 3D on Amigas. Other television series using Amigas for special effects included SeaQuest DSV and Max Headroom.\n\nIn addition, many celebrities and notable individuals have made use of the Amiga:",
        "Andy Warhol was an early user of the Amiga and appeared at the launch, where he made a computer artwork of Debbie Harry. Warhol used the Amiga to create a new style of art made with computers, and was the author of a multimedia opera called You Are the One, which consists of an animated sequence featuring images of actress Marilyn Monroe assembled in a short movie with a soundtrack. The video was discovered on two old Amiga floppies in a drawer in Warhol's studio and repaired in 2006 by the Detroit Museum of New Art. The pop artist has been quoted as saying: \"The thing I like most about doing this kind of work on the Amiga is that it looks like my work in other media\".",
        "Artist Jean \"Moebius\" Giraud credits the Amiga he bought for his son as a bridge to learning about \"using paint box programs\". He uploaded some of his early experiments to the file sharing forums on CompuServe.\n Futurist and science fiction author Arthur C. Clarke used an Amiga computer to calculate and explore Mandelbrot sets in the 1988 documentary film God, the Universe and Everything Else.\n The \"Weird Al\" Yankovic film UHF contains a computer-animated music video parody of the Dire Straits song \"Money for Nothing\", titled \"Money for Nothing/Beverly Hillbillies*\". According to the DVD commentary track, this spoof was created on an Amiga home computer.\n Rolf Harris used an Amiga to digitize his hand-drawn art work for animation on his television series Rolf's Cartoon Club.",
        "Rolf Harris used an Amiga to digitize his hand-drawn art work for animation on his television series Rolf's Cartoon Club.\n Debbie Harry appeared together with Andy Warhol (see above) at launch.\n Todd Rundgren's video \"Change Myself\" was produced with Toaster and Lightwave.\n Scottish pop artist Calvin Harris composed his 2007 debut album I Created Disco with an Amiga 1200.",
        "Todd Rundgren's video \"Change Myself\" was produced with Toaster and Lightwave.\n Scottish pop artist Calvin Harris composed his 2007 debut album I Created Disco with an Amiga 1200.\n Susumu Hirasawa, a Japanese progressive-electronic artist, is known for using Amigas to compose and perform music, aid his live shows and make his promotional videos. He has also been inspired by the Amiga, and has referenced it in his lyrics. His December 13, 1994 \"Adios Jay\" Interactive Live Show was dedicated to (then recently deceased) Jay Miner. He also used the Amiga to create the virtual drummer TAINACO, who was a CG rendered figure whose performance was made with Elan Performer and was projected with DCTV. He also composed and performed \"Eastern-boot\", the AmigaOS 4 boot jingle.",
        "Electronic musician Max Tundra created his three albums with an Amiga 500.\n Bob Casale, keyboardist and guitarist of the new wave band Devo, used Amiga computer graphics on the album cover to Devo's album Total Devo.\n Most of Pokémon Gold and Silver's music was created on an Amiga computer, converted to MIDI, and then reconverted to the game's music format.",
        "Special purpose applications\n Amigas were used in various NASA laboratories to keep track of low orbiting satellites until 2004. Amigas were used at Kennedy Space Center to run strip-chart recorders, to format and display data, and control stations of platforms for Delta rocket launches.\n Palomar Observatory used Amigas to calibrate and control the charge-coupled devices in their telescopes, as well as to display and store the digitized images they collected.\n London Transport Museum developed their own interactive multi-media software for the CD32 including a virtual tour of the museum.\n Amiga 500 motherboards were used, in conjunction with a LaserDisc player and genlock device, in arcade games manufactured by American Laser Games.",
        "Amiga 500 motherboards were used, in conjunction with a LaserDisc player and genlock device, in arcade games manufactured by American Laser Games.\n A custom Amiga 4000T motherboard was used in the HDI 1000 medical ultrasound system built by Advanced Technology Labs.\n, the Grand Rapids Public School district uses a Commodore Amiga 2000 with 1200 baud modem to automate its air conditioning and heating systems for the 19 schools covered by the GRPS district. The system has been operating day and night for decades.\n The Weather Network used Amigas to display the weather on TV.",
        "See also \n\nAmiga Forever\nList of Amiga games\nAmiga emulation\nSAGE Computer Technology\n\nNotes\n\nReferences\n\nWorks cited\n\nExternal links \n\n Official AmigaOS website\n History of the Amiga at Ars Technica\n Amiga, Inc. Website\n Amiga Software Database\n Amiga Hardware Database\n Big Book of Amiga Hardware\n Lemon Amiga: Amiga Fanbase\n \n RUN Magazine Issue 21, September 1985 article on the introduction of the Amiga\n Amiga.org: community forums and support\n English Amiga Board: Amiga community forums and support\n The Hall of Light: the database of Amiga games\n The Amiga Museum\n\n \n68k-based computers\nAmerican inventions\nComputer-related introductions in 1985\nDesktop computers\nHome computers"
    ],
    [
        "Ammeter\nAn ammeter (abbreviation of Ampere meter) is an instrument used to measure the current in a circuit. Electric currents are measured in amperes (A), hence the name. For direct measurement, the ammeter is connected in series with the circuit in which the current is to be measured. An ammeter usually has low resistance so that it does not cause a significant voltage drop in the circuit being measured.",
        "Instruments used to measure smaller currents, in the milliampere or microampere range, are designated as milliammeters or microammeters. Early ammeters were laboratory instruments that relied on the Earth's magnetic field for operation. By the late 19th century, improved instruments were designed which could be mounted in any position and allowed accurate measurements in electric power systems. It is generally represented by letter 'A' in a circuit.\n\nHistory",
        "History\n\nThe relation between electric current, magnetic fields and physical forces was first noted by Hans Christian Ørsted in 1820, who observed a compass needle was deflected from pointing North when a current flowed in an adjacent wire. The tangent galvanometer was used to measure currents using this effect, where the restoring force returning the pointer to the zero position was provided by the Earth's magnetic field. This made these instruments usable only when aligned with the Earth's field. Sensitivity of the instrument was increased by using additional turns of wire to multiply the effect – the instruments were called \"multipliers\".",
        "The word rheoscope as a detector of electrical currents was coined by Sir Charles Wheatstone about 1840 but is no longer used to describe electrical instruments.  The word makeup is similar to that of rheostat (also coined by Wheatstone) which was a device used to adjust the current in a circuit. Rheostat is a historical term for a variable resistance, though unlike rheoscope may still be encountered.\n\nTypes\n\nSome instruments are panel meters, meant to be mounted on some sort of control panel. Of these, the flat, horizontal or vertical type is often called an edgewise meter.\n\nMoving-coil",
        "Some instruments are panel meters, meant to be mounted on some sort of control panel. Of these, the flat, horizontal or vertical type is often called an edgewise meter.\n\nMoving-coil \n\nThe D'Arsonval galvanometer is a moving coil ammeter. It uses magnetic deflection, where current passing through a coil placed in the magnetic field of a permanent magnet causes the coil to move. The modern form of this instrument was developed by Edward Weston, and uses two spiral springs to provide the restoring force. The uniform air gap between the iron core and the permanent magnet poles make the deflection of the meter linearly proportional to current. These meters have linear scales. Basic meter movements can have full-scale deflection for currents from about 25 microamperes to 10 milliamperes.",
        "Because the magnetic field is polarised, the meter needle acts in opposite directions for each direction of current. A DC ammeter is thus sensitive to which polarity it is connected in; most are marked with a positive terminal, but some have centre-zero mechanisms\nand can display currents in either direction. A moving coil meter indicates the average (mean) of a varying current through it,\nwhich is zero for AC. For this reason, moving-coil meters are only usable directly for DC, not AC.\n\nThis type of meter movement is extremely common for both ammeters and other meters derived from them, such as voltmeters and ohmmeters.",
        "This type of meter movement is extremely common for both ammeters and other meters derived from them, such as voltmeters and ohmmeters.\n\nMoving magnet \nMoving magnet ammeters operate on essentially the same principle as moving coil, except that the coil is mounted in the meter case, and a permanent magnet moves the needle. Moving magnet Ammeters are able to carry larger currents than moving coil instruments, often several tens of Amperes, because the coil can be made of thicker wire and the current does not have to be carried by the hairsprings. Indeed, some Ammeters of this type do not have hairsprings at all, instead using a fixed permanent magnet to provide the restoring force.",
        "Electrodynamic \nAn electrodynamic ammeter uses an electromagnet instead of the permanent magnet of the d'Arsonval movement. This instrument can respond to both alternating and direct current and also indicates true RMS for AC. See Wattmeter for an alternative use for this instrument.\n\nMoving-iron",
        "Moving iron ammeters use a piece of iron which moves when acted upon by the electromagnetic force of a fixed coil of wire. The moving-iron meter was invented by Austrian engineer Friedrich Drexler in 1884.",
        "This type of meter responds to both direct and alternating currents (as opposed to the moving-coil ammeter, which works on direct current only). The iron element consists of a moving vane attached to a pointer, and a fixed vane, surrounded by a coil. As alternating or direct current flows through the coil and induces a magnetic field in both vanes, the vanes repel each other and the moving vane deflects against the restoring force provided by fine helical springs. The deflection of a moving iron meter is proportional to the square of the current. Consequently, such meters would normally have a nonlinear scale, but the iron parts are usually modified in shape to make the scale fairly linear over most of its range.  Moving iron instruments indicate the RMS value of any AC waveform applied.",
        "a nonlinear scale, but the iron parts are usually modified in shape to make the scale fairly linear over most of its range.  Moving iron instruments indicate the RMS value of any AC waveform applied. Moving iron ammeters are commonly used to measure current in industrial frequency AC circuits.",
        "Hot-wire \n\nIn a hot-wire ammeter, a current passes through a wire which expands as it heats. Although these instruments have slow response time and low accuracy, they were sometimes used in measuring radio-frequency current.\nThese also measure true RMS for an applied AC.\n\nDigital \nIn much the same way as the analogue ammeter formed the basis for a wide variety of derived meters, including voltmeters, the basic mechanism for a digital meter is a digital voltmeter mechanism, and other types of meter are built around this.",
        "Digital ammeter designs use a shunt resistor to produce a calibrated voltage proportional to the current flowing. This voltage is then measured by a digital voltmeter, through use of an analog-to-digital converter (ADC); the digital display is calibrated to display the current through the shunt. Such instruments are often calibrated to indicate the RMS value for a sine wave only, but many designs will indicate true RMS within limitations of the wave crest factor.\n\nIntegrating",
        "Integrating \n\nThere is also a range of devices referred to as integrating ammeters.\nIn these ammeters the current is summed over time, giving as a result the product of current and time; which is proportional to the electrical charge transferred with that current. These can be used for metering energy (the charge needs to be multiplied by the voltage to give energy) or for estimating the charge of a battery or capacitor.\n\nPicoammeter\nA picoammeter, or pico ammeter, measures very low electric current, usually from the picoampere range at the lower end to the milliampere range at the upper end. Picoammeters are used where the current being measured is below the limits of sensitivity of other devices, such as multimeters.",
        "Most picoammeters use a \"virtual short\" technique and have several different measurement ranges that must be switched between to cover multiple decades of measurement. Other modern picoammeters use log compression and a \"current sink\" method that eliminates range switching and associated voltage spikes.\nSpecial design and usage considerations must be observed in order to reduce leakage current which may swamp measurements such as special insulators and driven shields. Triaxial cable is often used for probe connections.\n\nApplication",
        "Ammeters must be connected in series with the circuit to be measured. For relatively small currents (up to a few amperes), an ammeter may pass the whole of the circuit current. For larger direct currents, a shunt resistor carries most of the circuit current and a small, accurately-known fraction of the current passes through the meter movement. For alternating current circuits, a current transformer may be used to provide a convenient small current to drive an instrument, such as 1 or 5 amperes, while the primary current to be measured is much larger (up to thousands of amperes). The use of a shunt or current transformer also allows convenient location of the indicating meter without the need to run heavy circuit conductors up to the point of observation. In the case of alternating",
        "of a shunt or current transformer also allows convenient location of the indicating meter without the need to run heavy circuit conductors up to the point of observation. In the case of alternating current, the use of a current transformer also isolates the meter from the high voltage of the primary circuit. A shunt provides no such isolation for a direct-current ammeter, but where high voltages are used it may be possible to place the ammeter in the \"return\" side of the circuit which may be at low potential with respect to earth.",
        "Ammeters must not be connected directly across a voltage source since their internal resistance is very low and excess current would flow. Ammeters are designed for a low voltage drop across their terminals, much less than one volt; the extra circuit losses produced by the ammeter are called its \"burden\" on the measured circuit(I).",
        "Ordinary Weston-type meter movements can measure only milliamperes at most, because the springs and practical coils can carry only limited currents. To measure larger currents, a resistor called a shunt is placed in parallel with the meter. The resistances of shunts is in the integer to fractional milliohm range. Nearly all of the current flows through the shunt, and only a small fraction flows through the meter. This allows the meter to measure large currents. Traditionally, the meter used with a shunt has a full-scale deflection (FSD) of , so shunts are typically designed to produce a voltage drop of  when carrying their full rated current.",
        "To make a multi-range ammeter, a selector switch can be used to connect one of a number of shunts across the meter. It must be a make-before-break switch to avoid damaging current surges through the meter movement when switching ranges.\n\nA better arrangement is the Ayrton shunt or universal shunt, invented by William E. Ayrton, which does not require a make-before-break switch. It also avoids any inaccuracy because of contact resistance. In the figure, assuming for example, a movement with a full-scale voltage of 50 mV and desired current ranges of 10 mA, 100 mA, and 1 A, the resistance values would be: R1 = 4.5 ohms, R2 = 0.45 ohm, R3 = 0.05 ohm. And if the movement resistance is 1000 ohms, for example, R1 must be adjusted to 4.525 ohms.",
        "Switched shunts are rarely used for currents above 10 amperes.\n\nZero-center ammeters are used for applications requiring current to be measured with both polarities, common in scientific and industrial equipment. Zero-center ammeters are also commonly placed in series with a battery. In this application, the charging of the battery deflects the needle to one side of the scale (commonly, the right side) and the discharging of the battery deflects the needle to the other side. A special type of zero-center ammeter for testing high currents in cars and trucks has a pivoted bar magnet that moves the pointer, and a fixed bar magnet to keep the pointer centered with no current. The magnetic field around the wire carrying current to be measured deflects the moving magnet.",
        "Since the ammeter shunt has a very low resistance, mistakenly wiring the ammeter in parallel with a voltage source will cause a short circuit, at best blowing a fuse, possibly damaging the instrument and wiring, and exposing an observer to injury.",
        "In AC circuits, a current transformer can be used to convert the large current in the main circuit into a smaller current more suited to a meter. Some designs of transformer are able to directly convert the magnetic field around a conductor into a small AC current, typically either  or  at full rated current, that can be easily read by a meter. In a similar way, accurate AC/DC non-contact ammeters have been constructed using Hall effect magnetic field sensors. A portable hand-held clamp-on ammeter is a common tool for maintenance of industrial and commercial electrical equipment, which is temporarily clipped over a wire to measure current. Some recent types have a parallel pair of magnetically soft probes that are placed on either side of the conductor.\n\nSee also",
        "See also\n\nClamp meter\nClass of accuracy in electrical measurements\nElectric circuit\nElectrical measurements\nElectrical current#Measurement\nElectronics\nList of electronics topics\nMeasurement category\nMultimeter\nOhmmeter\nRheoscope\nVoltmeter\n\nNotes\n\nReferences\n\nExternal links\n\n  — from Lessons in Electric Circuits series main page\n\nElectrical meters\nElectronic test equipment\nFlow meters"
    ],
    [
        "AMOS (programming language)\nAMOS BASIC is a dialect of the BASIC programming language for the Amiga computer. Following on from the successful STOS BASIC for the Atari ST, AMOS BASIC was written for the Amiga by François Lionet with Constantin Sotiropoulos and published by Europress Software in 1990.\n\nHistory \nAMOS competed on the Amiga platform with Acid Software's Blitz BASIC. Both BASICs differed from other dialects on different platforms, in that they allowed the easy creation of fairly demanding multimedia software, with full structured code and many high-level functions to load images, animations, sounds and display them in various ways.",
        "The original AMOS was a BASIC interpreter which, whilst working fine, suffered the same disadvantages of any language being run interpretively. By all accounts, AMOS was extremely fast among interpreted languages, being speedy enough that an extension called AMOS 3D could produce playable 3D games even on plain 7 MHz 68000 Amigas. Later, an AMOS compiler was developed that further increased speed. AMOS could also run MC68000 machine code, loaded into a program's memory banks.",
        "To simplify animation of sprites, AMOS included the AMOS Animation Language (AMAL), a compiled sprite scripting language which runs independently of the main AMOS BASIC program. It was also possible to control screen and \"rainbow\" effects using AMAL scripts. AMAL scripts in effect created CopperLists, small routines executed by the Amiga's Agnus chip.",
        "After the original version of AMOS, Europress released a compiler (AMOS Compiler), and two other versions of the language: Easy AMOS, a simpler version for beginners, and AMOS Professional, a more advanced version with added features, such as a better integrated development environment, ARexx support, a new user interface API and new flow control constructs. Neither of these new versions was significantly more popular than the original AMOS.\n\nAMOS was used mostly to make multimedia software, video games (platformers and graphical adventures) and educational software.\n\nThe language was mildly successful within the Amiga community. Its ease of use made it especially attractive to beginners.",
        "The language was mildly successful within the Amiga community. Its ease of use made it especially attractive to beginners.\n\nPerhaps AMOS BASIC's biggest disadvantage, stemming from its Atari ST lineage, was its incompatibility with the Amiga's operating system functions and interfaces. Instead, AMOS BASIC controlled the computer directly, which caused programs written in it to have a non-standard user interface, and also caused compatibility problems with newer versions of hardware.\n\nToday, the language has declined in popularity along with the Amiga computer for which it was written. Despite this, a small community of enthusiasts are still using it. The source code to AMOS was released around 2001 under a BSD style license by Clickteam, a company that includes the original programmer.",
        "Software \nSoftware written using AMOS BASIC includes:\n\n Miggybyte\n Scorched Tanks\n Games by Vulcan Software, amongst which was the Valhalla trilogy\n Amiga version of Ultimate Domain (called Genesia) by Microïds\n Flight of the Amazon Queen, by Interactive Binary Illusions\n Extreme Violence, included on an Amiga Power cover disk\n Jetstrike, a commercial game by Rasputin Software\n Black Dawn, a 1993 game for the Amiga personal computer\n\nReferences",
        "References\n\nExternal links \n Source code for AMOS Professional 68000 ASM from pianetaamiga.it (archived, ZIP)\n Source code for AMOS and STOS 68000 ASM from clickteam.com (archived, ZIP)\n The AMOS Factory (an AMOS support/community site)\n Amigacoding website (contains in-depth info and references for AMOS - Archived version 22 Sep 2015)\n History of STOS and AMOS: how they came to be published in the UK\n Amos Professional group on Facebook (one of the members is AMOS' original developer François Lionet)\n\nBASIC programming language family\nVideo game development software\nAmiga development software\nSoftware using the BSD license\nProgramming languages created in 1990"
    ],
    [
        "Amplitude modulation\nAmplitude modulation (AM) is a  modulation technique used in electronic communication, most commonly for transmitting messages with a radio wave. In amplitude modulation, the amplitude (signal strength) of the wave is varied in proportion to that of the message signal, such as an audio signal. This technique contrasts with angle modulation, in which either the frequency of the carrier wave is varied, as in frequency modulation, or its phase, as in phase modulation.",
        "AM was the earliest modulation method used for transmitting audio in radio broadcasting. It was developed during the first quarter of the 20th century beginning with Roberto Landell de Moura and Reginald Fessenden's radiotelephone experiments in 1900. This original form of AM is sometimes called double-sideband amplitude modulation (DSBAM), because the standard method produces sidebands on either side of the carrier frequency. Single-sideband modulation uses bandpass filters to eliminate one of the sidebands and possibly the carrier signal, which improves the ratio of message power to total transmission power, reduces power handling requirements of line repeaters, and permits better bandwidth utilization of the transmission medium.",
        "AM remains in use in many forms of communication in addition to AM broadcasting: shortwave radio, amateur radio, two-way radios, VHF aircraft radio, citizens band radio, and in computer modems in the form of QAM.\n\nFoundation \nIn electronics, telecommunications and mechanics, modulation means varying some aspect of a continuous wave carrier signal with an information-bearing modulation waveform, such as an audio signal which represents sound, or a video signal which represents images. In this sense, the carrier wave, which has a much higher frequency than the message signal, carries the information. At the receiving station, the message signal is extracted from the modulated carrier by demodulation.",
        "In general form, a modulation process of a sinusoidal carrier wave may be described by the following equation:\n\n.\n\nA(t) represents the time-varying amplitude of the sinusoidal carrier wave and the cosine-term is the carrier at its angular frequency , and the  instantaneous phase deviation . This description directly provides the two major groups of modulation, amplitude modulation and angle modulation. In  angle modulation, the term A(t) is constant and the second term of the equation has a functional relationship to the modulating message signal. Angle modulation provides two methods of modulation, frequency modulation and phase modulation.",
        "In amplitude modulation, the angle term is held constant and the first term, A(t), of the equation has a functional relationship to the modulating message signal.\n\nThe modulating message signal may be analog in nature, or it may be a digital signal, in which case the technique is generally called amplitude-shift keying.",
        "The modulating message signal may be analog in nature, or it may be a digital signal, in which case the technique is generally called amplitude-shift keying.\n\nFor example, in AM radio communication, a continuous wave radio-frequency signal has its amplitude modulated by an audio waveform before transmission.  The message signal determines the envelope of the  transmitted waveform. In the frequency domain, amplitude modulation produces a signal with power concentrated at the carrier frequency and two adjacent sidebands. Each sideband is equal in bandwidth to that of the modulating signal, and is a mirror image of the other. Standard AM is thus sometimes called \"double-sideband amplitude modulation\" (DSBAM).",
        "A disadvantage of all amplitude modulation techniques, not only standard AM, is that the receiver amplifies and detects noise and electromagnetic interference in equal proportion to the signal. Increasing the received signal-to-noise ratio, say, by a factor of 10 (a 10 decibel improvement), thus would require increasing the transmitter power by a factor of 10. This is in contrast to frequency modulation (FM) and digital radio where the effect of such noise following demodulation is strongly reduced so long as the received signal is well above the threshold for reception. For this reason AM broadcast is not favored for music and high fidelity broadcasting, but rather for voice communications and broadcasts (sports, news, talk radio etc.).",
        "AM is also inefficient in power usage; at least two-thirds of the power is concentrated in the carrier signal.  The carrier signal contains none of the original information being transmitted (voice, video, data, etc.). However its presence provides a simple means of demodulation using envelope detection, providing a frequency and phase reference to extract the modulation from the sidebands. In some modulation systems based on AM, a lower transmitter power is required through partial or total elimination of the carrier component, however receivers for these signals are more complex because they must provide a precise carrier frequency reference signal (usually as shifted to the intermediate frequency) from a greatly reduced \"pilot\" carrier (in reduced-carrier transmission or DSB-RC) to use",
        "must provide a precise carrier frequency reference signal (usually as shifted to the intermediate frequency) from a greatly reduced \"pilot\" carrier (in reduced-carrier transmission or DSB-RC) to use in the demodulation process. Even with the carrier totally eliminated in double-sideband suppressed-carrier transmission, carrier regeneration is possible using a Costas phase-locked loop. This does not work for single-sideband suppressed-carrier transmission (SSB-SC), leading to the characteristic \"Donald Duck\" sound from such receivers when slightly detuned. Single-sideband AM is nevertheless used widely in amateur radio and other voice communications because it has power and bandwidth efficiency (cutting the RF bandwidth in half compared to standard AM). On the other hand, in medium wave",
        "used widely in amateur radio and other voice communications because it has power and bandwidth efficiency (cutting the RF bandwidth in half compared to standard AM). On the other hand, in medium wave and short wave broadcasting, standard AM with the full carrier allows for reception using inexpensive receivers. The broadcaster absorbs the extra power cost to greatly increase potential audience.",
        "Shift keying \nA simple form of digital amplitude modulation which can be used for transmitting binary data is on–off keying, the simplest form of amplitude-shift keying, in which ones and zeros are represented by the presence or absence of a carrier. On–off keying is likewise used by radio amateurs to transmit Morse code where it is known as continuous wave (CW) operation, even though the transmission is not strictly \"continuous.\"  A more complex form of AM, quadrature amplitude modulation is now more commonly used with digital data, while making more efficient use of the available bandwidth.",
        "Analog telephony \nA simple form of amplitude modulation is the transmission of speech signals from a traditional analog telephone set using a common battery local loop. The direct current provided by the central office battery is a carrier with a frequency of 0 Hz. It is modulated by a microphone (transmitter) in the telephone set according to the acoustic signal from the speaker. The result is a varying amplitude direct current, whose AC-component is the speech signal extracted at the central office for transmission to another subscriber.",
        "Amplitude reference",
        "An additional function provided by the carrier in standard AM, but which is lost in either single or double-sideband suppressed-carrier transmission, is that it provides an amplitude reference. In the receiver, the automatic gain control (AGC) responds to the carrier so that the reproduced audio level stays in a fixed proportion to the original modulation. On the other hand, with suppressed-carrier transmissions there is no transmitted power during pauses in the modulation, so the AGC must respond to peaks of the transmitted power during peaks in the modulation. This typically involves a so-called fast attack, slow decay circuit which holds the AGC level for a second or more following such peaks, in between syllables or short pauses in the program. This is very acceptable for",
        "a so-called fast attack, slow decay circuit which holds the AGC level for a second or more following such peaks, in between syllables or short pauses in the program. This is very acceptable for communications radios, where compression of the audio aids intelligibility. However it is absolutely undesired for music or normal broadcast programming, where a faithful reproduction of the original program, including its varying modulation levels, is expected.",
        "ITU type designations\nIn 1982, the International Telecommunication Union (ITU) designated the types of amplitude modulation:\n\nHistory",
        "History\n\nAmplitude modulation was used in experiments of multiplex telegraph and telephone transmission in the late 1800s. However, the practical development of this technology is identified with the period between 1900 and 1920 of radiotelephone transmission, that is, the effort to send audio signals by radio waves.  The first radio transmitters, called spark gap transmitters, transmitted information by wireless telegraphy, using pulses of the carrier wave to spell out text messages in Morse code. They could not transmit audio because the carrier consisted of strings of damped waves, pulses of radio waves that declined to zero, and sounded like a buzz in receivers. In effect they were already amplitude modulated.",
        "Continuous waves\nThe first AM transmission was made by Canadian researcher Reginald Fessenden on 23 December 1900 using a spark gap transmitter with a specially designed high frequency 10 kHz interrupter, over a distance of one mile (1.6 km) at Cobb Island, Maryland, US. His first transmitted words were, \"Hello. One, two, three, four. Is it snowing where you are, Mr. Thiessen?\". The words were barely intelligible above the background buzz of the spark.",
        "Fessenden was a significant figure in the development of AM radio. He was one of the first researchers to realize, from experiments like the above, that the existing technology for producing radio waves, the spark transmitter, was not usable for amplitude modulation, and that a new kind of transmitter, one that produced sinusoidal continuous waves, was needed.  This was a radical idea at the time, because experts believed the impulsive spark was necessary to produce radio frequency waves, and Fessenden was ridiculed. He invented and helped develop one of the first continuous wave transmitters – the Alexanderson alternator, with which he made what is considered the first AM public entertainment broadcast on Christmas Eve, 1906.  He also discovered the principle on which AM is based,",
        "– the Alexanderson alternator, with which he made what is considered the first AM public entertainment broadcast on Christmas Eve, 1906.  He also discovered the principle on which AM is based, heterodyning, and invented one of the first detectors able to rectify and receive AM, the electrolytic detector or \"liquid baretter\", in 1902.  Other radio detectors invented for wireless telegraphy, such as the Fleming valve (1904) and the crystal detector (1906) also proved able to rectify AM signals, so the technological hurdle was generating AM waves; receiving them was not a problem.",
        "Early technologies",
        "Early experiments in AM radio transmission, conducted by Fessenden, Valdemar Poulsen, Ernst Ruhmer, Quirino Majorana, Charles Herrold, and Lee de Forest, were hampered by the lack of a technology for amplification.  The first practical continuous wave AM transmitters were based on either the huge, expensive Alexanderson alternator, developed 1906–1910, or versions of the Poulsen arc transmitter (arc converter), invented in 1903.  The modifications necessary to transmit AM were clumsy and resulted in very low quality audio.  Modulation was usually accomplished by a carbon microphone inserted directly in the antenna or ground wire; its varying resistance varied the current to the antenna.  The limited power handling ability of the microphone severely limited the power of the first",
        "directly in the antenna or ground wire; its varying resistance varied the current to the antenna.  The limited power handling ability of the microphone severely limited the power of the first radiotelephones; many of the microphones were water-cooled.",
        "Vacuum tubes",
        "The 1912 discovery of the amplifying ability of the Audion tube, invented in 1906 by Lee de Forest, solved these problems.  The vacuum tube feedback oscillator, invented in 1912 by Edwin Armstrong and Alexander Meissner, was a cheap source of continuous waves and could be easily modulated to make an AM transmitter.  Modulation did not have to be done at the output but could be applied to the signal before the final amplifier tube, so the microphone or other audio source didn't have to modulate a high-power radio signal.  Wartime research greatly advanced the art of AM modulation, and after the war the availability of cheap tubes sparked a great increase in the number of radio stations experimenting with AM transmission of news or music. The vacuum tube was responsible for the rise of AM",
        "war the availability of cheap tubes sparked a great increase in the number of radio stations experimenting with AM transmission of news or music. The vacuum tube was responsible for the rise of AM broadcasting around 1920, the first electronic mass communication medium.  Amplitude modulation was virtually the only type used for radio broadcasting until FM broadcasting began after World War II.",
        "At the same time as AM radio began, telephone companies such as AT&T were developing the other large application for AM: sending multiple telephone calls through a single wire by modulating them on separate carrier frequencies, called frequency division multiplexing.",
        "Single-sideband\nIn 1915, John Renshaw Carson formulated the first mathematical description of amplitude modulation, showing that a signal and carrier frequency combined in a nonlinear device creates a sideband on both sides of the carrier frequency. Passing the modulated signal through another nonlinear device can extract the original baseband signal. His analysis also showed that only one sideband was necessary to transmit the audio signal, and Carson patented single-sideband modulation (SSB) on 1 December 1915. This advanced variant of amplitude modulation was adopted by AT&T for longwave transatlantic telephone service beginning 7 January 1927. After WW-II, it was developed for military aircraft communication.\n\nAnalysis",
        "Analysis\n\nThe carrier wave (sine wave) of frequency fc and amplitude A is expressed by\n\n.\n\nThe message signal, such as an audio signal that is used for modulating the carrier, is m(t), and has a frequency fm, much lower than fc:\n\n,\n\nwhere m is the amplitude sensitivity, M is the amplitude of modulation. If m < 1, (1 + m(t)/A) is always positive for undermodulation. If m > 1 then overmodulation occurs and reconstruction of message signal from the transmitted signal would lead in loss of original signal. Amplitude modulation results when the carrier c(t) is multiplied by the positive quantity  (1 + m(t)/A):",
        "In this simple case m is identical to the modulation index, discussed below. With m = 0.5 the amplitude modulated signal y(t) thus corresponds to the top graph (labelled \"50% Modulation\") in figure 4.\n\nUsing prosthaphaeresis identities, y(t) can be shown to be the sum of three sine waves:\n\nTherefore, the modulated signal has three components: the carrier wave c(t) which is unchanged in frequency, and two sidebands with frequencies slightly above and below the carrier frequency fc.\n\nSpectrum",
        "A useful modulation signal m(t) is usually more complex than a single sine wave, as treated above. However, by the principle of Fourier decomposition, m(t) can be expressed as the sum of a set of sine waves of various frequencies, amplitudes, and phases. Carrying out the multiplication of  1 + m(t) with c(t) as above, the result consists of a sum of sine waves. Again, the carrier c(t) is present unchanged, but each frequency component of m at fi has two sidebands at frequencies fc + fi and  fc – fi. The collection of the former frequencies above the carrier frequency is known as the upper sideband, and those below constitute the lower sideband.  The modulation m(t) may be considered to consist of an equal mix of positive and negative frequency components, as shown in the top of figure 2.",
        "and those below constitute the lower sideband.  The modulation m(t) may be considered to consist of an equal mix of positive and negative frequency components, as shown in the top of figure 2. One can view the sidebands as that modulation m(t) having simply been shifted in frequency by fc as depicted at the bottom right of figure 2.",
        "The short-term spectrum of modulation, changing as it would for a human voice for instance, the frequency content (horizontal axis) may be plotted as a function of time (vertical axis), as in figure 3. It can again be seen that as the modulation frequency content varies, an upper sideband is generated according to those frequencies shifted above the carrier frequency, and the same content mirror-imaged in the lower sideband below the carrier frequency. At all times, the carrier itself remains constant, and of greater power than the total sideband power.",
        "Power and spectrum efficiency",
        "The RF bandwidth of an AM transmission (refer to figure 2, but only considering positive frequencies) is twice the bandwidth of the modulating (or \"baseband\") signal, since the upper and lower sidebands around the carrier frequency each have a bandwidth as wide as the highest modulating frequency. Although the bandwidth of an AM signal is narrower than one using frequency modulation (FM), it is twice as wide as single-sideband techniques; it thus may be viewed as spectrally inefficient. Within a frequency band, only half as many transmissions (or \"channels\") can thus be accommodated. For this reason analog television employs a variant of single-sideband (known as vestigial sideband, somewhat of a compromise in terms of bandwidth) in order to reduce the required channel spacing.",
        "Another improvement over standard AM is obtained through reduction or suppression of the carrier component of the modulated spectrum. In figure 2 this is the spike in between the sidebands; even with full (100%) sine wave modulation, the power in the carrier component is twice that in the sidebands, yet it carries no unique information. Thus there is a great advantage in efficiency in reducing or totally suppressing the carrier, either in conjunction with elimination of one sideband (single-sideband suppressed-carrier transmission) or with both sidebands remaining (double sideband suppressed carrier). While these suppressed carrier transmissions are efficient in terms of transmitter power, they require more sophisticated receivers employing synchronous detection and regeneration of the",
        "carrier). While these suppressed carrier transmissions are efficient in terms of transmitter power, they require more sophisticated receivers employing synchronous detection and regeneration of the carrier frequency. For that reason, standard AM continues to be widely used, especially in broadcast transmission, to allow for the use of inexpensive receivers using envelope detection. Even (analog) television, with a (largely) suppressed lower sideband, includes sufficient carrier power for use of envelope detection. But for communications systems where both transmitters and receivers can be optimized, suppression of both one sideband and the carrier represent a net advantage and are frequently employed.",
        "A technique used widely in broadcast AM transmitters is an application of the Hapburg carrier, first proposed in the 1930s but impractical with the technology then available. During periods of low modulation the carrier power would be reduced and would return to full power during periods of high modulation levels. This has the effect of reducing the overall power demand of the transmitter and is most effective on speech type programmes. Various trade names are used for its implementation by the transmitter manufacturers from the late 80's onwards.\n\nModulation index\nThe AM modulation index is a measure based on the ratio of the modulation excursions of the RF signal to the level of the unmodulated carrier. It is thus defined as:",
        "Modulation index\nThe AM modulation index is a measure based on the ratio of the modulation excursions of the RF signal to the level of the unmodulated carrier. It is thus defined as:\n\nwhere  and  are the modulation amplitude and carrier amplitude, respectively; the modulation amplitude is the peak (positive or negative) change in the RF amplitude from its unmodulated value. Modulation index is normally expressed as a percentage, and may be displayed on a meter connected to an AM transmitter.",
        "So if , carrier amplitude varies by 50% above (and below) its unmodulated level, as is shown in the first waveform, below. For , it varies by 100% as shown in the illustration below it. With 100% modulation the wave amplitude sometimes reaches zero, and this represents full modulation using standard AM and is often a target (in order to obtain the highest possible signal-to-noise ratio) but mustn't be exceeded. Increasing the modulating signal beyond that point, known as overmodulation, causes a standard AM modulator (see below) to fail, as the negative excursions of the wave envelope cannot become less than zero, resulting in distortion (\"clipping\") of the received modulation. Transmitters typically incorporate a limiter circuit to avoid overmodulation, and/or a compressor circuit",
        "become less than zero, resulting in distortion (\"clipping\") of the received modulation. Transmitters typically incorporate a limiter circuit to avoid overmodulation, and/or a compressor circuit (especially for voice communications) in order to still approach 100% modulation for maximum intelligibility above the noise. Such circuits are sometimes referred to as a vogad.",
        "However it is possible to talk about a modulation index exceeding 100%, without introducing distortion, in the case of double-sideband reduced-carrier transmission. In that case, negative excursions beyond zero entail a reversal of the carrier phase, as shown in the third waveform below. This cannot be produced using the efficient high-level (output stage) modulation techniques (see below) which are widely used especially in high power broadcast transmitters. Rather, a special modulator produces such a waveform at a low level followed by a linear amplifier. What's more, a standard AM receiver using an envelope detector is incapable of properly demodulating such a signal. Rather, synchronous detection is required. Thus double-sideband transmission is generally not referred to as \"AM\" even",
        "using an envelope detector is incapable of properly demodulating such a signal. Rather, synchronous detection is required. Thus double-sideband transmission is generally not referred to as \"AM\" even though it generates an identical RF waveform as standard AM as long as the modulation index is below 100%. Such systems more often attempt a radical reduction of the carrier level compared to the sidebands (where the useful information is present) to the point of double-sideband suppressed-carrier transmission where the carrier is (ideally) reduced to zero. In all such cases the term \"modulation index\" loses its value as it refers to the ratio of the modulation amplitude to a rather small (or zero) remaining carrier amplitude.",
        "Modulation methods\n\nModulation circuit designs may be classified as low- or high-level (depending on whether they modulate in a low-power domain—followed by amplification for transmission—or in the high-power domain of the transmitted signal).",
        "Low-level generation\nIn modern radio systems, modulated signals are generated via digital signal processing (DSP). With DSP many types of AM are possible with software control (including DSB with carrier, SSB suppressed-carrier and independent sideband, or ISB). Calculated digital samples are converted to voltages with a digital-to-analog converter, typically at a frequency less than the desired RF-output frequency. The analog signal must then be shifted in frequency and linearly amplified to the desired frequency and power level (linear amplification must be used to prevent modulation distortion).\nThis low-level method for AM is used in many Amateur Radio transceivers.\n\nAM may also be generated at a low level, using analog methods described in the next section.",
        "AM may also be generated at a low level, using analog methods described in the next section.\n\nHigh-level generation\nHigh-power AM transmitters (such as those used for AM broadcasting) are based on high-efficiency class-D and class-E power amplifier stages, modulated by varying the supply voltage.\n\nOlder designs (for broadcast and amateur radio) also generate AM by controlling the gain of the transmitter's final amplifier (generally class-C, for efficiency). The following types are for vacuum tube transmitters (but similar options are available with transistors):",
        "Plate modulation In plate modulation, the plate voltage of the RF amplifier is modulated with the audio signal. The audio power requirement is 50 percent of the RF-carrier power.\n Heising (constant-current) modulation RF amplifier plate voltage is fed through a choke (high-value inductor). The AM modulation tube plate is fed through the same inductor, so the modulator tube diverts current from the RF amplifier. The choke acts as a constant current source in the audio range. This system has a low power efficiency.\n Control grid modulation The operating bias and gain of the final RF amplifier can be controlled by varying the voltage of the control grid. This method requires little audio power, but care must be taken to reduce distortion.",
        "Clamp tube (screen grid) modulation The screen-grid bias may be controlled through a clamp tube, which reduces voltage according to the modulation signal. It is difficult to approach 100-percent modulation while maintaining low distortion with this system.\n Doherty modulation One tube provides the power under carrier conditions and another operates only for positive modulation peaks. Overall efficiency is good, and distortion is low.\n Outphasing modulation Two tubes are operated in parallel, but partially out of phase with each other. As they are differentially phase modulated their combined amplitude is greater or smaller. Efficiency is good and distortion low when properly adjusted.",
        "Pulse-width modulation (PWM) or pulse-duration modulation (PDM) A highly efficient high voltage power supply is applied to the tube plate. The output voltage of this supply is varied at an audio rate to follow the program.  This system was pioneered by Hilmer Swanson and has a number of variations, all of which achieve high efficiency and sound quality.",
        "Digital methods The Harris Corporation obtained a patent for synthesizing a modulated high-power carrier wave from a set of digitally selected low-power amplifiers, running in phase at the same carrier frequency. The input signal is sampled by a conventional audio analog-to-digital converter (ADC), and fed to a digital exciter, which modulates overall transmitter output power by switching a series of low-power solid-state RF amplifiers on and off. The combined output drives the antenna system.",
        "Demodulation methods\nThe simplest form of AM demodulator consists of a diode which is configured to act as envelope detector. Another type of demodulator, the product detector, can provide better-quality demodulation with additional circuit complexity.\n\nSee also\n AM stereo\n Shortwave radio\n Amplitude modulation signalling system (AMSS)\n Modulation sphere\n Types of radio emissions\n Airband\n DSB-SC\n\nReferences\n\nBibliography\n Newkirk, David and Karlquist, Rick (2004).  Mixers, modulators and demodulators.  In D. G. Reed (ed.), The ARRL Handbook for Radio Communications (81st ed.), pp. 15.1–15.36.  Newington: ARRL.  .",
        "External links\n Amplitude Modulation by Jakub Serych, Wolfram Demonstrations Project.\n Amplitude Modulation, by S Sastry.\n Amplitude Modulation, an introduction by Federation of American Scientists.\n Amplitude Modulation tutorial including related topics of modulators, demodulators, etc...\n Analog Modulation online interactive demonstration using Python in Google Colab Platform, by C Foh.\n\nRadio modulation modes"
    ],
    [
        "Anagram\nAn anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once. For example, the word anagram itself can be rearranged into nag a ram; which is an Easter egg in Google when searching for the word \"anagram\"; the word binary-into brainy and the word adobe-into abode.\n\nThe original word or phrase is known as the subject of the anagram. Any word or phrase that exactly reproduces the letters in another order is an anagram. Someone who creates anagrams may be called an \"anagrammatist\", and the goal of a serious or skilled anagrammatist is to produce anagrams that reflect or comment on their subject.\n\nExamples",
        "Examples\n\nAnagrams may be created as a commentary on the subject. They may be a parody, a criticism or satire. For example:\n \"New York Times\" = \"monkeys write\"\n \"Church of Scientology\" = \"rich-chosen goofy cult\"\n \"McDonald's restaurants\" = \"Uncle Sam's standard rot\"\n \"coronavirus\" = \"carnivorous\"\n \"She Sells Sanctuary\" = \"Santa; shy, less cruel\" or \"Satan; cruel, less shy\"\n\nAn anagram may also be a synonym of the original word. For example:\n \"evil\" = \"vile\"\n \"a gentleman\" = \"elegant man\"\n \"eleven plus two\" = \"twelve plus one\"\n\nAn anagram that has a meaning opposed to that of the original word or phrase is called an \"antigram\". For example:\n \"restful\" = \"fluster\"\n \"cheater\" = \"teacher\"\n \"funeral\" = \"real fun\"\n \"adultery\" = \"true lady\"\n \"forty five\" = \"over fifty\"\n \"Santa\" = \"Satan\"",
        "They can sometimes change from a proper noun or personal name into an appropriate sentence:\n \"William Shakespeare\" = \"I am a weakish speller\"\n \"Madam Curie\" = \"Radium came\"\n \"George Bush\" = \"He bugs Gore\"\n \"Tom Marvolo Riddle\" = \"I am Lord Voldemort\"\n\nThey can change part of speech, such as the adjective \"silent\" to the verb \"listen\".\n\n\"Anagrams\" itself can be anagrammatized as \"Ars magna\" (Latin, 'the great art').\n\nHistory",
        "They can change part of speech, such as the adjective \"silent\" to the verb \"listen\".\n\n\"Anagrams\" itself can be anagrammatized as \"Ars magna\" (Latin, 'the great art').\n\nHistory\n\nAnagrams can be traced back to the time of the ancient Greeks, and were used to find the hidden and mystical meaning in names.\nThey were popular throughout Europe during the Middle Ages, for example with the poet and composer Guillaume de Machaut. They are said to date back at least to the Greek poet Lycophron, in the third century BCE; but this relies on an account of Lycophron given by John Tzetzes in the 12th century.",
        "In the Talmudic and Midrashic literature, anagrams were used to interpret the Hebrew Bible, notably by Eleazar of Modi'im. Later, Kabbalists took this up with enthusiasm, calling anagrams temurah.\n\nAnagrams in Latin were considered witty over many centuries. Est vir qui adest, explained below, was cited as the example in Samuel Johnson's A Dictionary of the English Language. They became hugely popular in the early modern period, especially in Germany.",
        "Any historical material on anagrams must always be interpreted in terms of the assumptions and spellings that were current for the language in question. In particular, spelling in English only slowly became fixed. There were attempts to regulate anagram formation, an important one in English being that of George Puttenham's Of the Anagram or Posy Transposed in The Art of English Poesie (1589).",
        "Influence of Latin\nAs a literary game when Latin was the common property of the literate, Latin anagrams were prominent. Two examples are the change of Ave Maria, gratia plena, Dominus tecum (Latin: Hail Mary, full of grace, the Lord [is] with you) into Virgo serena, pia, munda et immaculata (Latin: Serene virgin, pious, clean and spotless), and the anagrammatic answer to Pilate's question, Quid est veritas? (Latin: What is truth?), namely, Est vir qui adest (Latin: It is the man who is here). The origins of these are not documented.",
        "Latin continued to influence letter values (such as I = J, U = V and W = VV). There was an ongoing tradition of allowing anagrams to be \"perfect\" if the letters were all used once, but allowing for these interchanges. This can be seen in a popular Latin anagram against the Jesuits: Societas Jesu turned into Vitiosa seces (Latin: Cut off the wicked things). Puttenham, in the time of Elizabeth I, wished to start from Elissabet Anglorum Regina (Latin: Elizabeth Queen of the English), to obtain Multa regnabis ense gloria (Latin: By thy sword shalt thou reign in great renown); he explains carefully that H is \"a note of aspiration only and no letter\", and that Z in Greek or Hebrew is a mere SS. The rules were not completely fixed in the 17th century. William Camden in his Remains commented,",
        "that H is \"a note of aspiration only and no letter\", and that Z in Greek or Hebrew is a mere SS. The rules were not completely fixed in the 17th century. William Camden in his Remains commented, singling out some letters—Æ, K, W, and Z—not found in the classical Roman alphabet:",
        "Early modern period",
        "When it comes to the 17th century and anagrams in English or other languages, there is a great deal of documented evidence of learned interest. The lawyer Thomas Egerton was praised through the anagram gestat honorem ('he carries honor'); the physician George Ent took the anagrammatic motto genio surget ('he rises through spirit/genius'), which requires his first name as Georgius. James I's courtiers discovered in \"James Stuart\" \"a just master\", and converted \"Charles James Stuart\" into \"Claims Arthur's seat\" (even at that point in time, the letters I and J were more-or-less interchangeable). Walter Quin, tutor to the future Charles I, worked hard on multilingual anagrams on the name of father James. A notorious murder scandal, the Overbury case, threw up two imperfect anagrams that were",
        "Walter Quin, tutor to the future Charles I, worked hard on multilingual anagrams on the name of father James. A notorious murder scandal, the Overbury case, threw up two imperfect anagrams that were aided by typically loose spelling and were recorded by Simonds D'Ewes: \"Francis Howard\" (for Frances Carr, Countess of Somerset, her maiden name spelled in a variant) became \"Car findes a whore\", with the letters E hardly counted, and the victim Thomas Overbury, as \"Thomas Overburie\", was written as \"O! O! a busie murther\" (an old form of \"murder\"), with a V counted as U.",
        "William Drummond of Hawthornden, in an essay On the Character of a Perfect Anagram, tried to lay down rules for permissible substitutions (such as S standing for Z) and letter omissions. William Camden provided a definition of \"Anagrammatisme\" as \"a dissolution of a name truly written into his letters, as his elements, and a new connection of it by artificial transposition, without addition, subtraction or change of any letter, into different words, making some perfect sense appliable (i.e., applicable) to the person named.\" Dryden in MacFlecknoe disdainfully called the pastime the \"torturing of one poor word ten thousand ways\".",
        "\"Eleanor Audeley\", wife of Sir John Davies, is said to have been brought before the High Commission in 1634 for extravagances, stimulated by the discovery that her name could be transposed to \"Reveale, O Daniel\", and to have been laughed out of court by another anagram submitted by Sir John Lambe, the dean of the Arches, \"Dame Eleanor Davies\", \"Never soe mad a ladie\".\n\nAn example from France was a flattering anagram for Cardinal Richelieu, comparing him to Hercules or at least one of his hands (Hercules being a kingly symbol), where Armand de Richelieu became Ardue main d'Hercule (\"difficult hand of Hercules\").",
        "Modern period\nExamples from the 19th century are the transposition of \"Horatio Nelson\" into Honor est a Nilo (Latin: Honor is from the Nile); and of \"Florence Nightingale\" into \"Flit on, cheering angel\". The Victorian love of anagramming as recreation is alluded to by the mathematician Augustus De Morgan using his own name as an example; \"Great Gun, do us a sum!\" is attributed to his son William De Morgan, but a family friend John Thomas Graves was prolific, and a manuscript with over 2,800 has been preserved.",
        "With the advent of surrealism as a poetic movement, anagrams regained the artistic respect they had had in the Baroque period. The German poet Unica Zürn, who made extensive use of anagram techniques, came to regard obsession with anagrams as a \"dangerous fever\", because it created isolation of the author. The surrealist leader André Breton coined the anagram Avida Dollars for Salvador Dalí, to tarnish his reputation by the implication of commercialism.",
        "Applications \nWhile anagramming is certainly a recreation first, there are ways in which anagrams are put to use, and these can be more serious, or at least not quite frivolous and formless. For example, psychologists use anagram-oriented tests, often called \"anagram solution tasks\", to assess the implicit memory of young adults and adults alike.\n\nEstablishment of priority\nNatural philosophers (astronomers and others) of the 17th century transposed their discoveries into Latin anagrams, to establish their priority. In this way they laid claim to new discoveries before their results were ready for publication.",
        "Galileo used  for  (Latin: I have observed the most distant planet to have a triple form) for discovering the rings of Saturn in 1610. Galileo announced his discovery that Venus had phases like the Moon in the form  (Latin: These immature ones have already been read in vain by me -oy), that is, when rearranged,  (Latin: The Mother of Loves [= Venus] imitates the figures of Cynthia [= the moon]). In both cases, Johannes Kepler had solved the anagrams incorrectly, assuming they were talking about the Moons of Mars () and a red spot on Jupiter (), respectively. By coincidence, he turned out to be right about the actual objects existing.",
        "In 1656, Christiaan Huygens, using a better telescope than those available to Galileo, figured that Galileo's earlier observations of Saturn actually meant it had a ring (Galileo's tools were only sufficient to see it as bumps) and, like Galileo, had published an anagram, . Upon confirming his observations, three years later he revealed it to mean  (Latin: It [Saturn] is surrounded by a thin, flat, ring, nowhere touching, inclined to the ecliptic).\n\nWhen Robert Hooke discovered Hooke's law in 1660, he first published it in anagram form, , for  (Latin: as the extension, so the force).",
        "When Robert Hooke discovered Hooke's law in 1660, he first published it in anagram form, , for  (Latin: as the extension, so the force).\n\nPseudonyms\nAnagrams are connected to pseudonyms, by the fact that they may conceal or reveal, or operate somewhere in between like a mask that can establish identity. For example, Jim Morrison used an anagram of his name in the Doors song \"L.A. Woman\", calling himself \"Mr. Mojo Risin'\". The use of anagrams and fabricated personal names may be to circumvent restrictions on the use of real names, as happened in the 18th century when Edward Cave wanted to get around restrictions imposed on the reporting of the House of Commons. In a genre such as farce or parody, anagrams as names may be used for pointed and satiric effect.",
        "Pseudonyms adopted by authors are sometimes transposed forms of their names; thus \"Calvinus\" becomes \"Alcuinus\" (here V = U) or \"François Rabelais\" = \"Alcofribas Nasier\". The name \"Voltaire\" of François Marie Arouet fits this pattern, and is allowed to be an anagram of \"Arouet, l[e] j[eune]\" (U = V, J = I) that is, \"Arouet the younger\". Other examples include:\n \"Damon Albarn\" = \"Dan Abnormal\"\n \"Dave Barry\" = \"Ray Adverb\"\n \"Arrigo Boito\" = \"Tobia Gorrio\"\n \"Buckethead\" = \"Death Cube K\"\n \"Daniel Clowes\" = \"Enid Coleslaw\"\n \"Siobhán Donaghy\" = \"Shanghai Nobody\"\n \"Glen Duncan\" = \"Declan Gunn\"\n \"(Theodor) Geisel\" = \"(Theo) Le Sieg\"\n \"Edward Gorey\" = \"Ogdred Weary\", = \"Regera Dowdy\" or = \"E. G. Deadworry\" (and others)\n \"Anna Madrigal\" = \"A man and a girl\"\n \"Ted Morgan\" = \"(Sanche) de Gramont\"",
        "\"Edward Gorey\" = \"Ogdred Weary\", = \"Regera Dowdy\" or = \"E. G. Deadworry\" (and others)\n \"Anna Madrigal\" = \"A man and a girl\"\n \"Ted Morgan\" = \"(Sanche) de Gramont\"\n \"Lorin Morgan-Richards\" = \"Marcil d'Hirson Garron\"\n \"Vladimir Nabokov\" = \"Vivian Darkbloom\", = \"Vivian Bloodmark\", = \"Blavdak Vinomori\", or = \"Dorian Vivalkomb\"",
        "Several of these are \"imperfect anagrams\", letters having been left out in some cases for the sake of easy pronunciation.",
        "Titles\nAnagrams used for titles afford scope for some types of wit. Examples:\n Homer Hickam's book Rocket Boys was adapted into the 1999 film October Sky.\n The tapes for the revival of the BBC show Doctor Who were labeled with the anagram Torchwood, which later went on to be used as the name for a spin-off show. In multi-episode shows, the program occasionally substitutes the anagram of an actor's name for the actual name to prevent revealing the true identity of the role (for instance, The Master) being played by the actor.\n The New Wave band Missing Persons' best-selling album was called Spring Session M.\n Hip-hop artist MF Doom recorded a 2004 album called Mm..Food.",
        "The New Wave band Missing Persons' best-selling album was called Spring Session M.\n Hip-hop artist MF Doom recorded a 2004 album called Mm..Food.\n Brian Eno's album Before and After Science includes a song entitled \"King's Lead Hat\", an anagram of \"Talking Heads\", a band Eno has worked with.\n Juan Maria Solare's piano ballad \"Jura ser anomalía\" (literally \"he/she swears to be an anomaly\") is an anagram of the composer's full name. His composition for English horn titled \"A Dot in Time\" is an anagram of \"Meditation\", which describes the piece. The title of his piano piece that is a homage to Claude Debussy is \"Seduce Us Badly\".",
        "Bill Evans's overdubbed piano elegy for fellow jazz pianist Sonny Clark is titled \"N.Y.C.'s No Lark\", and another composition, \"Re: Person I Knew\" is a tribute to his producer, Orrin Keepnews.\n The title of Imogen Heap's album iMegaphone is an anagram of her name.\n Progressive rock group Rush published a song on their 1989 album Presto titled \"Anagram (for Mongo)\" that makes use of anagrams in every line of the song.\n The title of the fifth album by American rock band Interpol, El Pintor, is an anagram of the band's name and also Spanish for \"the painter\".\n Many of the song titles on Aphex Twin's ...I Care Because You Do are anagrams of either \"Aphex Twin\", \"The Aphex Twin\", or \"Richard D. James\".",
        "Many of the song titles on Aphex Twin's ...I Care Because You Do are anagrams of either \"Aphex Twin\", \"The Aphex Twin\", or \"Richard D. James\".\n In Disney's 1964 film Mary Poppins, Dick Van Dyke played Mr. Dawes Sr. as the anagram of his name, Navckid Keyd. In the credits, the words unscrambled themselves to reveal his name.\n The title of King Crimson's 1982 song Thela Hun Ginjeet is an anagram of \"heat in the jungle\".\n Two albums released in 2022 by Australian rock band King Gizzard & the Lizard Wizard titled \"Made in Timeland\" and \"Laminated Denim\".",
        "Coincidences\nIn Hebrew, the name \"Gernot Zippe\" (גרנוט ציפה), the inventor of the Zippe-type centrifuge, is an anagram of the word \"centrifuge\" (צנטריפוגה).\n\nThe sentence \"Name is Anu Garg\", referring to anagrammer and founder of wordsmith.org Anu Garg, can be rearranged to spell \"Anagram genius\".\n\nGames and puzzles",
        "Games and puzzles\n\nAnagrams are in themselves a recreational activity, but they also make up part of many other games, puzzles and game shows. The Jumble is a puzzle found in many newspapers in the United States requiring the unscrambling of letters to find the solution. Cryptic crossword puzzles frequently use anagrammatic clues, usually indicating that they are anagrams by the inclusion of a descriptive term like \"confused\" or \"in disarray\". An example would be Businessman burst into tears (9 letters). The solution, stationer, is an anagram of into tears, the letters of which have burst out of their original arrangement to form the name of a type of businessman.",
        "Numerous other games and contests involve some element of anagram formation as a basic skill. Some examples:\n In Anagrams, players flip tiles over one at a time and race to take words. They can \"steal\" each other's words by rearranging the letters and extending the words.\n In a version of Scrabble called Clabbers, the name itself is an anagram of Scrabble. Tiles may be placed in any order on the board as long as they anagram to a valid word.\n On the British game show Countdown, contestants are given 30 seconds to make the longest word from nine random letters.\n In Boggle, players make constrained words from a grid of sixteen random letters, by joining adjacent cubes.",
        "In Boggle, players make constrained words from a grid of sixteen random letters, by joining adjacent cubes.\n On the British game show BrainTeaser, contestants are shown a word broken into randomly arranged segments and must announce the whole word. At the end of the game there is a \"Pyramid\" which starts with a three-letter word. A letter appears in the line below to which the player must add the existing letters to find a solution. The pattern continues until the player reaches the final eight-letter anagram. The player wins the game by solving all the anagrams within the allotted time.\n In Bananagrams, players place tiles from a pool into crossword-style word arrangements in a race to see who can finish the pool of tiles first.",
        "Ciphers\nMultiple anagramming is a technique used to solve some kinds of cryptograms, such as a permutation cipher, a transposition cipher, and the Jefferson disk. Solutions may be computationally found using a Jumble algorithm.",
        "Methods of construction\nSometimes, it is possible to \"see\" anagrams in words, unaided by tools, though the more letters involved the more difficult this becomes. The difficulty is that for a word of  different letters, there are  (factorial of ) different permutations and so  different anagrams of the word. Anagram dictionaries can also be used. Computer programs, known as \"anagram search\", \"anagram servers\", \"anagram solvers\", offer a much faster route to creating anagrams, and a large number of these programs are available on the Internet. Some programs use the Anatree algorithm to compute anagrams efficiently.",
        "The program or server carries out an exhaustive search of a database of words, to produce a list containing every possible combination of words or phrases from the input word or phrase using a jumble algorithm. Some programs (such as Lexpert) restrict to one-word answers. Many anagram servers (for example, The Words Oracle) can control the search results, by excluding or including certain words, limiting the number or length of words in each anagram, or limiting the number of results. Anagram solvers are often banned from online anagram games. The disadvantage of computer anagram solvers, especially when applied to multi-word anagrams, is their poor understanding of the meaning of the words they are manipulating. They usually cannot filter out meaningful or appropriate anagrams from large",
        "when applied to multi-word anagrams, is their poor understanding of the meaning of the words they are manipulating. They usually cannot filter out meaningful or appropriate anagrams from large numbers of nonsensical word combinations. Some servers attempt to improve on this using statistical techniques that try to combine only words that appear together often. This approach provides only limited success since it fails to recognize ironic and humorous combinations.",
        "Some anagrammatists indicate the method they used. Anagrams constructed without the aid of a computer are noted as having been done \"manually\" or \"by hand\"; those made by utilizing a computer may be noted \"by machine\" or \"by computer\", or may indicate the name of the computer program (using Anagram Genius).\n\nThere are also a few \"natural\" instances: English words unconsciously created by switching letters around. The French chaise longue (\"long chair\") became the American \"chaise lounge\" by metathesis (transposition of letters and/or sounds). It has also been speculated that the English \"curd\" comes from the Latin crudus (\"raw\"). Similarly, the ancient English word for bird was \"brid\".",
        "Notable anagrammatists\nThe French king Louis XIII had a man named Thomas Billon appointed as his Royal Anagrammatist with an annual salary of 1200 pounds. Among contemporary anagrammers, Anu Garg, created an Internet Anagram Server in 1994 together with the satirical anagram-based newspaper The Anagram Times. Mike Keith has anagrammed the complete text of Moby Dick. He, along with Richard Brodie, has published The Anagrammed Bible that includes anagrammed version of many books of the Bible. Popular television personality Dick Cavett is known for his anagrams of famous celebrities such as Alec Guinness and Spiro Agnew.\n\nAnagram animation\n\nAn animated anagram displays the letters of a word or phrase moving into their new positions.\n\nSee also",
        "Anagram animation\n\nAn animated anagram displays the letters of a word or phrase moving into their new positions.\n\nSee also\n\n Acronym\n Ambigram\n Anagrammatic poem\n Anagrams, a board game\n Ananym\n Blanagram\n Constrained writing\n Isogram\n Letter bank\n Lipogram\n List of geographic anagrams and ananyms\n List of taxa named by anagrams\n London Underground anagram map\n Palindrome\n Pangram\n Rebus\n Sator Square \n Spoonerism\n Tautonym\n Word play\n\nReferences\n\nFurther reading\n Henry Benjamin Wheatley. Of Anagrams: A Monograph Treating of Their History from the Earliest Ages to the Present Time. Williams & Norgate, 1862.\n Word Ways: The Journal of Recreational Linguistics.  Greenwood Periodicals et al., 1968–. .\n Howard W. Bergerson. Palindromes and Anagrams.  Dover Publications, 1973. .",
        "External links\n\n AnagramThis - An ad-free online anagram creator"
    ],
    [
        "Analog signal\nAn analog signal is any continuous-time signal representing some other quantity, i.e., analogous to another quantity. For example, in an analog audio signal, the instantaneous signal voltage varies continuously with the pressure of the sound waves.\n\nIn contrast, a digital signal represents the original time-varying quantity as a sampled sequence of quantized values. Digital sampling imposes some bandwidth and dynamic range constraints on the representation and adds quantization error.\n\nThe term analog signal usually refers to electrical signals; however, mechanical, pneumatic, hydraulic, and other systems may also convey or be considered analog signals.",
        "Representation\nAn analog signal uses some property of the medium to convey the signal's information. For example, an aneroid barometer uses rotary position as the signal to convey pressure information. In an electrical signal, the voltage, current, or frequency of the signal may be varied to represent the information.",
        "Any information may be conveyed by an analog signal; such a signal may be a measured response to changes in a physical variable, such as sound, light, temperature, position, or pressure. The physical variable is converted to an analog signal by a transducer. For example, sound striking the diaphragm of a microphone induces corresponding fluctuations in the current produced by a coil in an electromagnetic microphone or the voltage produced by a condenser microphone. The voltage or the current is said to be an analog of the sound.",
        "Noise\n\nAn analog signal is subject to electronic noise and distortion introduced by communication channels, recording and signal processing operations, which can progressively degrade the signal-to-noise ratio (SNR). As the signal is transmitted, copied, or processed, the unavoidable noise introduced in the signal path will accumulate as a generation loss, progressively and irreversibly degrading the SNR, until in extreme cases, the signal can be overwhelmed. Noise can show up as hiss and intermodulation distortion in audio signals, or snow in video signals. Generation loss is irreversible as there is no reliable method to distinguish the noise from the signal.",
        "Converting an analog signal to digital form introduces a low-level quantization noise into the signal due to finite resolution of digital systems. Once in digital form, the signal can be transmitted, stored, and processed without introducing additional noise or distortion using error detection and correction.\n\nNoise accumulation in analog systems can be minimized by electromagnetic shielding, balanced lines, low-noise amplifiers and high-quality electrical components.\n\nSee also\n\n Amplifier\n Analog computer\n Analog device\n Analog signal processing\n Magnetic tape\n Preamplifier\n\nReferences\n\nAnalog circuits\nElectronic design\nTelevision terminology\nVideo signal"
    ],
    [
        "Analytical engine\nThe analytical engine was a proposed mechanical general-purpose computer designed by English mathematician and computer pioneer Charles Babbage. It was first described in 1837 as the successor to Babbage's difference engine, which was a design for a simpler mechanical calculator.\n\nThe analytical engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete. In other words, the structure of the analytical engine was essentially the same as that which has dominated computer design in the electronic era. The analytical engine is one of the most successful achievements of Charles Babbage.",
        "Babbage was never able to complete construction of any of his machines due to conflicts with his chief engineer and inadequate funding. It was not until 1941 that Konrad Zuse built the first general-purpose computer, Z3, more than a century after Babbage had proposed the pioneering analytical engine in 1837.\n\nDesign \n\nBabbage's first attempt at a mechanical computing device, the Difference Engine, was a special-purpose machine designed to tabulate logarithms and trigonometric functions by evaluating finite differences to create approximating polynomials. Construction of this machine was never completed; Babbage had conflicts with his chief engineer, Joseph Clement, and ultimately the British government withdrew its funding for the project.",
        "During this project, Babbage realised that a much more general design, the analytical engine, was possible. The work on the design of the analytical engine started around 1833.\n\nThe input, consisting of programs (\"formulae\") and data, was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter, and a bell. The machine would also be able to punch numbers onto cards to be read in later. It employed ordinary base-10 fixed-point arithmetic.",
        "There was to be a store (that is, a memory) capable of holding 1,000 numbers of 50 decimal digits each (ca. 16.6 kB). An arithmetic unit (the \"mill\") would be able to perform all four arithmetic operations, plus comparisons and optionally square roots. Initially (1838) it was conceived as a difference engine curved back upon itself, in a generally circular layout, with the long store exiting off to one side. Later drawings (1858) depict a regularised grid layout. Like the central processing unit (CPU) in a modern computer, the mill would rely upon its own internal procedures, to be stored in the form of pegs inserted into rotating drums called \"barrels\", to carry out some of the more complex instructions the user's program might specify.",
        "The programming language to be employed by users was akin to modern day assembly languages. Loops and conditional branching were possible, and so the language as conceived would have been Turing-complete as later defined by Alan Turing. Three different types of punch cards were used: one for arithmetical operations, one for numerical constants, and one for load and store operations, transferring numbers from the store to the arithmetical unit or back. There were three separate readers for the three types of cards. Babbage developed some two dozen programs for the analytical engine between 1837 and 1840, and one program later. These programs treat polynomials, iterative formulas, Gaussian elimination, and Bernoulli numbers.",
        "In 1842, the Italian mathematician Luigi Federico Menabrea published a description of the engine in French, based on lectures Babbage gave when he visited Turin in 1840. In 1843, the description was translated into English and extensively annotated by Ada Lovelace, who had become interested in the engine eight years earlier. In recognition of her additions to Menabrea's paper, which included a way to calculate Bernoulli numbers using the machine (widely considered to be the first complete computer program), she has been described as the first computer programmer.\n\nConstruction \nLate in his life, Babbage sought ways to build a simplified version of the machine, and assembled a small part of it before his death in 1871.",
        "Construction \nLate in his life, Babbage sought ways to build a simplified version of the machine, and assembled a small part of it before his death in 1871.\n\nIn 1878, a committee of the British Association for the Advancement of Science described the analytical engine as \"a marvel of mechanical ingenuity\", but recommended against constructing it. The committee acknowledged the usefulness and value of the machine, but could not estimate the cost of building it, and were unsure whether the machine would function correctly after being built.",
        "Intermittently from 1880 to 1910, Babbage's son Henry Prevost Babbage was constructing a part of the mill and the printing apparatus. In 1910, it was able to calculate a (faulty) list of multiples of pi. This constituted only a small part of the whole engine; it was not programmable and had no storage. (Popular images of this section have sometimes been mislabelled, implying that it was the entire mill or even the entire engine.) Henry Babbage's \"analytical engine mill\" is on display at the Science Museum in London. Henry also proposed building a demonstration version of the full engine, with a smaller storage capacity: \"perhaps for a first machine ten (columns) would do, with fifteen wheels in each\". Such a version could manipulate 20 numbers of 25 digits each, and what it could be told",
        "a smaller storage capacity: \"perhaps for a first machine ten (columns) would do, with fifteen wheels in each\". Such a version could manipulate 20 numbers of 25 digits each, and what it could be told to do with those numbers could still be impressive. \"It is only a question of cards and time\", wrote Henry Babbage in 1888, \"... and there is no reason why (twenty thousand) cards should not be used if necessary, in an analytical engine for the purposes of the mathematician\".",
        "In 1991, the London Science Museum built a complete and working specimen of Babbage's Difference Engine No. 2, a design that incorporated refinements Babbage discovered during the development of the analytical engine. This machine was built using materials and engineering tolerances that would have been available to Babbage, quelling the suggestion that Babbage's designs could not have been produced using the manufacturing technology of his time.",
        "In October 2010, John Graham-Cumming started a \"Plan 28\" campaign to raise funds by \"public subscription\" to enable serious historical and academic study of Babbage's plans, with a view to then build and test a fully working virtual design which will then in turn enable construction of the physical analytical engine. As of May 2016, actual construction had not been attempted, since no consistent understanding could yet be obtained from Babbage's original design drawings. In particular it was unclear whether it could handle the indexed variables which were required for Lovelace's Bernoulli program. By 2017, the \"Plan 28\" effort reported that a searchable database of all catalogued material was available, and an initial review of Babbage's voluminous Scribbling Books had been completed.",
        "Many of Babbage's original drawings have been digitised and are publicly available online.\n\nInstruction set \n\nBabbage is not known to have written down an explicit set of instructions for the engine in the manner of a modern processor manual.  Instead he showed his programs as lists of states during their execution, showing what operator was run at each step with little indication of how the control flow would be guided.\n\nAllan G. Bromley has assumed that the card deck could be read in forwards and backwards directions as a function of conditional branching after testing for conditions, which would make the engine Turing-complete:\n\n...the cards could be ordered to move forward and reverse (and hence to loop)...",
        "...the cards could be ordered to move forward and reverse (and hence to loop)...\n\nThe introduction for the first time, in 1845, of user operations for a variety of service functions including, most importantly, an effective system for user control of looping in user programs.\n\nThere is no indication how the direction of turning of the operation and variable cards is specified. In the absence of other evidence I have had to adopt the minimal default assumption that both the operation and variable cards can only be turned backward as is necessary to implement the loops used in Babbage's sample programs. There would be no mechanical or microprogramming difficulty in placing the direction of motion under the control of the user.\n\nIn their emulator of the engine, Fourmilab say:",
        "In their emulator of the engine, Fourmilab say:\n\nThe Engine's Card Reader is not constrained to simply process the cards in a chain one after another from start to finish. It can, in addition, directed by the very cards it reads and advised by whether the Mill's run-up lever is activated, either advance the card chain forward, skipping the intervening cards, or backward, causing previously-read cards to be processed once again.\n\nThis emulator does provide a written symbolic instruction set, though this has been constructed by its authors rather than based on Babbage's original works.  For example, a factorial program would be written as:\n\n N0 6\n N1 1\n N2 1\n ×\n L1\n L0\n S1\n –\n L0\n L2\n S0\n L2\n L0\n CB?11",
        "N0 6\n N1 1\n N2 1\n ×\n L1\n L0\n S1\n –\n L0\n L2\n S0\n L2\n L0\n CB?11\n\nwhere the CB is the conditional branch instruction or \"combination card\" used to make the control flow jump, in this case backward by 11 cards.\n\nInfluence\n\nPredicted influence \nBabbage understood that the existence of an automatic computer would kindle interest in the field now known as algorithmic efficiency, writing in his Passages from the Life of a Philosopher, \"As soon as an analytical engine exists, it will necessarily guide the future course of the science. Whenever any result is sought by its aid, the question will then arise—By what course of calculation can these results be arrived at by the machine in the shortest time?\"",
        "Computer science \nFrom 1872 Henry continued diligently with his father's work and then intermittently in retirement in 1875.\n\nPercy Ludgate wrote about the engine in 1914 and published his own design for an analytical engine in 1909. It was drawn up in detail, but never built, and the drawings have never been found. Ludgate's engine would be much smaller (about , which corresponds to cube of side length ) than Babbage's, and hypothetically would be capable of multiplying two 20-decimal-digit numbers in about six seconds.",
        "In his work Essays on Automatics (1914) Leonardo Torres Quevedo, inspired by Babbage, designed a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also contains the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the  Electromechanical Arithmometer, which consisted of an arithmetic unit connected to a (possibly remote) typewriter, on which commands could be typed and the results printed automatically.",
        "Vannevar Bush's paper Instrumental Analysis (1936) included several references to Babbage's work. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.",
        "Despite this groundwork, Babbage's work fell into historical obscurity, and the analytical engine was unknown to builders of electromechanical and electronic computing machines in the 1930s and 1940s when they began their work, resulting in the need to re-invent many of the architectural innovations Babbage had proposed. Howard Aiken, who built the quickly-obsoleted electromechanical calculator, the Harvard Mark I, between 1937 and 1945, praised Babbage's work likely as a way of enhancing his own stature, but knew nothing of the analytical engine's architecture during the construction of the Mark I, and considered his visit to the constructed portion of the analytical engine \"the greatest disappointment of my life\". The Mark I showed no influence from the analytical engine and lacked the",
        "Mark I, and considered his visit to the constructed portion of the analytical engine \"the greatest disappointment of my life\". The Mark I showed no influence from the analytical engine and lacked the analytical engine's most prescient architectural feature, conditional branching. J. Presper Eckert and John W. Mauchly similarly were not aware of the details of Babbage's analytical engine work prior to the completion of their design for the first electronic general-purpose computer, the ENIAC.",
        "Comparison to other early computers \nIf the analytical engine had been built, it would have been digital, programmable and Turing-complete. It would, however, have been very slow. Luigi Federico Menabrea reported in Sketch of the Analytical Engine: \"Mr. Babbage believes he can, by his engine, form the product of two numbers, each containing twenty figures, in three minutes\".\nBy comparison the Harvard Mark I could perform the same task in just six seconds. A modern CPU could do the same thing in under a billionth of a second.",
        "In popular culture \n The cyberpunk novelists William Gibson and Bruce Sterling co-authored a steampunk novel of alternative history titled The Difference Engine in which Babbage's difference and analytical engines became available to Victorian society. The novel explores the consequences and implications of the early introduction of computational technology.",
        "Moriarty by Modem, a short story by Jack Nimersheim, describes an alternative history where Babbage's analytical engine was indeed completed and had been deemed highly classified by the British government. The characters of Sherlock Holmes and Moriarty had in reality been a set of prototype programs written for the analytical engine. This short story follows Holmes as his program is implemented on modern computers and he is forced to compete against his nemesis yet again in the modern counterparts of Babbage's analytical engine.",
        "A similar setting is used by Sydney Padua in the webcomic The Thrilling Adventures of Lovelace and Babbage. It features an alternative history where Ada Lovelace and Babbage have built the analytical engine and use it to fight crime at Queen Victoria's request. The comic is based on thorough research on the biographies of and correspondence between Babbage and Lovelace, which is then twisted for humorous effect.\n The Orion's Arm online project features the Machina Babbagenseii, fully sentient Babbage-inspired mechanical computers. Each is the size of a large asteroid, only capable of surviving in microgravity conditions, and processes data at 0.5% the speed of a human brain.",
        "References\n\nBibliography\n\nExternal links \n\n The Babbage Papers, Science Museum archive\nThe Analytical Engine at Fourmilab, includes historical documents and online simulations\n \n Image of a later Plan of Analytical Engine with grid layout (1858)\n First working Babbage \"barrel\" actually assembled, circa 2005\n Special issue, IEEE Annals of the History of Computing, Volume 22, Number 4, October–December 2000 \n Babbage, Science Museum, London (archived)\n \n Plan 28: Building Charles Babbage's Analytical Engine\n\nCharles Babbage\nComputer-related introductions in 1837\nEnglish inventions\nMechanical calculators\nMechanical computers\nOne-of-a-kind computers\nAda Lovelace"
    ],
    [
        "Andrew S. Tanenbaum\nAndrew Stuart Tanenbaum (born March 16, 1944), sometimes referred to by the handle ast, is an American-Dutch computer scientist and professor emeritus of computer science at the Vrije Universiteit Amsterdam in the Netherlands.\n\nHe is the author of MINIX, a free Unix-like operating system for teaching purposes, and has written multiple computer science textbooks regarded as standard texts in the field. He regards his teaching job as his most important work. Since 2004 he has operated Electoral-vote.com, a website dedicated to analysis of polling data in federal elections in the United States.",
        "Biography\nTanenbaum was born in New York City and grew up in suburban White Plains, New York, where he attended the White Plains High School. He is Jewish. His paternal grandfather was born in Khorostkiv in the Austro-Hungarian empire.\n\nHe received his Bachelor of Science degree in physics from MIT in 1965 and his PhD degree in astrophysics from the University of California, Berkeley in 1971. Tanenbaum also served as a lobbyist for the Sierra Club.\n\nHe moved to the Netherlands to live with his wife, who is Dutch, but he retains his United States citizenship. He taught courses on Computer Organization and Operating Systems and supervised the work of PhD candidates at the VU University Amsterdam. On July 9, 2014, he announced his retirement.\n\nTeaching",
        "Teaching\n\nBooks\nTanenbaum's textbooks on computer science include:\n Structured Computer Organization (1976)\n Computer Networks, co-authored with David J. Wetherall and Nickolas Feamster (1981)\n Operating Systems: Design and Implementation, co-authored with Albert Woodhull (1987)\n Modern Operating Systems (1992)\n Distributed Operating Systems (1994)\n Distributed Systems: Principles and Paradigms, co-authored with Maarten van Steen (2001)\n\nHis book, Operating Systems: Design and Implementation and MINIX were Linus Torvalds' inspiration for the Linux kernel. In his autobiography Just for Fun, Torvalds describes it as \"the book that launched me to new heights\".",
        "His books have been translated into many languages including Arabic, Basque, Bulgarian, Chinese, Dutch, French, German, Greek, Hebrew, Hungarian, Italian, Japanese, Korean, Macedonian, Mexican Spanish, Persian, Polish, Portuguese, Romanian, Russian, Serbian, and Spanish. They have appeared in over 175 editions and are used at universities around the world.\n\nDoctoral students\nTanenbaum has had a number of PhD students who themselves have gone on to become widely known computer science researchers.\nThese include:\n Henri Bal, professor at the Vrije Universiteit in Amsterdam\n Frans Kaashoek, professor at MIT\n Werner Vogels, Chief Technology Officer at Amazon.com",
        "Dean of the Advanced School for Computing and Imaging\nIn the early 1990s, the Dutch government began setting up a number of thematically oriented research schools that spanned multiple universities. These schools were intended to bring professors and PhD students from different Dutch (and later, foreign) universities together to help them cooperate and enhance their research.",
        "Tanenbaum was one of the cofounders and first Dean of the Advanced School for Computing and Imaging (ASCI). This school initially consisted of nearly 200 faculty members and PhD students from the Vrije Universiteit, University of Amsterdam, Delft University of Technology, and Leiden University. They were especially working on problems in advanced computer systems such as parallel computing and image analysis and processing.",
        "Tanenbaum remained dean for 12 years, until 2005, when he was awarded an Academy Professorship by the Royal Netherlands Academy of Arts and Sciences, at which time he became a full-time research professor. ASCI has since grown to include researchers from nearly a dozen universities in The Netherlands, Belgium, and France. ASCI offers PhD level courses, has an annual conference, and runs various workshops every year.\n\nProjects\n\nAmsterdam Compiler Kit\nThe Amsterdam Compiler Kit is a toolkit for producing portable compilers. It was started sometime before 1981 and Andrew Tanenbaum was the architect from the start until version 5.5.",
        "MINIX\nIn 1987, Tanenbaum wrote a clone of UNIX, called MINIX (MINi-unIX), for the IBM PC. It was targeted at students and others who wanted to learn how an operating system worked. Consequently, he wrote a book that listed the source code in an appendix and described it in detail in the text. The source code itself was available on a set of floppy disks. Within three months, a Usenet newsgroup, comp.os.minix, had sprung up with over 40,000 subscribers discussing and improving the system. One of these subscribers was a Finnish student named Linus Torvalds, who began adding new features to MINIX and tailoring it to his own needs. On October 5, 1991, Torvalds announced his own (POSIX-like) kernel, called Linux, which originally used the MINIX file system but is not based on MINIX code.",
        "Although MINIX and Linux have diverged, MINIX continues to be developed, now as a production system as well as an educational one. The focus is on building a highly modular, reliable, and secure operating system. The system is based on a microkernel, with only 5000 lines of code running in kernel mode. The rest of the operating system runs as a number of independent processes in user mode, including processes for the file system, process manager, and each device driver. The system continuously monitors each of these processes, and when a failure is detected is often capable of automatically replacing the failed process without a reboot, without disturbing running programs, and without the user even noticing. MINIX 3, as the current version is called, is available under the BSD license for",
        "the failed process without a reboot, without disturbing running programs, and without the user even noticing. MINIX 3, as the current version is called, is available under the BSD license for free.",
        "In 2017, Google discovered that the Intel Management Engine runs MINIX in ring -3. After Tanenbaum read about this, he published an open letter to Intel, detailing conversations with Intel software engineers that occurred several years ago, where they wanted his assistance in modifying MINIX to work on \"some secret project\". He believes that Intel chose MINIX for this purpose because it is licensed under the BSD-3-Clause license, which allowed Intel to modify the MINIX source code without freely distributing their modified version. In his letter, Tanenbaum claims that MINIX is the most widespread operating system, and this is interpreted by the community as Tanenbaum believing that he has won the Tanenbaum–Torvalds debate. It is hard to know if MINIX or Linux is more popular, as Intel",
        "operating system, and this is interpreted by the community as Tanenbaum believing that he has won the Tanenbaum–Torvalds debate. It is hard to know if MINIX or Linux is more popular, as Intel does not publish chipset sales figures, and there is no direct way to find the number of systems running Linux or MINIX. However, Linux has seen much more widespread use in the server space, is widely used in embedded systems, and also runs on all Android phones, which account for at least 3 billion active devices, meaning that it is likely that Linux is the most widespread operating system, although there is no concrete data to back up this claim.",
        "Research projects\nTanenbaum has also been involved in numerous other research projects in the areas of operating systems, distributed systems, and ubiquitous computing, often as supervisor of PhD students or a postdoctoral researcher. These projects include:\n Amoeba\n Globe\n Mansion\n Orca\n Paramecium\n RFID Guardian\n Turtle F2F",
        "Electoral-vote.com",
        "In 2004, Tanenbaum created Electoral-vote.com, a web site analyzing opinion polls for the 2004 U.S. Presidential Election, using them to project the outcome in the Electoral College. He stated that he created the site as an American who \"knows first hand what the world thinks of America and it is not a pretty picture at the moment. I want people to think of America as the land of freedom and democracy, not the land of arrogance and blind revenge. I want to be proud of America again.\" The site provided a color-coded map, updated each day with projections for each state's electoral votes. Through most of the campaign period Tanenbaum kept his identity secret, referring to himself as \"the Votemaster\" and acknowledging only that he personally preferred John Kerry. Mentioning that he supported",
        "most of the campaign period Tanenbaum kept his identity secret, referring to himself as \"the Votemaster\" and acknowledging only that he personally preferred John Kerry. Mentioning that he supported the Democrats, he revealed his identity on November 1, 2004, the day before the election, and also stating his reasons and qualifications for running the website.",
        "Through the site he also covered the 2006 midterm elections, correctly predicting the winner of all 33 Senate races that year.\n\nFor the 2008 elections, he got every state right except for Indiana, which he said McCain would win by 2% (Obama won by 1%) and Missouri, which he said was too close to call (McCain won by 0.1%). He correctly predicted all the winners in the Senate except for Minnesota, where he predicted a 1% win by Norm Coleman over Al Franken. After 7 months of legal battling and recounts, Franken won by 312 votes (0.01%).\n\nIn 2010, he correctly projected 35 out of 37 Senate races in the Midterm elections on the website. The exceptions were Colorado and Nevada.",
        "In 2010, he correctly projected 35 out of 37 Senate races in the Midterm elections on the website. The exceptions were Colorado and Nevada.\n\nElectoral-vote.com incorrectly predicted Hillary Clinton would win the 2016 United States presidential election. The website incorrectly predicted Clinton would win Wisconsin, Michigan, Pennsylvania, North Carolina, and Florida. Electoral-vote.com did not predict a winner for Nevada, which Clinton would win. The website predicted the winners of the remaining 44 states and the District of Columbia correctly.\n\nTanenbaum–Torvalds debate\nThe Tanenbaum–Torvalds debate was a famous debate between Tanenbaum and Linus Torvalds regarding kernel design on Usenet in 1992.",
        "Awards\n Fellow of the ACM\n Fellow of the IEEE for outstanding contributions to research and education in computer networks and operating systems.\n Member of the Royal Netherlands Academy of Arts and Sciences\n IEEE Computer Society Tech. Committee on Distributed Processing Outstanding Technical Achievement Award, 2022\n Eurosys Lifetime Achievement Award, 2015\n Honorary doctorate from Petru Maior University, Targu Mures, Romania, 2011\n Winner of the TAA McGuffey award for classic textbooks for Modern Operating Systems, 2010\n Coauthor of the Best Paper Award at the LADC Conference, 2009\n Winner of a 2.5 million euro European Research Council Advanced Grant, 2008\n USENIX Flame Award 2008  for his many contributions to systems design and to openness both in discussion and in source",
        "Winner of a 2.5 million euro European Research Council Advanced Grant, 2008\n USENIX Flame Award 2008  for his many contributions to systems design and to openness both in discussion and in source\n Honorary doctorate from Polytechnic University of Bucharest, Romania\n Coauthor of the Best Paper Award at the Real-Time and Network Systems Conf., 2008\n Winner of the 2007 IEEE James H. Mulligan, Jr. Education Medal\n Coauthor of the Best Paper Award at the USENIX LISA Conf., 2006\n Coauthor of the Best Paper for High Impact at the IEEE Percom Conf., 2006\n Academy Professor, 2004\n Winner of the 2005 PPAP Award for best education on computer science software\n Winner of the 2003 TAA McGuffey award for classic textbooks for Computer Networks\n Winner of the 2002 TAA Texty Award for new textbooks",
        "Winner of the 2003 TAA McGuffey award for classic textbooks for Computer Networks\n Winner of the 2002 TAA Texty Award for new textbooks\n Winner of the 1997 ACM SIGCSE for contributions to computer science education\n Winner of the 1994 ACM Karl V. Karlstrom Outstanding Educator Award\n Coauthor of the 1984 ACM SOSP Distinguished Paper Award",
        "Honorary doctorates",
        "On May 12, 2008, Tanenbaum received an honorary doctorate from Universitatea Politehnica din București. The award was given in the academic senate chamber, after which Tanenbaum gave a lecture on his vision of the future of the computer field. The degree was given in recognition of Tanenbaum's career work, which includes about 150 published papers, 18 books (which have been translated into over 20 languages), and the creation of a large body of open-source software, including the Amsterdam Compiler Kit, Amoeba, Globe, and MINIX.",
        "On October 7, 2011, Universitatea Petru Maior din Târgu Mureș (Petru Maior University of Târgu Mureș) granted Tanenbaum the Doctor Honoris Causa (honorary doctorate) title for his remarkable work in the field of computer science and achievements in education. The academic community is hereby honoring his devotion to teaching and research with this award. At the ceremony, the Chancellor, the Rector, the Dean of the Faculty of Sciences and Letters, and others all spoke about Tanenbaum and his work. The pro-rector then read the 'laudatio,' summarizing Tanenbaum's achievements. These include his work developing MINIX (the predecessor to Linux), the RFID Guardian, his work on Globe, Amoeba, and other systems, and his many books on computer science, which have been translated in many",
        "his work developing MINIX (the predecessor to Linux), the RFID Guardian, his work on Globe, Amoeba, and other systems, and his many books on computer science, which have been translated in many languages, including Romanian, and which are used at Petru Maior University.",
        "Keynote talks\nTanenbaum has been keynote speaker at numerous conferences, most recently\n ICDCS 2022 Bologna, Italy, July 12, 2022\n Qualcomm Security Summit San Diego, May 18, 2022\n RIOT Summit 2020 Online Event, September 14, 2020\n FrOSCon 2015  Sankt Augustin, Germany, August 22, 2015\n BSDCan 2015 Ottawa, Canada, June 12, 2015\n HAXPO 2015  Amsterdam May 28, 2015\n Codemotion 2015 Rome Italy, March 28, 2015\n SIREN 2010 Veldhoven, The Netherlands, November 2, 2010\n FOSDEM Brussels, Belgium, February 7, 2010\n NSCNE '09 Changsha, China, November 5, 2009\n E-Democracy 2009 Conference Athens, Greece, September 25, 2009\n Free and Open Source Conference Sankt Augustin, Germany, August 23, 2008\n XV Semana Informática  of the Instituto Superior Técnico, Lisbon, Portugal, March 13, 2008",
        "Free and Open Source Conference Sankt Augustin, Germany, August 23, 2008\n XV Semana Informática  of the Instituto Superior Técnico, Lisbon, Portugal, March 13, 2008\n NLUUG 25 year anniversary conference, Amsterdam, November 7, 2007\n linux.conf.au in Sydney, Australia, January 17, 2007\n Academic IT Festival in Cracow, Poland, February 23, 2006 (2nd edition)\n ACM Symposium on Operating System Principles, Brighton, England, October 24, 2005",
        "References\n\nExternal links\n\n Minix Article in Free Software Magazine contains an interview with Andrew Tanenbaum",
        "1944 births\nAmerican political writers\nAmerican male non-fiction writers\nAmerican technology writers\nComputer systems researchers\nAmerican computer scientists\nFellows of the Association for Computing Machinery\nFellow Members of the IEEE\nFree software programmers\nKernel programmers\nLiving people\nMassachusetts Institute of Technology School of Science alumni\nMembers of the Royal Netherlands Academy of Arts and Sciences\nMINIX\nScientists from New York City\nUniversity of California, Berkeley alumni\nAcademic staff of Vrije Universiteit Amsterdam\nInformation technology in the Netherlands\nComputer science educators\nJewish American writers\nEuropean Research Council grantees\n21st-century American Jews\nAmerican emigrants to the Netherlands"
    ],
    [
        "Andrew Tridgell\nAndrew \"Tridge\" Tridgell  (born 28 February 1967) is an Australian computer programmer.  He is the author of and a contributor to the Samba file server, and co-inventor of the rsync algorithm.\n\nHe has analysed complex proprietary protocols and algorithms, to allow compatible free and open source software implementations.\n\nProjects\nTridgell was a major developer of the Samba software, analyzing the Server Message Block protocol used for workgroup and network file sharing by Microsoft Windows products. He developed the  hierarchical memory allocator, originally as part of Samba.",
        "For his PhD dissertation, he co-developed rsync, including the rsync algorithm, a highly efficient file transfer and synchronisation tool. He was also the original author of rzip, which uses a similar algorithm to rsync. He developed spamsum, based on locality-sensitive hashing algorithms.\n\nHe is the author of KnightCap, a reinforcement-learning based chess engine.\n\nTridgell was also a leader in hacking the TiVo to make it work in Australia, which uses the PAL video format.",
        "In April 2005, Tridgell tried to produce free software (now known as SourcePuller) that interoperated with the BitKeeper source code repository. This was cited as the reason that BitMover revoked a license allowing Linux developers free use of their BitKeeper product. Linus Torvalds, the creator of the Linux kernel, and Tridgell were thus involved in a public debate about the events, in which Tridgell stated that, not having bought or owned BitKeeper – and thus having never agreed to its license – he could not violate it, and was analyzing the protocol ethically, as he had done with Samba. Tridgell's involvement in the project resulted in Torvalds accusing him of playing dirty tricks with BitKeeper. Tridgell claimed his analysis started with simply telneting to a BitKeeper server and",
        "Tridgell's involvement in the project resulted in Torvalds accusing him of playing dirty tricks with BitKeeper. Tridgell claimed his analysis started with simply telneting to a BitKeeper server and typing help.",
        "In 2011 Tridgell got involved with the software development of ArduPilot Mega, an open source Arduino-based UAV controller board, working on an entry for the UAV Challenge Outback Rescue.\n\nAcademic achievements\nTridgell completed a PhD at the Computer Sciences Laboratory of the Australian National University. His original doctorate work was in the area of speech recognition but was never completed. His submitted dissertation 'Efficient Algorithms for Sorting and Synchronization' was based on his work on the rsync algorithm.",
        "Awards and honours\n In October 2003, The Bulletin magazine judged Tridgell to be Australia's smartest Information and Communications Technology person.\n In July 2008, Tridgell was named \"Best Interoperator\" at the Google–O'Reilly Open Source Awards, for his work on Samba and rsync.\nTridgell (along with Jeremy Allison and Volker Lendecke) has been called a \"guru in its traditional Indian meaning\" by IT writer, Sam Varghese.\n On 11 December 2018, Tridgell was awarded the degree of Doctor of Science (Honoris Causa) by the Australian National University, for authoring Samba, co-inventing rsync; and contributions to free and open source software.",
        "On 26 January 2020, Tridgell was awarded the Medal (OAM) of the Order of Australia in the General Division for service to Information Technology. The biographical notes for the award noted his contributions to software development and education including his work on rsync, Samba, ArduPilot, MAVProxy as well as teaching at the Australian National University.",
        "References\n\nExternal links\n\n Andrew Tridgell's website, Newer Personal Website\n Andrew Tridgell's \"Junk Code\" collection\n Efficient Algorithms for Sorting and Synchronization (PhD thesis) – (406kB PDF)\n Active Directory in Samba 4 'an old story'\n FOSS folk who make us proud\n Patent Defence for Free Software, January 2010 presentation transcript\n\nAustralian computer programmers\nFree software programmers\nIBM employees\n1967 births\nLiving people\nPeople from the Australian Capital Territory\nUniversity of Sydney alumni\nAustralian National University alumni\nGeeknet\nPeople educated at Barker College\nRecipients of the Medal of the Order of Australia"
    ],
    [
        "Annealing\nAnnealing may refer to:\n\n Annealing (biology), in genetics\n Annealing (glass), heating a piece of glass to remove stress\n Annealing (materials science), a heat treatment that alters the microstructure of a material\n Quantum annealing, a method for solving combinatorial optimisation problems and ground states of glassy systems\n Simulated annealing, a numerical optimization technique"
    ],
    [
        "Antiderivative\nIn calculus, an antiderivative, inverse derivative, primitive function, primitive integral or indefinite integral of a function  is a differentiable function  whose derivative is equal to the original function . This can be stated symbolically as . The process of solving for antiderivatives is called antidifferentiation (or indefinite integration), and its opposite operation is called differentiation, which is the process of finding a derivative. Antiderivatives are often denoted by capital Roman letters such as  and .",
        "Antiderivatives are related to definite integrals through the second fundamental theorem of calculus: the definite integral of a function over a closed interval where the function is Riemann integrable is equal to the difference between the values of an antiderivative evaluated at the endpoints of the interval.\n\nIn physics, antiderivatives arise in the context of rectilinear motion (e.g., in explaining the relationship between position, velocity and acceleration). The discrete equivalent of the notion of antiderivative is antidifference.",
        "Examples\nThe function  is an antiderivative of , since the derivative of  is . And since the derivative of a constant is zero,  will have an infinite number of antiderivatives, such as , etc. Thus, all the antiderivatives of  can be obtained by changing the value of  in , where  is an arbitrary constant known as the constant of integration. Essentially, the graphs of antiderivatives of a given function are vertical translations of each other, with each graph's vertical location depending upon the value .\n\nMore generally, the power function  has antiderivative  if , and  if .",
        "More generally, the power function  has antiderivative  if , and  if .\n\nIn physics, the integration of acceleration yields velocity plus a constant. The constant is the initial velocity term that would be lost upon taking the derivative of velocity, because the derivative of a constant term is zero. This same pattern applies to further integrations and derivatives of motion (position, velocity, acceleration, and so on). Thus, integration produces the relations of acceleration, velocity and displacement:\n\nUses and properties\nAntiderivatives can be used to compute definite integrals, using the fundamental theorem of calculus: if  is an antiderivative of the integrable function  over the interval , then:",
        "Because of this, each of the infinitely many antiderivatives of a given function  may be called the \"indefinite integral\" of f and written using the integral symbol with no bounds:\n\nIf  is an antiderivative of , and the function  is defined on some interval, then every other antiderivative  of  differs from  by a constant: there exists a number  such that  for all .  is called the constant of integration. If the domain of  is a disjoint union of two or more (open) intervals, then a different constant of integration may be chosen for each of the intervals. For instance\n\nis the most general antiderivative of  on its natural domain \n\nEvery continuous function  has an antiderivative, and one antiderivative  is given by the definite integral of  with variable upper boundary:",
        "Every continuous function  has an antiderivative, and one antiderivative  is given by the definite integral of  with variable upper boundary:\n\nfor any  in the domain of . Varying the lower boundary produces other antiderivatives, but not necessarily all possible antiderivatives. This is another formulation of the fundamental theorem of calculus.\n\nThere are many functions whose antiderivatives, even though they exist, cannot be expressed in terms of elementary functions (like polynomials, exponential functions, logarithms, trigonometric functions, inverse trigonometric functions and their combinations). Examples of these are\n\n the error function \n the Fresnel function \n the sine integral \n the logarithmic integral function  and\n sophomore's dream",
        "the error function \n the Fresnel function \n the sine integral \n the logarithmic integral function  and\n sophomore's dream \n\nFor a more detailed discussion, see also Differential Galois theory.\n\nTechniques of integration\nFinding antiderivatives of elementary functions is often considerably harder than finding their derivatives (indeed, there is no pre-defined method for computing indefinite integrals). For some elementary functions, it is impossible to find an antiderivative in terms of other elementary functions. To learn more, see elementary functions and nonelementary integral.\n\nThere exist many properties and techniques for finding antiderivatives. These include, among others:",
        "The linearity of integration (which breaks complicated integrals into simpler ones)\n Integration by substitution, often combined with trigonometric identities or the natural logarithm\n The inverse chain rule method (a special case of integration by substitution)\n Integration by parts (to integrate products of functions)\n Inverse function integration (a formula that expresses the antiderivative of the inverse  of an invertible and continuous function , in terms of the antiderivative of  and of ).\n The method of partial fractions in integration (which allows us to integrate all rational functions—fractions of two polynomials)\n The Risch algorithm\n Additional techniques for multiple integrations (see for instance double integrals, polar coordinates, the Jacobian and the Stokes' theorem)",
        "The Risch algorithm\n Additional techniques for multiple integrations (see for instance double integrals, polar coordinates, the Jacobian and the Stokes' theorem)\n Numerical integration (a technique for approximating a definite integral when no elementary antiderivative exists, as in the case of )\n Algebraic manipulation of integrand (so that other integration techniques, such as integration by substitution, may be used)\nCauchy formula for repeated integration (to calculate the -times antiderivative of a function)",
        "Computer algebra systems can be used to automate some or all of the work involved in the symbolic techniques above, which is particularly useful when the algebraic manipulations involved are very complex or lengthy. Integrals which have already been derived can be looked up in a table of integrals.\n\nOf non-continuous functions\nNon-continuous functions can have antiderivatives. While there are still open questions in this area, it is known that:\n Some highly pathological functions with large sets of discontinuities may nevertheless have antiderivatives.\n In some cases, the antiderivatives of such pathological functions may be found by Riemann integration, while in other cases these functions are not Riemann integrable.\n\nAssuming that the domains of the functions are open intervals:",
        "A necessary, but not sufficient, condition for a function  to have an antiderivative is that  have the intermediate value property. That is, if  is a subinterval of the domain of  and  is any real number between  and , then there exists a  between  and  such that . This is a consequence of Darboux's theorem.\n The set of discontinuities of  must be a meagre set. This set must also be an F-sigma set (since the set of discontinuities of any function must be of this type). Moreover, for any meagre F-sigma set, one can construct some function  having an antiderivative, which has the given set as its set of discontinuities.",
        "If  has an antiderivative, is bounded on closed finite subintervals of the domain and has a set of discontinuities of Lebesgue measure 0, then an antiderivative may be found by integration in the sense of Lebesgue. In fact, using more powerful integrals like the Henstock–Kurzweil integral, every function for which an antiderivative exists is integrable, and its general integral coincides with its antiderivative.",
        "If  has an antiderivative  on a closed interval , then for any choice of partition  if one chooses sample points  as specified by the mean value theorem, then the corresponding Riemann sum telescopes to the value .  However if  is unbounded, or if  is bounded but the set of discontinuities of  has positive Lebesgue measure, a different choice of sample points  may give a significantly different value for the Riemann sum, no matter how fine the partition. See Example 4 below.",
        "Some examples\n\nBasic formulae \n\n If , then .\n\nSee also\n Antiderivative (complex analysis)\n Formal antiderivative\n Jackson integral\n Lists of integrals\n Symbolic integration\n Area\n\nNotes\n\nReferences\n\nFurther reading\n Introduction to Classical Real Analysis, by Karl R. Stromberg; Wadsworth, 1981 (see also)\n Historical Essay On Continuity Of Derivatives by Dave L. Renfro\n\nExternal links\n Wolfram Integrator — Free online symbolic integration with Mathematica\n Function Calculator from WIMS\n Integral at HyperPhysics\n Antiderivatives and indefinite integrals at the Khan Academy\n Integral calculator at Symbolab\n The Antiderivative at MIT\n Introduction to Integrals at SparkNotes\n Antiderivatives at Harvy Mudd College\n\nIntegral calculus\nLinear operators in calculus"
    ],
    [
        "AOL\nAOL (stylized as Aol., formerly a company known as AOL Inc. and originally known as America Online) is an American web portal and online service provider based in New York City. It is a brand marketed by the current incarnation of Yahoo! Inc.\n\nThe service traces its history to an online service known as PlayNET. PlayNET licensed its software to Quantum Link (Q-Link), that went online in November 1985. A new IBM PC client was launched in 1988, and eventually renamed as America Online in 1989. AOL grew to become the largest online service, displacing established players like CompuServe and The Source. By 1995, AOL had about three million active users.",
        "AOL was one of the early pioneers of the Internet in the early-1990s, and the most recognized brand on the web in the United States. It originally provided a dial-up service to millions of Americans, pioneered instant messaging, and in 1993 began adding internet access. In 1998, AOL purchased Netscape for US$4.2 billion. In 2001, at the height of its popularity, it purchased the media conglomerate Time Warner in the largest merger in U.S. history. AOL rapidly shrank thereafter, partly due to the decline of dial-up and rise of broadband. AOL was eventually spun off from Time Warner in 2009, with Tim Armstrong appointed the new CEO. Under his leadership, the company invested in media brands and advertising technologies.",
        "On June 23, 2015, AOL was acquired by Verizon Communications for $4.4 billion. On May 3, 2021, Verizon announced it would sell Yahoo and AOL to private equity firm Apollo Global Management for $5 billion. On September 1, 2021, AOL became part of the new Yahoo! Inc.\n\nHistory",
        "History\n\n1983–1991: early years \nAOL began in 1983, as a short-lived venture called Control Video Corporation (CVC), founded by William von Meister. Its sole product was an online service called GameLine for the Atari 2600 video game console, after von Meister's idea of buying music on demand was rejected by Warner Bros. Subscribers bought a modem from the company for $49.95 and paid a one-time $15 setup fee. GameLine permitted subscribers to temporarily download games and keep track of high scores, at a cost of $1 per game. The telephone disconnected and the downloaded game would remain in GameLine's Master Module and playable until the user turned off the console or downloaded another game.",
        "In January 1983, Steve Case was hired as a marketing consultant for Control Video on the recommendation of his brother, investment banker Dan Case. In May 1983, Jim Kimsey became a manufacturing consultant for Control Video, which was near bankruptcy. Kimsey was brought in by his West Point friend Frank Caufield, an investor in the company. In early 1985, von Meister left the company.",
        "On May 24, 1985, Quantum Computer Services, an online services company, was founded by Kimsey from the remnants of Control Video, with Kimsey as chief executive officer, and Marc Seriff as chief technology officer. The technical team consisted of Seriff, Tom Ralston, Ray Heinrich, Steve Trus, Ken Huntsman, Janet Hunter, Dave Brown, Craig Dykstra, Doug Coward, and Mike Ficco. In 1987, Case was promoted again to executive vice-president. Kimsey soon began to groom Case to take over the role of CEO, which he did when Kimsey retired in 1991.",
        "Kimsey changed the company's strategy, and in 1985, launched a dedicated online service for Commodore 64 and 128 computers, originally called Quantum Link (\"Q-Link\" for short). The Quantum Link software was based on software licensed from PlayNet, Inc, (founded in 1983 by Howard Goldberg and Dave Panzl). The service was different from other online services as it used the computing power of the Commodore 64 and the Apple II rather than just a \"dumb\" terminal. It passed tokens back and forth and provided a fixed price service tailored for home users. In May 1988, Quantum and Apple launched AppleLink Personal Edition for Apple II and Macintosh computers. In August 1988, Quantum launched PC Link, a service for IBM-compatible PCs developed in a joint venture with the Tandy Corporation. After",
        "Personal Edition for Apple II and Macintosh computers. In August 1988, Quantum launched PC Link, a service for IBM-compatible PCs developed in a joint venture with the Tandy Corporation. After the company parted ways with Apple in October 1989, Quantum changed the service's name to America Online. Case promoted and sold AOL as the online service for people unfamiliar with computers, in contrast to CompuServe, which was well established in the technical community.",
        "From the beginning, AOL included online games in its mix of products; many classic and casual games were included in the original PlayNet software system. the company introduced many innovative online interactive titles and games, including:\n Graphical chat environments Habitat (1986–1988) from LucasArts.\n The first online interactive fiction series QuantumLink Serial by Tracy Reed (1988).\n Quantum Space, the first fully automated play-by-mail game (1989–1991).\n\n1991–2006: Internet age, Time Warner merger",
        "1991–2006: Internet age, Time Warner merger \n\nIn February 1991, AOL for DOS was launched using a GeoWorks interface; it was followed a year later by AOL for Windows. This coincided with growth in pay-based online services, like Prodigy, CompuServe, and GEnie. 1991 also saw the introduction of an original Dungeons & Dragons title called Neverwinter Nights from Stormfront Studios; which was one of the first Multiplayer Online Role Playing Games to depict the adventure with graphics instead of text.",
        "During the early 1990s, the average subscription lasted for about 25 months and accounted for $350 in total revenue. Advertisements invited modem owners to \"Try America Online FREE\", promising free software and trial membership. AOL discontinued Q-Link and PC Link in late 1994. In September 1993, AOL added Usenet access to its features. This is commonly referred to as the \"Eternal September\", as Usenet's cycle of new users was previously dominated by smaller numbers of college and university freshmen gaining access in September and taking a few weeks to acclimate. This also coincided with a new \"carpet bombing\" marketing campaign by CMO Jan Brandt to distribute as many free trial AOL trial disks as possible through nonconventional distribution partners. At one point, 50% of the CDs",
        "a new \"carpet bombing\" marketing campaign by CMO Jan Brandt to distribute as many free trial AOL trial disks as possible through nonconventional distribution partners. At one point, 50% of the CDs produced worldwide had an AOL logo. AOL quickly surpassed GEnie, and by the mid-1990s, it passed Prodigy (which for several years allowed AOL advertising) and CompuServe. In November 1994, AOL purchased Booklink for its web browser, to give its users web access. In 1996, AOL replaced Booklink with a browser based on Internet Explorer, allegedly in exchange for inclusion of AOL in Windows.",
        "AOL launched services with the National Education Association, the American Federation of Teachers, National Geographic, the Smithsonian Institution, the Library of Congress, Pearson, Scholastic, ASCD, NSBA, NCTE, Discovery Networks, Turner Education Services (CNN Newsroom), NPR, The Princeton Review, Stanley Kaplan, Barron's, Highlights for Kids, the U.S. Department of Education, and many other education providers. AOL offered the first real-time homework help service (the Teacher Pager—1990; prior to this, AOL provided homework help bulletin boards), the first service by children, for children (Kids Only Online, 1991), the first online service for parents (the Parents Information Network, 1991), the first online courses (1988), the first omnibus service for teachers (the Teachers'",
        "children (Kids Only Online, 1991), the first online service for parents (the Parents Information Network, 1991), the first online courses (1988), the first omnibus service for teachers (the Teachers' Information Network, 1990), the first online exhibit (Library of Congress, 1991), the first parental controls, and many other online education firsts.",
        "AOL purchased search engine WebCrawler in 1995, but sold it to Excite the following year; the deal made Excite the sole search and directory service on AOL. After the deal closed in March 1997, AOL launched its own branded search engine, based on Excite, called NetFind. This was renamed to AOL Search in 1999.",
        "AOL charged its users an hourly fee until December 1996, when the company changed to a flat monthly rate of $19.95. During this time, AOL connections were flooded with users trying to connect, and many canceled their accounts due to constant busy signals. A commercial was made featuring Steve Case telling people AOL was working day and night to fix the problem. Within three years, AOL's user base grew to 10 million people. In 1995, AOL was headquartered at 8619 Westwood Center Drive in the Tysons Corner CDP in unincorporated Fairfax County, Virginia, near the Town of Vienna.",
        "AOL was quickly running out of room in October 1996 for its network at the Fairfax County campus. In mid-1996, AOL moved to 22000 AOL Way in Dulles, unincorporated Loudoun County, Virginia to provide room for future growth. In a five-year landmark agreement with the most popular operating system, AOL was bundled with Windows software.",
        "On March 31, 1996, the short-lived eWorld was purchased by AOL. In 1997, about half of all U.S. homes with Internet access had it through AOL. During this time, AOL's content channels, under Jason Seiken, including News, Sports, and Entertainment, experienced their greatest growth as AOL become the dominant online service internationally with more than 34 million subscribers. In November 1998, AOL announced it would acquire Netscape, best known for their web browser, in a major $4.2 billion deal. The deal closed on March 17, 1999. Another large acquisition in December 1999 was that of MapQuest, for $1.1 billion.",
        "In January 2000, as new broadband technologies were being rolled out around the New York City metropolitan area and elsewhere across the U.S., AOL and Time Warner announced plans to merge, forming AOL Time Warner, Inc. The terms of the deal called for AOL shareholders to own 55% of the new, combined company. The deal closed on January 11, 2001. The new company was led by executives from AOL, SBI, and Time Warner. Gerald Levin, who had served as CEO of Time Warner, was CEO of the new company. Steve Case served as chairman, J. Michael Kelly (from AOL) was the chief financial officer, Robert W. Pittman (from AOL) and Dick Parsons (from Time Warner) served as co-chief operating officers. In 2002, Jonathan Miller became CEO of AOL. The following year, AOL Time Warner dropped the \"AOL\" from its",
        "(from AOL) and Dick Parsons (from Time Warner) served as co-chief operating officers. In 2002, Jonathan Miller became CEO of AOL. The following year, AOL Time Warner dropped the \"AOL\" from its name. It was the largest merger in history when completed with the combined value of the companies at $360 billion. This value fell sharply, to as low as $120 billion, as markets repriced AOL's valuation as a pure internet firm more modestly when combined with the traditional media and cable business. This status did not last long, and the company's value rose again within three months. By the end of that year, the tide had turned against \"pure\" internet companies, with many collapsing under falling stock prices, and even the strongest companies in the field losing up to 75% of their market value.",
        "year, the tide had turned against \"pure\" internet companies, with many collapsing under falling stock prices, and even the strongest companies in the field losing up to 75% of their market value. The decline continued though 2001, but even with the losses, AOL was among the internet giants that continued to outperform brick and mortar companies.",
        "In 2004, along with the launch of AOL 9.0 Optimized, AOL also made available the option of personalized greetings which would enable the user to hear his or her name while accessing basic functions and mail alerts, or while logging in or out. In 2005, AOL broadcast the Live 8 concert live over the Internet, and thousands of users downloaded clips of the concert over the following months. In late 2005, AOL released AOL Safety & Security Center, a bundle of McAfee Antivirus, CA anti-spyware, and proprietary firewall and phishing protection software. News reports in late 2005 identified companies such as Yahoo!, Microsoft, and Google as candidates for turning AOL into a joint venture. Those plans were abandoned when it was revealed on December 20, 2005, that Google would purchase a 5% share",
        "such as Yahoo!, Microsoft, and Google as candidates for turning AOL into a joint venture. Those plans were abandoned when it was revealed on December 20, 2005, that Google would purchase a 5% share of AOL for $1 billion.",
        "2006–2009: rebranding and decline \n\nOn April 3, 2006, AOL announced that it would retire the full name America Online. The official name of the service became AOL, and the full name of the Time Warner subdivision became AOL LLC. On June 8, 2006, AOL offered a new program called AOL Active Security Monitor, a diagnostic tool to monitor and rate PC security status, and recommended additional security software from AOL or Download.com. Two months later, AOL released AOL Active Virus Shield, a free product developed by Kaspersky Lab, that did not require an AOL account, only an internet email address. The ISP side of AOL UK was bought by Carphone Warehouse in October 2006 to take advantage of its 100,000 LLU customers, making Carphone Warehouse the largest LLU provider in the UK.",
        "In August 2006, AOL announced that it would offer email accounts and software previously available only to its paying customers, provided that users accessed AOL or AOL.com through an access method not owned by AOL (otherwise known as \"third party transit\", \"bring your own access\" or \"BYOA\"). The move was designed to reduce costs associated with the \"walled garden\" business model by reducing usage of AOL-owned access points and shifting members with high-speed internet access from client-based usage to the more lucrative advertising provider AOL.com. The change from paid to free access was also designed to slow the rate at which members canceled their accounts and defected to Microsoft Hotmail, Yahoo! or other free email providers. The other free services included:",
        "AIM (AOL Instant Messenger)\n AOL Video, which featured professional content and allowed users to upload videos.\n AOL Local, comprising its CityGuide, Yellow Pages and Local Search services to help users find local information like restaurants, local events, and directory listings.\n AOL News\n AOL My eAddress, a custom domain name for email addresses. These email accounts could be accessed in a manner similar to those of other AOL and AIM email accounts.\n Xdrive, which allowed users to back up files over the Internet. It was acquired by AOL on August 4, 2005, and closed on December 31, 2008. It offered a free 5 GB account (free online file storage) to anyone with an AOL screenname. Xdrive also provided remote backup services and 50 GB of storage for $9.95 per month.",
        "Also in August, AOL informed its U.S. customers of an increase in the price of its dial-up access to $25.90. The increase was part of an effort to migrate the service's remaining dial-up users to broadband, as the increased price was the same as that of its monthly DSL access. However, AOL subsequently began offering unlimited dial-up access for $9.95 a month.\n\nOn November 16, 2006, Randy Falco succeeded Jonathan Miller as CEO. In December 2006, AOL closed its last remaining call center in the United States, \"taking the America out of America Online,\" according to industry pundits. Service centers based in India and the Philippines continue to provide customer support and technical assistance to subscribers.",
        "On September 17, 2007, AOL announced the relocation of one of its corporate headquarters from Dulles, Virginia to New York City and the combination of its advertising units into a new subsidiary called Platform A. This action followed several advertising acquisitions, most notably Advertising.com, and highlighted the company's new focus on advertising-driven business models. AOL management stressed that \"significant operations\" would remain in Dulles, which included the company's access services and modem banks.",
        "In October 2007, AOL announced the relocation of its other headquarters from Loudoun County, Virginia to New York City, while continuing to operate its Virginia offices. As part of the move to New York and the restructuring of responsibilities at the Dulles headquarters complex after the Reston move, Falco announced on October 15, 2007, plans to lay off 2,000 employees worldwide by the end of 2007, beginning \"immediately.\" The result was a layoff of approximately 40% of AOL's employees. Most compensation packages associated with the October 2007 layoffs included a minimum of 120 days of severance pay, 60 of which were offered in lieu of the 60-day advance notice requirement by provisions of the 1988 federal WARN Act.",
        "By November 2007, AOL's customer base had been reduced to 10.1 million subscribers, slightly more than the number of subscribers of Comcast and AT&T Yahoo!. According to Falco, as of December 2007, the conversion rate of accounts from paid access to free access was more than 80%.\n\nOn January 3, 2008, AOL announced the closing of its Reston, Virginia data center, which was sold to CRG West. On February 6, Time Warner CEO Jeff Bewkes announced that Time Warner would divide AOL's internet-access and advertising businesses, with the possibility of later selling the internet-access division.",
        "On March 13, 2008, AOL purchased the social networking site Bebo for $850 million (£417 million). On July 25, AOL announced that it was shuttering Xdrive, AOL Pictures and BlueString to save on costs and focus on its core advertising business. AOL Pictures was closed on December 31. On October 31, AOL Hometown (a web-hosting service for the websites of AOL customers) and the AOL Journal blog hosting service were eliminated.\n\n2009–2015: As a digital media company",
        "2009–2015: As a digital media company \n\nOn March 12, 2009, Tim Armstrong, formerly with Google, was named chairman and CEO of AOL. On May 28, Time Warner announced that it would position AOL as an independent company after Google's shares ceased at the end of the fiscal year. On November 23, AOL unveiled a new brand identity with the wordmark \"Aol.\" superimposed onto canvases created by commissioned artists. The new identity, designed by Wolff Olins, was integrated with all of AOL's services on December 10, the date upon which AOL traded independently for the first time since the Time Warner merger on the New York Stock Exchange under the symbol AOL.",
        "On April 6, 2010, AOL announced plans to shutter or sell Bebo. On June 16, the property was sold to Criterion Capital Partners for an undisclosed amount, believed to be approximately $10 million. In December, AIM eliminated access to AOL chat rooms, noting a marked decline in usage in recent months.\n\nUnder Armstrong's leadership, AOL followed a new business direction marked by a series of acquisitions. It announced the acquisition of Patch Media, a network of community-specific news and information sites focused on towns and communities. On September 28, 2010, at the San Francisco TechCrunch Disrupt Conference, AOL signed an agreement to acquire TechCrunch. On December 12, 2010, AOL acquired about.me, a personal profile and identity platform, four days after the platform's public launch.",
        "On January 31, 2011, AOL announced the acquisition of European video distribution network goviral. In March 2011, AOL acquired HuffPost for $315 million. Shortly after the acquisition was announced, Huffington Post co-founder Arianna Huffington replaced AOL content chief David Eun, assuming the role of president and editor-in-chief of the AOL Huffington Post Media Group. On March 10, AOL announced that it would cut approximately 900 workers following the HuffPost acquisition.\n\nOn September 14, 2011, AOL formed a strategic ad-selling partnership with two of its largest competitors, Yahoo and Microsoft. The three companies would begin selling inventory on each others' sites. The strategy was designed to help the companies compete with Google and advertising networks.",
        "On February 28, 2012, AOL partnered with PBS to launch MAKERS, a digital documentary series focusing on high-achieving women in industries perceived as male-dominated such as war, comedy, space, business, Hollywood and politics. Subjects for MAKERS episodes have included Oprah Winfrey, Hillary Clinton, Sheryl Sandberg, Martha Stewart, Indra Nooyi, Lena Dunham and Ellen DeGeneres.\n\nOn March 15, 2012, AOL announced the acquisition of Hipster, a mobile photo-sharing app, for an undisclosed amount. On April 9, 2012, AOL announced a deal to sell 800 patents to Microsoft for $1.056 billion. The deal included a perpetual license for AOL to use the patents.",
        "In April, AOL took several steps to expand its ability to generate revenue through online video advertising. The company announced that it would offer gross rating point (GRP) guarantee for online video, mirroring the television-ratings system and guaranteeing audience delivery for online-video advertising campaigns bought across its properties. This announcement came just days before the Digital Content NewFront (DCNF) a two-week event held by AOL, Google, Hulu, Microsoft, Vevo and Yahoo to showcase the participating sites' digital video offerings. The DCNF was conducted in advance of the traditional television upfronts in the hope of diverting more advertising money into the digital space. On April 24, the company launched the AOL On network, a single website for its video output.",
        "In February 2013, AOL reported its fourth quarter revenue of $599.5 million, its first growth in quarterly revenue in eight years.\n\nIn August 2013, Armstrong announced that Patch Media would scale back or sell hundreds of its local news sites. Not long afterward, layoffs began, with up to 500 out of 1,100 positions initially impacted. On January 15, 2014, Patch Media was spun off, and majority ownership was held by Hale Global. By the end of 2014, AOL controlled 0.74% of the global advertising market, well behind industry leader Google's 31.4%.",
        "On January 23, 2014, AOL acquired Gravity, a software startup that tracked users' online behavior and tailored ads and content based on their interests, for $83 million. The deal, which included approximately 40 Gravity employees and the company's personalization technology, was Armstrong's fourth-largest deal since taking command in 2009. Later that year, AOL acquired Vidible, a company that developed technology to help websites run video content from other publishers, and help video publishers sell their content to these websites. The deal, which was announced December 1, 2014, was reportedly worth roughly $50 million.",
        "On July 16, 2014, AOL earned an Emmy nomination for the AOL original series The Future Starts Here in the News and Documentary category. This came days after AOL earned its first Primetime Emmy Award nomination and win for Park Bench with Steve Buscemi in the Outstanding Short Form Variety Series. Created and hosted by Tiffany Shlain, the series focused on humans' relationship with technology and featured episodes such as \"The Future of Our Species,\" \"Why We Love Robots\" and \"A Case for Optimism.\"\n\n2015–2021: division of Verizon",
        "2015–2021: division of Verizon \n\nOn May 12, 2015, Verizon announced plans to buy AOL for $50 per share in a deal valued at $4.4 billion. The transaction was completed on June 23. Armstrong, who continued to lead the firm following regulatory approval, called the deal the logical next step for AOL. \"If you look forward five years, you're going to be in a space where there are going to be massive, global-scale networks, and there's no better partner for us to go forward with than Verizon.\" he said. \"It's really not about selling the company today. It's about setting up for the next five to 10 years.\"",
        "Analyst David Bank said he thought the deal made sense for Verizon. The deal will broaden Verizon's advertising sales platforms and increase its video production ability through websites such as HuffPost, TechCrunch, and Engadget. However, Craig Moffett said it was unlikely the deal would make a big difference to Verizon's bottom line. AOL had about two million dial-up subscribers at the time of the buyout. The announcement caused AOL's stock price to rise 17%, while Verizon's stock price dropped slightly.",
        "Shortly before the Verizon purchase, on April 14, 2015, AOL launched ONE by AOL, a digital marketing programmatic platform that unifies buying channels and audience management platforms to track and optimize campaigns over multiple screens. Later that year, on September 15, AOL expanded the product with ONE by AOL: Creative, which is geared towards creative and media agencies to similarly connect marketing and ad distribution efforts.",
        "On May 8, 2015, AOL reported its first-quarter revenue of $625.1 million, $483.5 million of which came from advertising and related operations, marking a 7% increase from Q1 2014. Over that year, the AOL Platforms division saw a 21% increase in revenue, but a drop in adjusted OIBDA due to increased investments in the company's video and programmatic platforms.",
        "On June 29, 2015, AOL announced a deal with Microsoft to take over the majority of its digital advertising business. Under the pact, as many as 1,200 Microsoft employees involved with the business will be transferred to AOL, and the company will take over the sale of display, video, and mobile ads on various Microsoft platforms in nine countries, including Brazil, Canada, the United States, and the United Kingdom. Additionally, Google Search will be replaced on AOL properties with Bing—which will display advertising sold by Microsoft. Both advertising deals are subject to affiliate marketing revenue sharing.",
        "On July 22, 2015, AOL received two News and Documentary Emmy nominations, one for MAKERS in the Outstanding Historical Programming category, and the other for True Trans With Laura Jane Grace, which documented the story of Laura Jane Grace, a transgender musician best known as the founder, lead singer, songwriter and guitarist of the punk rock band Against Me!, and her decision to come out publicly and overall transition experience.\n\nOn September 3, 2015, AOL agreed to buy Millennial Media for $238 million. On October 23, 2015, AOL completed the acquisition.",
        "On September 3, 2015, AOL agreed to buy Millennial Media for $238 million. On October 23, 2015, AOL completed the acquisition.\n\nOn October 1, 2015, Go90, a free ad-supported mobile video service aimed at young adult and teen viewers that Verizon owns and AOL oversees and operates launched its content publicly after months of beta testing. The initial launch line-up included content from Comedy Central, HuffPost, Nerdist News, Univision News, Vice, ESPN and MTV.\n\nOn April 20, 2016, AOL acquired virtual reality studio RYOT to bring immersive 360 degree video and VR content to HuffPost's global audience across desktop, mobile, and apps.",
        "On April 20, 2016, AOL acquired virtual reality studio RYOT to bring immersive 360 degree video and VR content to HuffPost's global audience across desktop, mobile, and apps.\n\nIn July 2016, Verizon Communications announced its intent to purchase the core internet business of Yahoo!. Verizon merged AOL with Yahoo into a new company called \"Oath Inc.\", which in January 2019 rebranded itself as Verizon Media.\n\nIn April 2018, Oath Inc. sold Moviefone to MoviePass Parent Helios and Matheson Analytics.\n\nIn November 2020 the Huffington Post was sold to BuzzFeed in a stock deal.",
        "In April 2018, Oath Inc. sold Moviefone to MoviePass Parent Helios and Matheson Analytics.\n\nIn November 2020 the Huffington Post was sold to BuzzFeed in a stock deal.\n\n2021–present: Apollo Global Management\nOn May 3, 2021, Verizon announced it would sell 90 percent of its Verizon Media division to Apollo Global Management for $5 billion. The division became the second incarnation of Yahoo! Inc.\n\nProducts and services\n\nContent \n\nAs of September 1, 2021, the following media brands became subsidiary of AOL's parent Yahoo Inc.\n Engadget\n Autoblog\n TechCrunch\n Built by Girls\n\nAOL's content contributors consists of over 20,000 bloggers, including politicians, celebrities, academics, and policy experts, who contribute on a wide range of topics making news.",
        "AOL's content contributors consists of over 20,000 bloggers, including politicians, celebrities, academics, and policy experts, who contribute on a wide range of topics making news.\n\nIn addition to mobile-optimized web experiences, AOL produces mobile applications for existing AOL properties like Autoblog, Engadget, The Huffington Post, TechCrunch, and products such as Alto, Pip, and Vivv.\n\nAdvertising \nAOL has a global portfolio of media brands and advertising services across mobile, desktop, and TV. Services include brand integration and sponsorships through its in-house branded content arm, Partner Studio by AOL, as well as data and programmatic offerings through ad technology stack, ONE by AOL.",
        "AOL acquired a number of businesses and technologies help to form ONE by AOL. These acquisitions included AdapTV in 2013 and Convertro, Precision Demand, and Vidible in 2014. ONE by AOL is further broken down into ONE by AOL for Publishers (formerly Vidible, AOL On Network and Be On for Publishers) and ONE by AOL for Advertisers, each of which have several sub-platforms.\n\nOn September 10, 2018, AOL's parent company Oath consolidated BrightRoll, One by AOL and Yahoo Gemini to 'simplify' adtech service by launching a single advertising proposition dubbed Oath Ad Platforms, now Yahoo! Ad Tech.\n\nMembership \nAOL offers a range of integrated products and properties including communication tools, mobile apps and services and subscription packages.",
        "In 2017, before the discontinuation of AIM, \"billions of messages\" were sent \"daily\" on it and AOL's other chat services.\n Dial-up Internet access – While 2.1 million people still used AOL's dial-up service as recently as 2015, only a few thousand were still subscribed as of 2021.\n AOL Mail – AOL Mail is AOL's proprietary email client. It is fully integrated with AIM and links to news headlines on AOL content sites.\n AOL Instant Messenger (AIM) – was AOL's proprietary instant-messaging tool. It was released in 1997. It lost market share to competition in the instant messenger market such as Google Chat, Facebook Messenger, and Skype. It also included a video-chat service, AV by AIM. On December 15, 2017, AOL discontinued AIM.",
        "AOL Plans – AOL Plans offers three online safety and assistance tools: ID protection, data security and a general online technical assistance service.",
        "AOL Desktop \n\nAOL Desktop is an internet suite produced by AOL from 2007 that integrates a web browser, a media player and an instant messenger client. Version 10.X was based on AOL OpenRide, it is an upgrade from such. The macOS version is based on WebKit.",
        "AOL Desktop version 10.X was different from previous AOL browsers and AOL Desktop versions. Its features are focused on web browsing as well as email. For instance, one does not have to sign into AOL in order to use it as a regular browser. In addition, non-AOL email accounts can be accessed through it. Primary buttons include \"MAIL\", \"IM\", and several shortcuts to various web pages. The first two require users to sign in, but the shortcuts to web pages can be used without authentication. AOL Desktop version 10.X was late marked as unsupported in favor of supporting the AOL Desktop 9.X versions.",
        "Version 9.8 was released, replacing the Internet Explorer components of the internet browser with CEF (Chromium Embedded Framework) to give users an improved web browsing experience closer to that of Chrome.\n\nVersion 11 of AOL Desktop, was a total rewrite but maintained a similar user interface to the previous 9.8.X series of releases.\n\nIn 2017, a new paid version called AOL Desktop Gold was released, available for $4.99 per month after trial. It replaced the previous free version. After the shutdown of AIM in 2017, AOL's original chat rooms continued to be accessible through AOL Desktop Gold, and some rooms remained active during peak hours. That chat system was shut down on December 15, 2020.",
        "In addition to AOL Desktop, the company also offered a browser toolbar Mozilla plug-in, AOL Toolbar, for several web browsers that provided quick access to AOL services. The toolbar was available from 2007 until 2018.\n\nCriticism \n\nIn its earlier incarnation as a \"walled garden\" community and service provider, AOL received criticism for its community policies, terms of service, and customer service. Prior to 2006, AOL was known for its direct mailing of CD-ROMs and 3.5-inch floppy disks containing its software. The disks were distributed in large numbers; at one point, half of the CDs manufactured worldwide had AOL logos on them. The marketing tactic was criticized for its environmental cost, and AOL CDs were recognized as PC Worlds most annoying tech product.",
        "Community leaders\nAOL used a system of volunteers to moderate its chat rooms, forums and user communities. The program dated back to AOL's early days, when it charged by the hour for access and one of its highest billing services was chat. AOL provided free access to community leaders in exchange for moderating the chat rooms, and this effectively made chat very cheap to operate, and more lucrative than AOL's other services of the era. There were 33,000 community leaders in 1996. All community leaders received hours of training and underwent a probationary period. While most community leaders moderated chat rooms, some ran AOL communities and controlled their layout and design, with as much as 90% of AOL's content being created or overseen by community managers until 1996.",
        "By 1996, ISPs were beginning to charge flat rates for unlimited access, which they could do at a profit because they only provided internet access. Even though AOL would lose money with such a pricing scheme, it was forced by market conditions to offer unlimited access in October 1996. In order to return to profitability, AOL rapidly shifted its focus from content creation to advertising, resulting in less of a need to carefully moderate every forum and chat room to keep users willing to pay by the minute to remain connected.",
        "After unlimited access, AOL considered scrapping the program entirely, but continued it with a reduced number of community leaders, with scaled-back roles in creating content. Although community leaders continued to receive free access, after 1996 they were motivated more by the prestige of the position and the access to moderator tools and restricted areas within AOL. By 1999, there were over 15,000 volunteers in the program.",
        "In May 1999, two former volunteers filed a class-action lawsuit alleging AOL violated the Fair Labor Standards Act by treating volunteers like employees. Volunteers had to apply for the position, commit to working for at least three to four hours a week, fill out timecards and sign a non-disclosure agreement. On July 22, AOL ended its youth corps, which consisted of 350 underage community leaders. At this time, the United States Department of Labor began an investigation into the program, but it came to no conclusions about AOL's practices.",
        "AOL ended its community leader program on June 8, 2005. The class action lawsuit dragged on for years, even after AOL ended the program and AOL declined as a major internet company. In 2010, AOL finally agreed to settle the lawsuit for $15 million. The community leader program was found to be an example of co-production in a 2009 article in International Journal of Cultural Studies.",
        "Billing disputes",
        "AOL has faced a number of lawsuits over claims that it has been slow to stop billing customers after their accounts have been canceled, either by the company or the user. In addition, AOL changed its method of calculating used minutes in response to a class action lawsuit. Previously, AOL would add 15 seconds to the time a user was connected to the service and round up to the next whole minute (thus, a person who used the service for 12 minutes and 46 seconds would be charged for 14 minutes). AOL claimed this was to account for sign on/sign off time, but because this practice was not made known to its customers, the plaintiffs won (some also pointed out that signing on and off did not always take 15 seconds, especially when connecting via another ISP). AOL disclosed its connection-time",
        "to its customers, the plaintiffs won (some also pointed out that signing on and off did not always take 15 seconds, especially when connecting via another ISP). AOL disclosed its connection-time calculation methods to all of its customers and credited them with extra free hours. In addition, the AOL software would notify the user of exactly how long they were connected and how many minutes they were being charged.",
        "AOL was sued by the Ohio Attorney General in October 2003 for improper billing practices. The case was settled on June 8, 2005. AOL agreed to resolve any consumer complaints filed with the Ohio AG's office. In December 2006, AOL agreed to provide restitution to Florida consumers to settle the case filed against them by the Florida Attorney General.",
        "Account cancellation",
        "Many customers complained that AOL personnel ignored their demands to cancel service and stop billing. In response to approximately 300 consumer complaints, the New York Attorney General's office began an inquiry of AOL's customer service policies. The investigation revealed that the company had an elaborate scheme for rewarding employees who purported to retain or \"save\" subscribers who had called to cancel their Internet service. In many instances, such retention was done against subscribers' wishes, or without their consent. Under the scheme, customer service personnel received bonuses worth tens of thousands of dollars if they could successfully dissuade or \"save\" half of the people who called to cancel service. For several years, AOL had instituted minimum retention or \"save\"",
        "worth tens of thousands of dollars if they could successfully dissuade or \"save\" half of the people who called to cancel service. For several years, AOL had instituted minimum retention or \"save\" percentages, which consumer representatives were expected to meet. These bonuses, and the minimum \"save\" rates accompanying them, had the effect of employees not honoring cancellations, or otherwise making cancellation unduly difficult for consumers.",
        "On August 24, 2005, America Online agreed to pay $1.25 million to the state of New York and reformed its customer service procedures. Under the agreement, AOL would no longer require its customer service representatives to meet a minimum quota for customer retention in order to receive a bonus. However the agreement only covered people in the state of New York.",
        "On June 13, 2006, Vincent Ferrari documented his account cancellation phone call in a blog post, stating he had switched to broadband years earlier. In the recorded phone call, the AOL representative refused to cancel the account unless the 30-year-old Ferrari explained why AOL hours were still being recorded on it. Ferrari insisted that AOL software was not even installed on the computer. When Ferrari demanded that the account be canceled regardless, the AOL representative asked to speak with Ferrari's father, for whom the account had been set up. The conversation was aired on CNBC. When CNBC reporters tried to have an account on AOL cancelled, they were hung up on immediately and it ultimately took more than 45 minutes to cancel the account.",
        "On July 19, 2006, AOL's entire retention manual was released on the Internet. On August 3, 2006, Time Warner announced that the company would be dissolving AOL's retention centers due to its profits hinging on $1 billion in cost cuts. The company estimated that it would lose more than six million subscribers over the following year.\n\nDirect marketing of disks \n\nPrior to 2006, AOL often sent unsolicited mass direct mail of 3\" floppy disks and CD-ROMs containing their software. They were the most frequent user of this marketing tactic, and received criticism for the environmental cost of the campaign. According to PC World, in the 1990s \"you couldn't open a magazine (PC World included) or your mailbox without an AOL disk falling out of it\".",
        "The mass distribution of these disks was seen as wasteful by the public and led to protest groups. One such was No More AOL CDs, a web-based effort by two IT workers to collect one million disks with the intent to return the disks to AOL. The website was started in August 2001, and an estimated 410,176 CDs were collected by August 2007 when the project was shut down.",
        "Software \nIn 2000, AOL was served with an $8 billion lawsuit alleging that its AOL 5.0 software caused significant difficulties for users attempting to use third-party Internet service providers. The lawsuit sought damages of up to $1000 for each user that had downloaded the software cited at the time of the lawsuit. AOL later agreed to a settlement of $15 million, without admission of wrongdoing. The AOL software then was given a feature called AOL Dialer, or AOL Connect on . This feature allowed users to connect to the ISP without running the full interface. This allowed users to use only the applications they wish to use, especially if they do not favor the AOL Browser.",
        "AOL 9.0 was once identified by Stopbadware as being under investigation for installing additional software without disclosure, and modifying browser preferences, toolbars, and icons. However, as of the release of AOL 9.0 VR (Vista Ready) on January 26, 2007, it was no longer considered badware due to changes AOL made in the software.",
        "Usenet newsgroups \nWhen AOL gave clients access to Usenet in 1993, they hid at least one newsgroup in standard list view: alt.aol-sucks. AOL did list the newsgroup in the alternative description view, but changed the description to \"Flames and complaints about America Online\". With AOL clients swarming Usenet newsgroups, the old, existing user base started to develop a strong distaste for both AOL and its clients, referring to the new state of affairs as Eternal September.\n\nAOL discontinued access to Usenet on June 25, 2005. No official details were provided as to the cause of decommissioning Usenet access, except providing users the suggestion to access Usenet services from a third-party, Google Groups. AOL then provided community-based message boards in lieu of Usenet.",
        "Terms of Service (TOS) \nAOL has a detailed set of guidelines and expectations for users on their service, known as the Terms of Service (TOS, also known as Conditions of Service, or COS in the UK). It is separated into three different sections: Member Agreement, Community Guidelines and Privacy Policy. All three agreements are presented to users at time of registration and digital acceptance is achieved when they access the AOL service. During the period when volunteer chat room hosts and board monitors were used, chat room hosts were given a brief online training session and test on Terms of Service violations.",
        "There have been many complaints over rules that govern an AOL user's conduct. Some users disagree with the TOS, citing the guidelines are too strict to follow coupled with the fact the TOS may change without users being made aware. A considerable cause for this was likely due to alleged censorship of user-generated content during the earlier years of growth for AOL.\n\nCertified email \nIn early 2005, AOL stated its intention to implement a certified email system called Goodmail, which will allow companies to send email to users with whom they have pre-existing business relationships, with a visual indication that the email is from a trusted source and without the risk that the email messages might be blocked or stripped by spam filters.",
        "This decision drew fire from MoveOn, which characterized the program as an \"email tax\", and the Electronic Frontier Foundation (EFF), which characterized it as a shakedown of non-profits. A website called Dearaol.com was launched, with an online petition and a blog that garnered hundreds of signatures from people and organizations expressing their opposition to AOL's use of Goodmail.\n\nEsther Dyson defended the move in an editorial in The New York Times, saying \"I hope Goodmail succeeds, and that it has lots of competition. I also think it and its competitors will eventually transform into services that more directly serve the interests of mail recipients. Instead of the fees going to Goodmail and AOL, they will also be shared with the individual recipients.\"",
        "Tim Lee of the Technology Liberation Front posted an article that questioned the Electronic Frontier Foundation's adopting a confrontational posture when dealing with private companies. Lee's article cited a series of discussions on Declan McCullagh's Politechbot mailing list on this subject between the EFF's Danny O'Brien and antispammer Suresh Ramasubramanian, who has also compared the EFF's tactics in opposing Goodmail to tactics used by Republican political strategist Karl Rove. SpamAssassin developer Justin Mason posted some criticism of the EFF's and Moveon's \"going overboard\" in their opposition to the scheme.\n\nThe dearaol.com campaign lost momentum and disappeared, with the last post to the now defunct dearaol.com blog—\"AOL starts the shakedown\" being made on May 9, 2006.",
        "The dearaol.com campaign lost momentum and disappeared, with the last post to the now defunct dearaol.com blog—\"AOL starts the shakedown\" being made on May 9, 2006.\n\nComcast, who also used the service, announced on its website that Goodmail had ceased operations and as of February 4, 2011, they no longer used the service.\n\nSearch data",
        "Comcast, who also used the service, announced on its website that Goodmail had ceased operations and as of February 4, 2011, they no longer used the service.\n\nSearch data \n\nOn August 4, 2006, AOL released a compressed text file on one of its websites containing 20 million search keywords for over 650,000 users over a 3-month period between March 1, 2006 and May 31, intended for research purposes. AOL pulled the file from public access by August 7, but not before its wide distribution on the Internet by others. Derivative research, titled A Picture of Search was published by authors Pass, Chowdhury and Torgeson for The First International Conference on Scalable Information Systems.",
        "The data were used by websites such as AOLstalker for entertainment purposes, where users of AOLstalker are encouraged to judge AOL clients based on the humorousness of personal details revealed by search behavior.\n\nUser list exposure \nIn 2003, Jason Smathers, an AOL employee, was convicted of stealing America Online's 92 million screen names and selling them to a known spammer. Smathers pled guilty to conspiracy charges in 2005. Smathers pled guilty to violations of the US CAN-SPAM Act of 2003. He was sentenced in August 2005 to 15 months in prison; the sentencing judge also recommended Smathers be forced to pay $84,000 in restitution, triple the $28,000 that he sold the addresses for.",
        "AOL's Computer Checkup \"scareware\"",
        "On February 27, 2012, a class action lawsuit was filed against Support.com, Inc. and partner AOL, Inc. The lawsuit alleged Support.com and AOL's Computer Checkup \"scareware\" (which uses software developed by Support.com) misrepresented that their software programs would identify and resolve a host of technical problems with computers, offered to perform a free \"scan,\" which often found problems with users' computers. The companies then offered to sell software—for which AOL allegedly charged $4.99 a month and Support.com $29—to remedy those problems. Both AOL, Inc. and Support.com, Inc. settled on May 30, 2013, for $8.5 million. This included $25.00 to each valid class member and $100,000 each to Consumer Watchdog and the Electronic Frontier Foundation. Judge Jacqueline Scott Corley",
        "settled on May 30, 2013, for $8.5 million. This included $25.00 to each valid class member and $100,000 each to Consumer Watchdog and the Electronic Frontier Foundation. Judge Jacqueline Scott Corley wrote: \"Distributing a portion of the [funds] to Consumer Watchdog will meet the interests of the silent class members because the organization will use the funds to help protect consumers across the nation from being subject to the types of fraudulent and misleading conduct that is alleged here,\" and \"EFF's mission includes a strong consumer protection component, especially in regards to online protection.\"",
        "AOL continues to market Computer Checkup.\n\nNSA PRISM program \nFollowing media reports about PRISM, NSA's massive electronic surveillance program, in June 2013, several technology companies were identified as participants, including AOL. According to leaks of said program, AOL joined the PRISM program in 2011.",
        "Hosting of user profiles changed, then discontinued \nAt one time, most AOL users had an online \"profile\" hosted by the AOL Hometown service. When AOL Hometown was discontinued, users had to create a new profile on Bebo. This was an unsuccessful attempt to create a social network that would compete with Facebook. When the value of Bebo decreased to a tiny fraction of the $850 million AOL paid for it, users were forced to recreate their profiles yet again, on a new service called AOL Lifestream.\n\nAOL took the decision to shut down Lifestream on February 24, 2017, and gave users one month's notice to save photos and videos that had been uploaded to Lifestream. Following the shutdown, AOL no longer provides any option for hosting user profiles.",
        "During the Hometown/Bebo/Lifestream era, another user's profile could be displayed by clicking the \"Buddy Info\" button in the AOL Desktop software. After the shutdown of Lifestream, this was no longer supported, but opened to the AIM home page (www.aim.com), which also became defunct, redirecting to AOL's home page.\n\nSee also \n\n Adrian Lamo – Inside-AOL.com\n AOHell\n Comparison of webmail providers\n David Shing\n Dot-com bubble\n List of acquisitions by AOL\n List of S&P 400 companies\n Live365\n Truveo\n\nReferences\n\nExternal links",
        "References\n\nExternal links \n\n \n\n \n1983 establishments in the United States\n2015 mergers and acquisitions\nCompanies based in Dulles, Virginia\nCompanies based in New York City\nCompanies formerly listed on the New York Stock Exchange\nCompanies in the PRISM network\nFormer WarnerMedia subsidiaries\nInternet properties established in 1983\nInternet properties established in 2009\nInternet service providers of the United States\nInternet services supporting OpenID\nPre–World Wide Web online services\nTelecommunications companies established in 1983\nTelecommunications companies established in 2009\nYahoo!\nWeb portals\nWeb service providers"
    ],
    [
        "Apache HTTP Server\nThe Apache HTTP Server ( ) is a free and open-source cross-platform web server software, released under the terms of Apache License 2.0. It is developed and maintained by a community of developers under the auspices of the Apache Software Foundation.\n\nThe vast majority of Apache HTTP Server instances run on a Linux distribution, but current versions also run on Microsoft Windows, OpenVMS, and a wide variety of Unix-like systems. Past versions also ran on NetWare, OS/2 and other operating systems, including ports to mainframes.",
        "Originally based on the NCSA HTTPd server, development of Apache began in early 1995 after work on the NCSA code stalled. Apache played a key role in the initial growth of the World Wide Web, quickly overtaking NCSA HTTPd as the dominant HTTP server. In 2009, it became the first web server software to serve more than 100 million websites.\n\n, Netcraft estimated that Apache served 23.04% of the million busiest websites, while Nginx served \n22.01%; Cloudflare at 19.53% and Microsoft Internet Information Services at 5.78% rounded out the top four. For some of Netcraft's other stats, Nginx is ahead of Apache. According to W3Techs' review of all web sites, in June 2022 Apache was ranked second at 31.4% and Nginx first at 33.6%, with Cloudflare Server third at 21.6%.",
        "Name\nAccording to The Apache Software Foundation, its name was chosen \"from respect for the various Native American nations collectively referred to as Apache, well-known for their superior skills in warfare strategy and their inexhaustible endurance\". This was in a context in which it seemed that the open internet -- based on free exchange of open source code -- appeared to be soon subjected to a kind of conquer by proprietary software vendor Microsoft; Apache co-creator Brian Behlendorf -- originator of the name -- saw his effort somewhat parallel that of Geronimo, Chief of the last of the free Apache peoples. But it conceded that the name \"also makes a cute pun on 'a patchy web server'—a server made from a series of patches\".",
        "There are other sources for the \"patchy\" software pun theory, including the project's official documentation in 1995, which stated: \"Apache is a cute name which stuck. It was based on some existing code and a series of software patches, a pun on 'A PAtCHy' server.\"\n\nBut in an April 2000 interview, Behlendorf asserted that the origins of Apache were not a pun, stating:\n\nIn January 2023, the US-based non-profit Natives in Tech accused the Apache Software Foundation of cultural appropriation and urged them to change the foundation's name, and consequently also the names of the software projects it hosts.\n\nWhen Apache is running under Unix, its process name is , which is short for \"HTTP daemon\".",
        "When Apache is running under Unix, its process name is , which is short for \"HTTP daemon\".\n\nFeature overview\nApache supports a variety of features, many implemented as compiled modules which extend the core functionality. These can range from authentication schemes to supporting server-side programming languages such as Perl, Python, Tcl and PHP. Popular authentication modules include mod_access, mod_auth, mod_digest, and mod_auth_digest, the successor to mod_digest. A sample of other features include Secure Sockets Layer and Transport Layer Security support (mod_ssl), a proxy module (mod_proxy), a URL rewriting module (mod_rewrite), custom log files (mod_log_config), and filtering support (mod_include and mod_ext_filter).",
        "Popular compression methods on Apache include the external extension module, mod_gzip, implemented to help with reduction of the size (weight) of web pages served over HTTP. ModSecurity is an open source intrusion detection and prevention engine for Web applications. Apache logs can be analyzed through a Web browser using free scripts, such as AWStats/W3Perl or Visitors.\n\nVirtual hosting allows one Apache installation to serve many different websites. For example, one computer with one Apache installation could simultaneously serve example.com, example.org, test47.test-server.example.edu, etc.\n\nApache features configurable error messages, DBMS-based authentication databases, content negotiation and supports several graphical user interfaces (GUIs).",
        "Apache features configurable error messages, DBMS-based authentication databases, content negotiation and supports several graphical user interfaces (GUIs).\n\nIt supports password authentication and digital certificate authentication. Because the source code is freely available, anyone can adapt the server for specific needs, and there is a large public library of Apache add-ons.",
        "A more detailed list of features is provided below:\n Loadable Dynamic Modules\n Multiple Request Processing modes (MPMs) including Event-based/Async, Threaded and Prefork.\n Highly scalable (easily handles more than 10,000 simultaneous connections)\n Handling of static files, index files, auto-indexing and content negotiation\n .htaccess per-directory configuration support\n Reverse proxy with caching\n Load balancing with in-band health checks\n Multiple load balancing mechanisms\n Fault tolerance and Failover with automatic recovery\n WebSocket, FastCGI, SCGI, AJP and uWSGI support with caching\n Dynamic configuration\n TLS/SSL with SNI and OCSP stapling support, via OpenSSL or wolfSSL.\n Name- and IP address-based virtual servers\n IPv6-compatible\n HTTP/2 support",
        "Dynamic configuration\n TLS/SSL with SNI and OCSP stapling support, via OpenSSL or wolfSSL.\n Name- and IP address-based virtual servers\n IPv6-compatible\n HTTP/2 support\n Fine-grained authentication and authorization access control\n gzip compression and decompression\n URL rewriting\n Headers and content rewriting\n Custom logging with rotation\n Concurrent connection limiting\n Request processing rate limiting\n Bandwidth throttling\n Server Side Includes\n IP address-based geolocation\n User and Session tracking\n WebDAV\n Embedded Perl, PHP and Lua scripting\n CGI support\n public_html per-user web-pages\n Generic expression parser\n Real-time status views\n FTP support (by a separate module)",
        "Performance\nInstead of implementing a single architecture, Apache provides a variety of MultiProcessing Modules (MPMs), which allow it to run in either a process-based mode, a hybrid (process and thread) mode, or an event-hybrid mode, in order to better match the demands of each particular infrastructure. Choice of MPM and configuration is therefore important. Where compromises in performance must be made, Apache is designed to reduce latency and increase throughput relative to simply handling more requests, thus ensuring consistent and reliable processing of requests within reasonable time-frames.",
        "For delivering static pages, Apache 2.2 series was considered significantly slower than nginx and varnish. To address this issue, the Apache developers created the Event MPM, which mixes the use of several processes and several threads per process in an asynchronous event-based loop. This architecture as implemented in the Apache 2.4 series performs at least as well as event-based web servers, according to Jim Jagielski and other independent sources. However, some independent but significantly outdated benchmarks show that it is still half as fast as nginx, e.g.\n\nLicensing\nThe Apache HTTP Server codebase was relicensed to the Apache 2.0 License (from the previous 1.1 license) in January 2004, and Apache HTTP Server 1.3.31 and 2.0.49 were the first releases using the new license.",
        "The OpenBSD project did not like the change and continued the use of pre-2.0 Apache versions, effectively forking Apache 1.3.x for its purposes. They initially replaced it with Nginx, and soon after made their own replacement, OpenBSD Httpd, based on the Relayd project.\n\nVersions\n\nVersion 1.1:\nThe Apache License 1.1 was approved by the ASF in 2000: The primary change from the 1.0 license is in the 'advertising clause' (section 3 of the 1.0 license); derived products are no longer required to include attribution in their advertising materials, only in their documentation.",
        "Version 2.0:\nThe ASF adopted the Apache License 2.0 in January 2004. The stated goals of the license included making the license easier for non-ASF projects to use, improving compatibility with GPL-based software, allowing the license to be included by reference instead of listed in every file, clarifying the license on contributions, and requiring a patent license on contributions that necessarily infringe a contributor's own patents.\n\nDevelopment",
        "Development\n\nThe Apache HTTP Server Project is a collaborative software development effort aimed at creating a robust, commercial-grade, feature-rich and freely available source code implementation of an HTTP (Web) server. The project is jointly managed by a group of volunteers located around the world, using the Internet and the Web to communicate, plan, and develop the server and its related documentation. This project is part of the Apache Software Foundation. In addition, hundreds of users have contributed ideas, code, and documentation to the project.\n\nApache 2.4 dropped support for BeOS, TPF, A/UX, NeXT, and Tandem platforms.\n\nSecurity",
        "Security\n\nApache, like other server software, can be hacked and exploited. The main Apache attack tool is Slowloris, which exploits a bug in Apache software. It creates many sockets and keeps each of them alive and busy by sending several bytes (known as \"keep-alive headers\") to let the server know that the computer is still connected and not experiencing network problems. The Apache developers have addressed Slowloris with several modules to limit the damage caused; the Apache modules mod_limitipconn, mod_qos, mod_evasive, mod security, mod_noloris, and mod_antiloris have all been suggested as means of reducing the likelihood of a successful Slowloris attack. Since Apache 2.2.15, Apache ships the module mod_reqtimeout as the official solution supported by the developers.\n\nSee also",
        "See also\n\n .htaccess\n .htpasswd\n ApacheBench\n Comparison of web server software\n IBM HTTP Server\n LAMP (software bundle)\n XAMPP\n List of Apache modules\nList of free and open-source software packages\n POSSE project\n suEXEC\n Apache Tomcat - another web server developed by the Apache Software Foundation\n\nReferences\n\nExternal links\n\n \n\n1995 software\nHTTP Server\n \nCross-platform free software\nFree software programmed in C\nFree web server software\nReverse proxy\nSoftware using the Apache license\nUnix network-related software\nWeb server software for Linux\nWeb server software"
    ],
    [
        "APL (programming language)\nAPL (named after the book A Programming Language) is a programming language developed in the 1960s by Kenneth E. Iverson. Its central datatype is the multidimensional array. It uses a large range of special graphic symbols to represent most functions and operators, leading to very concise code. It has been an important influence on the development of concept modeling, spreadsheets, functional programming, and computer math packages. It has also inspired several other programming languages.\n\nHistory",
        "History\n\nMathematical notation\nA mathematical notation for manipulating arrays was developed by Kenneth E. Iverson, starting in 1957 at Harvard University. In 1960, he began work for IBM where he developed this notation with Adin Falkoff and published it in his book A Programming Language in 1962. The preface states its premise:\n\nThis notation was used inside IBM for short research reports on computer systems, such as the Burroughs B5000 and its stack mechanism when stack machines versus register machines were being evaluated by IBM for upcoming computers.\n\nIverson also used his notation in a draft of the chapter A Programming Language, written for a book he was writing with Fred Brooks, Automatic Data Processing, which would be published in 1963.",
        "Iverson also used his notation in a draft of the chapter A Programming Language, written for a book he was writing with Fred Brooks, Automatic Data Processing, which would be published in 1963.\n\nIn 1979, Iverson received the Turing Award for his work on APL.\n\nDevelopment into a computer programming language \nAs early as 1962, the first attempt to use the notation to describe a complete computer system happened after Falkoff discussed with William C. Carter his work to standardize the instruction set for the machines that later became the IBM System/360 family.",
        "In 1963, Herbert Hellerman, working at the IBM Systems Research Institute, implemented a part of the notation on an IBM 1620 computer, and it was used by students in a special high school course on calculating transcendental functions by series summation. Students tested their code in Hellerman's lab. This implementation of a part of the notation was called Personalized Array Translator (PAT).",
        "In 1963, Falkoff, Iverson, and Edward H. Sussenguth Jr., all working at IBM, used the notation for a formal description of the IBM System/360 series machine architecture and functionality, which resulted in a paper published in IBM Systems Journal in 1964. After this was published, the team turned their attention to an implementation of the notation on a computer system. One of the motivations for this focus of implementation was the interest of John L. Lawrence who had new duties with Science Research Associates, an educational company bought by IBM in 1964. Lawrence asked Iverson and his group to help use the language as a tool to develop and use computers in education.",
        "After Lawrence M. Breed and Philip S. Abrams of Stanford University joined the team at IBM Research, they continued their prior work on an implementation programmed in FORTRAN IV for a part of the notation which had been done for the IBM 7090 computer running on the IBSYS operating system. This work was finished in late 1965 and later named IVSYS (for Iverson system). The basis of this implementation was described in detail by Abrams in a Stanford University Technical Report, \"An Interpreter for Iverson Notation\" in 1966. The academic aspect of this was formally supervised by Niklaus Wirth. Like Hellerman's PAT system earlier, this implementation did not include the APL character set but used special English reserved words for functions and operators. The system was later adapted for a",
        "Like Hellerman's PAT system earlier, this implementation did not include the APL character set but used special English reserved words for functions and operators. The system was later adapted for a time-sharing system and, by November 1966, it had been reprogrammed for the IBM System/360 Model 50 computer running in a time-sharing mode and was used internally at IBM.",
        "Hardware",
        "A key development in the ability to use APL effectively, before the wide use of cathode ray tube (CRT) terminals, was the development of a special IBM Selectric typewriter interchangeable typing element with all the special APL characters on it. This was used on paper printing terminal workstations using the Selectric typewriter and typing element mechanism, such as the IBM 1050 and IBM 2741 terminal. Keycaps could be placed over the normal keys to show which APL characters would be entered and typed when that key was struck. For the first time, a programmer could type in and see proper APL characters as used in Iverson's notation and not be forced to use awkward English keyword representations of them. Falkoff and Iverson had the special APL Selectric typing elements, 987 and 988,",
        "APL characters as used in Iverson's notation and not be forced to use awkward English keyword representations of them. Falkoff and Iverson had the special APL Selectric typing elements, 987 and 988, designed in late 1964, although no APL computer system was available to use them. Iverson cited Falkoff as the inspiration for the idea of using an IBM Selectric typing element for the APL character set.",
        "Many APL symbols, even with the APL characters on the Selectric typing element, still had to be typed in by over-striking two extant element characters. An example is the grade up character, which had to be made from a delta (shift-H) and a Sheffer stroke (shift-M). This was necessary because the APL character set was much larger than the 88 characters allowed on the typing element, even when letters were restricted to upper-case (capitals).\n\nCommercial availability \nThe first APL interactive login and creation of an APL workspace was in 1966 by Larry Breed using an IBM 1050 terminal at the IBM Mohansic Labs near Thomas J. Watson Research Center, the home of APL, in Yorktown Heights, New York.",
        "IBM was chiefly responsible for introducing APL to the marketplace. The first publicly available version of APL was released in 1968 for the IBM 1130. IBM provided APL\\1130 for free but without liability or support. It would run in as little as 8k 16-bit words of memory, and used a dedicated 1 megabyte hard disk.",
        "APL gained its foothold on mainframe timesharing systems from the late 1960s through the early 1980s, in part because it would support multiple users on lower-specification systems that had no dynamic address translation hardware. Additional improvements in performance for selected IBM System/370 mainframe systems included the APL Assist Microcode in which some support for APL execution was included in the processor's firmware, as distinct from being implemented entirely by higher-level software. Somewhat later, as suitably performing hardware was finally growing available in the mid- to late-1980s, many users migrated their applications to the personal computer environment.",
        "Early IBM APL interpreters for IBM 360 and IBM 370 hardware implemented their own multi-user management instead of relying on the host services, thus they were their own timesharing systems. First introduced for use at IBM in 1966, the APL\\360 system was a multi-user interpreter. The ability to programmatically communicate with the operating system for information and setting interpreter system variables was done through special privileged \"I-beam\" functions, using both monadic and dyadic operations.",
        "In 1973, IBM released APL.SV, which was a continuation of the same product, but which offered shared variables as a means to access facilities outside of the APL system, such as operating system files. In the mid-1970s, the IBM mainframe interpreter was even adapted for use on the IBM 5100 desktop computer, which had a small CRT and an APL keyboard, when most other small computers of the time only offered BASIC. In the 1980s, the VSAPL program product enjoyed wide use with Conversational Monitor System (CMS), Time Sharing Option (TSO), VSPC, MUSIC/SP, and CICS users.",
        "In 1973–1974, Patrick E. Hagerty directed the implementation of the University of Maryland APL interpreter for the 1100 line of the Sperry UNIVAC 1100/2200 series mainframe computers.  In 1974, student Alan Stebbens was assigned the task of implementing an internal function. Xerox APL was available from June 1975 for Xerox 560 and Sigma 6, 7, and 9 mainframes running CP-V and for Honeywell CP-6.",
        "In the 1960s and 1970s, several timesharing firms arose that sold APL services using modified versions of the IBM APL\\360 interpreter. In North America, the better-known ones were IP Sharp Associates, Scientific Time Sharing Corporation (STSC), Time Sharing Resources (TSR), and The Computer Company (TCC). CompuServe also entered the market in 1978 with an APL Interpreter based on a modified version of Digital Equipment Corp and Carnegie Mellon's, which ran on DEC's KI and KL 36-bit machines. CompuServe's APL was available both to its commercial market and the consumer information service. With the advent first of less expensive mainframes such as the IBM 4300, and later the personal computer, by the mid-1980s, the timesharing industry was all but gone.",
        "Sharp APL was available from IP Sharp Associates, first as a timesharing service in the 1960s, and later as a program product starting around 1979. Sharp APL was an advanced APL implementation with many language extensions, such as packages (the ability to put one or more objects into a single variable), a file system, nested arrays, and shared variables.\n\nAPL interpreters were available from other mainframe and mini-computer manufacturers also, notably Burroughs, Control Data Corporation (CDC), Data General, Digital Equipment Corporation (DEC), Harris, Hewlett-Packard (HP), Siemens, Xerox and others.",
        "Garth Foster of Syracuse University sponsored regular meetings of the APL implementers' community at Syracuse's Minnowbrook Conference Center in Blue Mountain Lake, New York. In later years, Eugene McDonnell organized similar meetings at the Asilomar Conference Grounds near Monterey, California, and at Pajaro Dunes near Watsonville, California. The SIGAPL special interest group of the Association for Computing Machinery continues to support the APL community.",
        "Microcomputers \nOn microcomputers, which became available from the mid-1970s onwards, BASIC became the dominant programming language. Nevertheless, some microcomputers provided APL instead – the first being the Intel 8008-based MCM/70 which was released in 1974 and which was primarily used in education. Another machine of this time was the VideoBrain Family Computer, released in 1977, which was supplied with its dialect of APL called APL/S.\n\nThe Commodore SuperPET, introduced in 1981, included an APL interpreter developed by the University of Waterloo.",
        "The Commodore SuperPET, introduced in 1981, included an APL interpreter developed by the University of Waterloo.\n\nIn 1976, Bill Gates claimed in his Open Letter to Hobbyists that Microsoft Corporation was implementing APL for the Intel 8080 and Motorola 6800 but had \"very little incentive to make [it] available to hobbyists\" because of software piracy. It was never released.",
        "APL2 \nStarting in the early 1980s, IBM APL development, under the leadership of Jim Brown, implemented a new version of the APL language that contained as its primary enhancement the concept of nested arrays, where an array can contain other arrays, and new language features which facilitated integrating nested arrays into program workflow. Ken Iverson, no longer in control of the development of the APL language, left IBM and joined I. P. Sharp Associates, where one of his major contributions was directing the evolution of Sharp APL to be more in accord with his vision. APL2 was first released for CMS and TSO in 1984. The APL2 Workstation edition (Windows, OS/2, AIX, Linux, and Solaris) followed later.",
        "As other vendors were busy developing APL interpreters for new hardware, notably Unix-based microcomputers, APL2 was almost always the standard chosen for new APL interpreter developments. Even today, most APL vendors or their users cite APL2 compatibility as a selling point for those products. IBM cites its use for problem solving, system design, prototyping, engineering and scientific computations, expert systems, for teaching mathematics and other subjects, visualization and database access.",
        "Modern implementations \nVarious implementations of APL by APLX, Dyalog, et al., include extensions for object-oriented programming, support for .NET, XML-array conversion primitives, graphing, operating system interfaces, and lambda calculus expressions. Freeware versions include GNU APL for Linux and NARS2000 for Windows (which runs on Linux under Wine). Both of these are fairly complete versions of APL2 with various language extensions.",
        "Derivative languages \nAPL has formed the basis of, or influenced, the following languages:\n A and A+, an alternative APL, the latter with graphical extensions.\n FP, a functional programming language.\nIvy, an interpreter for an APL-like language developed by Rob Pike, and which uses ASCII as input.\n J, which was also designed by Iverson, and which uses ASCII with digraphs instead of special symbols.\n K, a proprietary variant of APL developed by Arthur Whitney.\n MATLAB, a numerical computation tool.\n Nial, a high-level array programming language with a functional programming notation.\n Polymorphic Programming Language, an interactive, extensible language with a similar base language.\n S, a statistical programming language (usually now seen in the open-source version known as R).",
        "Polymorphic Programming Language, an interactive, extensible language with a similar base language.\n S, a statistical programming language (usually now seen in the open-source version known as R).\n Snap!, a low-code block-based programming language, born as an extended reimplementation of Scratch\n Speakeasy, a numerical computing interactive environment.\n Wolfram Language, the programming language of Mathematica.",
        "Language characteristics\n\nCharacter set \n\nAPL has been criticized and praised for its choice of a unique, non-standard character set. In the 1960s and 1970s, few terminal devices or even displays could reproduce the APL character set. The most popular ones employed the IBM Selectric print mechanism used with a special APL type element. One of the early APL line terminals (line-mode operation only, not full screen) was the Texas Instruments TI Model 745 () with the full APL character set which featured half and full duplex telecommunications modes, for interacting with an APL time-sharing service or remote mainframe to run a remote computer job, called an RJE.",
        "Over time, with the universal use of high-quality graphic displays, printing devices and Unicode support, the APL character font problem has largely been eliminated. However, entering APL characters requires the use of input method editors, keyboard mappings, virtual/on-screen APL symbol sets, or easy-reference printed keyboard cards which can frustrate beginners accustomed to other programming languages. With beginners who have no prior experience with other programming languages, a study involving high school students found that typing and using APL characters did not hinder the students in any measurable way.",
        "In defense of APL, it requires fewer characters to type, and keyboard mappings become memorized over time. Special APL keyboards are also made and in use today, as are freely downloadable fonts for operating systems such as Microsoft Windows. The reported productivity gains assume that one spends enough time working in the language to make it worthwhile to memorize the symbols, their semantics, and keyboard mappings, not to mention a substantial number of idioms for common tasks.",
        "Design \nUnlike traditionally structured programming languages, APL code is typically structured as chains of monadic or dyadic functions, and operators acting on arrays. APL has many nonstandard primitives (functions and operators) that are indicated by a single symbol or a combination of a few symbols. All primitives are defined to have the same precedence, and always associate to the right. Thus, APL is read or best understood from right-to-left.",
        "Early APL implementations ( or so) had no programming loop-flow control structures, such as do or while loops, and if-then-else constructs. Instead, they used array operations, and use of structured programming constructs was often not necessary, since an operation could be performed on a full array in one statement. For example, the iota function (ι) can replace for-loop iteration: ιN when applied to a scalar positive integer yields a one-dimensional array (vector), 1 2 3 ... N. More recent implementations of APL generally include comprehensive control structures, so that data structure and program control flow can be clearly and cleanly separated.",
        "The APL environment is called a workspace. In a workspace the user can define programs and data, i.e., the data values exist also outside the programs, and the user can also manipulate the data without having to define a program. In the examples below, the APL interpreter first types six spaces before awaiting the user's input. Its own output starts in column one.\n\nThe user can save the workspace with all values, programs, and execution status.",
        "The user can save the workspace with all values, programs, and execution status.\n\nAPL uses a set of non-ASCII symbols, which are an extension of traditional arithmetic and algebraic notation. Having single character names for single instruction, multiple data (SIMD) vector functions is one way that APL enables compact formulation of algorithms for data transformation such as computing Conway's Game of Life in one line of code. In nearly all versions of APL, it is theoretically possible to express any computable function in one expression, that is, in one line of code.",
        "Because of the unusual character set, many programmers use special keyboards with APL keytops to write APL code. Although there are various ways to write APL code using only ASCII characters, in practice it is almost never done. (This may be thought to support Iverson's thesis about notation as a tool of thought.) Most if not all modern implementations use standard keyboard layouts, with special mappings or input method editors to access non-ASCII characters. Historically, the APL font has been distinctive, with uppercase italic alphabetic characters and upright numerals and symbols. Most vendors continue to display the APL character set in a custom font.",
        "Advocates of APL claim that the examples of so-called write-only code (badly written and almost incomprehensible code) are almost invariably examples of poor programming practice or novice mistakes, which can occur in any language. Advocates also claim that they are far more productive with APL than with more conventional computer languages, and that working software can be implemented in far less time and with far fewer programmers than using other technology.",
        "They also may claim that because it is compact and terse, APL lends itself well to larger-scale software development and complexity, because the number of lines of code can be reduced greatly. Many APL advocates and practitioners also view standard programming languages such as COBOL and Java as being comparatively tedious. APL is often found where time-to-market is important, such as with trading systems.",
        "Terminology \nAPL makes a clear distinction between functions and operators. Functions take arrays (variables or constants or expressions) as arguments, and return arrays as results. Operators (similar to higher-order functions) take functions or arrays as arguments, and derive related functions. For example, the sum function is derived by applying the reduction operator to the addition function. Applying the same reduction operator to the maximum function (which returns the larger of two numbers) derives a function which returns the largest of a group (vector) of numbers. In the J language, Iverson substituted the terms verb for function and adverb or conjunction for operator.",
        "APL also identifies those features built into the language, and represented by a symbol, or a fixed combination of symbols, as primitives. Most primitives are either functions or operators. Coding APL is largely a process of writing non-primitive functions and (in some versions of APL) operators. However a few primitives are considered to be neither functions nor operators, most noticeably assignment.\n\nSome words used in APL literature have meanings that differ from those in both mathematics and the generality of computer science.\n\nSyntax \n\nAPL has explicit representations of functions, operators, and syntax, thus providing a basis for the clear and explicit statement of extended facilities in the language, and tools to experiment on them.\n\nExamples",
        "Examples\n\nHello, world \nThis displays \"Hello, world\":\n\n'Hello, world'A design theme in APL is to define default actions in some cases that would produce syntax errors in most other programming languages.\n\nThe 'Hello, world' string constant above displays, because display is the default action on any expression for which no action is specified explicitly (e.g. assignment, function parameter).",
        "The 'Hello, world' string constant above displays, because display is the default action on any expression for which no action is specified explicitly (e.g. assignment, function parameter).\n\nExponentiation \nAnother example of this theme is that exponentiation in APL is written as , which indicates raising 2 to the power 3 (this would be written as  or  in some languages, or relegated to a function call such as  in others). Many languages use  to signify multiplication, as in , but APL chooses to use . However, if no base is specified (as with the statement  in APL, or  in other languages), most programming languages one would see this as a syntax error. APL, however, assumes the missing base to be the natural logarithm constant e, and interprets  as .",
        "Simple statistics\nSuppose that  is an array of numbers. Then  gives its average. Reading right-to-left,  gives the number of elements in X, and since  is a dyadic operator, the term to its left is required as well. It is surrounded by parentheses since otherwise X would be taken (so that the summation would be of —each element of X divided by the number of elements in X), and  gives the sum of the elements of X. Building on this, the following expression computes standard deviation: \n\nNaturally, one would define this expression as a function for repeated use rather than rewriting it each time. Further, since assignment is an operator, it can appear within an expression, so the following would place suitable values into T, AV and SD:",
        "Pick 6 lottery numbers \nThis following immediate-mode expression generates a typical set of Pick 6 lottery numbers: six pseudo-random integers ranging from 1 to 40, guaranteed non-repeating, and displays them sorted in ascending order:\n\nx[⍋x←6?40]",
        "The above does a lot, concisely, although it may seem complex to a new APLer. It combines the following APL functions (also called primitives and glyphs):\n The first to be executed (APL executes from rightmost to leftmost) is dyadic function ? (named deal when dyadic) that returns a vector consisting of a select number (left argument: 6 in this case) of random integers ranging from 1 to a specified maximum (right argument: 40 in this case), which, if said maximum ≥ vector length, is guaranteed to be non-repeating; thus, generate/create 6 random integers ranging from 1 to 40.\n This vector is then assigned (←) to the variable x, because it is needed later.",
        "This vector is then assigned (←) to the variable x, because it is needed later.\n This vector is then sorted in ascending order by a monadic ⍋ function, which has as its right argument everything to the right of it up to the next unbalanced close-bracket or close-parenthesis. The result of ⍋ is the indices that will put its argument into ascending order.\n Then the output of ⍋ is used to index the variable x, which we saved earlier for this purpose, thereby selecting its items in ascending sequence.",
        "Since there is no function to the left of the left-most x to tell APL what to do with the result, it simply outputs it to the display (on a single line, separated by spaces) without needing any explicit instruction to do that.\n\n? also has a monadic equivalent called roll, which simply returns one random integer between 1 and its sole operand [to the right of it], inclusive. Thus, a role-playing game program might use the expression ?20 to roll a twenty-sided die.\n\nPrime numbers \nThe following expression finds all prime numbers from 1 to R. In both time and space, the calculation complexity is  (in Big O notation).\n\n(~R∊R∘.×R)/R←1↓⍳R",
        "Prime numbers \nThe following expression finds all prime numbers from 1 to R. In both time and space, the calculation complexity is  (in Big O notation).\n\n(~R∊R∘.×R)/R←1↓⍳R\n\nExecuted from right to left, this means:\n Iota ⍳ creates a vector containing integers from 1 to R (if R= 6 at the start of the program, ⍳R is 1 2 3 4 5 6)\n Drop first element of this vector (↓ function), i.e., 1. So 1↓⍳R is 2 3 4 5 6\n Set R to the new vector (←, assignment primitive), i.e., 2 3 4 5 6\n The / replicate operator is dyadic (binary) and the interpreter first evaluates its left argument (fully in parentheses):\n Generate outer product of R multiplied by R, i.e., a matrix that is the multiplication table of R by R (°.× operator), i.e.,",
        "Build a vector the same length as R with 1 in each place where the corresponding number in R is in the outer product matrix (∈, set inclusion or element of or Epsilon operator), i.e., 0 0 1 0 1\n Logically negate (not) values in the vector (change zeros to ones and ones to zeros) (∼, logical not or Tilde operator), i.e., 1 1 0 1 0\n Select the items in R for which the corresponding element is 1 (/ replicate operator), i.e., 2 3 5\n(Note, this assumes the APL origin is 1, i.e., indices start with 1. APL can be set to use 0 as the origin, so that ι6 is 0 1 2 3 4 5, which is convenient for some calculations.)\n\nSorting \nThe following expression sorts a word list stored in matrix X according to word length:\n\nX[⍋X+.≠' ';]",
        "Sorting \nThe following expression sorts a word list stored in matrix X according to word length:\n\nX[⍋X+.≠' ';]\n\nGame of Life \nThe following function \"life\", written in Dyalog APL, takes a boolean matrix and calculates the new generation according to Conway's Game of Life. It demonstrates the power of APL to implement a complex algorithm in very little code, but understanding it requires some advanced knowledge of APL (as the same program would in many languages).\n\nlife ← {⊃1 ⍵ ∨.∧ 3 4 = +/ +⌿ ¯1 0 1 ∘.⊖ ¯1 0 1 ⌽¨ ⊂⍵}\n\nHTML tags removal \nIn the following example, also Dyalog, the first line assigns some HTML code to a variable txt and then uses an APL expression to remove all the HTML tags (explanation):",
        "HTML tags removal \nIn the following example, also Dyalog, the first line assigns some HTML code to a variable txt and then uses an APL expression to remove all the HTML tags (explanation):\n\n      txt←'<html><body><p>This is <em>emphasized</em> text.</p></body></html>'\n      {⍵ /⍨ ~{⍵∨≠\\⍵}⍵∊'<>'} txt\nThis is emphasized text.\n\nNaming \nAPL derives its name from the initials of Iverson's book A Programming Language, even though the book describes Iverson's mathematical notation, rather than the implemented programming language described in this article. The name is used only for actual implementations, starting with APL\\360.\n\nAdin Falkoff coined the name in 1966 during the implementation of APL\\360 at IBM:",
        "Adin Falkoff coined the name in 1966 during the implementation of APL\\360 at IBM:\n\nAPL is occasionally re-interpreted as Array Programming Language or Array Processing Language, thereby making APL into a backronym.\n\nLogo \nThere has always been cooperation between APL vendors, and joint conferences were held on a regular basis from 1969 until 2010. At such conferences, APL merchandise was often handed out, featuring APL motifs or collection of vendor logos. Common were apples (as a pun on the similarity in pronunciation of apple and APL) and the code snippet  which are the symbols produced by the classic APL keyboard layout when holding the APL modifier key and typing \"APL\".",
        "Despite all these community efforts, no universal vendor-agnostic logo for the programming language emerged. As popular programming languages increasingly have established recognisable logos, Fortran getting one in 2020, British APL Association launched a campaign in the second half of 2021, to establish such a logo for APL, and after a community election and multiple rounds of feedback, a logo was chosen in May 2022.",
        "Use \nAPL is used for many purposes including financial and insurance applications, artificial intelligence,\nneural networks\nand robotics. It has been argued that APL is a calculation tool and not a programming language; its symbolic nature and array capabilities have made it popular with domain experts and data scientists who do not have or require the skills of a computer programmer.",
        "APL is well suited to image manipulation and computer animation, where graphic transformations can be encoded as matrix multiplications. One of the first commercial computer graphics houses, Digital Effects, produced an APL graphics product named Visions, which was used to create television commercials and animation for the 1982 film Tron. Latterly, the Stormwind boating simulator uses APL to implement its core logic, its interfacing to the rendering pipeline middleware and a major part of its physics engine.\n\nToday, APL remains in use in a wide range of commercial and scientific applications, for example\ninvestment management,\nasset management,\nhealth care,\nand DNA profiling, \nand by hobbyists.\n\nNotable implementations",
        "Notable implementations\n\nAPL\\360\nThe first implementation of APL using recognizable APL symbols was APL\\360 which ran on the IBM System/360, and was completed in November 1966 though at that time remained in use only within IBM. In 1973 its implementors, Larry Breed, Dick Lathwell and Roger Moore, were awarded the Grace Murray Hopper Award from the Association for Computing Machinery (ACM). It was given \"for their work in the design and implementation of APL\\360, setting new standards in simplicity, efficiency, reliability and response time for interactive systems.\"\n\nIn 1975, the IBM 5100 microcomputer offered APL\\360 as one of two built-in ROM-based interpreted languages for the computer, complete with a keyboard and display that supported all the special symbols used in the language.",
        "Significant developments to APL\\360 included CMS/APL, which made use of the virtual storage capabilities of CMS and APLSV, which introduced shared variables, system variables and system functions. It was subsequently ported to the IBM System/370 and VSPC platforms until its final release in 1983, after which it was replaced by APL2.\n\nAPL\\1130\nIn 1968, APL\\1130 became the first publicly available APL system, created by IBM for the IBM 1130. It became the most popular IBM Type-III Library software that IBM released.\n\nAPL*Plus and Sharp APL",
        "APL*Plus and Sharp APL are versions of APL\\360 with added business-oriented extensions such as data formatting and facilities to store APL arrays in external files. They were jointly developed by two companies, employing various members of the original IBM APL\\360 development team.",
        "The two companies were I. P. Sharp Associates (IPSA), an APL\\360 services company formed in 1964 by Ian Sharp, Roger Moore and others, and STSC, a time-sharing and consulting service company formed in 1969 by Lawrence Breed and others. Together the two developed APL*Plus and thereafter continued to work together but develop APL separately as APL*Plus and Sharp APL. STSC ported APL*Plus to many platforms with versions being made for the VAX 11, PC and UNIX, whereas IPSA took a different approach to the arrival of the personal computer and made Sharp APL available on this platform using additional PC-XT/360 hardware. In 1993, Soliton Incorporated was formed to support Sharp APL and it developed Sharp APL into SAX (Sharp APL for Unix). , APL*Plus continues as APL2000 APL+Win.",
        "In 1985, Ian Sharp, and Dan Dyer of STSC, jointly received the Kenneth E. Iverson Award for Outstanding Contribution to APL.\n\nAPL2\nAPL2 was a significant re-implementation of APL by IBM which was developed from 1971 and first released in 1984. It provides many additions to the language, of which the most notable is nested (non-rectangular) array support. The entire APL2 Products and Services Team was awarded the Iverson Award in 2007.\n\nIn 2021, IBM sold APL2 to Log-On Software, who develop and sell the product as Log-On APL2.",
        "In 2021, IBM sold APL2 to Log-On Software, who develop and sell the product as Log-On APL2.\n\nAPLGOL\nIn 1972, APLGOL was released as an experimental version of APL that added structured programming language constructs to the language framework. New statements were added for interstatement control, conditional statement execution, and statement structuring, as well as statements to clarify the intent of the algorithm. It was implemented for Hewlett-Packard in 1977.",
        "Dyalog APL\nDyalog APL was first released by British company Dyalog Ltd. in 1983 and, , is available for AIX, Linux (including on the Raspberry Pi), macOS and Microsoft Windows platforms. It is based on APL2, with extensions to support object-oriented programming, functional programming, and tacit programming. Licences are free for personal/non-commercial use.\n\nIn 1995, two of the development team – John Scholes and Peter Donnelly – were awarded the Iverson Award for their work on the interpreter. Gitte Christensen and Morten Kromberg were joint recipients of the Iverson Award in 2016.",
        "NARS2000\nNARS2000 is an open-source APL interpreter written by Bob Smith, a prominent APL developer and implementor from STSC in the 1970s and 1980s. NARS2000 contains advanced features and new datatypes and runs natively on Microsoft Windows, and other platforms under Wine. It is named after a development tool from the 1980s, NARS (Nested Arrays Research System).\n\nAPLX\n\nAPLX is a cross-platform dialect of APL, based on APL2 and with several extensions, which was first released by British company MicroAPL in 2002. Although no longer in development or on commercial sale it is now available free of charge from Dyalog.\n\nYork APL",
        "York APL\n\nYork APL was developed at the York University, Ontario around 1968, running on IBM 360 mainframes. One notable difference between it and APL\\360 was that it defined the \"shape\" (ρ) of a scalar as 1 whereas APL\\360 defined it as the more mathematically correct 0 — this made it easier to write functions that acted the same with scalars and vectors.\n\nGNU APL\nGNU APL is a free implementation of Extended APL as specified in ISO/IEC 13751:2001 and is thus an implementation of APL2. It runs on Linux, macOS, several BSD dialects, and on Windows (either using Cygwin for full support of all its system functions or as a native 64-bit Windows binary with some of its system functions missing). GNU APL uses Unicode internally and can be scripted. It was written by Jürgen Sauermann.",
        "Richard Stallman, founder of the GNU Project, was an early adopter of APL, using it to write a text editor as a high school student in the summer of 1969.",
        "Interpretation and compilation of APL \nAPL is traditionally an interpreted language, having language characteristics such as weak variable typing not well suited to compilation. However, with arrays as its core data structure it provides opportunities for performance gains through parallelism, parallel computing, massively parallel applications, and very-large-scale integration (VLSI), and from the outset APL has been regarded as a high-performance language – for example, it was noted for the speed with which it could perform complicated matrix operations \"because it operates on arrays and performs operations like matrix inversion internally\".",
        "Nevertheless, APL is rarely purely interpreted and compilation or partial compilation techniques that are, or have been, used include the following:\n\nIdiom recognition \nMost APL interpreters support idiom recognition and evaluate common idioms as single operations. For example, by evaluating the idiom BV/⍳⍴A as a single operation (where BV is a Boolean vector and A is an array), the creation of two intermediate arrays is avoided.",
        "Optimised bytecode \nWeak typing in APL means that a name may reference an array (of any datatype), a function or an operator. In general, the interpreter cannot know in advance which form it will be and must therefore perform analysis, syntax checking etc. at run-time. However, in certain circumstances, it is possible to deduce in advance what type a name is expected to reference and then generate bytecode which can be executed with reduced run-time overhead. This bytecode can also be optimised using compilation techniques such as constant folding or common subexpression elimination. The interpreter will execute the bytecode when present and when any assumptions which have been made are met. Dyalog APL includes support for optimised bytecode.",
        "Compilation \nCompilation of APL has been the subject of research and experiment since the language first became available; the first compiler is considered to be the Burroughs APL-700 which was released around 1971. In order to be able to compile APL, language limitations have to be imposed. APEX is a research APL compiler which was written by Robert Bernecky and is available under the GNU Public License.\n\nThe STSC APL Compiler is a hybrid of a bytecode optimiser and a compiler – it enables compilation of functions to machine code provided that its sub-functions and globals are declared, but the interpreter is still used as a runtime library and to execute functions which do not meet the compilation requirements.",
        "Standards \nAPL has been standardized by the American National Standards Institute (ANSI) working group X3J10 and International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC), ISO/IEC Joint Technical Committee 1 Subcommittee 22 Working Group 3. The Core APL language is specified in ISO 8485:1989, and the Extended APL language is specified in ISO/IEC 13751:2001.\n\nReferences\n\nFurther reading \n An APL Machine (1970 Stanford doctoral dissertation by Philip Abrams)\n A Personal History Of APL (1982 article by Michael S. Montalbano)\n \n \n A Programming Language by Kenneth E. Iverson\n APL in Exposition by Kenneth E. Iverson\n Brooks, Frederick P.; Kenneth Iverson (1965). Automatic Data Processing, System/360 Edition. .",
        "Video \n  – a 1974 talk show style interview with the original developers of APL.\n  – a 1975 live demonstration of APL by Professor Bob Spence, Imperial College London.\n  – a 2009 tutorial by John Scholes of Dyalog Ltd. which implements Conway's Game of Life in a single line of APL.\n  – a 2009 introduction to APL by Graeme Robertson.\n\nExternal links\n\nOnline resources \n TryAPL.org, an online APL primer\n \n APL2C, a source of links to APL compilers\n\n.NET programming languages\nAPL programming language family\nArray programming languages\nCommand shells\nDynamic programming languages\nDynamically typed programming languages\nFunctional languages\nIBM software\nProgramming languages created in 1964\nProgramming languages with an ISO standard\nProgramming languages\nHomoiconic programming languages"
    ],
    [
        "Apple I\nThe Apple Computer 1 (Apple-1), later known predominantly as the Apple I, is an 8-bit motherboard-only personal computer designed by Steve Wozniak and released by the Apple Computer Company (now Apple Inc.) in 1976. The company was initially formed to sell the Apple Iits first product and would later become the world's largest technology company. The idea of starting a company and selling the computer came from Wozniak's friend and Apple co-founder Steve Jobs.",
        "To finance its development, Wozniak and Jobs sold some of their possessions for a few hundred dollars. Wozniak demonstrated the first prototype in July 1976 at the Homebrew Computer Club in Palo Alto, California, impressing an early computer retailer. After securing an order for 50 computers, Jobs was able to order the parts on credit and deliver the first Apple products after ten days.\n\nThe Apple I was one of the first computers available that used the inexpensive MOS Technology 6502 microprocessor. An expansion included a BASIC interpreter, allowing users to utilize BASIC at home instead of at institutions with mainframe computers, greatly lowering the entry cost for computing with BASIC.",
        "Production was discontinued on September 30, 1977, after the June 10, 1977 introduction of its successor, the Apple II, which Byte magazine referred to as part of the \"1977 Trinity\" of personal computing (along with the PET 2001 from Commodore Business Machines and the TRS-80 Model I from Tandy Corporation). As relatively few computers were made before they were discontinued, coupled with their status as Apple's first product, surviving Apple I units are now displayed in computer museums.\n\nHistory\n\nDevelopment",
        "History\n\nDevelopment\n\nIn 1975, Steve Wozniak started attending meetings of the Homebrew Computer Club, which was a major source of inspiration for him. New microcomputers such as the Altair 8800 and the IMSAI 8080 inspired Wozniak to build a microprocessor into his video terminal circuit to make a complete computer. At the time the only microcomputer CPUs generally available were the $179 Intel 8080 (), and the $170 Motorola 6800 (). Wozniak preferred the 6800, but both were out of his price range. So he watched, and learned, and designed computers on paper, waiting for the day he could afford a CPU.",
        "When MOS Technology released its $20 () 6502 chip in 1976, Wozniak wrote a version of BASIC for it, then began to design a computer for it to run on. The 6502 was designed by the same people who designed the 6800, as many in Silicon Valley left employers to form their own companies. Wozniak's earlier 6800 paper-computer needed only minor changes to run on the new chip.",
        "By March 1, 1976, Wozniak completed the basic design of his computer. Wozniak originally offered the design to HP while working there, but was denied by the company on five occasions. When he demonstrated his computer at the Homebrew Computer Club, his friend and fellow club regular Steve Jobs was immediately interested in its commercial potential. Wozniak intended to share schematics of the machine for free, but Jobs advised him to start a business together and sell bare printed circuit boards for the computer. Wozniak, at first skeptical, was later convinced by Jobs that even if they were not successful they could at least say to their grandchildren that they had had their own company. To raise the money they needed to build the first batch of the circuit boards, Wozniak sold his HP",
        "not successful they could at least say to their grandchildren that they had had their own company. To raise the money they needed to build the first batch of the circuit boards, Wozniak sold his HP scientific calculator while Jobs sold his Volkswagen van.",
        "After the company was formed, Jobs and Wozniak made one last trip to the Homebrew Computer Club to give a presentation of the fully assembled \"Apple Computer A\". Paul Terrell, who was starting a new computer shop in Mountain View, California, called the Byte Shop, saw the presentation and was impressed by the machine. Terrell told Jobs that he would order 50 units of the Apple I and pay $500 () each on delivery, but only if they came fully assembledhe was not interested in buying bare printed circuit boards with no components.",
        "Jobs took the purchase order from the Byte Shop to national electronic parts distributor Cramer Electronics, and ordered the components needed. When asked by the credit manager how he would pay for the parts, Jobs replied, \"I have this purchase order from the Byte Shop chain of computer stores for 50 of my computers and the payment terms are COD. If you give me the parts on net 30-day terms I can build and deliver the computers in that time frame, collect my money from Terrell at the Byte Shop and pay you.\"",
        "To verify the purchase order, the credit manager called Paul Terrell, who assured him if the computers showed up, Jobs would have more than enough money for the parts order. The two Steves and their small crew spent day and night building and testing the computers, and delivered to Terrell on time. Terrell was surprised to receive a batch of assembled circuit boards, as he had expected complete computers with a case, monitor and keyboard. Nonetheless, he kept his word and paid the two Steves the money promised.\n\nAnnouncement and sales",
        "Announcement and sales\n\nThe Apple I went on sale in July 1976 at a price of . Wozniak later said he had no idea about the relation between the number and the mark of the beast, and that he came up with the price because he liked \"repeating digits\" and because it was a one-third markup on the  wholesale price. Jobs had managed to get the inventory into the nation's first four storefront microcomputer retailers: Byte Shop (Palo Alto, California), itty bitty machine company (Evanston, Illinois), Data Domain (Bloomington, Indiana), and Computer Mart (New York City).\n\nThe first unit produced was used in a high school math class, and donated to Liza Loop's public-access computer center. About 200 units were produced, and all but 25 were sold within nine or ten months.",
        "In April 1977, the price was dropped to . It continued to be sold through August 1977, despite the introduction of the Apple II in April 1977, which began shipping in June of that year. In October 1977, the Apple I was officially discontinued and removed from Apple's price list. As Wozniak was the only person who could answer most customer support questions about the computer, the company offered Apple I owners discounts and trade-ins for Apple IIs to persuade them to return their computers. These recovered boards were then destroyed by Apple, contributing to their later rarity.",
        "Both Steve Jobs and Steve Wozniak have stated that Apple did not assign serial numbers to the Apple l. Several boards have been found with numbered stickers affixed to them, which appear to be inspection stickers from the PCB manufacturer/assembler. A batch of boards is known to have numbers hand-written in black permanent marker on the back; these usually appear as \"01-00##\". As of January 2022, 29 Apple-1s with a serial number are known. The highest known number is . Two original Apple-1s have been analyzed by PSA, Los Angeles, concluding the serial numbers had been hand-written by Steve Jobs.\n\nHardware",
        "Hardware\n\nWozniak's design originally used a Motorola 6800 processor, which cost $175, but when MOS Technology introduced the much cheaper 6502 microprocessor ($25) he switched. The Apple I CPU ran at , a fraction () of the NTSC color carrier which simplified video circuitry. Memory used the new 4-Kbit DRAM chips, and was , expandable to  on board, or  externally. The board was designed to use the next generation of 16-Kbit memory chips when they became available. An optional $75 plug-in cassette interface card allowed users to store programs on ordinary audio cassette tapes. A BASIC interpreter, originally written by Wozniak, was provided that let users easily write programs and play simple games. An onboard AC power supply was included.",
        "Video\nThe Apple I's built-in computer terminal circuitry with TV composite output used shift register memory and a character generator.\n\nAll one needed was a television set and a ASCII keyboard. The Apple I did not come with a case. It was either used as-is or some chose to build custom (mostly wooden) cases. Competing machines such as the Altair 8800 generally were programmed with front-mounted toggle switches and used indicator lights (red LEDs, most commonly) for output, and had to be extended with separate hardware to allow connection to a computer terminal or a teletypewriter machine. This made the Apple I, along with earlier introduced Sphere 1 and other hobbyist microcomputers, an innovative machine for its day.",
        "The computer used a Signetics 2513 64×8×5 Character Generator, capable of displaying uppercase characters, numbers and basic punctuation and math symbols with a 5x8 pixel font:\n\nApple Cassette Interface expansion\n\nA cassette interface was available in the form of an optional add-on for the Apple I's expansion slot. A cassette deck plugged in to the expansion's phone connector ports could be written to and read from as a form of removable storage.\n\nThe expansion came with a free cassette tape with an Apple BASIC interpreter, and other software tapes were supplied \"at minimal cost\" including ported video games such as Hamurabi, Lunar Lander and Star Trek.\n\nIt is possible to save and play back recorded software using an iPod or other portable media player in place of a cassette deck.",
        "It is possible to save and play back recorded software using an iPod or other portable media player in place of a cassette deck.\n\nConservation\n\nOnly about 200 Apple I boards were produced, and  the whereabouts of 62 to 82 are known. After the success of the Apple II, and of Apple broadly, the Apple I was recognized as an important historical computer; according to the 1986 Apple IIe Owner's Guide, an Apple I was worth \"between $10,000 and $15,000\" and a board was reportedly sold for $50,000 in 1999.",
        "In November 2010, an Apple I with a cache of original documents and packaging sold for £133,250 ($210,000) at Christie's auction house in London. The documents included the return label showing Steve Jobs's parents' address, a personally typed and signed letter from Jobs (answering technical questions about the computer), and the invoice (listing \"Steven\" as the salesman). The computer was brought to Polytechnic University of Turin for restoration.\n\nIn October 2014 the Henry Ford Museum purchased an Apple I at a Bonhams auction for . The sale included the keyboard, monitor, cassette decks and a manual. In 2017, an Apple I removed from Steve Jobs's office in 1985 by Apple quality control engineer Don Hutmacher was placed on display at Living Computers: Museum + Labs.",
        "On May 30, 2015, an elderly woman reportedly dropped off boxes of electronics for disposal at an electronics recycling center in the Silicon Valley of Northern California. Included in the electronics (removed from her garage after the death of her husband) was an original Apple I computer, which the recycling firm sold for . When a discarded item is sold, it is the company's practice to give 50% of the proceeds to the original owner, but the woman has not been identified.\n\nApple I computers with original documents and memorabilia have frequently been auctioned for over $300,000 throughout the 2010s and 2020s. The production prototype for the Apple I survives in a badly damaged state and was itself auctioned in 2022 for $677,196.\n\nEmulation",
        "Emulation\n\nSeveral Apple I clones and replicas have been released in recent years. These are created by hobbyists and marketed to the hobbyist/collector community. Availability is usually limited to small runs in response to demand.\n\nEmulation software for the Apple I has been written for modern home computers and for web browsers. It has also been emulated on 1980s era computers including the SAM Coupé and Commodore 64.\n\nSee also\n Computer museum\n History of computer science\n History of computing\n KIM-1\n\nNotes\n\nReferences\n\nCitations\n\nSources \n\n \n Owad, Tom (2005). Apple I Replica Creation: Back to the Garage . Rockland, Mass.: Syngress Publishing. .\n Price, Rob (1987). So Far: The First Ten Years of a Vision. Cupertino, Calif.: Apple Computer. .\n\nExternal links",
        "External links\n\n Apple I Computer specifications\n Bugbook Computer Museum blog. Apple 1 display.\n Apple I Owners Club\n Apple I Operational Manual (browse)\n German making-of article to recreate the Apple I Operational Manual\n Apple I project on www.sbprojects.com\n Apple 1 Computer Registry\n Macintosh Prehistory: The Apple I\n John Calande III blog – Building the Apple I clone, including corrections on the early history of Apple Computer\nApple 1 | Cameron's Closet – includes display of the Apple 1's character set on real hardware, compared to on most emulators\n\nComputer-related introductions in 1976\nApple II family\nApple Inc. hardware\nEarly microcomputers\n6502-based home computers\nProducts and services discontinued in 1977\nDiscontinued Apple Inc. products\nSteve Jobs"
    ],
    [
        "Apple II series\nThe Apple II series (trademarked with square brackets as \"Apple ][\" and rendered on later models as \"Apple //\") is a family of home computers, one of the first highly successful mass-produced microcomputer products, designed primarily by Steve Wozniak, manufactured by Apple Computer (now Apple Inc.), and launched in 1977 with the original Apple II.\n\nIn  terms of ease of use, features, and expandability, the Apple II was a major advancement over its predecessor, the Apple I, a limited-production bare circuit board computer for electronics hobbyists. Through 1988, a number of models were introduced, with the most popular, the Apple IIe, remaining relatively unchanged into the 1990s.",
        "A model with more advanced graphics and sound and a 16-bit processor, the Apple IIGS, was added in 1986. It remained compatible with earlier Apple II models, but the IIGS had more in common with mid-1980s systems like the Atari ST, Amiga, and Acorn Archimedes.\n\nThe Apple II was first sold on June 10, 1977. By the end of production in 1993, somewhere between five and six million Apple II series computers (including about 1.25 million Apple IIGS models) had been produced. The Apple II was one of the longest running mass-produced home computer series, with models in production for just under 17 years.",
        "The Apple II became one of several recognizable and successful computers during the 1980s and early 1990s, although this was mainly limited to the US. It was aggressively marketed through volume discounts and manufacturing arrangements to educational institutions, which made it the first computer in widespread use in American secondary schools, displacing the early leader Commodore PET. The effort to develop educational and business software for the Apple II, including the 1979 release of the popular VisiCalc spreadsheet, made the computer especially popular with business users and families.",
        "Despite the introduction of the Motorola 68000-based Macintosh in 1984, the Apple II series still reportedly accounted for 85% of the company's hardware sales in the first quarter of fiscal 1985. Apple continued to sell Apple II systems alongside the Macintosh until terminating the IIGS in December 1992 and the IIe in November 1993. The last II-series Apple in production, the IIe card for Macintoshes, was discontinued on October 15, 1993. The total Apple II sales of all of its models during its 16-year production run were about 6 million units, with the peak occurring in 1983 when 1 million were sold.",
        "Hardware\nAll the machines in the series, except the //c, shared similar overall design elements. The plastic case was designed to look more like a home appliance than a piece of electronic equipment, and the machine could be opened without the use of tools, allowing access to the computer's internals.\n\nThe motherboard held eight expansion slots and an array of random access memory (RAM) sockets that could hold up to 48 kilobytes. Over the course of the Apple II series' life, an enormous amount of first- and third-party hardware was made available to extend the capabilities of the machine.\n\nThe //c was designed as a compact, portable unit, not intended to be disassembled, and could not use most of the expansion hardware sold for the other machines in the series.",
        "The //c was designed as a compact, portable unit, not intended to be disassembled, and could not use most of the expansion hardware sold for the other machines in the series.\n\nAll machines in the Apple II series had a built-in keyboard, with the exception of the IIgs which had a separate keyboard.\n\nApple IIs had color and high-resolution graphics modes, sound capabilities and a built-in BASIC programming language. The Apple II was targeted for the masses rather than just hobbyists and engineers, and influenced many of the microcomputers that followed it. Unlike preceding home microcomputers, it was sold as a finished consumer appliance rather than as a kit (unassembled or preassembled). The Apple II series eventually supported over 1,500 software programs.",
        "Apple marketed the machine as a durable product, including a 1981 ad in which an Apple II survived a fire started when a cat belonging to one early user knocked over a lamp.\n\nSoftware\nThe original Apple II provided an operating system in ROM along with a BASIC variant called Integer BASIC. The only form of storage available was cassette tape.\n\nWhen the Disk II floppy disk drive was released in 1978, a new operating system, Apple DOS, was commissioned from Shepardson Microsystems and developed by Paul Laughton, adding support for the disk drive. The final and most popular version of this software was Apple DOS 3.3.",
        "Apple DOS was superseded by ProDOS, which supported a hierarchical filesystem and larger storage devices. With an optional third-party Z80-based expansion card, the Apple II could boot into the CP/M operating system and run WordStar, dBase II, and other CP/M software. With the release of MousePaint in 1984 and the Apple IIGS in 1986, the platform took on the look of the Macintosh user interface, including a mouse.\n\nApple eventually released Applesoft BASIC, a more advanced variant of the language which users could run instead of Integer BASIC for more capabilities.\n\nSome commercial Apple II software booted directly and did not use standard DOS disk formats. This discouraged the copying or modifying of the software on the disks, and improved loading speed.\n\nModels\n\nApple II",
        "The first Apple II computers went on sale on June 10, 1977 with a MOS Technology 6502 (later Synertek) microprocessor running at 1.023 MHz, 4 KB of RAM, an audio cassette interface for loading programs and storing data, and the Integer BASIC programming language built into the ROMs. The video controller displayed 40 columns by 24 lines of monochrome, upper-case-only (the original character set matches ASCII characters 0x20 to 0x5F) text on the screen, with NTSC composite video output suitable for display on a TV monitor, or on a regular TV set by way of a separate RF modulator. The original retail price of the computer was US$1298(with 4 KB of RAM) and US$2638 (with the maximum 48 KB of RAM). To reflect the computer's color graphics capability, the Apple logo on the casing was represented",
        "price of the computer was US$1298(with 4 KB of RAM) and US$2638 (with the maximum 48 KB of RAM). To reflect the computer's color graphics capability, the Apple logo on the casing was represented using rainbow stripes, which remained a part of Apple's corporate logo until early 1998. The earliest Apple IIs were assembled in Silicon Valley, and later in Texas; printed circuit boards were manufactured in Ireland and Singapore.",
        "An external -inch floppy disk drive, the Disk II, attached via a controller card that plugged into one of the computer's expansion slots (usually slot 6), was used for data storage and retrieval to replace cassettes. The Disk II interface, created by Steve Wozniak, was regarded as an engineering masterpiece for its economy of electronic components.\n\nRather than having a dedicated sound-synthesis chip, the Apple II had a toggle circuit that could only emit a click through a built-in speaker or a line out jack; all other sounds (including two, three and, eventually, four-voice music and playback of audio samples and speech synthesis) were generated entirely by software that clicked the speaker at just the right times.",
        "The Apple II's multiple expansion slots permitted a wide variety of third-party devices, including Apple II peripheral cards such as serial controllers, display controllers, memory boards, hard disks, networking components, and realtime clocks. There were plug-in expansion cards – such as the Z-80 SoftCard – that permitted the Apple to use the Z80 processor and run a multitude of programs developed under the CP/M operating system, including the dBase II database and the WordStar word processor. There was also a third-party 6809 card that would allow OS-9 Level One to be run. Third-party sound cards greatly improved audio capabilities, allowing simple music synthesis and text-to-speech functions. Eventually, Apple II accelerator cards were created to double or quadruple the computer's",
        "cards greatly improved audio capabilities, allowing simple music synthesis and text-to-speech functions. Eventually, Apple II accelerator cards were created to double or quadruple the computer's speed.",
        "Rod Holt designed the Apple II's power supply. He employed a switched-mode power supply design, which was far smaller and generated less unwanted heat than the linear power supply some other home computers used.\n\nThe original Apple II was discontinued at the start of 1981, having been superseded by the Apple II+. By 1984, over six million machines had been sold.\n\nApple II Plus\n\nThe Apple II Plus, introduced in June 1979, included the Applesoft BASIC programming language in ROM. This Microsoft-authored dialect of BASIC, which was previously available as an upgrade, supported floating-point arithmetic, and became the standard BASIC dialect on the Apple II series (though it ran at a noticeably slower speed than Steve Wozniak's Integer BASIC).",
        "Except for improved graphics and disk-booting support in the ROM, and the removal of the 2k 6502 assembler/disassembler to make room for the floating point BASIC, the II+ was otherwise identical to the original II. RAM prices fell during 1980–81 and all II+ machines came from the factory with a full 48k of memory already installed.\n\nApple II Europlus and J-Plus\n\nAfter the success of the first Apple II in the United States, Apple expanded its market to include Europe, Australia and the Far East in 1979, with the Apple II Europlus (Europe, Australia) and the Apple II J-Plus (Japan). In these models, Apple made the necessary hardware, software and firmware changes in order to comply to standards outside of the US.\n\nApple IIe",
        "Apple IIe\n\nThe Apple II Plus was followed in 1983 by the Apple IIe, a cost-reduced yet more powerful machine that used newer chips to reduce the component count and add new features, such as the display of upper and lowercase letters and a standard 64 KB of RAM.\n\nThe IIe RAM was configured as if it were a 48 KB Apple II Plus with a language card. The machine had no slot 0, but instead had an auxiliary slot that could accept a 1 KB memory card to enable the 80-column display. This card contained only RAM; the hardware and firmware for the 80-column display was built into the Apple IIe. An \"extended 80-column card\" with more memory increased the machine's RAM to 128 KB.",
        "The Apple IIe was the most popular machine in the Apple II series. It has the distinction of being the longest-lived Apple computer of all time—it was manufactured and sold with only minor changes for nearly 11 years. The IIe was the last Apple II model to be sold, and was discontinued in November 1993.\n\nDuring its lifespan two variations were introduced: the Apple IIe Enhanced (four replacement chips to give it some of the features of the later model Apple IIc) and the Apple IIe Platinum (a modernized case color to match other Apple products of the era, along with the addition of a numeric keypad).\n\nSome of the feature of the IIe were carried over from the less successful Apple III, among them the ProDOS operating system.\n\nApple IIc",
        "Some of the feature of the IIe were carried over from the less successful Apple III, among them the ProDOS operating system.\n\nApple IIc\n\nThe Apple IIc was released in April 1984, billed as a portable Apple II because it could be easily carried due to its size and carrying handle, which could be flipped down to prop the machine up into a typing position. Unlike modern portables it lacked a built-in display and battery. It was the first of three Apple II models to be made in the Snow White design language, and the only one that used its unique creamy off-white color.",
        "The Apple IIc was the first Apple II to use the 65C02 low-power variant of the 6502 processor, and featured a built-in 5.25-inch floppy drive and 128 KB RAM, with a built-in disk controller that could control external drives, composite video (NTSC or PAL), serial interfaces for modem and printer, and a port usable by either a joystick or mouse. Unlike previous Apple II models, the IIc had no internal expansion slots at all.",
        "Two different monochrome LC displays were sold for use with the IIc's video expansion port, although both were short-lived due to high cost and poor legibility. The IIc had an external power supply that converted AC power to 15 V DC, though the IIc itself will accept between 12 V and 17 V DC, allowing third parties to offer battery packs and automobile power adapters that connected in place of the supplied AC adapter.\n\nApple IIGS",
        "Apple IIGS\n\nThe Apple IIGS, released on September 15, 1986, is the penultimate and most advanced model in the Apple II series, and a radical departure from prior models. It uses a 16-bit microprocessor, the 65C816 operating at 2.8 MHz with 24-bit addressing, allowing expansion up to 8 MB of RAM. The graphics are significantly improved, with 4096 colors and new modes with resolutions of 320×200 and 640×400. The audio capabilities are vastly improved, with a built-in music synthesizer that far exceeded any other home computer.",
        "The Apple IIGS evolved the platform while still maintaining near-complete backward compatibility. Its Mega II chip contains the functional equivalent of an entire Apple IIe computer (sans processor). This, combined with the 65816's ability to execute 65C02 code directly, provides full support for legacy software, while also supporting 16-bit software running under a new OS.\n\nThe OS eventually included a Macintosh-like graphical Finder for managing disks and files and opening documents and applications, along with desk accessories. Later, the IIGS gained the ability to read and write Macintosh disks and, through third-party software, a multitasking Unix-like shell and TrueType font support.",
        "The GS includes a 32-voice Ensoniq 5503 DOC sample-based sound synthesizer chip with 64 KB dedicated RAM, 256 KB (or later 1.125 MB) of standard RAM, built-in peripheral ports (switchable between IIe-style card slots and IIc-style onboard controllers for disk drives, mouse, RGB video, and serial devices) and, built-in AppleTalk networking.\n\nApple IIc Plus\n\nThe final Apple II model was the Apple IIc Plus introduced in 1988. It was the same size and shape as the IIc that came before it, but the 5.25-inch floppy drive had been replaced with a -inch drive, the power supply was moved inside the case, and the processor was a fast 4 MHz 65C02 processor that actually ran 8-bit Apple II software faster than the IIGS.",
        "The IIc Plus also featured a new keyboard layout that matched the Platinum IIe and IIGS. Unlike the IIe IIc and IIGS, the IIc Plus came only in one version (American) and was not officially sold anywhere outside the US. The Apple IIc Plus ceased production in 1990, with its two-year production run being the shortest of all the Apple II computers.\n\nApple IIe Card\n\nAlthough not an extension of the Apple II line, in 1990 the Apple IIe Card, an expansion card for the LC line of Macintosh computers, was released. Essentially a miniaturized Apple IIe computer on a card (using the Mega II chip from the Apple IIGS), it allowed the Macintosh to run 8-bit Apple IIe software through hardware emulation (although video was emulated in software and was slower at times than a IIe).",
        "Many of the LC's built-in Macintosh peripherals could be \"borrowed\" by the card when in Apple II mode (i.e. extra RAM, 3.5-inch floppy, AppleTalk networking, hard disk). The IIe card could not, however, run software intended for the 16-bit Apple IIGS.\n\nAdvertising, marketing, and packaging",
        "Mike Markkula, a retired Intel marketing manager, provided the early critical funding for Apple Computer. From 1977 to 1981, Apple used the Regis McKenna agency for its advertisements and marketing. In 1981, Chiat-Day acquired Regis McKenna's advertising operations and Apple used Chiat-Day. At Regis McKenna Advertising, the team assigned to launch the Apple II consisted of Rob Janoff, art director, Chip Schafer, copywriter and Bill Kelley, account executive. Janoff came up with the Apple logo with a bite out of it. The design was originally an olive green with matching company logotype all in lower case. Steve Jobs insisted on promoting the color capability of the Apple II by putting rainbow stripes on the Apple logo. In its letterhead and business card implementation, the rounded \"a\" of",
        "lower case. Steve Jobs insisted on promoting the color capability of the Apple II by putting rainbow stripes on the Apple logo. In its letterhead and business card implementation, the rounded \"a\" of the logotype echoed the \"bite\" in the logo. This logo was developed simultaneously with an advertisement and a brochure; the latter being produced for distribution initially at the first West Coast Computer Faire.",
        "Since the original Apple II, Apple has paid high attention to its quality of packaging, partly because of Steve Jobs' personal preferences and opinions on packaging and final product appearance. All of Apple's packaging for the Apple II series looked similar, featuring much clean white space and showing the Apple rainbow logo prominently. For several years up until the late 1980s, Apple used the Motter Tektura font for packaging, until changing to the Apple Garamond font.",
        "Apple ran the first advertisement for the Apple II, a two-page spread ad titled \"Introducing Apple II\", in BYTE in July 1977. The first brochure, was entitled \"Simplicity\" and the copy in both the ad and brochure pioneered \"demystifying\" language intended to make the new idea of a home computer more \"personal.\" The Apple II introduction ad was later run in the September 1977 issue of Scientific American.\n\nApple later aired eight television commercials for the Apple IIGS, emphasizing its benefits to education and students, along with some print ads.\n\nClones",
        "Apple later aired eight television commercials for the Apple IIGS, emphasizing its benefits to education and students, along with some print ads.\n\nClones\n\nThe Apple II was frequently cloned, both in the United States and abroad, in a similar way to the IBM PC. According to some sources (see below), more than 190 different models of Apple II clones were manufactured. Most could not be legally imported into the United States. Apple sued and sought criminal charges against clone makers in more than a dozen countries.\n\nData storage",
        "Data storage\n\nCassette\nOriginally the Apple II used Compact Cassette tapes for program and data storage. A dedicated tape recorder along the lines of the Commodore Datasette was never produced; Apple recommended using the Panasonic RQ309 in some of its early printed documentation. The uses of common consumer cassette recorders and a standard video monitor or television set (with a third party R-F modulator) made the total cost of owning an Apple II less expensive and helped contribute to the Apple II's success.",
        "Cassette storage may have been inexpensive, but it was also slow and unreliable. The Apple II's lack of a disk drive was \"a glaring weakness\" in what was otherwise intended to be a polished, professional product. Recognizing that the II needed a disk drive to be taken seriously, Apple set out to develop a disk drive and a DOS to run it. Wozniak spent the 1977 Christmas holidays designing a disk controller that reduced the number of chips used by a factor of 10 compared to existing controllers. Still lacking a DOS, and with Wozniak inexperienced in operating system design, Jobs approached Shepardson Microsystems with the project. On April 10, 1978, Apple signed a contract for $13,000 with Sheperdson to develop the DOS.",
        "Even after disk drives made the cassette tape interfaces obsolete they were still used by enthusiasts as simple one-bit audio input-output ports. Ham radio operators used the cassette input to receive slow scan TV (single frame images). A commercial speech recognition Blackjack program was available, after some user-specific voice training it would recognize simple commands (Hit, stand). Bob Bishop's \"Music Kaleidoscope\" was a simple program that monitored the cassette input port and based on zero-crossings created color patterns on the screen, a predecessor to current audio visualization plug-ins for media players. Music Kaleidoscope was especially popular on projection TV sets in dance halls.\n\nThe OS Disk",
        "The OS Disk\n\nApple and many third-party developers made software available on tape at first, but after the Disk II became available in 1978, tape-based Apple II software essentially disappeared from the market. The initial price of the Disk II drive and controller was US$595, although a $100 off coupon was available through the Apple newsletter \"Contact\". The controller could handle two drives and a second drive (without controller) retailed for $495.",
        "The Disk II single-sided floppy drive used 5.25-inch floppy disks; double-sided disks could be used, one side at a time, by turning them over and notching a hole for the write protect sensor. The first disk operating systems for the  were  and DOS 3.2, which stored 113.75 KB on each disk, organized into 35 tracks of 13 256-byte sectors each. After about two years, DOS 3.3 was introduced, storing 140 KB thanks to a minor firmware change on the disk controller that allowed it to store 16 sectors per track. (This upgrade was user-installable as two PROMs on older controllers.) After the release of DOS 3.3, the user community discontinued use of  except for running legacy software. Programs that required DOS 3.2 were fairly rare; however, as DOS 3.3 was not a major architectural change aside",
        "of DOS 3.3, the user community discontinued use of  except for running legacy software. Programs that required DOS 3.2 were fairly rare; however, as DOS 3.3 was not a major architectural change aside from the number of sectors per track, a program called MUFFIN was provided with DOS 3.3 to allow users to copy files from DOS 3.2 disks to DOS 3.3 disks. It was possible for software developers to create a DOS 3.2 disk which would also boot on a system with  firmware.",
        "Later, double-sided drives, with heads to read both sides of the disk, became available from third-party companies. (Apple only produced double-sided 5.25-inch disks for the Lisa 1 computer).",
        "On a DOS 3.x disk, tracks 0, 1, and most of track 2 were reserved to store the operating system. (It was possible, with a special utility, to reclaim most of this space for data if a disk did not need to be bootable.) A short ROM program on the disk controller had the ability to seek to track zero which it did without regard for the read/write head's current position, resulting in the characteristic \"chattering\" sound of a Disk II boot, which was the read/write head hitting the rubber stop block at the end of the rail – and read and execute code from sector 0. The code contained in there would then pull in the rest of the operating system. DOS stored the disk's directory on track 17, smack in the middle of the 35-track disks, in order to reduce the average seek time to the frequently used",
        "then pull in the rest of the operating system. DOS stored the disk's directory on track 17, smack in the middle of the 35-track disks, in order to reduce the average seek time to the frequently used directory track. The directory was fixed in size and could hold a maximum of 105 files. Subdirectories were not supported.",
        "Most game publishers did not include DOS on their floppy disks, since they needed the memory it occupied more than its capabilities; instead, they often wrote their own boot loaders and read-only file systems. This also served to discourage \"crackers\" from snooping around in the game's copy-protection code, since the data on the disk was not in files that could be accessed easily.\n\nSome third-party manufacturers produced floppy drives that could write 40 tracks to most 5.25-inch disks, yielding 160 KB of storage per disk, but the format did not catch on widely, and no known commercial software was published on 40-track media. Most drives, even Disk IIs, could write 36 tracks; a two byte modification to DOS to format the extra track was common.",
        "The Apple Disk II stored 140 KB on single-sided, \"single-density\" floppy disks, but it was very common for Apple II users to extend the capacity of a single-sided floppy disk to 280 KB by cutting out a second write-protect notch on the side of the disk using a \"disk notcher\" or hole puncher and inserting the disk flipped over. Double-sided disks, with notches on both sides, were available at a higher price, but in practice the magnetic coating on the reverse of nominally single-sided disks was usually of good enough quality to be used (both sides were coated in the same way to prevent warping, although only one side was certified for use). Early on, diskette manufacturers routinely warned that this technique would damage the read/write head of the drives or wear out the disk faster, and",
        "although only one side was certified for use). Early on, diskette manufacturers routinely warned that this technique would damage the read/write head of the drives or wear out the disk faster, and these warnings were frequently repeated in magazines of the day. In practice, however, this method was an inexpensive way to store twice as much data for no extra cost, and was widely used for commercially released floppies as well.",
        "Later, Apple IIs were able to use 3.5-inch disks with a total capacity of 800 KB and hard disks.  did not support these drives natively; third-party software was required, and disks larger than about 400 KB had to be split up into multiple \"virtual disk volumes.\"\n\nDOS 3.3 was succeeded by ProDOS, a 1983 descendant of the Apple ///'s SOS. It added support for subdirectories and volumes up to 32 MB in size. ProDOS became the  DOS of choice; AppleWorks and other newer programs required it.\n\nLegacy",
        "Industry impact",
        "The Apple II series of computers had an enormous impact on the technology industry and expanded the role of microcomputers in society. The Apple II was the first personal computer many people ever saw. Its price was within the reach of many middle-class families, and a partnership with MECC helped make the Apple II popular in schools. By the end of 1980 Apple had already sold over 100,000 Apple IIs. Its popularity bootstrapped the computer game and educational software markets and began the boom in the word processor and computer printer markets. The first spreadsheet application, VisiCalc, was initially released for the Apple II, and many businesses bought them just to run VisiCalc. Its success drove IBM in part to create the IBM PC, which many businesses purchased to run spreadsheet and",
        "initially released for the Apple II, and many businesses bought them just to run VisiCalc. Its success drove IBM in part to create the IBM PC, which many businesses purchased to run spreadsheet and word processing software, at first ported from Apple II versions.",
        "The Apple II's slots, allowing any peripheral card to take control of the bus and directly access memory, enabled an independent industry of card manufacturers who together created a flood of hardware products that let users build systems that were far more powerful and useful (at a lower cost) than any competing system, most of which were not nearly as expandable and were universally proprietary. The first peripheral card was a blank prototyping card intended for electronics enthusiasts who wanted to design their own peripherals for the Apple II.",
        "Specialty peripherals kept the Apple II in use in industry and education environments for many years after Apple Computer stopped supporting the Apple II. Well into the 1990s every clean-room (the super-clean facility where spacecraft are prepared for flight) at the Kennedy Space Center used an Apple II to monitor the environment and air quality. Most planetariums used Apple IIs to control their projectors and other equipment.",
        "Even the game port was unusually powerful and could be used for digital and analog input and output. The early manuals included instructions for how to build a circuit with only four commonly available components (one transistor and three resistors) and a software routine to drive a common Teletype Model 33 machine. Don Lancaster used the game I/O to drive a LaserWriter printer.",
        "Modern use\nToday, emulators for various Apple II models are available to run Apple II software on macOS, Linux, Microsoft Windows, homebrew enabled Nintendo DS and other operating systems. Numerous disk images of Apple II software are available free over the Internet for use with these emulators. AppleWin and MESS are among the best emulators compatible with most Apple II images. The MESS emulator supports recording and playing back of Apple II emulation sessions, as does Home Action Replay Page (a.k.a. HARP).\n\nIn addition, an active retrocomputing community of vintage Apple II collectors and users, continue to restore, maintain and develop hardware and software for daily use of these original computers. There is still a small annual convention, KansasFest, dedicated to the platform.",
        "In 2017, the band 8 Bit Weapon released the world's first 100% Apple II based music album entitled, \"Class Apples.\" The album featured dance-oriented cover versions of classical music by Bach, Beethoven, and Mozart recorded directly off the Apple II motherboard.\n\nSee also\nApple Industrial Design Group\nList of publications and periodicals devoted to the Apple II\nApple II peripheral cards\nApple II graphics\nList of Apple II application software\nList of Apple II games\nList of Apple IIGS games\n\nReferences\n\nExternal links",
        "References\n\nExternal links\n\nepocalc Apple II clones list\n\"These Pictures Of Apple's First Employees Are Absolutely Wonderful\", contains a c.1977 photograph taken inside Apple of early employees Chrisann Brennan, Mark Johnson, and Robert Martinengo standing in front of a stack of Apple IIs that they had tested, assembled, and were about to ship (Business Insider, December 26, 2013).\n\nApple II computers\nComputer-related introductions in 1977\nProducts and services discontinued in 1993\nDiscontinued Apple Inc. products"
    ],
    [
        "Apple III\nThe Apple III (styled as apple ///) is a business-oriented personal computer produced by Apple Computer and released in 1980. Running the Apple SOS operating system, it was intended as the successor to the Apple II series, but was largely considered a failure in the market. It was designed to provide key features business users wanted in a personal computer: a true typewriter-style upper/lowercase keyboard (the Apple II only supported uppercase) and an 80-column display.",
        "Work on the Apple III started in late 1978 under the guidance of Dr. Wendell Sander. It had the internal code name of \"Sara\", named after Sander's daughter. The system was announced on May 19, 1980 and released in late November that year. Serious stability issues required a design overhaul and a recall of the first 14,000 machines produced. The Apple III was formally reintroduced on November 9, 1981.\n\nDamage to the computer's reputation had already been done, however, and it failed to do well commercially. Development stopped, and the Apple III was discontinued on April 24, 1984. Its last successor, the III Plus, was dropped from the Apple product line in September 1985.",
        "An estimated 65,000–75,000 Apple III computers were sold. The Apple III Plus brought this up to approximately 120,000. Apple co-founder Steve Wozniak stated that the primary reason for the Apple III's failure was that the system was designed by Apple's marketing department, unlike Apple's previous engineering-driven projects. The Apple III's failure led Apple to reevaluate its plan to phase out the Apple II, prompting the eventual continuation of development of the older machine. As a result, later Apple II models incorporated some hardware and software technologies of the Apple III.\n\nOverview",
        "Overview\n\nDesign\nSteve Wozniak and Steve Jobs expected hobbyists to purchase the Apple II, but because of VisiCalc and Disk II, small businesses purchased 90% of the computers. The Apple III was designed to be a business computer and successor. Though the Apple II contributed to the inspirations of several important business products, such as VisiCalc, Multiplan, and Apple Writer, the computer's hardware architecture, operating system, and developer environment are limited. Apple management intended to clearly establish market segmentation by designing the Apple III to appeal to the 90% business market, leaving the Apple II to home and education users. Management believed that \"once the Apple III was out, the Apple II would stop selling in six months\", Wozniak said.",
        "The Apple III is powered by a 1.8-megahertz Synertek 6502A or 6502B 8-bit CPU and, like some of the later machines in the Apple II family, uses bank switching techniques to address memory beyond the 6502's traditional 64 KB limit, up to 256 KB in the III's case. Third-party vendors produced memory upgrade kits that allow the Apple III to reach up to 512 KB of random-access memory (RAM). Other Apple III built-in features include an 80-column, 24-line display with upper and lowercase characters, a numeric keypad, dual-speed (pressure-sensitive) cursor control keys, 6-bit (DAC) audio, and a built-in 140-kilobyte 5.25-inch floppy disk drive. Graphics modes include 560x192 in black and white, and 280x192 with 16 colors or shades of gray. Unlike the Apple II, the Disk III controller is part of",
        "140-kilobyte 5.25-inch floppy disk drive. Graphics modes include 560x192 in black and white, and 280x192 with 16 colors or shades of gray. Unlike the Apple II, the Disk III controller is part of the logic board.",
        "The Apple III is the first Apple product to allow the user to choose both a screen font and a keyboard layout: either QWERTY or Dvorak. These choices cannot be changed while programs were running, unlike the Apple IIc, which has a keyboard switch directly above the keyboard, allowing the user to switch on the fly.\n\nSoftware",
        "Software\n\nThe Apple III introduced an advanced operating system called Apple SOS, pronounced \"apple sauce\". Its ability to address resources by name allows the Apple III to be more scalable than the Apple II's addressing by physical location such as PR#6, CATALOG, D1. Apple SOS allows the full capacity of a storage device to be used as a single volume, such as the Apple ProFile hard disk drive, and it supports a hierarchical file system. Some of the features and code base of Apple SOS were later adopted into the Apple II's ProDOS and GS/OS operating systems, as well as Lisa 7/7 and Macintosh system software.",
        "With a starting price between , the Apple III was more expensive than many of the CP/M-based business computers that were available at the time. Few software applications other than VisiCalc are available for the computer; according to a presentation at KansasFest 2012, fewer than 50 Apple III-specific software packages were ever published, most shipping when the III Plus was released. Because Apple did not view the Apple III as suitable for hobbyists, it did not provide much of the technical software information that accompanies the Apple II. Originally intended as a direct replacement to the Apple II series, it was designed to be backward compatible with Apple II software. However, since Apple did not want to encourage continued development of the II platform, Apple II compatibility",
        "the Apple II series, it was designed to be backward compatible with Apple II software. However, since Apple did not want to encourage continued development of the II platform, Apple II compatibility exists only in a special Apple II Mode which is limited in its capabilities to the emulation of a basic Apple II Plus configuration with  of RAM. Special chips were intentionally added to prevent access from Apple II Mode to the III's advanced features such as its larger amount of memory.",
        "Peripherals\nThe Apple III has four expansion slots, a number that inCider in 1986 called \"miserly\". Apple II cards are compatible but risk violating government RFI regulations, and require Apple III-specific device drivers; BYTE stated that \"Apple provides virtually no information on how to write them\". As with software, Apple provided little hardware technical information with the computer but Apple III-specific products became available, such as one that made the computer compatible with the Apple IIe. Several new Apple-produced peripherals were developed for the Apple III. The original Apple III has a built-in real-time clock, which is recognized by Apple SOS. The clock was later removed from the \"revised\" model, and was instead made available as an add-on.",
        "Along with the built-in floppy drive, the Apple III can also handle up to three additional external Disk III floppy disk drives. The Disk III is only officially compatible with the Apple III. The Apple III Plus requires an adaptor from Apple to use the Disk III with its DB-25 disk port.\n\nWith the introduction of the revised Apple III a year after launch, Apple began offering the ProFile external hard disk system. Priced at $3,499 for 5 MB of storage, it also required a peripheral slot for its controller card.",
        "Backward compatibility\nThe Apple III has the built-in hardware capability to run Apple II software. In order to do so, an emulation boot disk is required that functionally turns the machine into a standard 48-kilobyte Apple II Plus, until it is powered off. The keyboard, internal floppy drive (and one external Disk III), display (color is provided through the 'B/W video' port) and speaker all act as Apple II peripherals. The paddle and serial ports can also function in Apple II mode, however with some limitations and compatibility issues.",
        "Apple engineers added specialized circuitry with the sole purpose of blocking access to its advanced features when running in Apple II emulation mode. This was done primarily to discourage further development and interest in the Apple II line, and to push the Apple III as its successor. For example, no more than  of RAM can be accessed, even if the machine has  of RAM or higher present. Many Apple II programs require a minimum of  of RAM, making them impossible to run on the Apple III. Similarly, access to lowercase support, 80 columns text, or its more advanced graphics and sound are blocked by this hardware circuitry, making it impossible for even skilled software programmers to bypass Apple's lockout. A third-party company, Titan Technologies, sold an expansion board called the III",
        "by this hardware circuitry, making it impossible for even skilled software programmers to bypass Apple's lockout. A third-party company, Titan Technologies, sold an expansion board called the III Plus II that allows Apple II mode to access more memory, a standard game port, and with a later released companion card, even emulate the Apple IIe.",
        "Certain Apple II slot cards can be installed in the Apple III and used in native III-mode with custom written SOS device drivers, including Grappler Plus and Liron 3.5 Controller.\n\nRevisions\n\nAfter overheating issues were attributed to serious design flaws, a redesigned logic board was introduced in mid-December 1981 – which included a lower power supply requirement, wider circuit traces and better-designed chip sockets. The $3,495 revised model also includes 256 KB of RAM as the standard configuration. The 14,000 units of the original Apple III sold were returned and replaced with the entirely new revised model.",
        "Apple III Plus\nApple discontinued the III in October 1983 because it violated FCC regulations, and the FCC required the company to change the redesigned computer's name. It introduced the Apple III Plus in December 1983 at a price of US$2,995. This newer version includes a built-in clock, video interlacing, standardized rear port connectors, 55-watt power supply, 256 KB of RAM as standard, and a redesigned, Apple IIe-like keyboard.",
        "Owners of the Apple III could purchase individual III Plus upgrades, like the clock and interlacing feature, and obtain the newer logic board as a service replacement. A keyboard upgrade kit, dubbed \"Apple III Plus upgrade kit\" was also made available – which included the keyboard, cover, keyboard encoder ROM, and logo replacements. This upgrade had to be installed by an authorized service technician.\n\nDesign flaws\n\nAccording to Wozniak, the Apple III \"had 100 percent hardware failures\". Former Apple executive Taylor Pohlman stated that:",
        "Jobs insisted on the idea of having no fan or air vents, in order to make the computer run quietly. He would later push this same ideology onto almost all Apple models he had control of, from the Apple Lisa and Macintosh 128K to the iMac. To allow the computer to dissipate heat, the base of the Apple III was made of heavy cast aluminum, which supposedly acts as a heat sink. One advantage to the aluminum case was a  reduction in RFI (Radio Frequency Interference), a problem which had plagued the Apple II series throughout its history. Unlike the Apple II series, the power supply was mounted – without its own shell – in a compartment separate from the logic board. The decision to use an aluminum shell ultimately led to engineering issues which resulted in the Apple III's reliability",
        "– without its own shell – in a compartment separate from the logic board. The decision to use an aluminum shell ultimately led to engineering issues which resulted in the Apple III's reliability problems. The lead time for manufacturing the shells was high, and this had to be done before the motherboard was finalized. Later, it was realized that there was not enough room on the motherboard for all of the components unless narrow traces were used.",
        "Many Apple IIIs were thought to have failed due to their inability to properly dissipate heat. inCider stated in 1986 that \"Heat has always been a formidable enemy of the Apple ///\", and some users reported that their Apple IIIs became so hot that the chips started dislodging from the board, causing the screen to display garbled data or their disk to come out of the slot \"melted\". BYTE wrote, \"the integrated circuits tended to wander out of their sockets\". It has been rumored Apple advised customers to tilt the front of the Apple III six inches above the desk and then drop it to reseat the chips as a temporary solution. Other analyses blame a faulty automatic chip insertion process, not heat.",
        "Case designer Jerry Manock denied the design flaw charges, insisting that tests proved that the unit adequately dissipated the internal heat. The primary cause, he claimed, was a major logic board design problem. The logic board used \"fineline\" technology that was not fully mature at the time, with narrow, closely spaced traces. When chips were \"stuffed\" into the board and wave-soldered, solder bridges would form between traces that were not supposed to be connected. This caused numerous short circuits, which required hours of costly diagnosis and hand rework to fix. Apple designed a new circuit board with more layers and normal-width traces. The new logic board was laid out by one designer on a huge drafting board, rather than using the costly CAD-CAM system used for the previous board,",
        "board with more layers and normal-width traces. The new logic board was laid out by one designer on a huge drafting board, rather than using the costly CAD-CAM system used for the previous board, and the new design worked.",
        "Earlier Apple III units came with a built-in real time clock. The hardware, however, would fail after prolonged use. Assuming that National Semiconductor would test all parts before shipping them, Apple did not perform this level of testing. Apple was soldering chips directly to boards and could not easily replace a bad chip if one was found. Eventually, Apple solved this problem by removing the real-time clock from the Apple III's specification rather than shipping the Apple III with the clock pre-installed, and then sold the peripheral as a level 1 technician add-on.\n\nBASIC",
        "BASIC\n\nMicrosoft and Apple each developed their own versions of BASIC for the Apple III. Apple III Microsoft BASIC was designed to run on the CP/M platform available for the Apple III. Apple Business BASIC shipped with the Apple III. Donn Denman ported Applesoft BASIC to SOS and reworked it to take advantage of the extended memory of the Apple III.",
        "Both languages introduced a number of new or improved features over Applesoft BASIC. Both languages replaced Applesoft's single-precision floating-point variables using 5-byte storage with the somewhat-reduced-precision 4-byte variables, while also adding a larger numerical format. Apple III Microsoft BASIC provides double-precision floating-point variables, taking 8 bytes of storage, while Apple Business BASIC offers an extra-long integer type, also taking 8 bytes for storage. Both languages also retain 2-byte integers, and maximum 255-character strings.",
        "Other new features common to both languages include:\nIncorporation of disk-file commands within the language.\nOperators for MOD and for integer-division.\nAn optional ELSE clause in IF...THEN statements.\nHEX$() function for hexadecimal-format output.\nINSTR function for finding a substring within a string.\nPRINT USING statement to control format of output. Apple Business BASIC had an option, in addition to directly specifying the format with a string expression, of giving the line number where an IMAGE statement gave the formatting expression, similar to a FORMAT statement in FORTRAN.\n\nSome features work differently in each language:",
        "Microsoft BASIC additional features\n function to replace Applesoft's  command.\n  statement to input an entire line of text, regardless of punctuation, into a single string variable.\n and   statements to automatically direct output to paper.\n and  statements to left- or right-justify a string expression within a given string variable's character length.\n function for output, and \"&\"- or \"&O\"-formatted expressions, for manipulating octal notation.\n function for generating blank spaces outside of a  statement, and  function to do likewise with any character.\n... statements, for loop structures built on general Boolean conditions without an index variable.\nBitwise Boolean (16-bit) operations (, , ), with additional operators , , .\nLine number specification in the  command.",
        "Bitwise Boolean (16-bit) operations (, , ), with additional operators , , .\nLine number specification in the  command.\n options of  (to skip to the statement after that which caused the error) or a specified line number (which replaces the idea of exiting error-handling by -line, thus avoiding Applesoft II's stack error problem).\nMultiple parameters in user-defined () functions.\nA return to the old Applesoft One concept of having multiple  functions at different addresses, by establishing ten different  functions, numbered  to , with separate  statements to define the address of each. The argument passed to a  function can be of any specific type, including string. The returned value can also be of any type, by default the same type as the argument passed.",
        "There is no support for graphics provided within the language, nor for reading analog controls or buttons; nor is there a means of defining the active window of the text screen.\n\nBusiness BASIC additional features\nApple Business BASIC eliminates all references to absolute memory addresses. Thus, the POKE command and PEEK() function were not included in the language, and new features replaced the CALL statement and USR() function. The functionality of certain features in Applesoft that had been achieved with various PEEK and POKE locations is now provided by:\n BUTTON() function to read game-controller buttons\n WINDOW statement to define the active window of the text screen by its coordinates\n KBD, HPOS, and VPOS system variables",
        "External binary subroutines and functions are loaded into memory by a single INVOKE disk-command that loads separately-assembled code modules. A PERFORM statement is then used to call an INVOKEd procedure by name, with an argument-list. INVOKEd functions would be referenced in expressions by EXFN. (floating-point) or EXFN%. (integer), with the function name appended, plus the argument-list for the function.\n\nGraphics are supported with an INVOKEd module, with features including displaying text within graphics in various fonts, within four different graphics modes available on the Apple III.\n\nReception",
        "Graphics are supported with an INVOKEd module, with features including displaying text within graphics in various fonts, within four different graphics modes available on the Apple III.\n\nReception\n\nDespite devoting the majority of its R&D to the Apple III and so ignoring the II that for a while dealers had difficulty in obtaining the latter, the III's technical problems made marketing the computer difficult. Ed Smith, who after designing the APF Imagination Machine worked as a distributor's representative, described the III as \"a complete disaster\". He recalled that he \"was responsible for going to every dealership, setting up the Apple III in their showroom, and then explaining to them the functions of the Apple III, which in many cases didn't really work\".",
        "Sales\nPohlman reported that Apple was only selling 500 units a month by late 1981, mostly as replacements. The company was able to eventually raise monthly sales to 5,000, but the IBM PC's successful launch had encouraged software companies to develop for it instead, prompting Apple to shift focus to the Lisa and Macintosh. The PC almost ended sales of the Apple III, the most closely comparable Apple computer model. By early 1984, sales were primarily to existing III owners, Apple itself—its 4,500 employees were equipped with some 3,000-4,500 units—and some small businesses. Apple finally discontinued the Apple III series on April 24, 1984, four months after introducing the III Plus, after selling only 65,000-75,000 units and replacing 14,000 defective units.",
        "Jobs said that the company lost \"infinite, incalculable amounts\" of money on the Apple III. Wozniak estimated that Apple had spent $100 million on the III, instead of improving the II and better competing against IBM. Pohlman claimed that there was a \"stigma\" at Apple associated with having contributed to the computer. Most employees who worked on the III reportedly left Apple.",
        "Legacy\nThe file system and some design ideas from Apple SOS, the Apple III's operating system, were part of Apple ProDOS and Apple GS/OS, the major operating systems for the Apple II series following the demise of the Apple III, as well as the Apple Lisa, which was the de facto business-oriented successor to the Apple III. The hierarchical file system influenced the evolution of the Macintosh: while the original Macintosh File System (MFS) was a flat file system designed for a floppy disk without subdirectories, subsequent file systems were hierarchical. By comparison, the IBM PC's first file system (again designed for floppy disks) was also flat and later versions (designed for hard disks) were hierarchical.",
        "In popular culture\nAt the start of the Walt Disney Pictures film TRON, lead character Kevin Flynn (played by Jeff Bridges) is seen hacking into the ENCOM mainframe using an Apple III.\n\nReferences\n\nSources\n\nExternal links\n\n The Ill-Fated Apple III\n Many manuals and diagrams\n Sara – Apple /// emulator\n The Ill-Fated Apple III Low End Mac\n Apple III Chaos: Apple's First Failure Low End Mac\n\nApple II family\nComputer-related introductions in 1980\nProducts and services discontinued in 1984\nDiscontinued Apple Inc. products\n8-bit computers"
    ],
    [
        "Apple Inc.\nApple Inc. is an American multinational technology company headquartered in Cupertino, California. , Apple is the world's biggest company by market capitalization, and with  the largest technology company by 2022 revenue. , Apple is the fourth-largest personal computer vendor by unit sales; the largest manufacturing company by revenue; and the second-largest mobile phone manufacturer in the world. It is considered one of the Big Five American information technology companies, alongside Alphabet (parent company of Google), Amazon, Meta Platforms, and Microsoft.",
        "Apple was founded as Apple Computer Company on April 1, 1976, by Steve Wozniak, Steve Jobs and Ronald Wayne to develop and sell Wozniak's Apple I personal computer. It was incorporated by Jobs and Wozniak as Apple Computer, Inc. in 1977. The company's second computer, the Apple II, became a best seller and one of the first mass-produced microcomputers. Apple went public in 1980 to instant financial success. The company developed computers featuring innovative graphical user interfaces, including the 1984 original Macintosh, announced that year in a critically acclaimed advertisement called \"1984\". By 1985, the high cost of its products, and power struggles between executives, caused problems. Wozniak stepped back from Apple and pursued other ventures, while Jobs resigned and founded NeXT,",
        "By 1985, the high cost of its products, and power struggles between executives, caused problems. Wozniak stepped back from Apple and pursued other ventures, while Jobs resigned and founded NeXT, taking some Apple employees with him.",
        "As the market for personal computers expanded and evolved throughout the 1990s, Apple lost considerable market share to the lower-priced duopoly of the Microsoft Windows operating system on Intel-powered PC clones (also known as \"Wintel\"). In 1997, weeks away from bankruptcy, the company bought NeXT to resolve Apple's unsuccessful operating system strategy and entice Jobs back to the company. Over the next decade, Jobs guided Apple back to profitability through a number of tactics including introducing the iMac, iPod, iPhone and iPad to critical acclaim, launching the \"Think different\" campaign and other memorable advertising campaigns, opening the Apple Store retail chain, and acquiring numerous companies to broaden the company's product portfolio. When Jobs resigned in 2011 for health",
        "and other memorable advertising campaigns, opening the Apple Store retail chain, and acquiring numerous companies to broaden the company's product portfolio. When Jobs resigned in 2011 for health reasons, and died two months later, he was succeeded as CEO by Tim Cook.",
        "Apple became the first publicly traded U.S. company to be valued at over $1 trillion in August 2018, then at $2 trillion in August 2020, and at $3 trillion in January 2022. In June 2023, it was valued at just over $3 trillion. The company receives criticism regarding the labor practices of its contractors, its environmental practices, and its business ethics, including anti-competitive practices and materials sourcing. Nevertheless, the company has a large following and enjoys a high level of brand loyalty. It has also been consistently ranked as one of the world's most valuable brands.\n\nHistory\n\n1976–1980: Founding and incorporation",
        "Apple Computer Company was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne as a partnership. The company's first product was the Apple I, a computer designed and hand-built entirely by Wozniak. To finance its creation, Jobs sold his Volkswagen Bus, and Wozniak sold his HP-65 calculator. Wozniak debuted the first prototype Apple I at the Homebrew Computer Club in July 1976. The Apple I was sold as a motherboard with CPU, RAM, and basic textual-video chips—a base kit concept which would not yet be marketed as a complete personal computer. It went on sale soon after debut for . Wozniak later said he was unaware of the coincidental mark of the beast in the number 666, and that he came up with the price because he liked \"repeating digits\".",
        "Apple Computer, Inc. was incorporated on January 3, 1977, without Wayne, who had left and sold his share of the company back to Jobs and Wozniak for $800 only twelve days after having co-founded Apple. Multimillionaire Mike Markkula provided essential business expertise and funding of  to Jobs and Wozniak during the incorporation of Apple. During the first five years of operations, revenues grew exponentially, doubling about every four months. Between September 1977 and September 1980, yearly sales grew from $775,000 to $118 million, an average annual growth rate of 533%.",
        "The Apple II, also invented by Wozniak, was introduced on April 16, 1977, at the first West Coast Computer Faire. It differed from its major rivals, the TRS-80 and Commodore PET, because of its character cell-based color graphics and open architecture. While the Apple I and early Apple II models used ordinary audio cassette tapes as storage devices, they were superseded by the introduction of a -inch floppy disk drive and interface called the Disk II in 1978.",
        "The Apple II was chosen to be the desktop platform for the first \"killer application\" of the business world: VisiCalc, a spreadsheet program released in 1979. VisiCalc created a business market for the Apple II and gave home users an additional reason to buy an Apple II: compatibility with the office, but Apple II market share remained behind home computers made by competitors such as Atari, Commodore, and Tandy.",
        "On December 12, 1980, Apple (ticker symbol \"AAPL\") went public selling 4.6 million shares at $22 per share ($.10 per share when adjusting for stock splits ), generating over $100 million, which was more capital than any IPO since Ford Motor Company in 1956. By the end of the day, 300 millionaires were created, from a stock price of $29 per share and a market cap of $1.778 billion.\n\n1980–1990: Success with Macintosh",
        "1980–1990: Success with Macintosh \n\nA critical moment in the company's history came in December 1979 when Jobs and several Apple employees, including human–computer interface expert Jef Raskin, visited Xerox PARC in to see a demonstration of the Xerox Alto, a computer using a graphical user interface. Xerox granted Apple engineers three days of access to the PARC facilities in return for the option to buy 100,000 shares (22.4 million split-adjusted shares ) of Apple at the pre-IPO price of $10 a share. After the demonstration, Jobs was immediately convinced that all future computers would use a graphical user interface, and development of a GUI began for the Apple Lisa, named after Jobs's daughter.",
        "The Lisa division would be plagued by infighting, and in 1982 Jobs was pushed off the project. The Lisa launched in 1983 and became the first personal computer sold to the public with a GUI, but was a commercial failure due to its high price and limited software titles.\n\nJobs, angered by being pushed off the Lisa team, took over the company's Macintosh division. Wozniak and Raskin had envisioned the Macintosh as a low-cost computer with a text-based interface like the Apple II, but a plane crash in 1981 forced Wozniak to step back from the project. Jobs quickly redefined the Macintosh as a graphical system that would be cheaper than the Lisa, undercutting his former division. Jobs was also hostile to the Apple II division, which at the time, generated most of the company's revenue.",
        "In 1984, Apple launched the Macintosh, the first personal computer to be sold without a programming language. Its debut was signified by \"1984\", a $1.5 million television advertisement directed by Ridley Scott that aired during the third quarter of Super Bowl XVIII on January 22, 1984. This is now hailed as a watershed event for Apple's success and was called a \"masterpiece\" by CNN and one of the greatest TV advertisements of all time by TV Guide.",
        "The advertisement created great interest in the original Macintosh, and sales were initially good, but began to taper off dramatically after the first three months as reviews started to come in. Jobs had made the decision to equip the original Macintosh with 128 kilobytes of RAM, attempting to reach a  price point, which limited its speed and the software that could be used. The Macintosh would eventually ship for , a price panned by critics in light of its slow performance. In early 1985, this sales slump triggered a power struggle between Steve Jobs and CEO John Sculley, who had been hired away from Pepsi two years earlier by Jobs saying, \"Do you want to sell sugar water for the rest of your life or come with me and change the world?\" Sculley decided to remove Jobs as the head of the",
        "away from Pepsi two years earlier by Jobs saying, \"Do you want to sell sugar water for the rest of your life or come with me and change the world?\" Sculley decided to remove Jobs as the head of the Macintosh division, with unanimous support from the Apple board of directors.",
        "The board of directors instructed Sculley to contain Jobs and his ability to launch expensive forays into untested products. Rather than submit to Sculley's direction, Jobs attempted to oust him from his leadership role at Apple. Informed by Jean-Louis Gassée, Sculley found out that Jobs had been attempting to organize a boardroom coup and called an emergency meeting at which Apple's executive staff sided with Sculley and stripped Jobs of all operational duties. Jobs resigned from Apple in September 1985 and took a number of Apple employees with him to found NeXT. Wozniak had also quit his active employment at Apple earlier in 1985 to pursue other ventures, expressing his frustration with Apple's treatment of the Apple II division and stating that the company had \"been going in the wrong",
        "employment at Apple earlier in 1985 to pursue other ventures, expressing his frustration with Apple's treatment of the Apple II division and stating that the company had \"been going in the wrong direction for the last five years.\" Despite Wozniak's grievances, he officially remained employed by Apple, and to this day continues to work for the company as a representative, receiving a stipend estimated to be $120,000 per year for this role. Both Jobs and Wozniak remained Apple shareholders after their departures.",
        "After the departures of Jobs and Wozniak, Sculley worked to improve the Macintosh in 1985 by quadrupling the RAM and introducing the LaserWriter, the first reasonably priced PostScript laser printer. PageMaker, an early desktop publishing application taking advantage of the PostScript language, was also released by Aldus Corporation in July 1985. It has been suggested that the combination of Macintosh, LaserWriter and PageMaker was responsible for the creation of the desktop publishing market.",
        "This dominant position in the desktop publishing market allowed the company to focus on higher price points, the so-called \"high-right policy\" named for the position on a chart of price vs. profits. Newer models selling at higher price points offered higher profit margin, and appeared to have no effect on total sales as power users snapped up every increase in speed. Although some worried about pricing themselves out of the market, the high-right policy was in full force by the mid-1980s, notably due to Jean-Louis Gassée's mantra of \"fifty-five or die\", referring to the 55% profit margins of the Macintosh II.",
        "This policy began to backfire in the last years of the decade as desktop publishing programs appeared on PC clones that offered some or much of the same functionality of the Macintosh, but at far lower price points. The company lost its dominant position in the desktop publishing market and estranged many of its original consumer customer base who could no longer afford their high-priced products. The Christmas season of 1989 was the first in the company's history to have declining sales, which led to a 20% drop in Apple's stock price. During this period, the relationship between Sculley and Gassée deteriorated, leading Sculley to effectively demote Gassée in January 1990 by appointing Michael Spindler as the chief operating officer. Gassée left the company later that year.",
        "1990–1997: Decline and restructuring \nThe company pivoted strategy and in October 1990 introduced three lower-cost models, the Macintosh Classic, the Macintosh LC, and the Macintosh IIsi, all of which saw significant sales due to pent-up demand. In 1991, Apple introduced the hugely successful PowerBook with a design that set the current shape for almost all modern laptops. The same year, Apple introduced System 7, a major upgrade to the Macintosh operating system, adding color to the interface and introducing new networking capabilities.",
        "The success of the lower-cost Macs and PowerBook brought increasing revenue. For some time, Apple was doing incredibly well, introducing fresh new products and generating increasing profits in the process. The magazine MacAddict named the period between 1989 and 1991 as the \"first golden age\" of the Macintosh.",
        "The success of Apple's lower-cost consumer models, especially the LC, also led to the cannibalization of their higher-priced machines. To address this, management introduced several new brands, selling largely identical machines at different price points, aimed at different markets: the high-end Quadra models, the mid-range Centris line, and the consumer-marketed Performa series. This led to significant market confusion, as customers did not understand the difference between models.",
        "The early 1990s also saw the discontinuation of the Apple II series, which was expensive to produce, and the company felt was still taking sales away from lower-cost Macintosh models. After the launch of the LC, Apple began encouraging developers to create applications for Macintosh rather than Apple II, and authorized salespersons to direct consumers towards Macintosh and away from Apple II. The Apple IIe was discontinued in 1993.\n\nThroughout this period, Microsoft continued to gain market share with its Windows graphical user interface that it sold to manufacturers of generally less expensive PC clones. While the Macintosh was more expensive, it offered a more tightly integrated user experience, but the company struggled to make the case to consumers.",
        "Apple also experimented with a number of other unsuccessful consumer targeted products during the 1990s, including digital cameras, portable CD audio players, speakers, video game consoles, the eWorld online service, and TV appliances. Most notably, enormous resources were invested in the problem-plagued Newton tablet division, based on John Sculley's unrealistic market forecasts.",
        "Throughout this period, Microsoft continued to gain market share with Windows by focusing on delivering software to inexpensive personal computers, while Apple was delivering a richly engineered but expensive experience. Apple relied on high profit margins and never developed a clear response; instead, they sued Microsoft for using a GUI similar to the Apple Lisa in Apple Computer, Inc. v. Microsoft Corp. The lawsuit dragged on for years before it was finally dismissed.\n\nThe major product flops and the rapid loss of market share to Windows sullied Apple's reputation, and in 1993 Sculley was replaced as CEO by Michael Spindler.",
        "The major product flops and the rapid loss of market share to Windows sullied Apple's reputation, and in 1993 Sculley was replaced as CEO by Michael Spindler.\n\nWith Spindler at the helm, Apple, IBM, and Motorola formed the AIM alliance in 1994 with the goal of creating a new computing platform (the PowerPC Reference Platform; PReP), which would use IBM and Motorola hardware coupled with Apple software. The AIM alliance hoped that PReP's performance and Apple's software would leave the PC far behind and thus counter the dominance of Windows. The same year, Apple introduced the Power Macintosh, the first of many Apple computers to use Motorola's PowerPC processor.",
        "In the wake of the alliance, Apple opened up to the idea of allowing Motorola and other companies to build Macintosh clones. Over the next two years, 75 distinct Macintosh clone models were introduced. However, by 1996, Apple executives were worried that the clones were cannibalizing sales of their own high-end computers, where profit margins were highest.\n\nIn 1996, Spindler was replaced by Gil Amelio as CEO. Hired for his reputation as a corporate rehabilitator, Amelio made deep changes, including extensive layoffs and cost-cutting.",
        "This period was also marked by numerous failed attempts to modernize the Macintosh operating system (MacOS). The original Macintosh operating system (System 1) was not built for multitasking (running several applications at once). The company attempted to correct this with by introducing cooperative multitasking in System 5, but the company still felt it needed a more modern approach. This led to the Pink project in 1988, A/UX that same year, Copland in 1994, and the attempted purchase of BeOS in 1996. Talks with Be stalled when the CEO, former Apple executive Jean-Louis Gassée, demanded $300 million instead of the $125 million Apple wanted to pay.",
        "Only weeks away from bankruptcy, Apple's board decided NeXTSTEP was a better choice for its next operating system and purchased NeXT in late 1996 for $400 million, bringing back Apple co-founder Steve Jobs.\n\n1997–2007: Return to profitability \nThe NeXT acquisition was finalized on February 9, 1997, and the board brought Jobs back to Apple as an advisor. On July 9, 1997, Jobs staged a boardroom coup that resulted in Amelio's resignation after overseeing a three-year record-low stock price and crippling financial losses.",
        "The board named Jobs as interim CEO and he immediately began a review of the company's products. Jobs would order 70% of the company's products to be cancelled, resulting in the loss of 3,000 jobs, and taking Apple back to the core of its computer offerings. The next month, in August 1997, Steve Jobs convinced Microsoft to make a $150 million investment in Apple and a commitment to continue developing software for the Mac. The investment was seen as an \"antitrust insurance policy\" for Microsoft who had recently settled with the Department of Justice over anti-competitive practices. Jobs also ended the Mac clone deals and in September 1997, purchased the largest clone maker, Power Computing. On November 10, 1997, Apple introduced the Apple Store website, which was tied to a new",
        "Jobs also ended the Mac clone deals and in September 1997, purchased the largest clone maker, Power Computing. On November 10, 1997, Apple introduced the Apple Store website, which was tied to a new build-to-order manufacturing that had been successfully used by PC manufacturer Dell.",
        "The moves paid off for Jobs; at the end of his first year as CEO, the company turned a $309 million profit.",
        "On May 6, 1998, Apple introduced a new all-in-one computer reminiscent of the original Macintosh: the iMac. The iMac was a huge success for Apple selling 800,000 units in its first five months and ushered in major shifts in the industry by abandoning legacy technologies like the -inch diskette, being an early adopter of the USB connector, and coming pre-installed with internet connectivity (the \"i\" in iMac) via Ethernet and a dial-up modem. The device also had a striking teardrop shape and translucent materials, designed by Jonathan Ive, who although hired by Amelio, would go on to work collaboratively with Jobs for the next decade to chart a new course the design of Apple's products.",
        "A little more than a year later on July 21, 1999, Apple introduced the iBook, a laptop for consumers. It was the culmination of a strategy established by Jobs to produce only four products: refined versions of the Power Macintosh G3 desktop and PowerBook G3 laptop for professionals, along with the iMac desktop and iBook laptop for consumers. Jobs felt the small product line allowed for a greater focus on quality and innovation.",
        "At around the same time, Apple also completed numerous acquisitions to create a portfolio of digital media production software for both professionals and consumers. Apple acquired of Macromedia's Key Grip digital video editing software project which was renamed Final Cut Pro when it was launched on the retail market in April 1999. The development of Key Grip also led to Apple's release of the consumer video-editing product iMovie in October 1999. Next, Apple successfully acquired the German company Astarte in April 2000, which had developed the DVD authoring software DVDirector, which Apple would sell as the professional-oriented DVD Studio Pro software product, and used the same technology to create iDVD for the consumer market. In 2000, Apple purchased the SoundJam MP audio player",
        "would sell as the professional-oriented DVD Studio Pro software product, and used the same technology to create iDVD for the consumer market. In 2000, Apple purchased the SoundJam MP audio player software from Casady & Greene. Apple renamed the program iTunes, while simplifying the user interface and adding the ability to burn CDs.",
        "2001 would be a pivotal year for the Apple with the company making three announcements that would change the course of the company.\n\nThe first announcement came on March 24, 2001, that Apple was nearly ready to release a new modern operating system, Mac OS X. The announcement came after numerous failed attempts in the early 1990s, and several years of development. Mac OS X was based on NeXTSTEP, OPENSTEP, and BSD Unix, with Apple aiming to combine the stability, reliability, and security of Unix with the ease of use afforded by an overhauled user interface, heavily influenced by NeXTSTEP. To aid users in migrating from Mac OS 9, the new operating system allowed the use of OS 9 applications within Mac OS X via the Classic Environment.",
        "In May 2001, the company opened its first two Apple Store retail locations in Virginia and California, offering an improved presentation of the company's products. At the time, many speculated that the stores would fail, but they went on to become highly successful, and the first of more than 500 stores around the world.\n\nOn October 23, 2001, Apple debuted the iPod portable digital audio player. The product, which was first sold on November 10, 2001, was phenomenally successful with over 100 million units sold within six years.",
        "In 2003, Apple's iTunes Store was introduced. The service offered music downloads for 99¢ a song and integration with the iPod. The iTunes Store quickly became the market leader in online music services, with over five billion downloads by June 19, 2008. Two years later, the iTunes Store was the world's largest music retailer.",
        "In 2002, Apple purchased Nothing Real for their advanced digital compositing application Shake, as well as Emagic for the music productivity application Logic. The purchase of Emagic made Apple the first computer manufacturer to own a music software company. The acquisition was followed by the development of Apple's consumer-level GarageBand application. The release of iPhoto in the same year completed the iLife suite.",
        "At the Worldwide Developers Conference keynote address on June 6, 2005, Jobs announced that Apple would move away from PowerPC processors, and the Mac would transition to Intel processors in 2006. On January 10, 2006, the new MacBook Pro and iMac became the first Apple computers to use Intel's Core Duo CPU. By August 7, 2006, Apple made the transition to Intel chips for the entire Mac product line—over one year sooner than announced. The Power Mac, iBook, and PowerBook brands were retired during the transition; the Mac Pro, MacBook, and MacBook Pro became their respective successors. On April 29, 2009, The Wall Street Journal reported that Apple was building its own team of engineers to design microchips. Apple also introduced Boot Camp in 2006 to help users install Windows XP or Windows",
        "29, 2009, The Wall Street Journal reported that Apple was building its own team of engineers to design microchips. Apple also introduced Boot Camp in 2006 to help users install Windows XP or Windows Vista on their Intel Macs alongside Mac OS X.",
        "Apple's success during this period was evident in its stock price. Between early 2003 and 2006, the price of Apple's stock increased more than tenfold, from around $6 per share (split-adjusted) to over $80. When Apple surpassed Dell's market cap in January 2006, Jobs sent an email to Apple employees saying Dell's CEO Michael Dell should eat his words. Nine years prior, Dell had said that if he ran Apple he would \"shut it down and give the money back to the shareholders\".\n\n2007–2011: Success with mobile devices",
        "2007–2011: Success with mobile devices \n\nDuring his keynote speech at the Macworld Expo on January 9, 2007, Jobs announced that Apple Computer, Inc. would thereafter be known as \"Apple Inc.\", because the company had shifted its emphasis from computers to consumer electronics. This event also saw the announcement of the iPhone and the Apple TV. The company sold 270,000 iPhone units during the first 30 hours of sales, and the device was called \"a game changer for the industry\".",
        "In an article posted on Apple's website on February 6, 2007, Jobs wrote that Apple would be willing to sell music on the iTunes Store without digital rights management (DRM), thereby allowing tracks to be played on third-party players if record labels would agree to drop the technology. On April 2, 2007, Apple and EMI jointly announced the removal of DRM technology from EMI's catalog in the iTunes Store, effective in May 2007. Other record labels eventually followed suit and Apple published a press release in January 2009 to announce that all songs on the iTunes Store are available without their FairPlay DRM.",
        "In July 2008, Apple launched the App Store to sell third-party applications for the iPhone and iPod Touch. Within a month, the store sold 60 million applications and registered an average daily revenue of $1 million, with Jobs speculating in August 2008 that the App Store could become a billion-dollar business for Apple. By October 2008, Apple was the third-largest mobile handset supplier in the world due to the popularity of the iPhone.",
        "On January 14, 2009, Jobs announced in an internal memo that he would be taking a six-month medical leave of absence from Apple until the end of June 2009 and would spend the time focusing on his health. In the email, Jobs stated that \"the curiosity over my personal health continues to be a distraction not only for me and my family, but everyone else at Apple as well\", and explained that the break would allow the company \"to focus on delivering extraordinary products\". Though Jobs was absent, Apple recorded its best non-holiday quarter (Q1 FY 2009) during the recession with revenue of $8.16 billion and profit of $1.21 billion.",
        "After years of speculation and multiple rumored \"leaks\", Apple unveiled a large screen, tablet-like media device known as the iPad on January 27, 2010. The iPad ran the same touch-based operating system as the iPhone, and all iPhone apps were compatible with the iPad. This gave the iPad a large app catalog on launch, though having very little development time before the release. Later that year on April 3, 2010, the iPad was launched in the U.S. It sold more than 300,000 units on its first day, and 500,000 by the end of the first week. In May of the same year, Apple's market cap exceeded that of competitor Microsoft for the first time since 1989.",
        "In June 2010, Apple released the iPhone 4, which introduced video calling using FaceTime, multitasking, and a new uninsulated stainless steel design that acted as the phone's antenna. Later that year, Apple again refreshed its iPod line of MP3 players by introducing a multi-touch iPod Nano, an iPod Touch with FaceTime, and an iPod Shuffle that brought back the clickwheel buttons of earlier generations. It also introduced the smaller, cheaper second generation Apple TV which allowed renting of movies and shows.",
        "On January 17, 2011, Jobs announced in an internal Apple memo that he would take another medical leave of absence for an indefinite period to allow him to focus on his health. Chief operating officer Tim Cook assumed Jobs's day-to-day operations at Apple, although Jobs would still remain \"involved in major strategic decisions\". Apple became the most valuable consumer-facing brand in the world. In June 2011, Jobs surprisingly took the stage and unveiled iCloud, an online storage and syncing service for music, photos, files, and software which replaced MobileMe, Apple's previous attempt at content syncing. This would be the last product launch Jobs would attend before his death.",
        "On August 24, 2011, Jobs resigned his position as CEO of Apple. He was replaced by Cook and Jobs became Apple's chairman. Apple did not have a chairman at the time and instead had two co-lead directors, Andrea Jung and Arthur D. Levinson, who continued with those titles until Levinson replaced Jobs as chairman of the board in November after Jobs' death.\n\n2011–present: Post-Jobs era, Cook's leadership \n\nOn October 5, 2011, Steve Jobs died, marking the end of an era for Apple. The first major product announcement by Apple following Jobs's passing occurred on January 19, 2012, when Apple's Phil Schiller introduced iBook's Textbooks for iOS and iBook Author for Mac OS X in New York City. Jobs stated in the biography \"Steve Jobs\" that he wanted to reinvent the textbook industry and education.",
        "From 2011 to 2012, Apple released the iPhone 4S and iPhone 5, which featured improved cameras, an intelligent software assistant named Siri, and cloud-synced data with iCloud; the third- and fourth-generation iPads, which featured Retina displays; and the iPad Mini, which featured a 7.9-inch screen in contrast to the iPad's 9.7-inch screen. These launches were successful, with the iPhone 5 (released September 21, 2012) becoming Apple's biggest iPhone launch with over two million pre-orders and sales of three million iPads in three days following the launch of the iPad Mini and fourth-generation iPad (released November 3, 2012). Apple also released a third-generation 13-inch MacBook Pro with a Retina display and new iMac and Mac Mini computers.",
        "On August 20, 2012, Apple's rising stock price increased the company's market capitalization to a then-record $624 billion. This beat the non-inflation-adjusted record for market capitalization previously set by Microsoft in 1999. On August 24, 2012, a US jury ruled that Samsung should pay Apple $1.05 billion (£665m) in damages in an intellectual property lawsuit. Samsung appealed the damages award, which was reduced by $450 million and further granted Samsung's request for a new trial. On November 10, 2012, Apple confirmed a global settlement that dismissed all existing lawsuits between Apple and HTC up to that date, in favor of a ten-year license agreement for current and future patents between the two companies. It is predicted that Apple will make $280 million a year from this deal",
        "and HTC up to that date, in favor of a ten-year license agreement for current and future patents between the two companies. It is predicted that Apple will make $280 million a year from this deal with HTC.",
        "In May 2014, the company confirmed its intent to acquire Dr. Dre and Jimmy Iovine's audio company Beats Electronics—producer of the \"Beats by Dr. Dre\" line of headphones and speaker products, and operator of the music streaming service Beats Music—for $3 billion, and to sell their products through Apple's retail outlets and resellers. Iovine believed that Beats had always \"belonged\" with Apple, as the company modeled itself after Apple's \"unmatched ability to marry culture and technology.\" The acquisition was the largest purchase in Apple's history.",
        "During a press event on September 9, 2014, Apple introduced a smartwatch, the Apple Watch. Initially, Apple marketed the device as a fashion accessory and a complement to the iPhone, that would allow people to look at their smartphones less. Over time, the company has focused on developing health and fitness-oriented features on the watch, in an effort to compete with dedicated activity trackers.\n\nIn January 2016, it was announced that one billion Apple devices were in active use worldwide.",
        "In January 2016, it was announced that one billion Apple devices were in active use worldwide.\n\nOn June 6, 2016, Fortune released Fortune 500, their list of companies ranked on revenue generation. In the trailing fiscal year (2015), Apple appeared on the list as the top tech company. It ranked third, overall, with $233 billion in revenue. This represents a movement upward of two spots from the previous year's list.",
        "In June 2017, Apple announced the HomePod, its smart speaker aimed to compete against Sonos, Google Home, and Amazon Echo. Towards the end of the year, TechCrunch reported that Apple was acquiring Shazam, a company that introduced its products at WWDC and specializing in music, TV, film and advertising recognition. The acquisition was confirmed a few days later, reportedly costing Apple $400 million, with media reports noting that the purchase looked like a move to acquire data and tools bolstering the Apple Music streaming service. The purchase was approved by the European Union in September 2018.",
        "Also in June 2017, Apple appointed Jamie Erlicht and Zack Van Amburg to head the newly formed worldwide video unit. In November 2017, Apple announced it was branching out into original scripted programming: a drama series starring Jennifer Aniston and Reese Witherspoon, and a reboot of the anthology series Amazing Stories with Steven Spielberg. In June 2018, Apple signed the Writers Guild of America's minimum basic agreement and Oprah Winfrey to a multi-year content partnership. Additional partnerships for original series include Sesame Workshop and DHX Media and its subsidiary Peanuts Worldwide, as well as a partnership with A24 to create original films.",
        "During the Apple Special Event in September 2017, the AirPower wireless charger was announced alongside the iPhone X, 8 and Watch Series 3. The AirPower was intended to wirelessly charge multiple devices, simultaneously. Though initially set to release in early 2018, the AirPower would be canceled in March 2019, marking the first cancellation of a device under Cook's leadership.\n\nOn August 19, 2020, Apple's share price briefly topped $467.77, making Apple the first US company with a market capitalization of $2 trillion.",
        "On August 19, 2020, Apple's share price briefly topped $467.77, making Apple the first US company with a market capitalization of $2 trillion.\n\nDuring its annual WWDC keynote speech on June 22, 2020, Apple announced it would move away from Intel processors, and the Mac would transition to processors developed in-house. The announcement was expected by industry analysts, and it has been noted that Macs featuring Apple's processors would allow for big increases in performance over current Intel-based models. On November 10, 2020, the MacBook Air, MacBook Pro, and the Mac Mini became the first Mac devices powered by an Apple-designed processor, the Apple M1.",
        "In April 2022, it was reported that Samsung Electro-Mechanics would be collaborating with Apple on its M2 chip instead of LG Innotek. Developer logs showed that at least nine Mac models with four different M2 chips were being tested.\n\nThe Wall Street Journal reported that an effort to develop its own chips left Apple better prepared to deal with the semiconductor shortage that emerged during the pandemic era and led to increased profitability, with sales of Mac computers that included M1 chips rising sharply in 2020 and 2021. It also inspired other companies like Tesla, Amazon, and Meta Platforms to pursue a similar path.",
        "In April 2022, Apple opened an online store that allowed anyone in the US to view repair manuals and order replacement parts for specific recent iPhones, although the difference in cost between this method and official repair is anticipated to be minimal.\n\nIn May 2022, a trademark was filed for RealityOS, an operating system reportedly intended for virtual and augmented reality headsets, first mentioned in 2017. According to Bloomberg, the headset may come out in 2023. Further insider reports state that the device uses iris scanning for payment confirmation and signing into accounts.\n\nOn June 18, 2022, the Apple Store in Towson, Maryland became the first to unionize in the U.S., with the employees voting to join the International Association of Machinists and Aerospace Workers.",
        "On June 18, 2022, the Apple Store in Towson, Maryland became the first to unionize in the U.S., with the employees voting to join the International Association of Machinists and Aerospace Workers.\n\nOn July 7, 2022, Apple added Lockdown Mode to macOS 13 and iOS 16, as a response to the earlier Pegasus revelations; the mode increases security protections for high-risk users against targeted zero-day malware.\n\nApple launched a buy now, pay later service called 'Apple Pay Later' for its Apple Wallet users in March 2023. The program allows its users to apply for loans between $50 and $1,000 to make online or in-app purchases and then repaying them through four installments spread over six weeks without any interest or fees.\n\nProducts\n\nMac",
        "Products\n\nMac \n\nThe Mac is Apple's family of personal computers. Macs are known for their ease of use and distinctive aluminium, minimalist designs. Macs have been popular among students, creative professionals, and software engineers. The current lineup consists of the MacBook Air and MacBook Pro laptops, and the iMac, Mac mini, Mac Studio and Mac Pro desktop computers.\n\nOften described as a walled garden, Macs use Apple silicon chips, run the macOS operating system, and include Apple software like the Safari web browser, iMovie for home movie editing, GarageBand for music creation, and the iWork productivity suite. Apple also sells pro apps: Final Cut Pro for video production, Logic Pro for musicians and producers, and Xcode for software developers.",
        "Apple also sells a variety of accessories for Macs, including the Pro Display XDR, Apple Studio Display, Magic Mouse, Magic Trackpad, and Magic Keyboard.\n\niPhone \n\nThe iPhone is Apple's line of smartphones, which run the iOS operating system. The first iPhone was unveiled by Steve Jobs on January 9, 2007. Since then, new models have been released annually. When it was introduced, its multi-touch screen was described as \"revolutionary\" and a \"game-changer\" for the mobile phone industry. The device has been credited with creating the app economy.",
        ", the iPhone has 15% market share, yet represents 50% of global smartphone revenues, with Android phones accounting for the rest. The iPhone has generated large profits for the company, and is credited with helping to make Apple one of the world's most valuable publicly traded companies.\n\nThe most recent iPhones are the iPhone 15, iPhone 15 Plus, iPhone 15 Pro and iPhone 15 Pro Max.\n\niPad",
        "The most recent iPhones are the iPhone 15, iPhone 15 Plus, iPhone 15 Pro and iPhone 15 Pro Max.\n\niPad \n\nThe iPad is Apple's line of tablets which run iPadOS. The first-generation iPad was announced on January 27, 2010. The iPad is mainly marketed for consuming multimedia, creating art, working on documents, videoconferencing, and playing games. The iPad lineup consists of several base iPad models, and the smaller iPad Mini, upgraded iPad Air, and high-end iPad Pro. Apple has consistently improved the iPad's performance, with the iPad Pro adopting the same M1 and M2 chips as the Mac; but the iPad still receives criticism for its limited OS.",
        "Apple has sold more than 500 million iPads, though sales peaked in 2013. The iPad still remains the most popular tablet computer by sales , and accounted for nine percent of the company's revenue .\n\nApple sells several iPad accessories, including the Apple Pencil, Smart Keyboard, Smart Keyboard Folio, Magic Keyboard, and several adapters.\n\nOther products \n\nApple also makes several other products that it categorizes as \"Wearables, Home and Accessories\". These products include the AirPods line of wireless headphones, Apple TV digital media players, Apple Watch smartwatches, Beats headphones and HomePod Mini smart speakers.\n\n, this broad line of products comprises about 11% of the company's revenues.",
        ", this broad line of products comprises about 11% of the company's revenues.\n\nAt WWDC 2023, Apple introduced its new VR headset, Vision Pro, along with visionOS. Apple announced that it will be partnering with Unity to bring existing 3D apps to Vision Pro using Unity's PolySpatial technology.\n\nServices \nApple also offers a broad line of services that it earns revenue on, including advertising in the App Store and Apple News app, the AppleCare+ extended warranty plan, the iCloud+ cloud-based data storage service, payment services through the Apple Card credit card and the Apple Pay processing platform, a digital content services including Apple Books, Apple Fitness+, Apple Music, Apple News+, Apple TV+, and the iTunes Store.",
        ", services comprise about 19% of the company's revenue. Many of the services have been launched  when Apple announced it would be making a concerted effort to expand its service revenues.\n\nMarketing\n\nBranding \n\nAccording to Steve Jobs, the company's name was inspired by his visit to an apple farm while he was on a fruitarian diet. Jobs thought the name \"Apple\" was \"fun, spirited and not intimidating.\" Steve Jobs and Steve Wozniak were fans of the Beatles, but Apple Inc. had name and logo trademark issues with Apple Corps Ltd., a multimedia company started by the Beatles in 1968. This resulted in a series of lawsuits and tension between the two companies. These issues ended with the settling of their lawsuit in 2007.",
        "Apple's first logo, designed by Ron Wayne, depicts Sir Isaac Newton sitting under an apple tree. It was almost immediately replaced by Rob Janoff's \"rainbow Apple\", the now-familiar rainbow-colored silhouette of an apple with a bite taken out of it. On August 27, 1999, Apple officially dropped the rainbow scheme and began to use monochromatic logos nearly identical in shape to the previous rainbow incarnation.\n\nApple evangelists were actively engaged by the company at one time, but this was after the phenomenon had already been firmly established. Apple evangelist Guy Kawasaki has called the brand fanaticism \"something that was stumbled upon,\" while Ive claimed in 2014 that \"people have an incredibly personal relationship\" with Apple's products.",
        "Fortune magazine named Apple the most admired company in the United States in 2008, and in the world from 2008 to 2012. On September 30, 2013, Apple surpassed Coca-Cola to become the world's most valuable brand in the Omnicom Group's \"Best Global Brands\" report. Boston Consulting Group has ranked Apple as the world's most innovative brand every year .\n\n there were 1.65 billion Apple products in active use. In February 2023 that number exceeded 2 billion devices.\n\nAdvertising",
        "there were 1.65 billion Apple products in active use. In February 2023 that number exceeded 2 billion devices.\n\nAdvertising \n\nApple's first slogan, \"Byte into an Apple\", was coined in the late 1970s. From 1997 to 2002, the slogan \"Think different\" was used in advertising campaigns, and is still closely associated with Apple. Apple also has slogans for specific product lines—for example, \"iThink, therefore iMac\" was used in 1998 to promote the iMac, and \"Say hello to iPhone\" has been used in iPhone advertisements. \"Hello\" was also used to introduce the original Macintosh, Newton, iMac (\"hello (again)\"), and iPod.",
        "From the introduction of the Macintosh in 1984, with the 1984 Super Bowl advertisement to the more modern Get a Mac adverts, Apple has been recognized for its efforts towards effective advertising and marketing for its products. However, claims made by later campaigns were criticized, particularly the 2005 Power Mac ads. Apple's product advertisements gained significant attention as a result of their eye-popping graphics and catchy tunes. Musicians who benefited from an improved profile as a result of their songs being included on Apple advertisements include Canadian singer Feist with the song \"1234\" and Yael Naïm with the song \"New Soul\".\n\nStores",
        "Stores \n\nThe first Apple Stores were originally opened as two locations in May 2001 by then-CEO Steve Jobs, after years of attempting but failing store-within-a-store concepts. Seeing a need for improved retail presentation of the company's products, he began an effort in 1997 to revamp the retail program to get an improved relationship to consumers, and hired Ron Johnson in 2000. Jobs relaunched Apple's online store in 1997, and opened the first two physical stores in 2001. The media initially speculated that Apple would fail, but its stores were highly successful, bypassing the sales numbers of competing nearby stores and within three years reached US$1 billion in annual sales, becoming the fastest retailer in history to do so.",
        "Over the years, Apple has expanded the number of retail locations and its geographical coverage, with 499 stores across 22 countries worldwide . Strong product sales have placed Apple among the top-tier retail stores, with sales over $16 billion globally in 2011. Apple Stores underwent a period of significant redesign, beginning in May 2016. This redesign included physical changes to the Apple Stores, such as open spaces and re-branded rooms, as well as changes in function to facilitate interaction between consumers and professionals.",
        "Many Apple Stores are located inside shopping malls, but Apple has built several stand-alone \"flagship\" stores in high-profile locations. It has been granted design patents and received architectural awards for its stores' designs and construction, specifically for its use of glass staircases and cubes. The success of Apple Stores have had significant influence over other consumer electronics retailers, who have lost traffic, control and profits due to a perceived higher quality of service and products at Apple Stores. Due to the popularity of the brand, Apple receives a large number of job applications, many of which come from young workers. Although Apple Store employees receive above-average pay, are offered money toward education and health care, and receive product discounts, there",
        "applications, many of which come from young workers. Although Apple Store employees receive above-average pay, are offered money toward education and health care, and receive product discounts, there are limited or no paths of career advancement.",
        "Market power \nOn March 16, 2020, France fined Apple €1.1 billion for colluding with two wholesalers to stifle competition and keep prices high by handicapping independent resellers. The arrangement created aligned prices for Apple products such as iPads and personal computers for about half the French retail market. According to the French regulators, the abuses occurred between 2005 and 2017 but were first discovered after a complaint by an independent reseller, eBizcuss, in 2012.",
        "On August 13, 2020, Epic Games, the maker of the popular game Fortnite, sued Apple and Google after its hugely popular video game was removed from Apple and Google's App Store. The suits come after both Apple and Google blocked the game after it introduced a direct payment system, effectively shutting out the tech titans from collecting fees. In September 2020 Epic Games founded the Coalition for App Fairness together with other thirteen companies, which aims for better conditions for the inclusion of apps in the app stores. Later in December 2020, Facebook agreed to assist Epic in their legal game against Apple, planning to support the company by providing materials and documents to Epic. Facebook had, however, stated that the company will not participate directly with the lawsuit,",
        "legal game against Apple, planning to support the company by providing materials and documents to Epic. Facebook had, however, stated that the company will not participate directly with the lawsuit, although did commit to helping with the discovery of evidence relating to the trial of 2021. In the months prior to their agreement, Facebook had been dealing with feuds against Apple relating to the prices of paid apps as well as privacy rule changes. Head of ad products for Facebook Dan Levy commented, saying that \"this is not really about privacy for them, this is about an attack on personalized ads and the consequences it's going to have on small-business owners,\" commenting on the full-page ads placed by Facebook in various newspapers in December 2020.",
        "Customer privacy",
        "Apple has a notable pro-privacy stance, actively making privacy-conscious features and settings part of its conferences, promotional campaigns, and public image. With its iOS 8 mobile operating system in 2014, the company started encrypting all contents of iOS devices through users' passcodes, making it impossible at the time for the company to provide customer data to law enforcement requests seeking such information. With the popularity rise of cloud storage solutions, Apple began a technique in 2016 to do deep learning scans for facial data in photos on the user's local device and encrypting the content before uploading it to Apple's iCloud storage system. It also introduced \"differential privacy\", a way to collect crowdsourced data from many users, while keeping individual users",
        "the content before uploading it to Apple's iCloud storage system. It also introduced \"differential privacy\", a way to collect crowdsourced data from many users, while keeping individual users anonymous, in a system that Wired described as \"trying to learn as much as possible about a group while learning as little as possible about any individual in it\". Users are explicitly asked if they want to participate, and can actively opt-in or opt-out.",
        "With Apple's release of an update to iOS 14, Apple required all developers of iPhone, iPad, and iPod Touch applications to directly ask iPhone users permission to track them. The feature, titled \"App Tracking Transparency\", received heavy criticism from Facebook, whose primary business model revolves around the tracking of users' data and sharing such data with advertisers so users can see more relevant ads, a technique commonly known as targeted advertising. Despite Facebook's measures, including purchasing full-page newspaper advertisements protesting App Tracking Transparency, Apple released the update in mid-spring 2021. A study by Verizon subsidiary Flurry Analytics reported only 4% of iOS users in the United States and 12% worldwide have opted into tracking.",
        "However, Apple aids law enforcement in criminal investigations by providing iCloud backups of users' devices, and the company's commitment to privacy has been questioned by its efforts to promote biometric authentication technology in its newer iPhone models, which do not have the same level of constitutional privacy as a passcode in the United States.",
        "Prior to the release of iOS 15, Apple announced new efforts at combating child sexual abuse material on iOS and Mac platforms. Parents of minor iMessage users can now be alerted if their child sends or receives nude photographs. Additionally, on-device hashing would take place on media destined for upload to iCloud, and hashes would be compared to a list of known abusive images provided by law enforcement; if enough matches were found, Apple would be alerted and authorities informed. The new features received praise from law enforcement and victims rights advocates, however privacy advocates, including the Electronic Frontier Foundation, condemned the new features as invasive and highly prone to abuse by authoritarian governments.",
        "Ireland's Data Protection Commission launched a privacy investigation to examine whether Apple complied with the EU's GDPR law following an investigation into how the company processes personal data with targeted ads on its platform.\n\nIn December 2019, a report found that the iPhone 11 Pro continues tracking location and collecting user data even after users have disabled location services. In response, an Apple engineer said the Location Services icon \"appears for system services that do not have a switch in settings.\"",
        "According to published reports by Bloomberg News on March 30, 2022, Apple turned over data such as phone numbers, physical addresses, and IP addresses to hackers posing as law enforcement officials using forged documents. The law enforcement requests sometimes included forged signatures of real or fictional officials. When asked about the allegations, an Apple representative referred the reporter to a section of the company policy for law enforcement guidelines, which stated, \"We review every data request for legal sufficiency and use advanced systems and processes to validate law enforcement requests and detect abuse.\"\n\nCorporate affairs\n\nLeadership",
        "Leadership\n\nSenior management \nAs of March 16, 2021, the management of Apple Inc. includes:\n Tim Cook (chief executive officer)\n Jeff Williams (chief operating officer)\n Luca Maestri (senior vice president and chief financial officer)\n Katherine L. Adams (senior vice president and general counsel)\n Eddy Cue (senior vice president – Internet Software and Services)\n Craig Federighi (senior vice president – Software Engineering)\n John Giannandrea (senior vice president – Machine Learning and AI Strategy)\n Deirdre O'Brien (senior vice president – Retail + People)\n John Ternus (senior vice president – Hardware Engineering)\n Greg Josiwak (senior vice president – Worldwide Marketing)\n Johny Srouji (senior vice president – Hardware Technologies)\n Sabih Khan (senior vice president – Operations)",
        "Board of directors \nAs of January 20, 2023, the board of directors of Apple Inc. includes:\n Arthur D. Levinson (chairman)\n Tim Cook (executive director and CEO)\n James A. Bell\n Al Gore\n Alex Gorsky\n Andrea Jung\n Monica Lozano\n Ronald Sugar\n Susan Wagner\n\nPrevious CEOs \n Michael Scott (1977–1981)\n Mike Markkula (1981–1983)\n John Sculley (1983–1993)\n Michael Spindler (1993–1996)\n Gil Amelio (1996–1997)\n Steve Jobs (1997–2011)\n\nCorporate culture",
        "Corporate culture \n\nApple is one of several highly successful companies founded in the 1970s that bucked the traditional notions of corporate culture. Jobs often walked around the office barefoot even after Apple became a Fortune 500 company. By the time of the \"1984\" television advertisement, Apple's informal culture had become a key trait that differentiated it from its competitors. According to a 2011 report in Fortune, this has resulted in a corporate culture more akin to a startup rather than a multinational corporation. In a 2017 interview, Wozniak credited watching Star Trek and attending Star Trek conventions while in his youth as a source of inspiration for his co-founding Apple.",
        "As the company has grown and been led by a series of differently opinionated chief executives, it has arguably lost some of its original character. Nonetheless, it has maintained a reputation for fostering individuality and excellence that reliably attracts talented workers, particularly after Jobs returned to the company. Numerous Apple employees have stated that projects without Jobs's involvement often took longer than projects with it.",
        "To recognize the best of its employees, Apple created the Apple Fellows program which awards individuals who make extraordinary technical or leadership contributions to personal computing while at the company. The Apple Fellowship has so far been awarded to individuals including Bill Atkinson, Steve Capps, Rod Holt, Alan Kay, Guy Kawasaki, Al Alcorn, Don Norman, Rich Page, Steve Wozniak, and Phil Schiller.",
        "At Apple, employees are intended to be specialists who are not exposed to functions outside their area of expertise. Jobs saw this as a means of having \"best-in-class\" employees in every role. For instance, Ron Johnson—Senior Vice President of Retail Operations until November 1, 2011—was responsible for site selection, in-store service, and store layout, yet had no control of the inventory in his stores. This was done by Tim Cook, who had a background in supply-chain management. Apple is known for strictly enforcing accountability. Each project has a \"directly responsible individual\" or \"DRI\" in Apple jargon. As an example, when iOS senior vice president Scott Forstall refused to sign Apple's official apology for numerous errors in the redesigned Maps app, he was forced to resign. Unlike",
        "in Apple jargon. As an example, when iOS senior vice president Scott Forstall refused to sign Apple's official apology for numerous errors in the redesigned Maps app, he was forced to resign. Unlike other major U.S. companies, Apple provides a relatively simple compensation policy for executives that does not include perks enjoyed by other CEOs like country club fees or private use of company aircraft. The company typically grants stock options to executives every other year.",
        "In 2015, Apple had 110,000 full-time employees. This increased to 116,000 full-time employees the next year, a notable hiring decrease, largely due to its first revenue decline. Apple does not specify how many of its employees work in retail, though its 2014 SEC filing put the number at approximately half of its employee base. In September 2017, Apple announced that it had over 123,000 full-time employees.\n\nApple has a strong culture of corporate secrecy, and has an anti-leak Global Security team that recruits from the National Security Agency, the Federal Bureau of Investigation, and the United States Secret Service.",
        "In December 2017, Glassdoor said Apple was the 48th best place to work, having originally entered at rank 19 in 2009, peaking at rank 10 in 2012, and falling down the ranks in subsequent years.\n\nIn 2023, Bloomberg Mark Gurman revealed the existence of Apple's Exploratory Design Group (XDG), which was working to add glucose monitoring to the Apple Watch. Gurman compared XDG to Alphabet's X \"moonshot factory\".\n\nOffices",
        "Offices \n\nApple Inc.'s world corporate headquarters are located in Cupertino, in the middle of California's Silicon Valley, at Apple Park, a massive circular groundscraper building with a circumference of . The building opened in April 2017 and houses more than 12,000 employees. Apple co-founder Steve Jobs wanted Apple Park to look less like a business park and more like a nature refuge, and personally appeared before the Cupertino City Council in June 2011 to make the proposal, in his final public appearance before his death.",
        "Apple also operates from the Apple Campus (also known by its address, 1 Infinite Loop), a grouping of six buildings in Cupertino that total  located about  to the west of Apple Park. The Apple Campus was the company's headquarters from its opening in 1993, until the opening of Apple Park in 2017. The buildings, located at 1–6 Infinite Loop, are arranged in a circular pattern around a central green space, in a design that has been compared to that of a university.",
        "In addition to Apple Park and the Apple Campus, Apple occupies an additional thirty office buildings scattered throughout the city of Cupertino, including three buildings that also served as prior headquarters: \"Stephens Creek Three\" (1977–1978), Bandley One\" (1978–1982), and \"Mariani One\" (1982–1993). In total, Apple occupies almost 40% of the available office space in the city.\n\nApple's headquarters for Europe, the Middle East and Africa (EMEA) are located in Cork in the south of Ireland, called the Hollyhill campus. The facility, which opened in 1980, houses 5,500 people and was Apple's first location outside of the United States. Apple's international sales and distribution arms operate out of the campus in Cork.",
        "Apple has two campuses near Austin, Texas: a  campus opened in 2014 houses 500 engineers who work on Apple silicon and a  campus opened in 2021 where 6,000 people work in technical support, supply chain management, online store curation, and Apple Maps data management.\n\nThe company also has several other locations in Boulder, Colorado, Culver City, California, Herzliya (Israel), London, New York, Pittsburgh, San Diego, and Seattle that each employ hundreds of people.\n\nLitigation",
        "Apple has been a participant in various legal proceedings and claims since it began operation. In particular, Apple is known for and promotes itself as actively and aggressively enforcing its intellectual property interests. Some litigation examples include Apple v. Samsung, Apple v. Microsoft, Motorola Mobility v. Apple Inc., and Apple Corps v. Apple Computer. Apple has also had to defend itself against charges on numerous occasions of violating intellectual property rights. Most have been dismissed in the courts as shell companies known as patent trolls, with no evidence of actual use of patents in question. On December 21, 2016, Nokia announced that in the U.S. and Germany, it has filed a suit against Apple, claiming that the latter's products infringe on Nokia's patents. Most",
        "of patents in question. On December 21, 2016, Nokia announced that in the U.S. and Germany, it has filed a suit against Apple, claiming that the latter's products infringe on Nokia's patents. Most recently, in November 2017, the United States International Trade Commission announced an investigation into allegations of patent infringement in regards to Apple's remote desktop technology; Aqua Connect, a company that builds remote desktop software, has claimed that Apple infringed on two of its patents. In January 2022, Ericsson sued Apple over payment of royalty of 5G technology.",
        "Finances \n\nApple is the world's largest technology company by revenue, the world's largest technology company by total assets, and the world's second-largest mobile phone manufacturer after Samsung.\n\nIn its fiscal year ending in September 2011, Apple Inc. reported a total of $108 billion in annual revenues—a significant increase from its 2010 revenues of $65 billion—and nearly $82 billion in cash reserves. On March 19, 2012, Apple announced plans for a $2.65-per-share dividend beginning in fourth quarter of 2012, per approval by their board of directors.",
        "The company's worldwide annual revenue in 2013 totaled $170 billion. In May 2013, Apple entered the top ten of the Fortune 500 list of companies for the first time, rising 11 places above its 2012 ranking to take the sixth position. , Apple has around US$234 billion of cash and marketable securities, of which 90% is located outside the United States for tax purposes.\n\nApple amassed 65% of all profits made by the eight largest worldwide smartphone manufacturers in quarter one of 2014, according to a report by Canaccord Genuity. In the first quarter of 2015, the company garnered 92% of all earnings.\n\nOn April 30, 2017, The Wall Street Journal reported that Apple had cash reserves of $250 billion, officially confirmed by Apple as specifically $256.8 billion a few days later.",
        "On April 30, 2017, The Wall Street Journal reported that Apple had cash reserves of $250 billion, officially confirmed by Apple as specifically $256.8 billion a few days later.\n\n, Apple was the largest publicly traded corporation in the world by market capitalization. On August 2, 2018, Apple became the first publicly traded U.S. company to reach a $1 trillion market value. Apple was ranked No. 4 on the 2018 Fortune 500 rankings of the largest United States corporations by total revenue.",
        "In July 2022, Apple reported an 11% decline in Q3 profits compared to 2021. Its revenue in the same period rose 2% year-on-year to $83 billion, though this figure was also lower than in 2021, where the increase was at 36%. The general downturn is reportedly caused by the slowing global economy and supply chain disruptions in China.",
        "In May 2023, Apple reported a decline in its sales for the first quarter of 2023. Compared to that of 2022, revenue for 2023 fell by 3%. This is Apple's second consecutive quarter of sales decline. This fall is attributed to the slowing economy and consumers putting off purchases of iPads and computers due to increased pricing. However, iPhone sales held up with a year-on-year increase of 1.5%. According to Apple, demands for such devices were strong, particularly in Latin America and South Asia.",
        "Taxes \nApple has created subsidiaries in low-tax places such as Ireland, the Netherlands, Luxembourg, and the British Virgin Islands to cut the taxes it pays around the world. According to The New York Times, in the 1980s Apple was among the first tech companies to designate overseas salespeople in high-tax countries in a manner that allowed the company to sell on behalf of low-tax subsidiaries on other continents, sidestepping income taxes. In the late 1980s, Apple was a pioneer of an accounting technique known as the \"Double Irish with a Dutch sandwich\", which reduces taxes by routing profits through Irish subsidiaries and the Netherlands and then to the Caribbean.",
        "British Conservative Party Member of Parliament Charlie Elphicke published research on October 30, 2012, which showed that some multinational companies, including Apple Inc., were making billions of pounds of profit in the UK, but were paying an effective tax rate to the UK Treasury of only 3 percent, well below standard corporate tax rates. He followed this research by calling on the Chancellor of the Exchequer George Osborne to force these multinationals, which also included Google and The Coca-Cola Company, to state the effective rate of tax they pay on their UK revenues. Elphicke also said that government contracts should be withheld from multinationals who do not pay their fair share of UK tax.",
        "According to a US Senate report on the company's offshore tax structure concluded in May 2013, Apple has held billions of dollars in profits in Irish subsidiaries to pay little or no taxes to any government by using an unusual global tax structure. The main subsidiary, a holding company that includes Apple's retail stores throughout Europe, has not paid any corporate income tax in the last five years. \"Apple has exploited a difference between Irish and U.S. tax residency rules\", the report said.\n\nOn May 21, 2013, Apple CEO Tim Cook defended his company's tax tactics at a Senate hearing.",
        "On May 21, 2013, Apple CEO Tim Cook defended his company's tax tactics at a Senate hearing.\n\nApple says that it is the single largest taxpayer in the U.S., with an effective tax rate of approximately of 26% as of Q2 FY2016. In an interview with the German newspaper FAZ in October 2017, Tim Cook stated that Apple was the biggest taxpayer worldwide.",
        "In 2016, after a two-year investigation, the European Commission claimed that Apple's use of a hybrid Double Irish tax arrangement constituted \"illegal state aid\" from Ireland, and ordered Apple to pay 13 billion euros ($14.5 billion) in unpaid taxes, the largest corporate tax fine in history. This was later annulled, after the European General Court ruled that the Commission had provided insufficient evidence. In 2018, Apple repatriated $285 billion to America, resulting in a $38 billion tax payment spread over the following 8 years.",
        "Charity \nApple is a partner of (PRODUCT)RED, a fundraising campaign for AIDS charity. In November 2014, Apple arranged for all App Store revenue in a two-week period to go to the fundraiser, generating more than US$20 million, and in March 2017, it released an iPhone 7 with a red color finish.",
        "Apple contributes financially to fundraisers in times of natural disasters. In November 2012, it donated $2.5 million to the American Red Cross to aid relief efforts after Hurricane Sandy, and in 2017 it donated $5 million to relief efforts for both Hurricane Irma and Hurricane Harvey, as well as for the 2017 Central Mexico earthquake. The company has also used its iTunes platform to encourage donations in the wake of environmental disasters and humanitarian crises, such as the 2010 Haiti earthquake, the 2011 Japan earthquake, Typhoon Haiyan in the Philippines in November 2013, and the 2015 European migrant crisis. Apple emphasizes that it does not incur any processing or other fees for iTunes donations, sending 100% of the payments directly to relief efforts, though it also acknowledges",
        "migrant crisis. Apple emphasizes that it does not incur any processing or other fees for iTunes donations, sending 100% of the payments directly to relief efforts, though it also acknowledges that the Red Cross does not receive any personal information on the users donating and that the payments may not be tax deductible.",
        "On April 14, 2016, Apple and the World Wide Fund for Nature (WWF) announced that they have engaged in a partnership to, \"help protect life on our planet.\" Apple released a special page in the iTunes App Store, Apps for Earth. In the arrangement, Apple has committed that through April 24, WWF will receive 100% of the proceeds from the applications participating in the App Store via both the purchases of any paid apps and the In-App Purchases. Apple and WWF's Apps for Earth campaign raised more than $8 million in total proceeds to support WWF's conservation work. WWF announced the results at WWDC 2016 in San Francisco.\n\nDuring the COVID-19 pandemic, Apple's CEO Cook announced that the company will be donating \"millions\" of masks to health workers in the United States and Europe.",
        "During the COVID-19 pandemic, Apple's CEO Cook announced that the company will be donating \"millions\" of masks to health workers in the United States and Europe.\n\nOn January 13, 2021, Apple announced a $100 million \"Racial Equity and Justice Initiative\" to help combat institutional racism worldwide.\n\nEnvironment",
        "On January 13, 2021, Apple announced a $100 million \"Racial Equity and Justice Initiative\" to help combat institutional racism worldwide.\n\nEnvironment\n\nApple Energy \nApple Energy, LLC is a wholly-owned subsidiary of Apple Inc. that sells solar energy. , Apple's solar farms in California and Nevada have been declared to provide 217.9 megawatts of solar generation capacity. In addition to the company's solar energy production, Apple has received regulatory approval to construct a landfill gas energy plant in North Carolina. Apple will use the methane emissions to generate electricity. Apple's North Carolina data center is already powered entirely with energy from renewable sources.",
        "Energy and resources \nIn 2010, Climate Counts, a nonprofit organization dedicated to directing consumers toward the greenest companies, gave Apple a score of 52 points out of a possible 100, which puts Apple in their top category \"Striding\". This was an increase from May 2008, when Climate Counts only gave Apple 11 points out of 100, which placed the company last among electronics companies, at which time Climate Counts also labeled Apple with a \"stuck icon\", adding that Apple at the time was \"a choice to avoid for the climate-conscious consumer\".",
        "Following a Greenpeace protest, Apple released a statement on April 17, 2012, committing to ending its use of coal and shifting to 100% renewable clean energy. By 2013, Apple was using 100% renewable energy to power their data centers. Overall, 75% of the company's power came from clean renewable sources.\n\nIn May 2015, Greenpeace evaluated the state of the Green Internet and commended Apple on their environmental practices saying, \"Apple's commitment to renewable energy has helped set a new bar for the industry, illustrating in very concrete terms that a 100% renewable Internet is within its reach, and providing several models of intervention for other companies that want to build a sustainable Internet.\"",
        ", Apple states that 100% of its U.S. operations run on renewable energy, 100% of Apple's data centers run on renewable energy and 93% of Apple's global operations run on renewable energy. However, the facilities are connected to the local grid which usually contains a mix of fossil and renewable sources, so Apple carbon offsets its electricity use. The Electronic Product Environmental Assessment Tool (EPEAT) allows consumers to see the effect a product has on the environment. Each product receives a Gold, Silver, or Bronze rank depending on its efficiency and sustainability. Every Apple tablet, notebook, desktop computer, and display that EPEAT ranks achieves a Gold rating, the highest possible. Although Apple's data centers recycle water 35 times, the increased activity in retail,",
        "tablet, notebook, desktop computer, and display that EPEAT ranks achieves a Gold rating, the highest possible. Although Apple's data centers recycle water 35 times, the increased activity in retail, corporate and data centers also increase the amount of water use to  in 2015.",
        "During an event on March 21, 2016, Apple provided a status update on its environmental initiative to be 100% renewable in all of its worldwide operations. Lisa P. Jackson, Apple's vice president of Environment, Policy and Social Initiatives who reports directly to CEO, Tim Cook, announced that , 93% of Apple's worldwide operations are powered with renewable energy. Also featured was the company's efforts to use sustainable paper in their product packaging; 99% of all paper used by Apple in the product packaging comes from post-consumer recycled paper or sustainably managed forests, as the company continues its move to all paper packaging for all of its products. Apple working in partnership with Conservation Fund, have preserved 36,000 acres of working forests in Maine and North Carolina.",
        "continues its move to all paper packaging for all of its products. Apple working in partnership with Conservation Fund, have preserved 36,000 acres of working forests in Maine and North Carolina. Another partnership announced is with the World Wildlife Fund to preserve up to  of forests in China. Featured was the company's installation of a 40 MW solar power plant in the Sichuan province of China that was tailor-made to coexist with the indigenous yaks that eat hay produced on the land, by raising the panels to be several feet off of the ground so the yaks and their feed would be unharmed grazing beneath the array. This installation alone compensates for more than all of the energy used in Apple's Stores and Offices in the whole of China, negating the company's energy carbon footprint in",
        "beneath the array. This installation alone compensates for more than all of the energy used in Apple's Stores and Offices in the whole of China, negating the company's energy carbon footprint in the country. In Singapore, Apple has worked with the Singaporean government to cover the rooftops of 800 buildings in the city-state with solar panels allowing Apple's Singapore operations to be run on 100% renewable energy. Liam was introduced to the world, an advanced robotic disassembler and sorter designed by Apple Engineers in California specifically for recycling outdated or broken iPhones. Reuses and recycles parts from traded in products.",
        "Apple announced on August 16, 2016, that Lens Technology, one of its major suppliers in China, has committed to power all its glass production for Apple with 100 percent renewable energy by 2018. The commitment is a large step in Apple's efforts to help manufacturers lower their carbon footprint in China. Apple also announced that all 14 of its final assembly sites in China are now compliant with UL's Zero Waste to Landfill validation. The standard, which started in January 2015, certifies that all manufacturing waste is reused, recycled, composted, or converted into energy (when necessary). Since the program began, nearly 140,000 metric tons of waste have been diverted from landfills.",
        "On July 21, 2020, Apple announced its plan to become carbon neutral across its entire business, manufacturing supply chain, and product life cycle by 2030. In the next 10 years, Apple will try to lower emissions with a series of innovative actions, including: low carbon product design, expanding energy efficiency, renewable energy, process and material innovations, and carbon removal.\n\nIn April 2021, Apple said that it had started a $200 million fund in order to combat climate change by removing 1 million metric tons of carbon dioxide from the atmosphere each year.",
        "In April 2021, Apple said that it had started a $200 million fund in order to combat climate change by removing 1 million metric tons of carbon dioxide from the atmosphere each year.\n\nIn February 2022, the NewClimate Institute, a German environmental policy think tank, published a survey evaluating the transparency and progress of the climate strategies and carbon neutrality pledges announced by 25 major companies in the United States that found that Apple's carbon neutrality pledge and climate strategy was unsubstantiated and misleading.",
        "Toxins",
        "Following further campaigns by Greenpeace, in 2008, Apple became the first electronics manufacturer to eliminate all polyvinyl chloride (PVC) and brominated flame retardants (BFRs) in its complete product line. In June 2007, Apple began replacing the cold cathode fluorescent lamp (CCFL) backlit LCD displays in its computers with mercury-free LED-backlit LCD displays and arsenic-free glass, starting with the upgraded MacBook Pro. Apple offers comprehensive and transparent information about the CO2e, emissions, materials, and electrical usage concerning every product they currently produce or have sold in the past (and which they have enough data needed to produce the report), in their portfolio on their homepage. Allowing consumers to make informed purchasing decisions on the products they",
        "sold in the past (and which they have enough data needed to produce the report), in their portfolio on their homepage. Allowing consumers to make informed purchasing decisions on the products they offer for sale. In June 2009, Apple's iPhone 3GS was free of PVC, arsenic, and BFRs. All Apple products now have mercury-free LED-backlit LCD displays, arsenic-free glass, and non-PVC cables. All Apple products have EPEAT Gold status and beat the latest Energy Star guidelines in each product's respective regulatory category.",
        "In November 2011, Apple was featured in Greenpeace's Guide to Greener Electronics, which ranks electronics manufacturers on sustainability, climate and energy policy, and how \"green\" their products are. The company ranked fourth of fifteen electronics companies (moving up five places from the previous year) with a score of 4.6/10. Greenpeace praised Apple's sustainability, noting that the company exceeded its 70% global recycling goal in 2010. Apple continues to score well on product ratings, with all of their products now being free of PVC plastic and BFRs. However, the guide criticized Apple on the Energy criteria for not seeking external verification of its greenhouse gas emissions data, and for not setting any targets to reduce emissions. In January 2012, Apple requested that its",
        "Apple on the Energy criteria for not seeking external verification of its greenhouse gas emissions data, and for not setting any targets to reduce emissions. In January 2012, Apple requested that its cable maker, Volex, begin producing halogen-free USB and power cables.",
        "Green bonds \nIn February 2016, Apple issued a US$1.5 billion green bond (climate bond), the first ever of its kind by a U.S. tech company. The green bond proceeds are dedicated to the financing of environmental projects.\n\nSupply chain \n\nApple products were made in America in Apple-owned factories until the late 1990s; however, as a result of outsourcing initiatives in the 2000s, almost all of its manufacturing is now handled abroad. According to a report by The New York Times, Apple insiders \"believe the vast scale of overseas factories, as well as the flexibility, diligence and industrial skills of foreign workers, have so outpaced their American counterparts that \"Made in the USA\" is no longer a viable option for most Apple products\".",
        "The company's manufacturing, procurement, and logistics enable it to execute massive product launches without having to maintain large, profit-sapping inventories. In 2011, Apple's profit margins were 40 percent, compared with between 10 and 20 percent for most other hardware companies. Cook's catchphrase to describe his focus on the company's operational arm is: \"Nobody wants to buy sour milk.\"",
        "In May 2017, the company announced a $1 billion funding project for \"advanced manufacturing\" in the United States, and subsequently invested $200 million in Corning Inc., a manufacturer of toughened Gorilla Glass technology used in its iPhone devices. The following December, Apple's chief operating officer, Jeff Williams, told CNBC that the \"$1 billion\" amount was \"absolutely not\" the final limit on its spending, elaborating that \"We're not thinking in terms of a fund limit... We're thinking about, where are the opportunities across the U.S. to help nurture companies that are making the advanced technology— and the advanced manufacturing that goes with that— that quite frankly is essential to our innovation.\"",
        ", Apple uses components from 43 countries. The majority of assembling is done by Taiwanese original design manufacturer firms Foxconn, Pegatron, Wistron and Compal Electronics with factories mostly located inside China, but also Brazil, and India.\n\nTaiwan Semiconductor Manufacturing Co., (TSMC) is a pure-play semiconductor manufacturing company. They make the majority of Apple's smartphone SoCs, with Samsung Semiconductor, playing a minority role. Apple, alone accounted for over 25% of TSMC's total income in 2021. Apple's Bionic lineup of smartphone SoCs, are currently made exclusively by TSMC from the A7 bionic onwards, previously manufacturing was shared with Samsung. The M series of Apple SoC for consumer computers and tablets is made by TSMC as well.",
        "During the Mac's early history Apple generally refused to adopt prevailing industry standards for hardware, instead creating their own. This trend was largely reversed in the late 1990s, beginning with Apple's adoption of the PCI bus in the 7500/8500/9500 Power Macs. Apple has since joined the industry standards groups to influence the future direction of technology standards such as USB, AGP, HyperTransport, Wi-Fi, NVMe, PCIe and others in its products. FireWire is an Apple-originated standard that was widely adopted across the industry after it was standardized as IEEE 1394 and is a legally mandated port in all Cable TV boxes in the United States.",
        "Apple has gradually expanded its efforts in getting its products into the Indian market. In July 2012, during a conference call with investors, CEO Tim Cook said that he \"[loves] India\", but that Apple saw larger opportunities outside the region. India's requirement that 30% of products sold be manufactured in the country was described as \"really adds cost to getting product to market\". In May 2016, Apple opened an iOS app development center in Bangalore and a maps development office for 4,000 staff in Hyderabad. In March, The Wall Street Journal reported that Apple would begin manufacturing iPhone models in India \"over the next two months\", and in May, the Journal wrote that an Apple manufacturer had begun production of iPhone SE in the country, while Apple told CNBC that the",
        "iPhone models in India \"over the next two months\", and in May, the Journal wrote that an Apple manufacturer had begun production of iPhone SE in the country, while Apple told CNBC that the manufacturing was for a \"small number\" of units. In April 2019, Apple initiated manufacturing of iPhone 7 at its Bengaluru facility, keeping in mind demand from local customers even as they seek more incentives from the government of India. At the beginning of 2020, Tim Cook announced that Apple schedules the opening of its first physical outlet in India for 2021, while an online store is to be launched by the end of the year.",
        "During the 2022 COVID-19 protests in China, Chinese state-owned company Wingtech was reported by The Wall Street Journal to gain an additional foothold in Apple's supply chain following protests at a Foxconn factory in the Zhengzhou Airport Economy Zone.\n\nWorker organizations \n\nIn 2006, one complex of factories in Shenzhen, China that assembled the iPod and other items had over 200,000 workers living and working within it. Employees regularly worked more than 60 hours per week and made around $100 per month. A little over half of the workers' earnings was required to pay for rent and food from the company.",
        "Apple immediately launched an investigation after the 2006 media report, and worked with their manufacturers to ensure acceptable working conditions. In 2007, Apple started yearly audits of all its suppliers regarding worker's rights, slowly raising standards and pruning suppliers that did not comply. Yearly progress reports have been published . In 2011, Apple admitted that its suppliers' child labor practices in China had worsened.",
        "The Foxconn suicides occurred between January and November 2010, when 18 Foxconn () employees attempted suicide, resulting in 14 deaths—the company was the world's largest contract electronics manufacturer, for clients including Apple, at the time. The suicides drew media attention, and employment practices at Foxconn were investigated by Apple. Apple issued a public statement about the suicides, and company spokesperson Steven Dowling said:",
        "The statement was released after the results from the company's probe into its suppliers' labor practices were published in early 2010. Foxconn was not specifically named in the report, but Apple identified a series of serious labor violations of labor laws, including Apple's own rules, and some child labor existed in a number of factories. Apple committed to the implementation of changes following the suicides.",
        "Also in 2010, workers in China planned to sue iPhone contractors over poisoning by a cleaner used to clean LCD screens. One worker claimed that he and his coworkers had not been informed of possible occupational illnesses. After a high suicide rate in a Foxconn facility in China making iPads and iPhones, albeit a lower rate than that of China as a whole, workers were forced to sign a legally binding document guaranteeing that they would not kill themselves. Workers in factories producing Apple products have also been exposed to hexane, a neurotoxin that is a cheaper alternative than alcohol for cleaning the products.",
        "A 2014 BBC investigation found excessive hours and other problems persisted, despite Apple's promise to reform factory practice after the 2010 Foxconn suicides. The Pegatron factory was once again the subject of review, as reporters gained access to the working conditions inside through recruitment as employees. While the BBC maintained that the experiences of its reporters showed that labor violations were continuing , Apple publicly disagreed with the BBC and stated: \"We are aware of no other company doing as much as Apple to ensure fair and safe working conditions\".",
        "In December 2014, the Institute for Global Labour and Human Rights published a report which documented inhumane conditions for the 15,000 workers at a Zhen Ding Technology factory in Shenzhen, China, which serves as a major supplier of circuit boards for Apple's iPhone and iPad. According to the report, workers are pressured into 65-hour work weeks which leaves them so exhausted that they often sleep during lunch breaks. They are also made to reside in \"primitive, dark and filthy dorms\" where they sleep \"on plywood, with six to ten workers in each crowded room.\" Omnipresent security personnel also routinely harass and beat the workers.\n\nIn 2019, there were reports stating that some of Foxconn's managers had used rejected parts to build iPhones and that Apple was investigating the issue.",
        "In 2019, there were reports stating that some of Foxconn's managers had used rejected parts to build iPhones and that Apple was investigating the issue.\n\nSee also \n List of Apple Inc. media events\n Pixar\n\nNotes\n\nReferences\n\nBibliography\n\nFurther reading\n\nExternal links",
        "1976 establishments in California\n1980s initial public offerings\nAmerican brands\nCompanies based in Cupertino, California\nCompanies in the Dow Jones Industrial Average\nCompanies in the PRISM network\nCompanies listed on the Nasdaq\nComputer companies established in 1976\nComputer companies of the United States\nDisplay technology companies\nElectronics companies of the United States\nHome computer hardware companies\nMobile phone manufacturers\nMultinational companies headquartered in the United States\nNetworking hardware companies\nPortable audio player manufacturers\nRetail companies of the United States\nSoftware companies based in the San Francisco Bay Area\nSoftware companies established in 1976\nSteve Jobs\nTechnology companies based in the San Francisco Bay Area",
        "Retail companies of the United States\nSoftware companies based in the San Francisco Bay Area\nSoftware companies established in 1976\nSteve Jobs\nTechnology companies based in the San Francisco Bay Area\nTechnology companies established in 1976\nTechnology companies of the United States"
    ],
    [
        "Applesoft BASIC\nApplesoft BASIC is a dialect of Microsoft BASIC, developed by Marc McDonald and Ric Weiland,  supplied with the Apple II series of computers. It supersedes Integer BASIC and is the BASIC in ROM in all Apple II series computers after the original Apple II model. It is also referred to as FP BASIC (from floating point) because of the Apple DOS command used to invoke it, instead of INT for Integer BASIC.",
        "Applesoft BASIC was supplied by Microsoft and its name is derived from the names of both Apple Computer and Microsoft. Apple employees, including Randy Wigginton, adapted Microsoft's interpreter for the Apple II and added several features. The first version of Applesoft was released in 1977 on cassette tape and lacked proper support for high-resolution graphics. Applesoft II, which was made available on cassette and disk and in the ROM of the Apple II Plus and subsequent models, was released in 1978. It is this latter version, which has some syntax differences and support for the Apple II high-resolution graphics modes, that is usually synonymous with the term \"Applesoft.\"\n\nA compiler for Applesoft BASIC, TASC (The Applesoft Compiler), was released by Microsoft in 1981.",
        "A compiler for Applesoft BASIC, TASC (The Applesoft Compiler), was released by Microsoft in 1981.\n\nHistory\nWhen Steve Wozniak wrote Integer BASIC for the Apple II, he did not implement support for floating-point arithmetic because he was primarily interested in writing games, a task for which integers alone were sufficient. In 1976, Microsoft had developed Microsoft BASIC for the MOS Technology 6502, but at the time there was no production computer that used it. Upon learning that Apple had a 6502 machine, Microsoft asked if the company were interested in licensing BASIC, but Steve Jobs replied that Apple already had one.",
        "The Apple II was unveiled to the public at the West Coast Computer Faire in April 1977 and became available for sale in June. One of the most common customer complaints about the computer was BASIC's lack of floating-point math. Making things more problematic was that the rival Commodore PET personal computer had a floating point-capable BASIC interpreter from the beginning. As Wozniak—the only person who understood Integer BASIC well enough to add floating point features—was busy with the Disk II drive and controller and with Apple DOS, Apple turned to Microsoft.",
        "Apple reportedly obtained an eight-year license for Applesoft BASIC from Microsoft for a flat fee of $31,000, renewing it in 1985 through an arrangement that gave Microsoft the rights and source code for Apple's Macintosh version of BASIC. Applesoft was designed to be backwards-compatible with Integer BASIC and uses the core of Microsoft's 6502 BASIC implementation, which includes using the GET command for detecting key presses and not requiring any spaces on program lines. While Applesoft BASIC is slower than Integer BASIC, it has many features that the older BASIC lacks:",
        "Atomic strings: A string is no longer an array of characters (as in Integer BASIC and C); it is instead a garbage-collected object (as in Scheme and Java). This allows for string arrays;  creates an array of eleven string variables numbered 0–10.\n Multidimensional arrays (numbers or strings)\n Single-precision floating-point variables with an 8-bit exponent and a 31-bit significand and improved math capabilities, including trigonometry and logarithmic functions\n Commands for high-resolution graphics\n DATA statements, with READ and RESTORE commands, for representing numerical and string values in quantity\n CHR$, STR$, and VAL functions for converting between string and numeric types (both languages did have the ASC function)",
        "CHR$, STR$, and VAL functions for converting between string and numeric types (both languages did have the ASC function)\n User-defined functions: simple one-line functions written in BASIC, with a single parameter\n Error-trapping: allowing BASIC programs to handle unexpected errors via subroutine written in BASIC",
        "Conversely, Applesoft lacks the MOD (remainder) operator from Integer BASIC.\n\nAdapting BASIC for the Apple II was a tedious job as Apple received a source listing for Microsoft 6502 BASIC which proved to be buggy and also required the addition of Integer BASIC commands. Since Apple had no 6502 assembler on hand, the development team was forced to send the source code over the phone lines to Call Computer, an outfit that offered compiler services. This was an extremely tedious, slow process and after Call Computer lost the source code due to an equipment malfunction, one of the programmers, Cliff Huston, used his own IMSAI 8080 computer to cross assemble the BASIC source.",
        "Features\nApplesoft is similar to Commodore's BASIC 2.0 aside from features inherited from Integer BASIC. There are a few minor differences such as Applesoft's lack of bitwise operators; otherwise most BASIC programs that do not use hardware-dependent features will run on both BASICs.\n\nThe  statement redirects output to an expansion card, and  redirects input from an expansion card. The slot number of the card is specified after the  or  within the statement. The computer locks-up if there is no card present in the slot.  restores output to the 40 column screen and  to the keyboard.\n\nThe  statement can be used to redirect output to the printer (e.g. ) where x is the slot number containing the printer port card. To send a BASIC program listing to the printer, the user types .",
        "The  statement can be used to redirect output to the printer (e.g. ) where x is the slot number containing the printer port card. To send a BASIC program listing to the printer, the user types .\n\nUsing  on a slot with a disk drive (usually in slot 6) causes Applesoft to boot the disk drive.  Using  on a slot with an 80 column card (usually in slot 1) switches to 80 column text mode.",
        "Using  on a slot with a disk drive (usually in slot 6) causes Applesoft to boot the disk drive.  Using  on a slot with an 80 column card (usually in slot 1) switches to 80 column text mode.\n\nAs with Commodore BASIC, numeric variables are stored as 40-bit floating point; each variable requires five bytes of memory. The programmer may designate variables as integer by following them with a percent sign, in which case they use two bytes and are limited to a range of -32768 to 32767; however BASIC internally converts them back to floating point when performing calculations, while each percent sign also takes an additional byte of program code, so in practice this feature is only useful for reducing the memory usage of large array variables, as it offers no performance benefit.",
        "The RND function generates a pseudorandom fractional number between 0 and 1.  returns the most recently generated random number.  with a negative number will jump to a point in the sequence determined by the particular negative number used. RND with any positive value generates the next number in the sequence, not dependent on the actual value given.\n\nLike other implementations of Microsoft BASIC, Applesoft discards spaces (outside of strings and comments) on program lines. LIST adds spaces when displaying code for the sake of readability. Since  adds a space before and after every tokenized keyword, it often produces two spaces in a row where one would suffice for readability.\n\nThe default prompt for INPUT is a question mark. PRINT does not add a leading space in front of numbers.",
        "The default prompt for INPUT is a question mark. PRINT does not add a leading space in front of numbers.\n\nLimitations\nThrough several early models of the Apple II, Applesoft BASIC did not support the use of lowercase letters in programs, except in strings. PRINT is a valid command but print and Print result in a syntax error.\n\nApplesoft lacks several commands and functions common to most of the non-6502 Microsoft BASIC interpreters, such as:\n INSTR (search for a substring in a string)\n PRINT USING (format numbers in printed output)\n INKEY$ (check for a keypress without stopping the program; although a PEEK to location $C000 achieves this action)\n LPRINT (output to a printer instead of the screen)",
        "Applesoft does not have commands for file or disk handling, other than to save and load programs via cassette tape. The Apple II disk operating system, known simply as DOS, augments the language to provide such abilities.\n\nOnly the first two letters of variables names are significant. For example, \"LOW\" and \"LOSS\" are treated as the same variable, and attempting to assign a value to \"LOSS\" overwrites any value assigned to \"LOW\". A programmer also has to avoid consecutive letters that are Applesoft commands or operations. The name \"SCORE\" for a variable is interpreted as containing the OR Boolean operator, rendered as SC OR E. \"BACKGROUND\" contains GR, the command to invoke the low-resolution graphics mode, and results in a syntax error.",
        "Sound and graphics\nThe only built-in sound support is the option to PRINT an ASCII bell character to sound the system alert beep.\n\nApplesoft supports drawing in the Apple II's low resolution and high resolution modes. There are commands to plot pixels and draw horizontal and vertical lines in low resolution. High resolution allows arbitrary lines and vector-based shape tables for drawing scaled and rotated objects. The only provision for mixing text and graphics is the four lines of text at the bottom of a graphic display.",
        "Beginning with the Apple IIe, a \"double-high resolution\" mode became available on machines with 128k of memory. This mode essentially duplicates the resolution of the original high resolution mode, but including all 16 colors of the low resolution palette. Applesoft does not provide direct support for this mode. Apple IIGS-specific modes are likewise not supported.",
        "Extensions\nApplesoft BASIC can be extended by two means: the ampersand () command and the  function. These are two features that call low-level machine-language routines stored in memory, which is useful for routines that need to be fast or require direct access to arbitrary functions or data in memory. The  function takes one argument, and can be programmed to derive and return a calculated function value to be used in a numerical expression.  is effectively a shorthand for , with an address that is predefined. By calling routines in the Applesoft ROM, it is possible for ampersand routines to parse values that follow the ampersand. Numerous third-party commercial packages were available to extend Applesoft using ampersand routines.",
        "Bugs\nA deficiency with error-trapping via ONERR means that the system stack is not reset if an error-handling routine does not invoke RESUME, potentially leading to a crash. The built-in pseudorandom number generator function RND is capable of producing a predictable series of outputs due to the manner in which the generator is seeded when first powering on. This behavior is contrary to how Apple's documentation describes the function.\n\nPerformance\nWozniak originally referred to his Integer BASIC as \"Game BASIC\" (having written it so he could implement a Breakout clone for his new computer). Few action games were written in Applesoft BASIC, in large part because the use of floating-point numbers for all math operations degrades performance.",
        "Applesoft BASIC programs are stored as a linked list of lines; a GOTO or GOSUB takes linear time. Some programs have the subroutines at the top to reduce the time for calling them.\n\nUnlike Integer BASIC, Applesoft does not convert literal numbers (like 100) in the source code to binary when a line is entered. Rather, the ASCII string is converted whenever the line is executed. Since variable lookup is often faster than this conversion, it can be faster to store numeric constants used inside loops in variables before the loop is entered.\n\nSample code",
        "Hello World in Applesoft BASIC can be entered as the following:\n10TEXT:HOME\n20?\"HELLO WORLD\"\nMultiple commands can be included on the same line of code if separated by a colon (:). The ? can be used in Applesoft BASIC (and almost all versions of Microsoft BASIC) as a shortcut for \"PRINT\", though spelling out the word is not only acceptable but canonical—Applesoft converted \"?\" in entered programs to the same token as \"PRINT\" (thus no memory is actually saved by using \"?\"), thus either appears as \"PRINT\" when a program is listed. The program above appears in a LIST command as:\n10  TEXT : HOME\n20  PRINT \"HELLO WORLD\"",
        "10  TEXT : HOME\n20  PRINT \"HELLO WORLD\"\nWhen Applesoft II BASIC was initially released in mid-1978, it came on cassette tape and could be loaded into memory via the Apple II's machine language monitor. When the enhanced Apple II+ replaced the original II in 1979, Applesoft was now included in ROM and automatically started on power-up if no bootable floppy disk was present. Conversely, Integer BASIC was now removed from ROM and turned into an executable file on the DOS 3.3 disk.",
        "Early evolution\nThe original Applesoft, stored in RAM as documented in its Reference Manual of November 1977, has smaller interpreter code than the later Applesoft II, occupying 8½ KB of memory, instead of the 10 KB used by the later Applesoft II. Consequently, it lacks a number of command features developed for the later, mainstream version:\n All commands supporting Apple's \"high resolution\" graphics (9 total)\n Error-trapping with ONERR...GOTO and RESUME\n Machine-routine shorthand call \"&\"\n Screen-clearing HOME (a call to a system ROM routine)\n Text-output control NORMAL, INVERSE, FLASH and SPEED=\n The print-space function SPC() is listed among reserved words in the manual, but is not otherwise documented (the TAB() print-function is documented)",
        "The print-space function SPC() is listed among reserved words in the manual, but is not otherwise documented (the TAB() print-function is documented)\n Cassette tape storage of numerical arrays: STORE and RECALL\n Device response: WAIT\nas well as several the later version would have, that had already been present in Apple's Integer BASIC:\n Program-line deletion: DEL\n Machine-routine access: CALL\n Peripheral device access: IN# and PR# (although IN without \"#\" is listed among reserved words)\n Memory range control: HIMEM: and LOMEM:\n Execution tracking for debugging: TRACE and NOTRACE\n Screen-positioning: HTAB and VTAB\n Subroutine aborting POP\n Functions PDL() to read the analog controllers, and SCRN() to read the low-resolution graphics screen (both accessing system ROM routines)",
        "Screen-positioning: HTAB and VTAB\n Subroutine aborting POP\n Functions PDL() to read the analog controllers, and SCRN() to read the low-resolution graphics screen (both accessing system ROM routines)\nIn addition, its low-resolution graphics commands have different names from their Integer BASIC/Applesoft II counterparts. All command names are of the form PLTx such that GR, COLOR=, PLOT, HLIN and VLIN are called PLTG, PLTC, PLTP, PLTH, and PLTV, respectively. The command for returning to text mode, known as TEXT in other versions, is simply TEX, and carries the proviso that it has to be the last statement in a program line.",
        "Applesoft BASIC 1.x was closer to Microsoft's original 6502 BASIC code than the later Applesoft II; it retained the Memory Size? prompt and displayed a Microsoft copyright notice. To maintain consistency with Integer BASIC, the \"Ok\" prompt from Microsoft's code was replaced by a ] character. Applesoft 1.x also prompted the user upon loading if he wished to disable the REM statement and the LET keyword in assignment statements in exchange for lores graphics commands.",
        "The USR() function is also defined differently, serving as a stand-in for the absent CALL command. Its argument is not for passing a numerical value to the machine-language routine, but is instead the call-address of the routine itself; there is no \"hook\" to pre-define the address. All of several examples in the manual use the function only to access \"system monitor ROM\" routines, or short user-routines to manipulate the ROM routines. No mention is made of any code to calculate the value returned by the function itself; the function is always shown being assigned to \"dummy\" variables, which, without action to set a value by user-code, just receive a meaningless value handed back to them. Even accessed ROM routines that return values (in examples, those that provide the service of PDL()",
        "without action to set a value by user-code, just receive a meaningless value handed back to them. Even accessed ROM routines that return values (in examples, those that provide the service of PDL() and SCRN() functions) merely have their values stored, by user-routines, in locations that are separately PEEKed in a subsequent statement.",
        "Unlike in Integer BASIC and Applesoft II, the Boolean operators AND, OR and NOT perform bitwise operations on 16-bit integer values. If they are given values outside that range, an error results.\n\nThe terms OUT and PLT (and the aforementioned IN) appear in the list of reserved words, but are not explained anywhere in the manual.\n\nRelated BASICs\nColeco claimed that its Adam home computer's SmartBASIC was source-code compatible with Applesoft.\n\nMicrosoft licensed a BASIC compatible with Applesoft to VTech for its Laser 128 clone.\n\nSee also\n ALF's Formula Transfer Link, speed enhancement for Applesoft BASIC\n Chinese BASIC, a Chinese-localized version of Applesoft BASIC\n Apple III BASICs from Apple and Microsoft\n\nReferences\n\nThis article includes text from Everything2, licensed under GFDL.",
        "References\n\nThis article includes text from Everything2, licensed under GFDL.\n\nExternal links\n Disassembled ROM\n AppleSoft BASIC in JavaScript\n\nApple II software\nBASIC interpreters\nDiscontinued Microsoft BASICs\nBASIC programming language family\nMicrosoft programming languages"
    ],
    [
        "Applet\nIn computing, an applet is any small application that performs one specific task that runs within the scope of a dedicated widget engine or a larger program, often as a plug-in. The term is frequently used to refer to a Java applet, a program written in the Java programming language that is designed to be placed on a web page.  Applets are typical examples of transient and auxiliary applications that do not monopolize the user's attention. Applets are not full-featured application programs, and are intended to be easily accessible.",
        "History\nThe word applet was first used in 1990 in PC Magazine. However, the concept of an applet, or more broadly a small interpreted program downloaded and executed by the user, dates at least to RFC 5 (1969) by Jeff Rulifson, which described the Decode-Encode Language, which was designed to allow remote use of the oN-Line System over ARPANET, by downloading small programs to enhance the interaction. This has been specifically credited as a forerunner of Java's downloadable programs in RFC 2555.\n\nApplet as an extension of other software\nIn some cases, an applet does not run independently. These applets must run either in a container provided by a host program, through a plugin, or a variety of other applications including mobile devices that support the applet programming model.",
        "Web-based applets\nApplets were used to provide interactive features to web applications that historically could not be provided by HTML alone. They could capture mouse input and also had controls like buttons or check boxes. In response to the user action, an applet could change the provided graphic content. This made applets well suited for demonstration, visualization, and teaching. There were online applet collections for studying various subjects, from physics to heart physiology. Applets were also used to create online game collections that allowed players to compete against live opponents in real-time.",
        "An applet could also be a text area only, providing, for instance, a cross-platform command-line interface to some remote system. If needed, an applet could leave the dedicated area and run as a separate window. However, applets had very little control over web page content outside the applet dedicated area, so they were less useful for improving the site appearance in general (while applets like news tickers or WYSIWYG editors are also known). Applets could also play media in formats that are not natively supported by the browser.\n\nHTML pages could embed parameters that were passed to the applet. Hence, the same applet could appear differently depending on the parameters that were passed.\n\nExamples of Web-based applets include:",
        "Examples of Web-based applets include:\n\n QuickTime movies\n Flash movies\n Windows Media Player applets, used to display embedded video files in Internet Explorer (and other browsers that supported the plugin)\n 3D modeling display applets, used to rotate and zoom a model\n Browser games that were applet-based, though some developed into fully functional applications that required installation.\n\nApplet Vs. Subroutine\nA larger application distinguishes its applets through several features:",
        "Applet Vs. Subroutine\nA larger application distinguishes its applets through several features:\n\n Applets execute only on the \"client\" platform environment of a system, as contrasted from \"Servlet\". As such, an applet provides functionality or performance beyond the default capabilities of its container (the browser).\n The container restricts applets' capabilities.\n Applets are written in a language different from the scripting or HTML language that invokes it. The applet is written in a compiled language, whereas the scripting language of the container is an interpreted language, hence the greater performance or functionality of the applet. Unlike a subroutine, a complete web component can be implemented as an applet.\n\nJava applets",
        "Java applets\n\nA Java applet is a Java program that is launched from HTML and run in a web browser. It takes code from server and run in a web browser. It can provide web applications with interactive features that cannot be provided by HTML. Since Java's bytecode is platform-independent, Java applets can be executed by browsers running under many platforms, including Windows, Unix, macOS, and Linux. When a Java technology-enabled web browser processes a page that contains an applet, the applet's code is transferred to the client's system and executed by the browser's Java virtual machine. An HTML page references an applet either via the deprecated  tag or via its replacement, the  tag.",
        "Security\nRecent developments in the coding of applications, including mobile and embedded systems, have led to the awareness of the security of applets.\n\nOpen platform applets\nApplets in an open platform environment should provide secure interactions between different applications. A compositional approach can be used to provide security for open platform applets.  Advanced compositional verification methods have been developed for secure applet interactions.\n\nJava applets\nA Java applet contains different security models: unsigned Java applet security, signed Java applet security, and self-signed Java applet security.",
        "Java applets\nA Java applet contains different security models: unsigned Java applet security, signed Java applet security, and self-signed Java applet security.\n\nWeb-based applets\nIn an applet-enabled web browser, many methods can be used to provide applet security for malicious applets. A malicious applet can infect a computer system in many ways, including denial of service, invasion of privacy, and annoyance.  A typical solution for malicious applets is to make the web browser to monitor applets' activities. This will result in a web browser that will enable the manual or automatic stopping of malicious applets.\n\nSee also\n \n Application posture\n Bookmarklet\n Java applet\n Widget engine\n Abstract Window Toolkit\n\nReferences\n\nExternal links",
        "See also\n \n Application posture\n Bookmarklet\n Java applet\n Widget engine\n Abstract Window Toolkit\n\nReferences\n\nExternal links\n\nTechnology neologisms\nComponent-based software engineering\nJava (programming language) libraries"
    ],
    [
        "AppleTalk\nAppleTalk is a discontinued proprietary suite of networking protocols developed by Apple Computer for their Macintosh computers. AppleTalk includes a number of features that allow local area networks to be connected with no prior setup or the need for a centralized router or server of any sort. Connected AppleTalk-equipped systems automatically assign addresses, update the distributed namespace, and configure any required inter-networking routing.\n\nAppleTalk was released in 1985 and was the primary protocol used by Apple devices through the 1980s and 1990s. Versions were also released for the IBM PC and compatibles and the Apple IIGS. AppleTalk support was also available in most networked printers (especially laser printers), some file servers, and a number of routers.",
        "The rise of TCP/IP during the 1990s led to a reimplementation of most of these types of support on that protocol, and AppleTalk became unsupported as of the release of Mac OS X v10.6 in 2009. Many of AppleTalk's more advanced autoconfiguration features have since been introduced in Bonjour, while Universal Plug and Play serves similar needs.\n\nHistory",
        "History\n\nAppleNet\nAfter the release of the Apple Lisa computer in January 1983, Apple invested considerable effort in the development of a local area networking (LAN) system for the machines. Known as AppleNet, it was based on the seminal Xerox XNS protocol stack but running on a custom 1 Mbit/s coaxial cable system rather than Xerox's 2.94 Mbit/s Ethernet. AppleNet was announced early in 1983 with a full introduction at the target price of $500 for plug-in AppleNet cards for the Lisa and the Apple II.",
        "At that time, early LAN systems were just coming to market, including Ethernet, Token Ring, Econet, and ARCNET. This was a topic of major commercial effort at the time, dominating shows like the National Computer Conference (NCC) in Anaheim in May 1983. All of the systems were jockeying for position in the market, but even at this time, Ethernet's widespread acceptance suggested it was to become a de facto standard. It was at this show that Steve Jobs asked Gursharan Sidhu a seemingly innocuous question: \"Why has networking not caught on?\"",
        "Four months later, in October, AppleNet was cancelled. At the time, they announced that \"Apple realized that it's not in the business to create a networking system. We built and used AppleNet in-house, but we realized that if we had shipped it, we would have seen new standards coming up.\" In January, Jobs announced that they would instead be supporting IBM's Token Ring, which he expected to come out in a \"few months\".",
        "AppleBus\nThrough this period, Apple was deep in development of the Macintosh computer. During development, engineers had made the decision to use the Zilog 8530 serial controller chip (SCC) instead of the lower-cost and more common UART to provide serial port connections. The SCC cost about $5 more than a UART, but offered much higher speeds of up to 250 kilobits per second (or higher with additional hardware) and internally supported a number of basic networking-like protocols like IBM's Bisync.",
        "The SCC was chosen because it would allow multiple devices to be attached to the port. Peripherals equipped with similar SCCs could communicate using the built-in protocols, interleaving their data with other peripherals on the same bus. This would eliminate the need for more ports on the back of the machine, and allowed for the elimination of expansion slots for supporting more complex devices. The initial concept was known as AppleBus, envisioning a system controlled by the host Macintosh polling \"dumb\" devices in a fashion similar to the modern Universal Serial Bus.",
        "AppleBus networking\nThe Macintosh team had already begun work on what would become the LaserWriter and had considered a number of other options to answer the question of how to share these expensive machines and other resources. A series of memos from Bob Belleville clarified these concepts, outlining the Mac, LaserWriter, and a file server system which would become the Macintosh Office. By late 1983 it was clear that IBM's Token Ring would not be ready in time for the launch of the Mac, and might miss the launch of these other products as well. In the end, Token Ring would not ship until October 1985.",
        "Jobs' earlier question to Sidhu had already sparked a number of ideas. When AppleNet was cancelled in October, Sidhu led an effort to develop a new networking system based on the AppleBus hardware. This new system would not have to conform to any existing preconceptions, and was designed to be worthy of the Mac – a system that was user-installable, had zero configuration, and no fixed network addresses – in short, a true plug-and-play network. Considerable effort was needed, but by the time the Mac was released, the basic concepts had been outlined, and some of the low-level protocols were on their way to completion. Sidhu mentioned the work to Belleville only two hours after the Mac was announced.",
        "The \"new\" AppleBus was announced in early 1984, allowing direct connection from the Mac or Lisa through a small box that is plugged into the serial port and connected via cables to the next computer upstream and downstream. Adaptors for Apple II and Apple III were also announced. Apple also announced that AppleBus networks could be attached to, and would appear to be a single node within, a Token Ring system. Details of how this would work were sketchy.\n\nAppleTalk Personal Network\nJust prior to its release in early 1985, AppleBus was renamed AppleTalk. Initially marketed as AppleTalk Personal Network, it comprised a family of network protocols and a physical layer.",
        "The physical layer had a number of limitations, including a speed of only 230.4 kbit/s, a maximum distance of  from end to end, and only 32 nodes per LAN. But as the basic hardware was built into the Mac, adding nodes only cost about $50 for the adaptor box. In comparison, Ethernet or Token Ring cards cost hundreds or thousands of dollars. Additionally, the entire networking stack required only about 6 kB of RAM, allowing it to run on any Mac.",
        "The relatively slow speed of AppleTalk allowed further reductions in cost. Instead of using RS-422's balanced transmit and receive circuits, the AppleTalk cabling used a single common electrical ground, which limited speeds to about 500 kbit/s, but allowed one conductor to be removed. This meant that common three-conductor cables could be used for wiring. Additionally, the adaptors were designed to be \"self-terminating\", meaning that nodes at the end of the network could simply leave their last connector unconnected. There was no need for the wires to be connected back together into a loop, nor the need for hubs or other devices.",
        "The system was designed for future expansion; the addressing system allowed for expansion to 255 nodes in a LAN (although only 32 could be used at that time), and by using \"bridges\" (which came to be known as \"routers\", although technically not the same) one could interconnect LANs into larger collections. \"Zones\" allowed devices to be addressed within a bridge-connected internet. Additionally, AppleTalk was designed from the start to allow use with any potential underlying physical link, and within a few years, the physical layer would be renamed LocalTalk, so as to differentiate it from the AppleTalk protocols.",
        "The main advantage of AppleTalk was that it was completely maintenance-free. To join a device to a network, a user simply plugged the adaptor into the machine, then connected a cable from it to any free port on any other adaptor. The AppleTalk network stack negotiated a network address, assigned the computer a human-readable name, and compiled a list of the names and types of other machines on the network so the user could browse the devices through the Chooser. AppleTalk was so easy to use that ad hoc networks tended to appear whenever multiple Macs were in the same room. Apple would later use this in an advertisement showing a network being created between two seats in an airplane.",
        "PhoneNet and other adaptors\nA thriving 3rd party market for AppleTalk devices developed over the next few years. One particularly notable example was an alternate adaptor designed by BMUG and commercialized by Farallon as PhoneNET in 1987. This was essentially a replacement for Apple's connector that had conventional phone jacks instead of Apple's round connectors. PhoneNet allowed AppleTalk networks to be connected together using normal telephone wires, and with very little extra work, could run analog phones and AppleTalk on a single four-conductor phone cable.",
        "Other companies took advantage of the SCC's ability to read external clocks in order to support higher transmission speeds, up to 1 Mbit/s. In these systems, the external adaptor also included its own clock, and used that to signal the SCC's clock input pins. The best-known such system was Centram's FlashTalk, which ran at 768 kbit/s, and was intended to be used with their TOPS networking system. A similar solution was the 850 kbit/s DaynaTalk, which used a separate box that plugged in between the computer and a normal LocalTalk/PhoneNet box. Dayna also offered a PC expansion card that ran up to 1.7 Mbit/s when talking to other Dayna PC cards. Several other systems also existed with even higher performance, but these often required special cabling that was incompatible with",
        "that ran up to 1.7 Mbit/s when talking to other Dayna PC cards. Several other systems also existed with even higher performance, but these often required special cabling that was incompatible with LocalTalk/PhoneNet, and also required patches to the networking stack that often caused problems.",
        "AppleTalk over Ethernet \nAs Apple expanded into more commercial and education markets, they needed to integrate AppleTalk into existing network installations. Many of these organizations had already invested in a very expensive Ethernet infrastructure and there was no direct way to connect a Macintosh to Ethernet. AppleTalk included a protocol structure for interconnecting AppleTalk subnets and so as a solution, EtherTalk was initially created to use the Ethernet as a backbone between LocalTalk subnets. To accomplish this, organizations would need to purchase a LocalTalk-to-Ethernet bridge and Apple left it to third parties to produce these products. A number of companies responded, including Hayes and a few newly formed companies like Kinetics.",
        "LocalTalk, EtherTalk, TokenTalk, and AppleShare",
        "By 1987, Ethernet was clearly winning the standards battle over Token Ring, and in the middle of that year, Apple introduced EtherTalk 1.0, an implementation of the AppleTalk protocol over the Ethernet physical layer. Introduced for the newly released Macintosh II computer, Apple's first Macintosh with expansion slots, the operating system included a new Network control panel that allowed the user to select which physical connection to use for networking (from \"Built-in\" or \"EtherTalk\"). At introduction, Ethernet interface cards were available from 3Com and Kinetics that plugged into a Nubus slot in the machine. The new networking stack also expanded the system to allow a full 255 nodes per LAN. With EtherTalk's release, AppleTalk Personal Network was renamed LocalTalk, the name it would",
        "slot in the machine. The new networking stack also expanded the system to allow a full 255 nodes per LAN. With EtherTalk's release, AppleTalk Personal Network was renamed LocalTalk, the name it would be known under for the bulk of its life. Token Ring would later be supported with a similar TokenTalk product, which used the same Network control panel and underlying software. Over time, many third-party companies would introduce compatible Ethernet and Token Ring cards that used these same drivers.",
        "The appearance of a Macintosh with a direct Ethernet connection also magnified the Ethernet and LocalTalk compatibility problem: Networks with new and old Macs needed some way to communicate with each other. This could be as simple as a network of Ethernet Mac II's trying to talk to a LaserWriter that only connected to LocalTalk. Apple initially relied on the aforementioned LocalTalk-to-Ethernet bridge products, but contrary to Apple's belief that these would be low-volume products, by the end of 1987, 130,000 such networks were in use. AppleTalk was at that time the most used networking system in the world, with over three times the installations of any other vendor.",
        "1987 also marked the introduction of the AppleShare product, a dedicated file server that ran on any Mac with 512 kB of RAM or more. A common AppleShare machine was the Mac Plus with an external SCSI hard drive. AppleShare was the #3 network operating system in the late 1980s, behind Novell NetWare and Microsoft's MS-Net. AppleShare was effectively the replacement for the failed Macintosh Office efforts, which had been based on a dedicated file server device.",
        "AppleTalk Phase II and other developments",
        "A significant re-design was released in 1989 as AppleTalk Phase II. In many ways, Phase II can be considered an effort to make the earlier version (never called Phase I) more generic. LANs could now support more than 255 nodes, and zones were no longer associated with physical networks but were entirely virtual constructs used simply to organize nodes. For instance, one could now make a \"Printers\" zone that would list all the printers in an organization, or one might want to place that same device in the \"2nd Floor\" zone to indicate its physical location. Phase II also included changes to the underlying inter-networking protocols to make them less \"chatty\", which had previously been a serious problem on networks that bridged over wide-area networks.",
        "By this point, Apple had a wide variety of communications products under development, and many of these were announced along with AppleTalk Phase II. These included updates to EtherTalk and TokenTalk, AppleTalk software and LocalTalk hardware for the IBM PC, EtherTalk for Apple's A/UX operating system allowing it to use LaserPrinters and other network resources, and the Mac X.25 and MacX products.",
        "Ethernet had become almost universal by 1990, and it was time to build Ethernet into Macs direct from the factory. However, the physical wiring used by these networks was not yet completely standardized. Apple solved this problem using a single port on the back of the computer into which the user could plug an adaptor for any given cabling system. This FriendlyNet system was based on the industry-standard Attachment Unit Interface or AUI, but deliberately chose a non-standard connector that was smaller and easier to use, which they called \"Apple AUI\", or AAUI. FriendlyNet was first introduced on the Quadra 700 and Quadra 900 computers, and used across much of the Mac line for some time. As with LocalTalk, a number of 3rd party FriendlyNet adaptors quickly appeared.",
        "As 10BASE-T became the de facto cabling system for Ethernet, second-generation Power Macintosh machines added a 10BASE-T port in addition to AAUI. The PowerBook 3400c and lower-end Power Macs also added 10BASE-T. The Power Macintosh 7300/8600/9600 were the final Macs to include AAUI, and 10BASE-T became universal starting with the Power Macintosh G3 and PowerBook G3.",
        "The capital-I Internet",
        "From the beginning of AppleTalk, users wanted to connect the Macintosh to the TCP/IP network environments. In 1984, Bill Croft at Stanford University pioneered the development of IP packets encapsulated in DDP as part of the SEAGATE (Stanford Ethernet–AppleTalk Gateway) project. SEAGATE was commercialized by Kinetics in their LocalTalk-to-Ethernet bridge as an additional routing option. A few years later, MacIP, was separated from the SEAGATE code and became the de facto method for IP packets to be routed over LocalTalk networks. By 1986, Columbia University released the first version of the Columbia AppleTalk Package (CAP) that allowed higher integration of Unix, TCP/IP, and AppleTalk environments. In 1988, Apple released MacTCP, a system that allowed the Mac to support TCP/IP on",
        "of the Columbia AppleTalk Package (CAP) that allowed higher integration of Unix, TCP/IP, and AppleTalk environments. In 1988, Apple released MacTCP, a system that allowed the Mac to support TCP/IP on machines with suitable Ethernet hardware. However, this left many universities with the problem of supporting IP on their many LocalTalk-equipped Macs. It was soon common to include MacIP support in LocalTalk-to-Ethernet bridges. MacTCP would not become a standard part of the Classic Mac OS until 1994, by which time it also supported SNMP and PPP.",
        "For some time in the early 1990s, the Mac was a primary client on the rapidly expanding Internet. Among the better-known programs in wide use were Fetch, Eudora, eXodus, NewsWatcher, and the NCSA packages, especially NCSA Mosaic and its offspring, Netscape Navigator. Additionally, a number of server products appeared that allowed the Mac to host Internet content. Through this period, Macs had about 2 to 3 times as many clients connected to the Internet as any other platform, despite the relatively small overall microcomputer market share.",
        "As the world quickly moved to IP for both LAN and WAN uses, Apple was faced with maintaining two increasingly outdated code bases on an ever-wider group of machines as well as the introduction of the PowerPC based machines. This led to the Open Transport efforts, which re-implemented both MacTCP and AppleTalk on an entirely new code base adapted from the Unix standard STREAMS. Early versions had problems and did not become stable for some time.  By that point, Apple was deep in their ultimately doomed Copland efforts.",
        "Legacy and abandonment\nWith the purchase of NeXT and subsequent development of Mac OS X, AppleTalk was strictly a legacy system. Support was added to OS X in order to provide support for a large number of existing AppleTalk devices, notably laser printers and file shares, but alternate connection solutions common in this era, notably USB for printers, limited their demand. As Apple abandoned many of these product categories, and all new systems were based on IP, AppleTalk became less and less common. AppleTalk support was finally removed from the MacOS in Mac OS X v10.6 in 2009.",
        "However, the loss of AppleTalk did not reduce the desire for networking solutions that combined its ease of use with IP routing. Apple has led the development of many such efforts, from the introduction of the AirPort router to the development of the Zero-configuration networking system and their implementation of it, Bonjour.\n\nAs of 2020, AppleTalk support has been completely removed from legacy support with macOS 11 Big Sur.\n\nDesign \nThe AppleTalk design rigorously followed the OSI model of protocol layering.  Unlike most of the early LAN systems, AppleTalk was not built using the archetypal Xerox XNS system. The intended target was not Ethernet, and it did not have 48-bit addresses to route. Nevertheless, many portions of the AppleTalk system have direct analogs in XNS.",
        "One key differentiation for AppleTalk was it contained two protocols aimed at making the system completely self-configuring. The AppleTalk address resolution protocol (AARP) allowed AppleTalk hosts to automatically generate their own network addresses, and the Name Binding Protocol (NBP) was a dynamic system for mapping network addresses to user-readable names. Although systems similar to AARP existed in other systems, Banyan VINES for instance.  Beginning about 2002 Multicast DNS provided capabilities similar to NBP.",
        "Both AARP and NBP had defined ways to allow \"controller\" devices to override the default mechanisms. The concept was to allow routers to provide the information or  \"hardwire\" the system to known addresses and names.  On larger networks where AARP could cause problems as new nodes searched for free addresses, the addition of a router could reduce \"chattiness.\"  Together AARP and NBP made AppleTalk an easy-to-use networking system. New machines were added to the network by plugging them in and optionally giving them a name.  The NBP lists were examined and displayed by a program known as the Chooser which would display a list of machines on the local network, divided into classes such as file-servers and printers.",
        "Addressing",
        "An AppleTalk address was a four-byte quantity. This consisted of a two-byte network number, a one-byte node number, and a one-byte socket number. Of these, only the network number required any configuration, being obtained from a router. Each node dynamically chose its own node number, according to a protocol (originally the LocalTalk Link Access Protocol LLAP and later, for Ethernet/EtherTalk, the AppleTalk Address Resolution Protocol, AARP) which handled contention between different nodes accidentally choosing the same number. For socket numbers, a few well-known numbers were reserved for special purposes specific to the AppleTalk protocol itself. Apart from these, all application-level protocols were expected to use dynamically-assigned socket numbers at both the client and server end.",
        "Because of this dynamism, users could not be expected to access services by specifying their address. Instead, all services had names which, being chosen by humans, could be expected to be meaningful to users, and also could be sufficiently long to minimize the chance of conflicts.",
        "As NBP names translated to an address, which included a socket number as well as a node number, a name in AppleTalk mapped directly to a service being provided by a machine, which was entirely separate from the name of the machine itself. Thus, services could be moved to a different machine and, so long as they kept the same service name, there was no need for users to do anything different in order to continue accessing the service. And the same machine could host any number of instances of services of the same type, without any network connection conflicts.",
        "Contrast this with A records in the DNS, in which a name translates to a machine's address, not including the port number that might be providing a service. Thus, if people are accustomed to using a particular machine name to access a particular service, their access will break when the service is moved to a different machine. This can be mitigated somewhat by insistence on using CNAME records indicating service rather than actual machine names to refer to the service, but there is no way of guaranteeing that users will follow such a convention. Some newer protocols, such as Kerberos and Active Directory use DNS SRV records to identify services by name, which is much closer to the AppleTalk model.\n\nProtocols",
        "Protocols\n\nAppleTalk Address Resolution Protocol \nAARP resolves AppleTalk addresses to link layer addresses. It is functionally equivalent to ARP and obtains address resolution by a method very similar to ARP.",
        "AARP is a fairly simple system. When powered on, an AppleTalk machine broadcasts an AARP probe packet asking for a network address, intending to hear back from controllers such as routers. If no address is provided, one is picked at random from the \"base subnet\", 0. It then broadcasts another packet saying \"I am selecting this address\", and then waits to see if anyone else on the network complains. If another machine has that address, it will pick another address, and keep trying until it finds a free one. On a network with many machines it may take several tries before a free address is found, so for performance purposes the successful address is \"written down\" in NVRAM and used as the default address in the future. This means that in most real-world setups where machines are added a few",
        "so for performance purposes the successful address is \"written down\" in NVRAM and used as the default address in the future. This means that in most real-world setups where machines are added a few at a time, only one or two tries are needed before the address effectively become constant.",
        "AppleTalk Data Stream Protocol \nThis was a comparatively late addition to the AppleTalk protocol suite, done when it became clear that a TCP-style reliable connection-oriented transport was needed. Significant differences from TCP were that:\n a connection attempt could be rejected\n there were no \"half-open\" connections; once one end initiated a tear-down of the connection, the whole connection would be closed (i.e., ADSP is full-duplex, not dual simplex).",
        "there were no \"half-open\" connections; once one end initiated a tear-down of the connection, the whole connection would be closed (i.e., ADSP is full-duplex, not dual simplex).\n AppleTalk had an included attention message system which allowed short messages to be sent which would bypass the normal stream data flow. These were delivered reliably but out of order with respect to the stream. Any attention message would be delivered as soon as possible instead of waiting for the current stream byte sequence point to become current.",
        "Apple Filing Protocol \nThe Apple Filing Protocol (AFP), formerly AppleTalk Filing Protocol, is the protocol for communicating with AppleShare file servers. Built on top of AppleTalk Session Protocol (for legacy AFP over DDP) or the Data Stream Interface (for AFP over TCP), it provides services for authenticating users (extensible to different authentication methods including two-way random-number exchange) and for performing operations specific to the Macintosh HFS filesystem. AFP is still in use in macOS, even though most other AppleTalk protocols have been deprecated.",
        "AppleTalk Session Protocol \nASP was an intermediate protocol, built on top of ATP, which in turn was the foundation of AFP. It provided basic services for requesting responses to arbitrary commands d performing out-of-band status queries. It also allowed the server to send asynchronous attention messages to the client.\n\nDatagram Delivery Protocol \nDDP was the lowest-level data-link-independent transport protocol. It provided a datagram service with no guarantees of delivery. All application-level protocols, including the infrastructure protocols NBP, RTMP and ZIP, were built on top of DDP. AppleTalk's DDP corresponds closely to the Network layer of the Open Systems Interconnection (OSI) communication model.",
        "Name Binding Protocol \nName Binding Protocol was a dynamic, distributed system for managing AppleTalk names. When a service started up on a machine, it registered a name for itself as chosen by a human administrator. At this point, NBP provided a system for checking that no other machine had already registered the same name. Later, when a client wanted to access that service, it used NBP to query machines to find that service. NBP provided browsability (\"what are the names of all the services available?\") as well as the ability to find a service with a particular name. Names were human readable, containing spaces, upper and lower case letters, and including support for searching.",
        "AppleTalk Echo Protocol\nAEP (AppleTalk Echo Protocol) is a transport layer protocol designed to test the reachability of network nodes. AEP generates packets to be sent to the network node and is identified in the Type field of a packet as an AEP packet. The packet is first passed to the source DDP. After it is identified as an AEP packet, it is forwarded to the node where the packet is examined by the DDP at the destination. After the packet is identified as an AEP packet, the packet is then copied and a field in the packet is altered to create an AEP reply packet, and is then returned to the source node.",
        "Printer Access Protocol \nPAP was the standard way of communicating with PostScript printers. It was built on top of ATP. When a PAP connection was opened, each end sent the other an ATP request which basically meant \"send me more data\". The client's response to the server was to send a block of PostScript code, while the server could respond with any diagnostic messages that might be generated as a result, after which another \"send-more-data\" request was sent. This use of ATP provided automatic flow control; each end could only send data to the other end if there was an outstanding ATP request to respond to.",
        "PAP also provided for out-of-band status queries, handled by separate ATP transactions. Even while it was busy servicing a print job from one client, a PAP server could continue to respond to status requests from any number of other clients. This allowed other Macintoshes on the LAN that were waiting to print to display status messages indicating that the printer was busy, and what the job was that it was busy with.\n\nRouting Table Maintenance Protocol \nRTMP was the protocol by which routers kept each other informed about the topology of the network. This was the only part of AppleTalk that required periodic unsolicited broadcasts: every 10 seconds, each router had to send out a list of all the network numbers it knew about and how far away it thought they were.",
        "Zone Information Protocol \nZIP was the protocol by which AppleTalk network numbers were associated with zone names. A zone was a subdivision of the network that made sense to humans (for example, \"Accounting Department\"); but while a network number had to be assigned to a topologically-contiguous section of the network, a zone could include several different discontiguous portions of the network.\n\nPhysical implementation",
        "Physical implementation \n\nThe initial default hardware implementation for AppleTalk was a high-speed serial protocol known as LocalTalk that used the Macintosh's built-in RS-422 ports at 230.4 kbit/s. LocalTalk used a splitter box in the RS-422 port to provide an upstream and downstream cable from a single port. The topology was a bus: cables were daisy-chained from each connected machine to the next, up to the maximum of 32 permitted on any LocalTalk segment. The system was slow by today's standards, but at the time the additional cost and complexity of networking on PC machines was such that it was common that Macs were the only networked personal computers in an office. Other larger computers, such as UNIX or VAX workstations, would commonly be networked via Ethernet.",
        "Other physical implementations were also available. A very popular replacement for LocalTalk was PhoneNET, a 3rd party solution from Farallon Computing, Inc. (renamed Netopia, acquired by Motorola in 2007) that also used the RS-422 port and was indistinguishable from LocalTalk as far as Apple's LocalTalk port drivers were concerned, but ran over the two unused wires in standard four-wire phone cabling.  Foreshadowing today's network hubs and switches, Farallon provided solutions for PhoneNet to be used in \"star\" as well as bus configurations, with both \"passive\" star connections (with the phone wires simply bridged to each other at a central point), and \"active\" star with \"PhoneNet Star Controller\" hub hardware.  Apple's LocalTalk connectors didn't have a locking feature, so connectors",
        "wires simply bridged to each other at a central point), and \"active\" star with \"PhoneNet Star Controller\" hub hardware.  Apple's LocalTalk connectors didn't have a locking feature, so connectors could easily come loose, and the bus configuration resulted in any loose connector bringing down the whole network, and being hard to track down. PhoneNet RJ-11 connectors, on the other hand, snapped into place, and in a star configuration any wiring issue only affected one device, and problems were easy to pinpoint. PhoneNet's low cost, flexibility, and easy troubleshooting resulted in it being the dominant choice for Mac networks into the early 1990s.",
        "AppleTalk protocols also came to run over Ethernet (first coaxial and then twisted pair) and Token Ring physical layers, labeled by Apple as EtherTalk and TokenTalk, respectively. EtherTalk gradually became the dominant implementation method for AppleTalk as Ethernet became generally popular in the PC industry throughout the 1990s. Besides AppleTalk and TCP/IP, any Ethernet network could also simultaneously carry other protocols such as DECnet and IPX.\n\nNetworking model\n\nVersions",
        "Networking model\n\nVersions\n\nCross-platform solutions\nWhen AppleTalk was first introduced, the dominant office computing platform was the PC compatible running MS-DOS. Apple introduced the AppleTalk PC Card in early 1987, allowing PCs to join AppleTalk networks and print to LaserWriter printers. A year later AppleShare PC was released, allowing PCs to access AppleShare file servers.",
        "The \"TOPS Teleconnector\" MS-DOS networking system over AppleTalk system enabled MS-DOS PCs to communicate over AppleTalk network hardware; it comprised an AppleTalk interface card for the PC and a suite of networking software allowing such functions as file, drive and printer sharing. As well as allowing the construction of a PC-only AppleTalk network, it allowed communication between PCs and Macs with TOPS software installed. (Macs without TOPS installed could use the same network but only to communicate with other Apple machines.) The Mac TOPS software did not match the quality of Apple's own either in ease of use or in robustness and freedom from crashes, but the DOS software was relatively simple to use in DOS terms, and was robust.",
        "The BSD and Linux operating systems support AppleTalk through an open source project called Netatalk, which implements the complete protocol suite and allows them to both act as native file or print servers for Macintosh computers, and print to LocalTalk printers over the network.",
        "The Windows Server operating systems supported AppleTalk starting with Windows NT and ending after Windows Server 2003.  Miramar included AppleTalk in its PC MacLAN product which was discontinued by CA in 2007. GroupLogic continues to bundle its AppleTalk protocol with its ExtremeZ-IP server software for Macintosh-Windows integration which supports Windows Server 2008 and Windows Vista as well prior versions. HELIOS Software GmbH offers a proprietary implementation of the AppleTalk protocol stack, as part of their HELIOS UB2 server. This is essentially a File and Print Server suite that runs on a whole range of different platforms.",
        "In addition, Columbia University released the Columbia AppleTalk Package (CAP) which implemented the protocol suite for various Unix flavors including Ultrix, SunOS, BSD and IRIX.  This package is no longer actively maintained.\n\nSee also\nNetatalk is a free, open-source implementation of the AppleTalk suite of protocols.\nNetwork File System\nRemote File Sharing\nSamba\nServer Message Block\n\nNotes\n\nReferences\n\nCitations\n\nBibliography\n\n \n \n \n\nApple Inc. software\nNetwork operating systems\nNetwork protocols"
    ],
    [
        "Arcadia 2001\nThe Arcadia 2001 is a second-generation 8-bit home video game console released by Emerson Radio in May 1982 for a price of US$ 99, several months before the release of ColecoVision. It was discontinued only 18 months later, with a total of 35 games having been released. Emerson licensed the Arcadia 2001 to Bandai, which released it in Japan. Over 30 Arcadia 2001 clones exist.\n\nThe unrelated Arcadia Corporation, manufacturer of the Atari 2600 Supercharger add-on, was sued by Emerson for trademark infringement. Arcadia Corporation then changed its name to Starpath.",
        "The unrelated Arcadia Corporation, manufacturer of the Atari 2600 Supercharger add-on, was sued by Emerson for trademark infringement. Arcadia Corporation then changed its name to Starpath.\n\nDescription\nThe Arcadia is much smaller than its contemporary competitors and is powered by a standard 12 volt power supply so it can be used in a boat or a vehicle. It has two headphone jacks on the far left and right sides of the back.\n\nThe system came with two Intellivision-style controllers with a 12-button keypad and \"fire\" buttons on the sides. The direction pads have a removable joystick attachment. Most games came with BoPET overlays that can be applied to the controller's keypads. The console itself has five buttons: Power, Start, Reset, Option, and Select.",
        "There are at least three different cartridge case styles and artwork, with variations on each. Emerson-family cartridges come in two different lengths (short and long) of black plastic cases.\n\nTechnical specifications\n\nMain Processor: Signetics 2650 CPU (some variants run a Signetics 2650A)\nRAM: 1 KB\nROM: None\nVideo display: 128 × 208 / 128 × 104, 8 Colours\nVideo display controller: Signetics 2637 UVI @ 3.58 MHz (NTSC), 3.55 MHz (PAL)\nSound: Single Channel \"Beeper\" + Single Channel \"Noise\"\nHardware Sprites: 4 independent, single color\nControllers: 2 × 2 way\nKeypads: 2 × 12 button (more buttons on some variants)",
        "Console variants and clones\nMany variants and clones of the Arcadia 2001 have been released by various companies in different countries. These systems are mostly compatible with each other. In 1982, the Bandai Arcadia was released only in Japan. Four exclusive games were released for the system.\n\nBandai Arcadia\n\nIn 1982, the Bandai Arcadia, a variant of the Emerson Arcadia 2001, was licensed and distributed to Japan by Bandai for a price of 19,800 yen. There were four Japan-exclusive games released by Bandai.\n\nDoraemon\nDr. Slump\nMobile Suit Gundam\nSuper Dimension Fortress Macross\n\nReception",
        "Doraemon\nDr. Slump\nMobile Suit Gundam\nSuper Dimension Fortress Macross\n\nReception\n\nAfter seeing the Arcadia 2001 at the summer 1982 Consumer Electronics Show, Danny Goodman of Creative Computing Video & Arcade Games reported that its graphics were similar to the Atari 2600's, and that \"our overall impression of the game play was favorable for a system in this price range, though no cartridge stands out as being an exciting original creation\". He called the controller offering both Intellivision-like disc and joystick functionality \"A great idea\".\n\nGames\nEmerson planned to launch the console with 19 games. Some Arcadia 2001 games are ports of lesser-known arcade games such as Route 16, Jungler, and Jump Bug, which were not available on other home systems.",
        "Emerson actually created many popular arcade titles including Pac-Man, Galaxian and Defender for the Arcadia, but never had them manufactured as Atari started to sue its competitor companies for releasing games to which it had exclusive-rights agreements. Early marketing showed popular arcade games, but they were later released as clones. For instance, the Arcadia 2001 game Space Raiders is a clone of Defender, and Breakaway is a clone of Breakout.\n\nReleased games \nThere are 47 games known to have been released for the Arcadia 2001 and its clones.\n\nBandai Arcadia Only\nSee here\n\nUnreleased games\n\nReferences\n\nExternal links",
        "Released games \nThere are 47 games known to have been released for the Arcadia 2001 and its clones.\n\nBandai Arcadia Only\nSee here\n\nUnreleased games\n\nReferences\n\nExternal links\n\n Video Game Console Library entry on the Arcadia 2001\n TheGameConsole.com entry on the Arcadia 2001\n The Dot Eaters entry on the Arcadia 2001\n www.old-computers.com Emerson Arcadia 2001 museum entry\n www.old-computers.com Article about Arcadia 2001 and clones\n Arcadia 2001 retrospective at IGN\n\nHome video game consoles\nSecond-generation video game consoles\nBandai consoles\nProducts introduced in 1982\nProducts and services discontinued in 1984\n1980s toys\nDiscontinued video game consoles"
    ],
    [
        "ArgoUML\nArgoUML is an UML diagramming application written in Java and released under the open source Eclipse Public License.  By virtue of being a Java application, it is available on any platform supported by Java SE.\n\nHistory\nArgoUML was originally developed at UC Irvine by Jason E. Robbins, leading to his Ph.D. It was an open source project hosted by Tigris.org and moved in 2019 to GitHub. The ArgoUML project included more than 19,000 registered users and over 150 developers.\n\nIn 2003, ArgoUML won the Software Development Magazine's annual Readers' Choice Award in the “Design and Analysis Tools” category.\n\nArgoUML development has suffered from lack of manpower. For example, Undo has been a perpetually requested feature since 2003 but has not been implemented yet.",
        "Features\nAccording to the official feature list, ArgoUML is capable of the following: \nAll 9 UML 1.4 diagrams are supported.\nClosely follows the UML standard.\nPlatform independent – Java 1.5+ and C++.\nClick and Go! with Java Web Start (no setup required, starts from your web browser).\nStandard UML 1.4 Metamodel.\nXMI support.\nExport diagrams as GIF, PNG, PS, EPS, PGML and SVG.\nAvailable in ten languages: EN, EN-GB, DE, ES, IT, RU, FR, NB, PT, ZH.\nAdvanced diagram editing and zoom.\nBuilt-in design critics provide unobtrusive review of design and suggestions for improvements.\nExtensible modules interface.\nOCL support.\nForward engineering (code generation supports C++ and C#, Java, PHP 4, PHP 5, Ruby and, with less mature modules, Ada, Delphi and SQL).",
        "Extensible modules interface.\nOCL support.\nForward engineering (code generation supports C++ and C#, Java, PHP 4, PHP 5, Ruby and, with less mature modules, Ada, Delphi and SQL).\nReverse engineering / JAR/class file import.",
        "Weaknesses\nArgoUML does not yet completely implement the UML standard.\n Partial undo feature (working for graphics edits )\n Java Web Start launching may no longer work reliably. See Java Web Start.\n\nSee also\nList of UML tools\nMetaCASE tool\n\nReferences\n\nExternal links\n\nJava platform software\nFree UML tools\n1999 software\nSoftware using the Eclipse license"
    ],
    [
        "Array (data structure)\nIn computer science, an array is a data structure consisting of a collection of elements (values or variables), of same memory size, each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.",
        "For example, an array of ten 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as ten words at memory addresses 2000, 2004, 2008, ..., 2036, (in hexadecimal: 0x7D0, 0x7D4, 0x7D8, ..., 0x7F4) so that the element with index i has the address 2000 + (i × 4).\nThe memory address of the first element of an array is called first address, foundation address, or base address.",
        "Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called \"matrices\". In some cases the term \"vector\" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word \"table\" is sometimes used as a synonym of array.",
        "Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.",
        "Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.",
        "The term \"array\" may also refer to an array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.\n\nThe term is also used, especially in the description of algorithms, to mean associative array or \"abstract array\", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays.",
        "History\nThe first digital computers used machine-language programming to set up and access array structures for data tables, vector and matrix computations, and for many other purposes. John von Neumann wrote the first array-sorting program (merge sort) in 1945, during the building of the first stored-program computer. Array indexing was originally done by self-modifying code, and later using index registers and indirect addressing. Some mainframes designed in the 1960s, such as the Burroughs B5000 and its successors, used memory segmentation to perform index-bounds checking in hardware.",
        "Assembly languages generally have no special support for arrays, other than what the machine itself provides. The earliest high-level programming languages, including FORTRAN (1957), Lisp (1958), COBOL (1960), and ALGOL 60 (1960), had support for multi-dimensional arrays, and so has C (1972). In C++ (1983), class templates exist for multi-dimensional arrays whose dimension is fixed at runtime as well as for runtime-flexible arrays.\n\nApplications\nArrays are used to implement mathematical vectors and matrices, as well as other kinds of rectangular tables. Many databases, small and large, consist of (or include) one-dimensional arrays whose elements are records.",
        "Arrays are used to implement other data structures, such as lists, heaps, hash tables, deques, queues, stacks, strings, and VLists. Array-based implementations of other data structures are frequently simple and space-efficient (implicit data structures), requiring little space overhead, but may have poor space complexity, particularly when modified, compared to tree-based data structures (compare a sorted array to a search tree).\n\nOne or more large arrays are sometimes used to emulate in-program dynamic memory allocation, particularly memory pool allocation. Historically, this has sometimes been the only way to allocate \"dynamic memory\" portably.",
        "Arrays can be used to determine partial or complete control flow in programs, as a compact alternative to (otherwise repetitive) multiple IF statements. They are known in this context as control tables and are used in conjunction with a purpose built interpreter whose control flow is altered according to values contained in the array. The array may contain subroutine pointers (or relative subroutine numbers that can be acted upon by SWITCH statements) that direct the path of the execution.\n\nElement identifier and addressing formulas\nWhen data objects are stored in an array, individual objects are selected by an index that is usually a non-negative scalar integer. Indexes are also called subscripts. An index maps the array value to a stored object.",
        "There are three ways in which the elements of an array can be indexed:\n\n 0 (zero-based indexing) The first element of the array is indexed by subscript of 0.\n 1 (one-based indexing) The first element of the array is indexed by subscript of 1.\n n (n-based indexing) The base index of an array can be freely chosen. Usually programming languages allowing n-based indexing also allow negative index values and other scalar data types like enumerations, or characters may be used as an array index.\n\nUsing zero based indexing is the design choice of many influential programming languages, including C, Java and Lisp. This leads to simpler implementation where the subscript refers to an offset from the starting position of an array, so the first element has an offset of zero.",
        "Arrays can have multiple dimensions, thus it is not uncommon to access an array using multiple indices. For example, a two-dimensional array A with three rows and four columns might provide access to the element at the 2nd row and 4th column by the expression A[1][3] in the case of a zero-based indexing system. Thus two indices are used for a two-dimensional array, three for a three-dimensional array, and n for an n-dimensional array.\n\nThe number of indices needed to specify an element is called the dimension, dimensionality, or rank of the array.\n\nIn standard arrays, each index is restricted to a certain range of consecutive integers (or consecutive values of some enumerated type), and the address of an element is computed by a \"linear\" formula on the indices.",
        "One-dimensional arrays\nA one-dimensional array (or single dimension array) is a type of linear array. Accessing its elements involves a single subscript which can either represent a row or column index.\n\nAs an example consider the C declaration int anArrayName[10]; which declares a one-dimensional array of ten integers. Here, the array can store ten elements of type int . This array has indices starting from zero through nine. For example, the expressions anArrayName[0] and anArrayName[9] are the first and last elements respectively.\n\nFor a vector with linear addressing, the element with index i is located at the address , where B is a fixed base address and c a fixed constant, sometimes called the address increment or stride.",
        "For a vector with linear addressing, the element with index i is located at the address , where B is a fixed base address and c a fixed constant, sometimes called the address increment or stride.\n\nIf the valid element indices begin at 0, the constant B is simply the address of the first element of the array. For this reason, the C programming language specifies that array indices always begin at 0; and many programmers will call that element \"zeroth\" rather than \"first\".",
        "However, one can choose the index of the first element by an appropriate choice of the base address B. For example, if the array has five elements, indexed 1 through 5, and the base address B is replaced by , then the indices of those same elements will be 31 to 35. If the numbering does not start at 0, the constant B may not be the address of any element.\n\nMultidimensional arrays\nFor a multidimensional array, the element with indices i,j would have address B + c · i + d · j, where the coefficients c and d are the row and column address increments, respectively.\n\nMore generally, in a k-dimensional array, the address of an element with indices i1, i2, ..., ik is\n B + c1 · i1 + c2 · i2 + … + ck · ik.\n\nFor example: int a[2][3];",
        "More generally, in a k-dimensional array, the address of an element with indices i1, i2, ..., ik is\n B + c1 · i1 + c2 · i2 + … + ck · ik.\n\nFor example: int a[2][3];\n\nThis means that array a has 2 rows and 3 columns, and the array is of integer type. Here we can store 6 elements they will be stored linearly but starting from first row linear then continuing with second row. The above array will be stored as a11, a12, a13, a21, a22, a23.\n\nThis formula requires only k multiplications and k additions, for any array that can fit in memory. Moreover, if any coefficient is a fixed power of 2, the multiplication can be replaced by bit shifting.\n\nThe coefficients ck must be chosen so that every valid index tuple maps to the address of a distinct element.",
        "The coefficients ck must be chosen so that every valid index tuple maps to the address of a distinct element.\n\nIf the minimum legal value for every index is 0, then B is the address of the element whose indices are all zero. As in the one-dimensional case, the element indices may be changed by changing the base address B. Thus, if a two-dimensional array has rows and columns indexed from 1 to 10 and 1 to 20, respectively, then replacing B by  will cause them to be renumbered from 0 through 9 and 4 through 23, respectively. Taking advantage of this feature, some languages (like FORTRAN 77) specify that array indices begin at 1, as in mathematical tradition while other languages (like Fortran 90, Pascal and Algol) let the user choose the minimum value for each index.",
        "Dope vectors\nThe addressing formula is completely defined by the dimension d, the base address B, and the increments c1, c2, ..., ck. It is often useful to pack these parameters into a record called the array's descriptor or stride vector or dope vector. The size of each element, and the minimum and maximum values allowed for each index may also be included in the dope vector. The dope vector is a complete handle for the array, and is a convenient way to pass arrays as arguments to procedures. Many useful array slicing operations (such as selecting a sub-array, swapping indices, or reversing the direction of the indices) can be performed very efficiently by manipulating the dope vector.\n\nCompact layouts",
        "Compact layouts\n\nOften the coefficients are chosen so that the elements occupy a contiguous area of memory. However, that is not necessary. Even if arrays are always created with contiguous elements, some array slicing operations may create non-contiguous sub-arrays from them.\n\nThere are two systematic compact layouts for a two-dimensional array. For example, consider the matrix\n\nIn the row-major order layout (adopted by C for statically declared arrays), the elements in each row are stored in consecutive positions and all of the elements of a row have a lower address than any of the elements of a consecutive row:\n {| class=\"wikitable\"\n|-\n| 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9\n|}",
        "In column-major order (traditionally used by Fortran), the elements in each column are consecutive in memory and all of the elements of a column have a lower address than any of the elements of a consecutive column:\n {| class=\"wikitable\"\n|-\n| 1 || 4 || 7 || 2 || 5 || 8 || 3 || 6 || 9\n|}\n\nFor arrays with three or more indices, \"row major order\" puts in consecutive positions any two elements whose index tuples differ only by one in the last index. \"Column major order\" is analogous with respect to the first index.",
        "In systems which use processor cache or virtual memory, scanning an array is much faster if successive elements are stored in consecutive positions in memory, rather than sparsely scattered. This is known as spatial locality, which is a type of locality of reference. Many algorithms that use multidimensional arrays will scan them in a predictable order. A programmer (or a sophisticated compiler) may use this information to choose between row- or column-major layout for each array. For example, when computing the product A·B of two matrices, it would be best to have A stored in row-major order, and B in column-major order.\n\nResizing",
        "Resizing\n\nStatic arrays have a size that is fixed when they are created and consequently do not allow elements to be inserted or removed. However, by allocating a new array and copying the contents of the old array to it, it is possible to effectively implement a dynamic version of an array; see dynamic array. If this operation is done infrequently, insertions at the end of the array require only amortized constant time.\n\nSome array data structures do not reallocate storage, but do store a count of the number of elements of the array in use, called the count or size. This effectively makes the array a dynamic array with a fixed maximum size or capacity; Pascal strings are examples of this.",
        "Non-linear formulas\nMore complicated (non-linear) formulas are occasionally used. For a compact two-dimensional triangular array, for instance, the addressing formula is a polynomial of degree 2.\n\nEfficiency\nBoth store and select take (deterministic worst case) constant time. Arrays take linear (O(n)) space in the number of elements n that they hold.",
        "In an array with element size k and on a machine with a cache line size of B bytes, iterating through an array of n elements requires the minimum of ceiling(nk/B) cache misses, because its elements occupy contiguous memory locations. This is roughly a factor of B/k better than the number of cache misses needed to access n elements at random memory locations. As a consequence, sequential iteration over an array is noticeably faster in practice than iteration over many other data structures, a property called locality of reference (this does not mean however, that using a perfect hash or trivial hash within the same (local) array, will not be even faster - and achievable in constant time). Libraries provide low-level optimized facilities for copying ranges of memory (such as memcpy) which",
        "hash within the same (local) array, will not be even faster - and achievable in constant time). Libraries provide low-level optimized facilities for copying ranges of memory (such as memcpy) which can be used to move contiguous blocks of array elements significantly faster than can be achieved through individual element access. The speedup of such optimized routines varies by array element size, architecture, and implementation.",
        "Memory-wise, arrays are compact data structures with no per-element overhead. There may be a per-array overhead (e.g., to store index bounds) but this is language-dependent. It can also happen that elements stored in an array require less memory than the same elements stored in individual variables, because several array elements can be stored in a single word; such arrays are often called packed arrays. An extreme (but commonly used) case is the bit array, where every bit represents a single element. A single octet can thus hold up to 256 different combinations of up to 8 different conditions, in the most compact form.\n\nArray accesses with statically predictable access patterns are a major source of data parallelism.\n\nComparison with other data structures",
        "Array accesses with statically predictable access patterns are a major source of data parallelism.\n\nComparison with other data structures\n\nDynamic arrays or growable arrays are similar to arrays but add the ability to insert and delete elements; adding and deleting at the end is particularly efficient. However, they reserve linear (Θ(n)) additional storage, whereas arrays do not reserve additional storage.\n\nAssociative arrays provide a mechanism for array-like functionality without huge storage overheads when the index values are sparse. For example, an array that contains values only at indexes 1 and 2 billion may benefit from using such a structure. Specialized associative arrays with integer keys include Patricia tries, Judy arrays, and van Emde Boas trees.",
        "Balanced trees require O(log n) time for indexed access, but also permit inserting or deleting elements in O(log n) time, whereas growable arrays require linear (Θ(n)) time to insert or delete elements at an arbitrary position.\n\nLinked lists allow constant time removal and insertion in the middle but take linear time for indexed access. Their memory use is typically worse than arrays, but is still linear.",
        "An Iliffe vector is an alternative to a multidimensional array structure. It uses a one-dimensional array of references to arrays of one dimension less. For two dimensions, in particular, this alternative structure would be a vector of pointers to vectors, one for each row(pointer on c or c++). Thus an element in row i and column j of an array A would be accessed by double indexing (A[i][j] in typical notation). This alternative structure allows jagged arrays, where each row may have a different size—or, in general, where the valid range of each index depends on the values of all preceding indices. It also saves one multiplication (by the column address increment) replacing it by a bit shift (to index the vector of row pointers) and one extra memory access (fetching the row address),",
        "indices. It also saves one multiplication (by the column address increment) replacing it by a bit shift (to index the vector of row pointers) and one extra memory access (fetching the row address), which may be worthwhile in some architectures.",
        "Dimension\nThe dimension of an array is the number of indices needed to select an element. Thus, if the array is seen as a function on a set of possible index combinations, it is the dimension of the space of which its domain is a discrete subset. Thus a one-dimensional array is a list of data, a two-dimensional array is a rectangle of data, a three-dimensional array a block of data, etc.\n\nThis should not be confused with the dimension of the set of all matrices with a given domain, that is, the number of elements in the array. For example, an array with 5 rows and 4 columns is two-dimensional, but such matrices form a 20-dimensional space. Similarly, a three-dimensional vector can be represented by a one-dimensional array of size three.\n\nSee also",
        "See also\n\n Dynamic array\n Parallel array\n Variable-length array\n Bit array\n Array slicing\n Offset (computer science)\n Row- and column-major order\n Stride of an array\n\nReferences\n\nExternal links"
    ],
    [
        "Artificial intelligence\nArtificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or animals. It is also the field of study in computer science that develops and studies intelligent machines. \"AI\" may also refer to the machines themselves.\n\nAI technology is widely used throughout industry, government and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), and competing at the highest level in strategic games (such as chess and Go).",
        "Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding, but after 2012, when deep learning surpassed all previous AI techniques, there was a vast increase in funding and interest.",
        "The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals.\nTo solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience and many other fields.",
        "Goals \nThe general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\n\nReasoning, problem-solving \nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.",
        "Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": they became exponentially slower as the problems grew larger.\nEven humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.\nAccurate and efficient reasoning is an unsolved problem.\n\nKnowledge representation",
        "Knowledge representation \n\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\n\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as:\nobjects, properties, categories and relations between objects;",
        "situations, events, states and time;\ncauses and effects;\nknowledge about knowledge (what we know about what other people know);\ndefault reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\n\nAmong the most difficult problems in KR are: the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).",
        "Knowledge acquisition is the difficult problem of obtaining knowledge for AI applications. Modern AI gathers knowledge by \"scraping\" the internet (including Wikipedia). The knowledge itself was collected by the volunteers and professionals who published the information (who may or may not have agreed to provide their work to AI companies). This \"crowd sourced\" technique does not guarantee that the knowledge is correct or reliable. The knowledge of Large Language Models (such as ChatGPT) is highly unreliable -- it generates misinformation and falsehoods (known as \"hallucinations\"). Providing accurate knowledge for these modern AI applications is an unsolved problem.\n\nPlanning and decision making",
        "Planning and decision making \n\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.\nIn automated planning, the agent has a specific goal. In automated decision making, the agent has preferences – there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.",
        "In classical planning, the agent knows exactly what the effect of any action will be.\nIn most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning) or the agent can seek information to improve its preferences.\nInformation value theory can be used to weigh the value of exploratory or experimental actions.",
        "Information value theory can be used to weigh the value of exploratory or experimental actions.\nThe space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain what the outcome will be.",
        "A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way, and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g. by iteration), be heuristic, or it can be learned.\n\nGame theory describes rational behavior of multiple interacting agents, and is used in AI programs that make decisions that involve other agents.\n\nLearning \nMachine learning is the study of programs that can improve their performance on a given task automatically.\nIt has been a part of AI from the beginning.",
        "There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.\nSupervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\nIn reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".",
        "In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".\nTransfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.",
        "Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\n\nNatural language processing \n\nNatural language processing (NLP) allows programs to read, write and communicate in human languages such as English.\nSpecific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\n\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation\nunless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem).",
        "Modern deep learning techniques for NLP include word embedding (how often one word appears near another), transformers (which finds patterns in text), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023 these models were able to get human-level scores on the bar exam, SAT, GRE, and many other real-world applications.\n\nPerception \n\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\nThe field includes speech recognition,\nimage classification,\nfacial recognition, object recognition,\nand robotic perception.\n\nRobotics",
        "Robotics \n\nRobotics uses AI.\n\nSocial intelligence \n\nAffective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.\nFor example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\nHowever, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.",
        "General intelligence \n\nA machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\n\nTools \nAI research uses a wide variety of tools to accomplish the goals above.\n\nSearch and optimization \nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.\n\nState space search \nState space search searches through a tree of possible states to try to find a goal state.\nFor example, Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.",
        "Simple exhaustive searches\nare rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.\n\"Heuristics\" or \"rules of thumb\" can help to prioritize choices that are more likely to reach a goal.\n\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.\n\nLocal search",
        "Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.\n\nLocal search \n\nLocal search uses mathematical optimization to find a numeric solution to a problem. It begins with some form of a guess and then refines the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. This process is called stochastic gradient descent.",
        "Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses).\n\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\n\nNeural networks and statistical classifiers (discussed below), also use a form of local search, where the \"landscape\" to be searched is formed by learning.",
        "Neural networks and statistical classifiers (discussed below), also use a form of local search, where the \"landscape\" to be searched is formed by learning.\n\nLogic \nFormal Logic is used for reasoning and knowledge representation.\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\")\nand predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").",
        "Logical inference (or deduction) is the process of proving a new statement (conclusion) from other statements that are already known to be true (the premises).\nA logical knowledge base also handles queries and assertions as a special case of inference.\nAn inference rule describes what is a valid step in a proof. The most general inference rule is  resolution.\nInference can be reduced to performing a search to find a path that leads from premises to conclusions, where each step is the application of an inference rule.\nInference performed this way is intractable except for short proofs in restricted domains. No efficient, powerful and general method has been discovered.",
        "Fuzzy logic assigns a \"degree of truth\" between 0 and 1 and handles uncertainty and probabilistic situations.\nNon-monotonic logics are designed to handle default reasoning.\nOther specialized versions of logic have been developed to describe many complex domains (see knowledge representation above).\n\nProbabilistic methods for uncertain reasoning \n\nMany problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.",
        "Bayesian networks\nare a very general tool that can be used for many problems, including reasoning (using the Bayesian inference algorithm),\nlearning (using the expectation-maximization algorithm),\nplanning (using decision networks)\nand perception (using dynamic Bayesian networks).\n\nProbabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\n\nPrecise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,\nand information value theory.\nThese tools include models such as Markov decision processes,",
        "dynamic decision networks,\ngame theory and mechanism design.\n\nClassifiers and statistical learning methods \nThe simplest AI applications can be divided into two types: classifiers (e.g. \"if shiny then diamond\"), on one hand, and controllers (e.g. \"if diamond then pick up\"), on the other hand. Classifiers\nare functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.",
        "There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.\nNeural networks are also used as classifiers.\n\nArtificial neural networks",
        "Artificial neural networks \n\nArtificial neural networks were inspired by the design of the human brain: a simple \"neuron\" N accepts input from other neurons, each of which, when activated (or \"fired\"), casts a weighted \"vote\" for or against whether neuron N should itself activate. In practice, the input \"neurons\" are a list of numbers, the \"weights\" are a matrix, the next layer is the dot product (i.e., several weighted sums) scaled by an increasing function, such as the logistic function. \"The resemblance to real neural cells and structures is superficial\", according to Russell and Norvig.",
        "Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.\nNeural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.",
        "In feedforward neural networks the signal passes in only one direction.\nRecurrent neural networks feed the output signal back into the input, which allows  short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.\nPerceptrons\nuse only a single layer of neurons, deep learning uses multiple layers.\nConvolutional neural networks strengthen the connection between neurons that are \"close\" to each other – this is especially important in image processing, where a local set of neurons must identify an \"edge\" before the network can identify an object.\n\nDeep learning",
        "Deep learning \n\nDeep learning\nuses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.",
        "Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification\nand others. The reason that deep learning performs so well in so many applications is not known as of 2023.\nThe sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)",
        "but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.",
        "Specialized hardware and software\n\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow  software, had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.\nHistorically, specialized languages, such as Lisp, Prolog, and others, had been used.\n\nApplications",
        "Applications \n\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search),\ntargeting online advertisements,\nrecommendation systems (offered by Netflix, YouTube or Amazon),\ndriving internet traffic,\ntargeted advertising (AdSense, Facebook),\nvirtual assistants (such as Siri or Alexa),\nautonomous vehicles (including drones,\nADAS and self-driving cars),\nautomatic language translation (Microsoft Translator, Google Translate),\nfacial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and\nimage labeling (used by Facebook, Apple's iPhoto and TikTok).",
        "There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported they had incorporated \"AI\" in some offerings or processes.\nA few examples are energy storage,\nmedical diagnosis,\nmilitary logistics,\napplications that predict the result of judicial decisions,\nforeign policy,\nor supply chain management.",
        "Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.",
        "In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then it defeated Ke Jie in 2017, who at the time continuously held the world No. 1 ranking for two years. Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus and Cepheus. DeepMind in the 2010s developed a \"generalized artificial intelligence\" that could learn many diverse Atari games on its own.",
        "In the early 2020s, generative AI gained widespread prominence. ChatGPT, based on GPT-3, and other large language models, were tried by 14% of Americans adults. The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion\nsparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat,\nthe fictional arrest of Donald Trump,\nand a hoax of an attack on the Pentagon,\nas well as the usage in professional creative arts.\n\nAlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.",
        "AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.\n\nEthics\nAI, like any powerful technology, has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified.\n\nRisks and harm\n\nPrivacy and copyright \n\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.",
        "Privacy and copyright \n\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright. \n\nTechnology companies collect a wide range of data from their users, including online activity, geolocation data, video and audio.\nFor example, in order to build speech recognition algorithms, Amazon others have recorded millions of private conversations and allowed temps to listen to and transcribe some of them.\nOpinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.",
        "AI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.\nSince 2016, some privacy experts, such as Cynthia Dwork, began to view privacy in terms of fairness -- Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\".",
        "Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under a rationale of \"fair use\". Experts disagree about how well, and under what circumstances, this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.\n\nMisinformation",
        "YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S.",
        "true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem.",
        "In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films or human writing.  It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda. This technology has been widely distributed at minimal cost. Geoffrey Hinton (who was an instrumental developer of these tools) expressed his concerns about AI disinformation. He quit his job at Google to freely criticize the companies developing AI.\n\nAlgorithmic bias and fairness",
        "Machine learning applications will be biased if they learn from biased data.\nAnyone looking to use machine learning as part of real-world, in-production systems needs to factor ethics into their AI training processes and strive to avoid bias. This is especially true when using AI algorithms that are inherently unexplainable in deep learning. The developers may not be aware that the bias exists.\nBias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.",
        "Fairness in machine learning is the study of how to prevent the harm caused by algorithmic bias. It has become serious area of academic study within AI. Researchers have discovered it is not always possible to define \"fairness\" in a way that satisfies all stakeholders.",
        "On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a  \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.",
        "COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist.\nIn 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different -- the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.",
        "A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".\nMoritz Hardt said “the most robust fact in this research area is that fairness through blindness doesn't work.”",
        "Criticism of COMPAS highlighted a deeper problem with the misuse of AI. Machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. Unfortunately, if an applications then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is necessarily descriptive and not proscriptive.",
        "Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\n\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022) the Association for Computing Machinery, in Seoul, South Korea, presented and published findings recommending that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\n\nLack of transparency",
        "Lack of transparency \n\nMost modern AI applications can not explain how they have reached a decision. The large amount of relationships between inputs and outputs in deep neural networks and resulting complexity makes it difficult for even an expert to explain how they produced their outputs, making them a black box.",
        "There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example,  Justin Ko and Roberto Novoa developed a system that could identify skin diseases better than medical professionals, however it classified any image with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. A more dangerous example was discovered by Rich Caruana in 2015: a machine learning system that accurately predicted risk of death classified a patient that was over 65, asthma and difficulty breathing as \"low risk\". Further research showed that in high-risk cases like this, the hospital would allocate more resources and save the patient's life, decreasing the",
        "65, asthma and difficulty breathing as \"low risk\". Further research showed that in high-risk cases like this, the hospital would allocate more resources and save the patient's life, decreasing the risk measured by the program. Mistakes like these become obvious when we know how the program has reached a decision. Without an explanation, these problems may not not be discovered until after they have caused harm.",
        "A second issue is that people who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are required to clearly and completely explain the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\n\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try and solve these problems.",
        "There are several potential solutions to the transparency problem. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network have learned and produce output that can suggest what the network is learning. Supersparse linear integer models use learning to identify the most important features, rather than the classification. Simple addition of these features can then make the classification (i.e. learning is used to create a scoring system classifier, which is transparent).\n\nBad actors and weaponized AI",
        "Bad actors and weaponized AI \n\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. By 2015, over fifty countries were reported to be researching battlefield robots. These weapons are considered especially dangerous for several reasons: if they kill an innocent person it is not clear who should be held accountable, it is unlikely they will reliably choose targets, and, if produced at scale, they are potentially weapons of mass destruction. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the  United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.",
        "AI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes and generative AI aid in producing misinformation; advanced AI can make authoritarian centralized decision making more competitive with liberal and decentralized systems such as markets.\n\nTerrorists, criminals and rogue states can use weaponized AI such as advanced digital warfare and lethal autonomous weapons.",
        "Terrorists, criminals and rogue states can use weaponized AI such as advanced digital warfare and lethal autonomous weapons.\n\nMachine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.\n\nTechnological unemployment \n\nFrom the early days of the development of artificial intelligence there have been arguments, for example those put forward by Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement. \n\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.",
        "In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology (rather than",
        "only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology (rather than social policy) creates unemployment (as opposed to redundancies).",
        "Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\n\nIn April 2023, it was reported that 70% of the jobs for Chinese video game illlustrators had been eliminated by generative artificial intelligence.\n\nExistential risk",
        "In April 2023, it was reported that 70% of the jobs for Chinese video game illlustrators had been eliminated by generative artificial intelligence.\n\nExistential risk \n\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as the physicist Stephen Hawking puts it, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.",
        "First, AI does not require human-like \"sentience\" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".",
        "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.",
        "The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, Elon Musk have expressed concern about existential risk from AI. \nIn the early 2010's, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. \nHowever, after 2016, the study of current and future risks and possible solutions became a serious area of research.",
        "However, after 2016, the study of current and future risks and possible solutions became a serious area of research.\nIn 2023, AI pioneers including Geoffrey Hinton, Yoshua Bengio, Demis Hassabis, and Sam Altman issued the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".",
        "Ethical machines and alignment \n\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\n\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\nThe field of machine ethics is also called computational morality,\nand was founded at an AAAI symposium in 2005.",
        "Other approaches include Wendell Wallach's \"artificial moral agents\"\nand Stuart J. Russell's three principles for developing provably beneficial machines.\n\nRegulation",
        "The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.\nThe regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.\nBetween 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.",
        "Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.\nMost EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.\nThe Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.",
        "In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.",
        "In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.\nIn a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\n\nHistory",
        "History \n\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate both mathematical deduction and formal reasoning, which is known as the Church–Turing thesis. This, along with concurrent discoveries in cybernetics and information theory, led researchers to consider the possibility of building an \"electronic brain\". The first paper later recognized as \"AI\" was McCullouch and Pitts design for Turing-complete \"artificial neurons\" in 1943.",
        "The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.\n\nBy the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".",
        "They had, however, underestimated the difficulty of the problem. Both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks approach would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.",
        "In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.",
        "Many researchers began to doubt that the current practices would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches. Robotics researchers, such as Rodney Brooks, rejected \"representation\" in general and focussed directly on engineering machines that move and survive.. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize",
        "was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.",
        "AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).\nBy 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\".\n\nSeveral academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.",
        "Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\nFor many specific tasks, other methods were abandoned.\nDeep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing)\nand access to large amounts of data (including curated datasets, such as ImageNet).",
        "Deep learning's success led to an enormous increase in interest and funding in AI.\nThe amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019,\nand WIPO reported that AI was the most prolific emerging technology in terms of the number of patent applications and granted patents\nAccording to 'AI Impacts', about $50 billion annually was invested in \"AI\" around 2022 in the US alone and about 20% of new US Computer Science PhD graduates have specialized in \"AI\";\nabout 800,000 \"AI\"-related US job openings existed in 2022.",
        "In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\n\nPhilosophy\n\nDefining artificial intelligence",
        "Philosophy\n\nDefining artificial intelligence \n\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"\nHe advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".\nHe devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks\"",
        "Russell and Norvig agree with Turing that AI must be defined in terms of \"acting\" and not \"thinking\". However, they are critical that the test compares machines to people. \"Aeronautical engineering texts,\" they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons. AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".",
        "McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world.\" Another AI founder, Marvin Minsky similarly defines it as \"the ability to solve hard problems\". These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\n\nAnother definition has been adopted by Google, a major practitioner in the field of AI.\nThis definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.",
        "Evaluating approaches to AI \n\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.",
        "Symbolic AI and its limits\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"",
        "However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.\nPhilosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.\nAlthough his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.",
        "The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n\nNeat vs. scruffy",
        "Neat vs. scruffy \n\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 70s and 80s,\nbut eventually was seen as irrelevant. Modern AI has elements of both.\n\nSoft vs. hard computing",
        "Soft vs. hard computing \n\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n\nNarrow vs. general AI",
        "Narrow vs. general AI \n\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.\nGeneral intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.\n\nMachine consciousness, sentience and mind",
        "Machine consciousness, sentience and mind \n\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n\nConsciousness",
        "David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.",
        "Computationalism and functionalism \n\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.",
        "Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"\nSearle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.\n\nRobot rights",
        "Robot rights \n\nIf a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so it could also suffer; it has been argued that this could entitle it to certain rights.\nAny hypothetical robot rights would lie on a spectrum with animal rights and human rights.\nThis issue has been considered in fiction for centuries,\nand is now being considered by, for example, California's Institute for the Future; however, critics argue that the discussion is premature.\n\nFuture\n\nSuperintelligence and the singularity \n\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.",
        "Future\n\nSuperintelligence and the singularity \n\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.\n\nIf research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".",
        "However, most technologies do not improve exponentially indefinitely, but rather follow an S-curve, slowing when they reach the physical limits of what the technology can do. Consider, for example, transportation: speed increased exponentially from 1830 to 1970, but then the trend abruptly stopped when it reached physical limits.\n\nTranshumanism \n\nRobot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.",
        "Edward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.\n\nIn fiction \n\nThought-capable artificial beings have appeared as storytelling devices since antiquity,\nand have been a persistent theme in science fiction.",
        "In fiction \n\nThought-capable artificial beings have appeared as storytelling devices since antiquity,\nand have been a persistent theme in science fiction.\n\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.",
        "Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;\nwhile almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.",
        "Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\nSee also\n\nExplanatory notes\n\nReferences\n\nAI textbooks \nThe two most widely used textbooks in 2023. (See the Open Syllabus).\n \n \n\nThese were the four the most widely used AI textbooks in 2008:\n\n \n \n .\n \n\nLater editions.\n\nHistory of AI \n\n .\n .\n\nOther sources",
        "These were the four the most widely used AI textbooks in 2008:\n\n \n \n .\n \n\nLater editions.\n\nHistory of AI \n\n .\n .\n\nOther sources \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  was introduced by Kunihiko Fukushima in 1980.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n |\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n .\n  Presidential Address to the Association for the Advancement of Artificial Intelligence.\n \n \n \n \n \n  Later published as\n \n \n \n \n .\n \n \n \n \n AI & ML in Fusion\n AI & ML in Fusion, video lecture\n\nFurther reading",
        "Autor, David H., \"Why Are There Still So Many Jobs? The History and Future of Workplace Automation\" (2015) 29(3) Journal of Economic Perspectives 3.\n Boden, Margaret, Mind As Machine, Oxford University Press, 2006.",
        "Boden, Margaret, Mind As Machine, Oxford University Press, 2006.\n Cukier, Kenneth, \"Ready for Robots? How to Think about the Future of AI\", Foreign Affairs, vol. 98, no. 4 (July/August 2019), pp. 192–98. George Dyson, historian of computing, writes (in what might be called \"Dyson's Law\") that \"Any system simple enough to be understandable will not be complicated enough to behave intelligently, while any system complicated enough to behave intelligently will be too complicated to understand.\" (p. 197.) Computer scientist Alex Pentland writes: \"Current AI machine-learning algorithms are, at their core, dead simple stupid. They work, but they work by brute force.\" (p. 198.)",
        "Domingos, Pedro, \"Our Digital Doubles: AI will serve our species, not control it\", Scientific American, vol. 319, no. 3 (September 2018), pp. 88–93.\n Gertner, Jon. (2023) \"Wikipedia's Moment of Truth: Can the online encyclopedia help teach A.I. chatbots to get their facts right — without destroying itself in the process?\" New York Times Magazine (July 18, 2023) online",
        "Hughes-Castleberry, Kenna, \"A Murder Mystery Puzzle: The literary puzzle Cain's Jawbone, which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\", Scientific American, vol. 329, no. 4 (November 2023), pp. 81–82. \"This murder mystery competition has revealed that although NLP (natural-language processing) models are capable of incredible feats, their abilities are very much limited by the amount of context they receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyze ancient languages. In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose.\" (p. 82.)",
        "Johnston, John (2008) The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI, MIT Press.\n \n \n Gary Marcus, \"Artificial Confidence: Even the newest, buzziest systems of artificial general intelligence are stymmied by the same old problems\", Scientific American, vol. 327, no. 4 (October 2022), pp. 42–45.\n \n  Introduced DQN, which produced human-level performance on some Atari games.",
        "Introduced DQN, which produced human-level performance on some Atari games.\n Eka Roivainen, \"AI's IQ: ChatGPT aced a [standard intelligence] test but showed that intelligence cannot be measured by IQ alone\", Scientific American, vol. 329, no. 1 (July/August 2023), p. 7. \"Despite its high IQ, ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world.... ChatGPT seemed unable to reason logically and tried to rely on its vast database of... facts derived from online texts.\"\n \n \n Ashish Vaswani, Noam Shazeer, Niki Parmar et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017). Seminal paper on transformers.",
        "External links \n\n \n \n Artificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005).\n Theranostics and AI—The Next Advance in Cancer Precision Medicine\n\nArtificial intelligence\nCybernetics\nComputational neuroscience\nComputational fields of study\nData science\nEmerging technologies\nFormal sciences\nIntelligence by type\nUnsolved problems in computer science"
    ],
    [
        "ASCII art\nASCII art is a graphic design technique that uses computers for presentation and consists of pictures pieced together from the 95 printable (from a total of 128) characters defined by the ASCII Standard from 1963 and ASCII compliant character sets with proprietary extended characters (beyond the 128 characters of standard 7-bit ASCII). The term is also loosely used to refer to text-based visual art in general. ASCII art can be created with any text editor, and is often used with free-form languages. Most examples of ASCII art require a fixed-width font (non-proportional fonts, as on a traditional typewriter) such as Courier for presentation.",
        "Among the oldest known examples of ASCII art are the\ncreations by computer-art pioneer Kenneth Knowlton from around 1966, who was working for Bell Labs at the time. \"Studies in Perception I\" by Knowlton and Leon Harmon from 1966 shows some examples of their early ASCII art.\n\nASCII art was invented, in large part, because early printers often lacked graphics ability and thus, characters were used in place of graphic marks. Also, to mark divisions between different print jobs from different users, bulk printers often used ASCII art to print large banner pages, making the division easier to spot so that the results could be more easily separated by a computer operator or clerk. ASCII art was also used in early e-mail when images could not be embedded.\n\nHistory\n\nTypewriter art",
        "History\n\nTypewriter art\n\nSince 1867, typewriters have been used for creating visual art.\n\nTTY and RTTY\nTTY stands for \"TeleTYpe\" or \"TeleTYpewriter\", and is also known as Teleprinter or Teletype.\nRTTY stands for Radioteletype; character sets such as Baudot code, which predated ASCII, were used. According to a chapter in the \"RTTY Handbook\", text images have been sent via teletypewriter as early as 1923. However, none of the \"old\" RTTY art has been discovered yet. What is known is that text images appeared frequently on radioteletype in the 1960s and the 1970s.",
        "Line-printer art\nIn the 1960s, Andries van Dam published a representation of an electronic circuit produced on an IBM 1403 line printer. At the same time, Kenneth Knowlton was producing realistic images, also on line printers, by overprinting several characters on top of one another.\nNote that it was not ASCII art in a sense that the 1403 was driven by an EBCDIC-coded platform and the character sets and trains available on the 1403 were derived from EBCDIC rather than ASCII, despite some glyphs commonalities.",
        "ASCII art\nThe widespread usage of ASCII art can be traced to the computer bulletin board systems of the late 1970s and early 1980s. The limitations of computers of that time period necessitated the use of text characters to represent images. Along with ASCII's use in communication, however, it also began to appear in the underground online art groups of the period. An ASCII comic is a form of webcomic which uses ASCII text to create images. In place of images in a regular comic, ASCII art is used, with the text or dialog usually placed underneath.",
        "During the 1990s, graphical browsing and variable-width fonts became increasingly popular, leading to a decline in ASCII art. Despite this, ASCII art continued to survive through online MUDs, an acronym for \"Multi-User Dungeon\", (which are textual multiplayer role-playing video games), Internet Relay Chat, Email, message boards, and other forms of online communication which commonly employ the needed fixed-width.\n\nIt is seen to this day on the CLI app Neofetch, which displays the logo of the OS on which it is invoked.\n\nANSI\nASCII and more importantly, ANSI were staples of the early technological era; terminal systems relied on coherent presentation using color and control signals standard in the terminal protocols.",
        "Over the years, warez groups began to enter the ASCII art scene. Warez groups usually release .nfo files with their software, cracks or other general software reverse-engineering releases. The ASCII art will usually include the warez group's name and maybe some ASCII borders on the outsides of the release notes, etc.\n\nBBS systems were based on ASCII and ANSI art, as were most DOS and similar console applications, and the precursor to AOL.\n\nUses",
        "Uses\n\nASCII art is used wherever text can be more readily printed or transmitted than graphics, or in some cases, where the transmission of pictures is not possible. This includes typewriters, teleprinters, non-graphic computer terminals, printer separators, in early computer networking (e.g., BBSes), email, and Usenet news messages. ASCII art is also used within the source code of computer programs for representation of company or product logos, and flow control or other diagrams. In some cases, the entire source code of a program is a piece of ASCII art – for instance, an entry to one of the earlier International Obfuscated C Code Contest is a program that adds numbers, but visually looks like a binary adder drawn in logic ports.",
        "Some electronic schematic archives represent the circuits using ASCII art.\n\nExamples of ASCII-style art predating the modern computer era can be found in the June 1939, July 1948 and October 1948 editions of Popular Mechanics.\n\nEarly computer games played on terminals frequently used ASCII art to simulate graphics, most notably the roguelike genre using ASCII art to visually represent dungeons and monsters within them. \"0verkill\" is a 2D platform multiplayer shooter game designed entirely in color ASCII art. MPlayer and VLC media player can display videos as ASCII art through the AAlib library. ASCII art is used in the making of DOS-based ZZT games.",
        "Many game walkthrough guides come as part of a basic .txt file; this file often contains the name of the game in ASCII art. Such as below, word art is created using backslashes and other ASCII values in order to create the illusion of 3D.\n\nTypes and styles\nDifferent techniques could be used in ASCII art to obtain different artistic effects.\n\n\"Typewriter-style\" lettering, made from individual letter characters:\nLine art, for creating shapes: \n .--.     /\\                \n '--'    /__\\    (^._.^)~ <(o.o )>\n\nSolid art, for creating filled objects:\n .g@8g.  db\n 'Y8@P' d88b\n\nShading, using symbols with various intensities for creating gradients or contrasts:\n :$#$: \"4b. ':.\n :$#$:   \"4b. ':.",
        "Solid art, for creating filled objects:\n .g@8g.  db\n 'Y8@P' d88b\n\nShading, using symbols with various intensities for creating gradients or contrasts:\n :$#$: \"4b. ':.\n :$#$:   \"4b. ':.\n\nCombinations of the above, often used as signatures, for example, at the end of an email:\n   |\\_/|        ****************************    (\\_/)\n  / @ @ \\       *  \"Purrrfectly pleasant\"  *   (='.'=)\n ( > º < )      *       Poppy Prinz        *   (\")_(\")\n  `>>x<<´       *   (pprinz@example.com)   *\n  /  O  \\       ****************************",
        "As-pixel characters use combinations of ░ , █ , ▄, ▀ (Block Elements), and/or ⣿, ⣴, ⢁, etc (Braille ASCII) to make pictures:\n ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠿⠿⢿⣿⣿⣿⣿⣿⣿\n ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⢁⣴⣾⣿⣷⣦⣌⠙⢿⣿⣿⣿\n ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⢁⣴⣿⣿⣿⣿⣿⣿⣿⣷⡈⢻⣿⣿\n ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⢁⣴⣿⣿⠟⠋⣉⠙⢻⣿⣿⣿⣷⠀⣿⣿\n ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⢁⣴⣿⣿⠟⢁⣴⣿⣿⡷⢀⣿⣿⣿⡿⠀⣿⣿\n ⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⢁⣴⣿⣿⠟⢁⣴⣿⣿⡿⠋⣠⣾⣿⣿⠟⢁⣼⣿⣿\n ⣿⣿⣿⣿⣿⣿⣿⠟⢁⣴⣿⣿⠟⢁⣴⣿⣿⡿⠋⣠⣾⣿⣿⠟⢁⣴⣿⣿⣿⣿\n ⣿⣿⣿⣿⣿⠟⢁⣴⣿⣿⠟⢁⣴⣿⣿⡿⠋⣠⣾⣿⣿⠟⢁⣴⣿⣿⣿⣿⣿⣿\n ⣿⣿⣿⠟⢁⣴⣿⣿⣿⣿⣶⣿⣿⡿⠋⣠⣾⣿⣿⠟⢁⣴⣿⣿⣿⣿⣿⣿⣿⣿\n ⣿⣿⠁⣴⣿⣿⣿⣿⣿⣿⣿⡿⠋⣠⣾⣿⣿⠟⢁⣴⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿\n ⣿⣿⠀⢿⣿⣿⣿⣿⣿⡿⠋⣠⣾⣿⣿⠟⢁⣴⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿\n ⣿⣿⣧⡈⠻⢿⣿⡿⠋⣠⣾⣿⣿⡟⢁⣴⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿\n ⣿⣿⣿⣿⣷⣶⣶⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿\n\nEmoticons and verticons",
        "Emoticons and verticons\n\nThe simplest forms of ASCII art are combinations of two or three characters for expressing emotion in text. They are commonly referred to as 'emoticon', 'smilie', or 'smiley'. There is another type of one-line ASCII art that does not require the mental rotation of pictures, which is widely known in Japan as kaomoji (literally \"face characters\".)\n\nMore complex examples use several lines of text to draw large symbols or more complex figures. Hundreds of different text smileys have developed over time, but only a few are generally accepted, used and understood.\n\nASCII comic\nAn ASCII comic is a form of webcomic.",
        "ASCII comic\nAn ASCII comic is a form of webcomic.\n\nThe Adventures of Nerd Boy\nThe Adventures of Nerd Boy, or just Nerd Boy, was an ASCII comic, published by Joaquim Gândara between 5 August 2001 and 17 July 2007, and consisting of 600 strips. They were posted to ASCII art newsgroup alt.ascii-art and on the website. Some strips have been translated to Polish and French.\n\nStyles of the computer underground text art scene",
        "Styles of the computer underground text art scene\n\nAtari 400/800 ATASCII\nThe Atari 400/800, which were released in 1979, did not follow the ASCII standard and had their own character set, called ATASCII. The emergence of ATASCII art coincided with the growing popularity of BBS Systems caused by availability of the acoustic couplers that were compatible with the 8-bit home computers. ATASCII text animations are also referred to as \"break animations\" by the Atari sceners.\n\nC-64 PETSCII\nThe Commodore 64, which was released in 1982, also did not follow the ASCII standard. The C-64 character set is called PETSCII, an extended form of ASCII-1963. As with the Atari's ATASCII art, C-64 fans developed a similar scene that used PETSCII for their creations.",
        "\"Block ASCII\" / \"High ASCII\" style ASCII art on the IBM PC\n\nSo-called \"block ASCII\" or \"high ASCII\" uses the extended characters of the 8-bit code page 437, which is a proprietary standard introduced by IBM in 1979 (ANSI Standard x3.16) for the IBM PC DOS and MS-DOS operating systems. \"Block ASCIIs\" were widely used on the PC during the 1990s until the Internet replaced BBSes as the main communication platform. Until then, \"block ASCIIs\" dominated the PC Text Art Scene.",
        "The first art scene group that focused on the extended character set of the PC in their art work was called \"Aces of ANSI Art\" (). Some members left in 1990, and formed a group called \"ANSI Creators in Demand\" (ACiD). In that same year the second major underground art scene group was founded, ICE, \"Insane Creators Enterprise\".\n\nThere is some debate between ASCII and block ASCII artists, with \"Hardcore\" ASCII artists maintaining that block ASCII art is in fact not ASCII art, because it does not use the 128 characters of the original ASCII standard. On the other hand, block ASCII artists argue that if their art uses only characters of the computers character set, then it is to be called ASCII, regardless if the character set is proprietary or not.",
        "Microsoft Windows does not support the ANSI Standard x3.16. One can view block ASCIIs with a text editor using the font \"Terminal\", but it will not look exactly as it was intended by the artist. With a special ASCII/ANSI viewer, such as ACiDView for Windows (see ASCII and ANSI art viewers), one can see block ASCII and ANSI files properly. An example that illustrates the difference in appearance is part of this article. Alternatively, one could look at the file using the TYPE command in the command prompt.\n\n\"Amiga\"/\"Oldskool\" style ASCII art",
        "\"Amiga\"/\"Oldskool\" style ASCII art\n\nIn the art scene one popular ASCII style that used the 7-bit standard ASCII character set was the so-called \"Oldskool\" style. It is also called \"Amiga style\", due to its origin and widespread use on the Commodore Amiga computers. The style uses primarily the characters: _/\\-+=.()<>:. The \"oldskool\" art looks more like the outlined drawings of shapes than real pictures.\nThis is an example of \"Amiga style\" (also referred to as \"old school\" or \"oldskool\" style) scene ASCII art.",
        "The Amiga ASCII scene surfaced in 1992, seven years after the introduction of the Commodore Amiga 1000. The Commodore 64 PETSCII scene did not make the transition to the Commodore Amiga as the C64 demo and warez scenes did. Among the first Amiga ASCII art groups were ART, Epsilon Design, Upper Class, Unreal (later known as \"DeZign\"). This means that the text art scene on the Amiga was actually younger than the text art scene on the PC. The Amiga artists also did not call their ASCII art style \"Oldskool\". That term was introduced on the PC. When and by whom is unknown and lost in history.",
        "The Amiga style ASCII artwork was most often released in the form of a single text file, which included all the artwork (usually requested), with some design parts in between, as opposed to the PC art scene where the art work was released as a ZIP archive with separate text files for each piece. Furthermore, the releases were usually called \"ASCII collections\" and not \"art packs\" like on the IBM PC.\n\nIn text editors\n _ ___  _      _   \n|  ___|_ _/ ___| | ___| |_ \n| |_   | | |  _| |/ _ \\ __|\n|  _|  | | |_| | |  __/ |_ \n|_|   |___\\|_|\\___|\\__|\nThis kind of ASCII art is handmade in a text editor. Popular editors used to make this kind of ASCII art include Microsoft Notepad, CygnusEditor aka. CED (Amiga), and EditPlus2 (PC).",
        "Oldskool font example from the PC, which was taken from the ASCII editor FIGlet.\n\nNewskool style ASCII art\n\n\"Newskool\" is a popular form of ASCII art which capitalizes on character strings like \"$#Xxo\". In spite of its name, the style is not \"new\"; on the contrary, it was very old but fell out of favor and was replaced by \"Oldskool\" and \"Block\" style ASCII art. It was dubbed \"Newskool\" upon its comeback and renewed popularity at the end of the 1990s.\n\nNewskool changed significantly as the result of the introduction of extended proprietary characters. The classic 7-bit standard ASCII characters remain predominant, but the extended characters are often used for \"fine tuning\" and \"tweaking\". The style developed further after the introduction and adaptation of Unicode.",
        "Methods for generating ASCII art\n\nWhile some prefer to use a simple text editor to produce ASCII art, specialized programs, such as JavE have been developed that often simulate the features and tools in bitmap image editors. For Block ASCII art and ANSI art the artist almost always uses a special text editor, because to generate the required characters on a standard keyboard, one needs to know the Alt code for each character. For example, + will produce ▓, + will produce ▒, and + will produce ◘.",
        "The special text editors have sets of special characters assigned to existing keys on the keyboard. Popular DOS-based editors, such as TheDraw and ACiDDraw had multiple sets of different special characters mapped to the function keys to make the use of those characters easier for the artist who can switch between individual sets of characters via basic keyboard shortcuts. PabloDraw is one of the very few special ASCII/ANSI art editors that were developed for Windows.",
        "Image to text conversion\nOther programs allow one to automatically convert an image to text characters, which is a special case of vector quantization. A method is to sample the image down to grayscale with less than 8-bit precision, and then assign a character for each value. Such ASCII art generators often allow users to choose the intensity and contrast of the generated image.\n\nThree factors limit the fidelity of the conversion, especially of photographs:\n\n depth (solutions: reduced line spacing; bold style; block elements; colored background; good shading);\n sharpness (solutions: a longer text, with a smaller font; a greater set of characters; variable width fonts);\n ratio (solutions with compatibility issues: font with a square grid; stylized without extra line spacing).",
        "Examples of converted images are given below.\n\nThis is one of the earliest forms of ASCII art, dating back to the early days of the 1960s minicomputers and teletypes. During the 1970s, it was popular in US malls to get a t-shirt with a photograph printed in ASCII art on it from an automated kiosk containing a computer, and London's Science Museum had a similar service to produce printed portraits. With the advent of the web, HTML and CSS, many ASCII conversion programs will now quantize to a full RGB colorspace, enabling colorized ASCII images.",
        "Still images or movies can also be converted to ASCII on various UNIX and UNIX-like systems using the AAlib (black and white) or libcaca (colour) graphics device driver, or the VLC media player or mpv under Windows, Linux or macOS; all of which render the screen using ASCII symbols instead of pixels.\n\nThere are also a number of smartphone applications, such as ASCII cam for Android, that generate ASCII art in real-time using input from the phone's camera. These applications typically allow the ASCII art to be saved as either a text file or as an image made up of ASCII text.",
        "Non fixed-width ASCII\nMost ASCII art is created using a monospaced font, such as Courier, where all characters are identical in width. Early computers in use when ASCII art came into vogue had monospaced fonts for screen and printer displays. Today, most of the more commonly used fonts in word processors, web browsers and other programs are proportional fonts, such as Helvetica or Times Roman, where different widths are used for different characters. ASCII art drawn for a fixed width font will usually appear distorted, or even unrecognizable when displayed in a proportional font.",
        "Some ASCII artists have produced art for display in proportional fonts. These ASCIIs, rather than using a purely shade-based correspondence, use characters for slopes and borders and use block shading. These ASCIIs generally offer greater precision and attention to detail than fixed-width ASCIIs for a lower character count, although they are not as universally accessible since they are usually relatively font-specific.\n\nAnimated ASCII art\nAnimated ASCII art started in 1970 from so-called VT100 animations produced on VT100 terminals. These animations were simply text with cursor movement instructions, deleting and erasing the characters necessary to appear animated. Usually, they represented a long hand-crafted process undertaken by a single person to tell a story.",
        "Contemporary web browser revitalized animated ASCII art again. It became possible to display animated ASCII art via JavaScript or Java applets. Static ASCII art pictures are loaded and displayed one after another, creating the animation, very similar to how movie projectors unreel film reel and project the individual pictures on the big screen at movie theaters. A new term was born: \"ASCIImation\" – another name of animated ASCII art. A seminal work in this arena is the Star Wars ASCIImation. More complicated routines in JavaScript generate more elaborate ASCIImations showing effects like Morphing effects, star field emulations, fading effects and calculated images, such as mandelbrot fractal animations.",
        "There are now many tools and programs that can transform raster images into text symbols; some of these tools can operate on streaming video. For example, the music video for American singer Beck's song \"Black Tambourine\" is made up entirely of ASCII characters that approximate the original footage. VLC, a media player software, can render any video in colored ASCII through the libcaca module.\n\nOther text-based visual art\nThere are a variety of other types of art using text symbols from character sets other than ASCII and/or some form of color coding. Despite not being pure ASCII, these are still often referred to as \"ASCII art\". The character set portion designed specifically for drawing is known as the line drawing characters or pseudo-graphics.\n\nANSI art",
        "The IBM PC graphics hardware in text mode uses 16 bits per character. It supports a variety of configurations, but in its default mode under DOS they are used to give 256 glyphs from one of the IBM PC code pages (Code page 437 by default), 16 foreground colors, eight background colors, and a flash option. Such art can be loaded into screen memory directly. ANSI.SYS, if loaded, also allows such art to be placed on screen by outputting escape sequences that indicate movements of the screen cursor and color/flash changes. If this method is used then the art becomes known as ANSI art. The IBM PC code pages also include characters intended for simple drawing which often made this art appear much cleaner than that made with more traditional character sets. Plain text files are also seen with",
        "code pages also include characters intended for simple drawing which often made this art appear much cleaner than that made with more traditional character sets. Plain text files are also seen with these characters, though they have become far less common since Windows GUI text editors (using the Windows ANSI code page) have largely replaced DOS-based ones.",
        "Shift_JIS and Japan\n\nIn Japan, ASCII art (AA) is mainly known as Shift_JIS art. Shift JIS offers a larger selection of characters than plain ASCII (including characters from Japanese scripts and fullwidth forms of ASCII characters), and may be used for text-based art on Japanese websites.\n\nOften, such artwork is designed to be viewed with the default Japanese font on a platform, such as the proportional MS P Gothic.\n\nKaomoji\n\nUsers on ASCII-NET, in which the word ASCII refers to the ASCII Corporation rather than the American Standard Code for Information Interchange, popularised a style of  in which the face appears upright rather than rotated.\n\nUnicode",
        "Unicode\n\nUnicode would seem to offer the ultimate flexibility in producing text based art with its huge variety of characters. However, finding a suitable fixed-width font is likely to be difficult if a significant subset of Unicode is desired. (Modern UNIX-style operating systems do provide complete fixed-width Unicode fonts, e.g. for xterm. Windows has the Courier New font, which includes characters like ┌╥─╨┐♥☺Ƹ̵̡Ӝ̵̨̄Ʒ). Also, the common practice of rendering Unicode with a mixture of variable width fonts is likely to make predictable display hard, if more than a tiny subset of Unicode is used. ≽ʌⱷ҅ᴥⱷʌ≼ is an adequate representation of a cat's face in a font with varying character widths.\n\nControl and combining characters",
        "The combining characters mechanism of Unicode provides considerable ways of customizing the style, even obfuscating the text (e.g. via an online generator like Obfuscator, which focuses on the filters). Glitcher is one example of Unicode art, initiated in 2012: These symbols, intruding up and down, are made by combining lots of diacritical marks. It’s a kind of art. There’s quite a lot of artists who use the Internet or specific social networks as their canvas. The corresponding creations are favored in web browsers (thanks to their always better support), as geekily stylized usernames for social networks. With a fair compatibility, and among different online tools, [Facebook symbols] showcases various types of Unicode art, mainly for aesthetic purpose (Ɯıḳĭƥḙȡḯả Wîkipêȡıẚ Ẉǐḳîṗȅḍȉā",
        "social networks. With a fair compatibility, and among different online tools, [Facebook symbols] showcases various types of Unicode art, mainly for aesthetic purpose (Ɯıḳĭƥḙȡḯả Wîkipêȡıẚ Ẉǐḳîṗȅḍȉā Ẃįḵįṗẻḑìẵ Ẉĭḵɪṕḗdïą Ẇïƙỉpểɗĭà Ẅȉḱïṕȩđĩẵ etc.). Besides, the creations can be hand-crafted (by programming), or pasted from mobile applications (e.g. the category of 'fancy text' tools on Android). The underlying technique dates back to the old systems that incorporated control characters, though. E.g. the German composite ö would be imitated on ZX Spectrum by overwriting \" after backspace and o.",
        "Overprinting (surprint)\nIn the 1970s and early 1980s it was popular to produce a kind of text art that relied on overprinting. This could be produced either on a screen or on a printer by typing a character, backing up, and then typing another character, just as on a typewriter. This developed into sophisticated graphics in some cases, such as the PLATO system (circa 1973), where superscript and subscript allowed a wide variety of graphic effects. A common use was for emoticons, with WOBTAX and VICTORY both producing convincing smiley faces. Overprinting had previously been used on typewriters, but the low-resolution pixelation of characters on video terminals meant that overprinting here produced seamless pixel graphics, rather than visibly overstruck combinations of letters on paper.",
        "Beyond pixel graphics, this was also used for printing photographs, as the overall darkness of a particular character space dependent on how many characters, as well as the choice of character, were printed in a particular place. Thanks to the increased granularity of tone, photographs were often converted to this type of printout. Even manual typewriters or daisy wheel printers could be used. The technique has fallen from popularity since all cheap printers can easily print photographs, and a normal text file (or an e-mail message or Usenet posting) cannot represent overprinted text. However, something similar has emerged to replace it: shaded or colored ASCII art, using ANSI video terminal markup or color codes (such as those found in HTML, IRC, and many internet message boards) to add",
        "something similar has emerged to replace it: shaded or colored ASCII art, using ANSI video terminal markup or color codes (such as those found in HTML, IRC, and many internet message boards) to add a bit more tone variation. In this way, it is possible to create ASCII art where the characters only differ in color.",
        "See also\n Micrography\n Types and styles: Alt code, ASCII stereogram, box-drawing characters, emoticon, FILE ID.DIZ, .nfo (release info file)\n Pre-ASCII history: Calligram, Concrete poetry, Typewriter, Typewriter mystery game, Teleprinter, Radioteletype\n Related art: ANSI art, ASCII porn, ATASCII, Fax art, PETSCII, Shift JIS art, Text semigraphics\n Related context: Bulletin board system (BBS), Computer art scene, :Category:Artscene groups\n Software: AAlib, cowsay\n Unicode: Homoglyph, Duplicate characters in Unicode\n\nReferences\n\nFurther reading\n\n \n \n \n \n  (Polish translators: Ania Górecka [ag], Asia Mazur [as], Błażej Kozłowski [bug], Janusz [jp], Łukasz Dąbrowski [luk], Łukasz Tyrała [lt.], Łukasz Wilk [wilu], Marcin Gliński [fsc])\n\nExternal links",
        "External links\n\n \n media4u.ch - ASCII Art (ASCII Art Movie. The Matrix in ASCII Art)\n TexArt.io ASCII Art collection\n Textfiles.com archive\n Sixteen Colors ANSI Art and ASCII Art Archive\n Defacto2.net Scene NFO Files Archive\n Chris.com ASCII art collection\n \"As-Pixel Characters\" ASCII art collection \n ASCII Art Animation of Star Wars, \"ASCIIMATION\"\n ASCII Keyboard Art Collection\n Animasci\n Video to ASCII Demonstration in 4 stages\n\n \nComputer art\nDigital art\nNew media art\nInternet art\nMultimedia\nWikipedia articles with ASCII art"
    ],
    [
        "ASIC (disambiguation)\nIn the realm of electronic technology, ASIC stands for application-specific integrated circuit, an integrated circuit customized for a specific task.",
        "ASIC may also refer to:\n Accreditation Service for International Colleges, an educational accreditation agency in the UK\n Acid-sensing ion channels, a protein family\n Air and Space Interoperability Council , former name of the Air Force Interoperability Council\n Arfoire Syndicate of International Crime, the antagonist group in the video game Hyperdimension Neptunia mk2\n ASIC programming language, a dialect of BASIC\n Associated Signature Containers (ASiC), specifies the use of container structures to bind together one or more signed objects with either advanced electronic signatures or time-stamp tokens into one single digital container\n Association Scientifique Internationale pour le Café, a scientific organization based in France",
        "Association Scientifique Internationale pour le Café, a scientific organization based in France\n Australian Securities & Investments Commission, Australia's corporate regulator\n Aviation Security Identification Card, an Australian identification card",
        "See also\n Asics, an athletic equipment company"
    ],
    [
        "ASP\nASP may refer to:\n\nCombat\n ASP pistol\n ASP, Inc., law enforcement weapon manufacturer\n A type of extending baton\n Ammunition Supply Point, military storage facility for live ammunition and explosives\n\nComputing\n Active Server Pages, a web-scripting interface by Microsoft\n ASP.NET, a web-application framework by Microsoft\n Advanced Simple Profile, an MPEG-4 video codec profile\n Answer set programming, a declarative programming paradigm\n Application service provider, to customers over a network\n AppleTalk Session Protocol\n Association of Software Professionals\n Attached Support Processor, IBM hardware system\n Auxiliary storage pool, a feature of the IBM operating system\n\nEducation\n American School of Paris\n Jan Matejko Academy of Fine Arts, Kraków, Poland",
        "Education\n American School of Paris\n Jan Matejko Academy of Fine Arts, Kraków, Poland\n\nElectronics\n Audio Signal Processor, large-scale digital signal processor\n Anti-skip protection or electronic skip protection in CD playback\n Angle-sensitive pixel, a light sensor\n\nEntertainment\n ASP (band), a German gothic metal band\n ASP (Japanese group), Japanese idol girl group\n Adult service provider (disambiguation)\n Apparent Sensory Perception, a thought recording and reproduction device in William Gibson's fiction",
        "Medicine and biology\n Aspartic acid, α-amino acid used in the biosynthesis of proteins\n Acylation stimulating protein\n American Society for Photobiology\n Amnesic shellfish poisoning\n Complement component 3, a protein in the complement system\n Antimicrobial stewardship, effort to educate and persuade prescribers of antimicrobials to follow evidence-based prescribing\n American Society of Primatologists\n\nPolitics\n Act of the Scottish Parliament\n American Solidarity Party, a Christian Democratic political party in the United States\n Australian Sex Party\n Afro-Shirazi Party, political party in Tanzania\n Assembly for the Sovereignty of the Peoples, a political party in Bolivia\n Assembly of States Parties, the legislative body of the International Criminal Court",
        "Other uses\n Airborne Surveillance Platform, Indian defence project\n Albany Student Press, newspaper of the University at Albany, The State University of New York\n Alice Springs Airport (IATA airport code)\n Allegany State Park\n Appalachia Service Project, for housing improvement\n Arkansas State Police\n Asociación de Scouts del Perú\n Aspatria railway station, UK, National Rail code\n Assault system pod or A.S.P., for the G.I. Joe doll\n Association of Surfing Professionals, former name of World Surf League\n Assistant superintendent of police\n Astronomical Society of the Pacific\n Authorized service provider\n Avenal State Prison in California, USA\n Average selling price of goods\n\nSee also\n ASPS (disambiguation)\n Asp (disambiguation)"
    ],
    [
        "Aspect ratio\nThe aspect ratio of a geometric shape is the ratio of its sizes in different dimensions. For example, the aspect ratio of a rectangle is the ratio of its longer side to its shorter side—the ratio of width to height, when the rectangle is oriented as a \"landscape\".\n\nThe aspect ratio is most often expressed as two integer numbers separated by a colon (x:y), less commonly as a simple or decimal fraction. The values x and y do not represent actual widths and heights but, rather, the proportion between width and height. As an example, 8:5, 16:10, 1.6:1,  and 1.6 are all ways of representing the same aspect ratio.\n\nIn objects of more than two dimensions, such as hyperrectangles, the aspect ratio can still be defined as the ratio of the longest side to the shortest side.",
        "Applications and uses\nThe term is most commonly used with reference to:\n Graphic / image\n Image aspect ratio\n Display aspect ratio\n Paper size\n Standard photographic print sizes\n Motion picture film formats\n Standard ad size\n Pixel aspect ratio\n Photolithography: the aspect ratio of an etched, or deposited structure is the ratio of the height of its vertical side wall to its width.\n HARMST High Aspect Ratios allow the construction of tall microstructures without slant\n Tire code\n Tire sizing\n Turbocharger impeller sizing\n Wing aspect ratio of an aircraft or bird\n Astigmatism of an optical lens\n Nanorod dimensions\n Shape factor (image analysis and microscopy)\n Finite Element Analysis\n\nAspect ratios of simple shapes\n\nRectangles",
        "Aspect ratios of simple shapes\n\nRectangles\n\nFor a rectangle, the aspect ratio denotes the ratio of the width to the height of the rectangle. A square has the smallest possible aspect ratio of 1:1.\n\nExamples:\n 4:3 = 1.: Some (not all) 20th century computer monitors (VGA, XGA, etc.), standard-definition television\n : international paper sizes (ISO 216)\n 3:2 = 1.5: 35mm still camera film, iPhone (until iPhone 5) displays\n 16:10 = 1.6: commonly used widescreen computer displays (WXGA)\n Φ:1 = 1.618...: golden ratio, close to 16:10\n 5:3 = 1.: super 16 mm,  a standard film gauge in many European countries\n 16:9 = 1.: widescreen TV and most laptops\n 2:1 = 2: dominoes\n 64:27 = 2.: ultra-widescreen, 21:9\n 32:9 = 3.: super ultra-widescreen",
        "Ellipses\nFor an ellipse, the aspect ratio denotes the ratio of the major axis to the minor axis. An ellipse with an aspect ratio of 1:1 is a circle.\n\nAspect ratios of general shapes\nIn geometry, there are several alternative definitions to aspect ratios of general compact sets in a d-dimensional space:\n The diameter-width aspect ratio (DWAR) of a compact set is the ratio of its diameter to its width. A circle has the minimal DWAR which is 1. A square has a DWAR of .\n The cube-volume aspect ratio (CVAR) of a compact set is the d-th root of the ratio of the d-volume of the smallest enclosing axes-parallel d-cube, to the set's own d-volume. A square has the minimal CVAR which is 1. A circle has a CVAR of . An axis-parallel rectangle of width W and height H, where W>H, has a CVAR of .",
        "If the dimension d is fixed, then all reasonable definitions of aspect ratio are equivalent to within constant factors.\n\nNotations\nAspect ratios are mathematically expressed as x:y (pronounced \"x-to-y\").\n\nCinematographic aspect ratios are usually denoted as a (rounded) decimal multiple of width vs unit height, while photographic and videographic aspect ratios are usually defined and denoted by whole number ratios of width to height. In digital images there is a subtle distinction between the display aspect ratio (the image as displayed) and the storage aspect ratio (the ratio of pixel dimensions); see Distinctions.\n\nSee also\n Axial ratio\n Ratio\n Equidimensional ratios in 3D\n List of film formats\n Squeeze mapping\n Scale (ratio)\n Vertical orientation\n\nReferences\n\nRatios"
    ],
    [
        "Assembly language\nIn computer programming,  assembly language (alternatively assembler language or symbolic machine code), often referred to simply as assembly and commonly abbreviated as ASM or asm, is any low-level programming language with a very strong correspondence between the instructions in the language and the architecture's machine code instructions. Assembly language usually has one statement per machine instruction (1:1), but  constants, comments, assembler directives, symbolic labels of, e.g., memory locations, registers, and macros are generally also supported.",
        "The first assembly code in which a language is used to represent machine code instructions is found in Kathleen and Andrew Donald Booth's 1947 work, Coding for A.R.C.. Assembly code is converted into executable machine code by a utility program referred to as an assembler. The term \"assembler\" is generally attributed to Wilkes, Wheeler and Gill in their 1951 book The Preparation of Programs for an Electronic Digital Computer, who, however, used the term to mean \"a program that assembles another program consisting of several sections into a single program\". The conversion process is referred to as assembly, as in assembling the source code. The computational step when an assembler is processing a program is called assembly time.",
        "Because assembly depends on the machine code instructions, each assembly language is specific to a particular computer architecture.",
        "Sometimes there is more than one assembler for the same architecture, and sometimes an assembler is specific to an operating system or to particular operating systems. Most assembly languages do not provide specific syntax for operating system calls, and most assembly languages can be used universally with any operating system, as the language provides access to all the real capabilities of the processor, upon which all system call mechanisms ultimately rest. In contrast to assembly languages, most high-level programming languages are generally portable across multiple architectures but require interpreting or compiling, much more complicated tasks than assembling.",
        "In the first decades of computing, it was commonplace for both systems programming and application programming to take place entirely in assembly language.  While still irreplaceable for some purposes, the majority of programming is now conducted in higher-level interpreted and compiled languages.  In \"No Silver Bullet\", Fred Brooks summarised the effects of the switch away from assembly language programming: \"Surely the most powerful stroke for software productivity, reliability, and simplicity has been the progressive use of high-level languages for programming. Most observers credit that development with at least a factor of five in productivity, and with concomitant gains in reliability, simplicity, and comprehensibility.\"",
        "Today, it is typical to use small amounts of assembly language code within larger systems implemented in a higher-level language, for performance reasons or to interact directly with hardware in ways unsupported by the higher-level language.  For instance, just under 2% of version 4.9 of the Linux kernel source code is written in assembly; more than 97% is written in C.",
        "Assembly language syntax",
        "Assembly language uses a mnemonic to represent, e.g., each low-level machine instruction or opcode, each directive, typically also each architectural register, flag, etc. Some of the mnemonics may be built in and some user defined. Many operations require one or more operands in order to form a complete instruction. Most assemblers permit named constants, registers, and labels for program and memory locations, and can calculate expressions for operands. Thus, programmers are freed from tedious repetitive calculations and assembler programs are much more readable than machine code. Depending on the architecture, these elements may also be combined for specific instructions or addressing modes using offsets or other data as well as fixed addresses. Many assemblers offer additional",
        "on the architecture, these elements may also be combined for specific instructions or addressing modes using offsets or other data as well as fixed addresses. Many assemblers offer additional mechanisms to facilitate program development, to control the assembly process, and to aid debugging.",
        "Some are column oriented, with specific fields in specific columns; this was very common for machines using punched cards in the 1950s and early 1960s. Some assemblers have free-form syntax, with fields separated by delimiters, e.g., punctuation, white space. Some assemblers are hybrid, with, e.g., labels, in a specific column and other fields separated by delimiters; this became more common than column oriented syntax in the 1960s.",
        "Terminology\n A macro assembler is an assembler that includes a macroinstruction facility so that (parameterized) assembly language text can be represented by a name, and that name can be used to insert the expanded text into other code.\n Open code refers to any assembler input outside of a macro definition.",
        "A cross assembler (see also cross compiler) is an assembler that is run on a computer or operating system (the host system) of a different type from the system on which the resulting code is to run (the target system). Cross-assembling facilitates the development of programs for systems that do not have the resources to support software development, such as an embedded system or a microcontroller. In such a case, the resulting object code must be transferred to the target system, via read-only memory (ROM, EPROM, etc.), a programmer (when the read-only memory is integrated in the device, as in microcontrollers), or a data link using either an exact bit-by-bit copy of the object code or a text-based representation of that code (such as Intel hex or Motorola S-record).",
        "A high-level assembler is a program that provides language abstractions more often associated with high-level languages, such as advanced control structures (IF/THEN/ELSE, DO CASE, etc.) and high-level abstract data types, including structures/records, unions, classes, and sets.\n A microassembler is a program that helps prepare a microprogram, called firmware, to control the low level operation of a computer.",
        "A microassembler is a program that helps prepare a microprogram, called firmware, to control the low level operation of a computer.\n A meta-assembler is \"a program that accepts the syntactic and semantic description of an assembly language, and generates an assembler for that language\", or that accepts an assembler source file along with such a description and assembles the source file in accordance with that description. \"Meta-Symbol\" assemblers for the SDS 9 Series and SDS Sigma series of computers are meta-assemblers. Sperry Univac also provided a Meta-Assembler for the UNIVAC 1100/2200 series.\n inline assembler (or embedded assembler) is assembler code contained within a high-level language program. This is most often used in systems programs which need direct access to the hardware.",
        "Key concepts\n\nAssembler\nAn assembler program creates object code by translating combinations of mnemonics and syntax for operations and addressing modes into their numerical equivalents. This representation typically includes an operation code (\"opcode\") as well as other control bits and data. The assembler also calculates constant expressions and resolves symbolic names for memory locations and other entities. The use of symbolic references is a key feature of assemblers, saving tedious calculations and manual address updates after program modifications. Most assemblers also include macro facilities for performing textual substitution – e.g., to generate common short sequences of instructions as inline, instead of called subroutines.",
        "Some assemblers may also be able to perform some simple types of instruction set-specific optimizations. One concrete example of this may be the ubiquitous x86 assemblers from various vendors. Called jump-sizing, most of them are able to perform jump-instruction replacements (long jumps replaced by short or relative jumps) in any number of passes, on request. Others may even do simple rearrangement or insertion of instructions, such as some assemblers for RISC architectures that can help optimize a sensible instruction scheduling to exploit the CPU pipeline as efficiently as possible.",
        "Assemblers have been available since the 1950s, as the first step above machine language and before high-level programming languages such as Fortran, Algol, COBOL and Lisp. There have also been several classes of translators and semi-automatic code generators with properties similar to both assembly and high-level languages, with Speedcode as perhaps one of the better-known examples.",
        "There may be several assemblers with different syntax for a particular CPU or instruction set architecture. For instance, an instruction to add memory data to a register in a x86-family processor might be add eax,[ebx], in original Intel syntax, whereas this would be written addl (%ebx),%eax in the AT&T syntax used by the GNU Assembler. Despite different appearances, different syntactic forms generally generate the same numeric machine code. A single assembler may also have different modes in order to support variations in syntactic forms as well as their exact semantic interpretations (such as FASM-syntax, TASM-syntax, ideal mode, etc., in the special case of x86 assembly programming).",
        "Number of passes\nThere are two types of assemblers based on how many passes through the source are needed (how many times the assembler reads the source) to produce the object file.\n One-pass assemblers process the source code once.  For symbols used before they are defined, the assembler will emit \"errata\" after the eventual definition, telling the linker or the loader to patch the locations where the as yet undefined symbols had been used.\n Multi-pass assemblers create a table with all symbols and their values in the first passes, then use the table in later passes to generate code.",
        "Multi-pass assemblers create a table with all symbols and their values in the first passes, then use the table in later passes to generate code.\nIn both cases, the assembler must be able to determine the size of each instruction on the initial passes in order to calculate the addresses of subsequent symbols. This means that if the size of an operation referring to an operand defined later depends on the type or distance of the operand, the assembler will make a pessimistic estimate when first encountering the operation, and if necessary, pad it with one or more",
        "\"no-operation\" instructions in a later pass or the errata. In an assembler with peephole optimization, addresses may be recalculated between passes to allow replacing pessimistic code with code tailored to the exact distance from the target.",
        "The original reason for the use of one-pass assemblers was memory size and speed of assembly – often a second pass would require storing the symbol table in memory (to handle forward references), rewinding and rereading the program source on tape, or rereading a deck of cards or punched paper tape. Later computers with much larger memories (especially disc storage), had the space to perform all necessary processing without such re-reading. The advantage of the multi-pass assembler is that the absence of errata makes the linking process (or the program load if the assembler directly produces executable code) faster.",
        "Example: in the following code snippet, a one-pass assembler would be able to determine the address of the backward reference BKWD when assembling statement S2, but would not be able to determine the address of the forward reference FWD when assembling the branch statement S1; indeed, FWD may be undefined. A two-pass assembler would determine both addresses in pass 1, so they would be known when generating code in pass 2.\n    B    \n   ...\n    EQU *\n   ...\n   EQU *\n   ...\n     B",
        "High-level assemblers\nMore sophisticated high-level assemblers provide language abstractions such as:\n High-level procedure/function declarations and invocations\n Advanced control structures (IF/THEN/ELSE, SWITCH)\n High-level abstract data types, including structures/records, unions, classes, and sets\n Sophisticated macro processing (although available on ordinary assemblers since the late 1950s for, e.g., the IBM 700 series and IBM 7000 series, and since the 1960s for IBM System/360 (S/360), amongst other machines)\n Object-oriented programming features such as classes, objects, abstraction, polymorphism, and inheritance\nSee Language design below for more details.",
        "Assembly language\nA program written in assembly language consists of a series of mnemonic processor instructions and meta-statements (known variously as declarative operations, directives, pseudo-instructions, pseudo-operations and pseudo-ops), comments and data. Assembly language instructions usually consist of an opcode mnemonic followed by an operand, which might be a list of data, arguments or parameters.  Some instructions may be \"implied\", which means the data upon which the instruction operates is implicitly defined by the instruction itself—such an instruction does not take an operand.  The resulting statement is translated by an assembler into machine language instructions that can be loaded into memory and executed.",
        "For example, the instruction below tells an x86/IA-32 processor to move an immediate 8-bit value into a register. The binary code for this instruction is 10110 followed by a 3-bit identifier for which register to use. The identifier for the AL register is 000, so the following machine code loads the AL register with the data 01100001.\n 10110000 01100001\nThis binary computer code can be made more human-readable by expressing it in hexadecimal as follows.\n B0 61",
        "10110000 01100001\nThis binary computer code can be made more human-readable by expressing it in hexadecimal as follows.\n B0 61\nHere, B0 means 'Move a copy of the following value into AL, and 61 is a hexadecimal representation of the value 01100001, which is 97 in decimal. Assembly language for the 8086 family provides the mnemonic MOV (an abbreviation of move) for instructions such as this, so the machine code above can be written as follows in assembly language, complete with an explanatory comment if required, after the semicolon. This is much easier to read and to remember.\nMOV AL, 61h       ; Load AL with 97 decimal (61 hex)",
        "In some assembly languages (including this one) the same mnemonic, such as MOV, may be used for a family of related instructions for loading, copying and moving data, whether these are immediate values, values in registers, or memory locations pointed to by values in registers or by immediate (a.k.a. direct) addresses.  Other assemblers may use separate opcode mnemonics such as L for \"move memory to register\", ST for \"move register to memory\", LR for \"move register to register\", MVI for \"move immediate operand to memory\", etc.",
        "If the same mnemonic is used for different instructions, that means that the mnemonic corresponds to several different binary instruction codes, excluding data (e.g. the 61h in this example), depending on the operands that follow the mnemonic.  For example, for the x86/IA-32 CPUs, the Intel assembly language syntax MOV AL, AH represents an instruction that moves the contents of register AH into register AL. The hexadecimal form of this instruction is:\n 88 E0\nThe first byte, 88h, identifies a move between a byte-sized register and either another register or memory, and the second byte, E0h, is encoded (with three bit-fields) to specify that both operands are registers, the source is AH, and the destination is AL.",
        "In a case like this where the same mnemonic can represent more than one binary instruction, the assembler determines which instruction to generate by examining the operands.  In the first example, the operand 61h is a valid hexadecimal numeric constant and is not a valid register name, so only the B0 instruction can be applicable.  In the second example, the operand AH is a valid register name and not a valid numeric constant (hexadecimal, decimal, octal, or binary), so only the 88 instruction can be applicable.",
        "Assembly languages are always designed so that this sort of lack of ambiguity is universally enforced by their syntax.  For example, in the Intel x86 assembly language, a hexadecimal constant must start with a numeral digit, so that the hexadecimal number 'A' (equal to decimal ten) would be written as 0Ah or 0AH, not AH, specifically so that it cannot appear to be the name of register AH.  (The same rule also prevents ambiguity with the names of registers BH, CH, and DH, as well as with any user-defined symbol that ends with the letter H and otherwise contains only characters that are hexadecimal digits, such as the word \"BEACH\".)",
        "Returning to the original example, while the x86 opcode 10110000 (B0) copies an 8-bit value into the AL register, 10110001 (B1) moves it into CL and 10110010 (B2) does so into DL. Assembly language examples for these follow.\nMOV AL, 1h        ; Load AL with immediate value 1\nMOV CL, 2h        ; Load CL with immediate value 2\nMOV DL, 3h        ; Load DL with immediate value 3\nThe syntax of MOV can also be more complex as the following examples show.\nMOV EAX, [EBX]\t  ; Move the 4 bytes in memory at the address contained in EBX into EAX\nMOV [ESI+EAX], CL ; Move the contents of CL into the byte at address ESI+EAX\nMOV DS, DX        ; Move the contents of DX into segment register DS",
        "In each case, the MOV mnemonic is translated directly into one of the opcodes 88-8C, 8E, A0-A3, B0-BF, C6 or C7 by an assembler, and the programmer normally does not have to know or remember which.",
        "Transforming assembly language into machine code is the job of an assembler, and the reverse can at least partially be achieved by a disassembler. Unlike high-level languages, there is a one-to-one correspondence between many simple assembly statements and machine language instructions. However, in some cases, an assembler may provide pseudoinstructions (essentially macros) which expand into several machine language instructions to provide commonly needed functionality. For example, for a machine that lacks a \"branch if greater or equal\" instruction, an assembler may provide a pseudoinstruction that expands to the machine's \"set if less than\" and \"branch if zero (on the result of the set instruction)\". Most full-featured assemblers also provide a rich macro language (discussed below)",
        "that expands to the machine's \"set if less than\" and \"branch if zero (on the result of the set instruction)\". Most full-featured assemblers also provide a rich macro language (discussed below) which is used by vendors and programmers to generate more complex code and data sequences. Since the information about pseudoinstructions and macros defined in the assembler environment is not present in the object program, a disassembler cannot reconstruct the macro and pseudoinstruction invocations but can only disassemble the actual machine instructions that the assembler generated from those abstract assembly-language entities. Likewise, since comments in the assembly language source file are ignored by the assembler and have no effect on the object code it generates, a disassembler is always",
        "assembly-language entities. Likewise, since comments in the assembly language source file are ignored by the assembler and have no effect on the object code it generates, a disassembler is always completely unable to recover source comments.",
        "Each computer architecture has its own machine language.  Computers differ in the number and type of operations they support, in the different sizes and numbers of registers, and in the representations of data in storage. While most general-purpose computers are able to carry out essentially the same functionality, the ways they do so differ; the corresponding assembly languages reflect these differences.\n\nMultiple sets of mnemonics or assembly-language syntax may exist for a single instruction set, typically instantiated in different assembler programs. In these cases, the most popular one is usually that supplied by the CPU manufacturer and used in its documentation.",
        "Two examples of CPUs that have two different sets of mnemonics are the Intel 8080 family and the Intel 8086/8088.  Because Intel claimed copyright on its assembly language mnemonics (on each page of their documentation published in the 1970s and early 1980s, at least), some companies that independently produced CPUs compatible with Intel instruction sets invented their own mnemonics.  The Zilog Z80 CPU, an enhancement of the Intel 8080A, supports all the 8080A instructions plus many more; Zilog invented an entirely new assembly language, not only for the new instructions but also for all of the 8080A instructions.  For example, where Intel uses the mnemonics MOV, MVI, LDA, STA, LXI, LDAX, STAX, LHLD, and SHLD for various data transfer instructions, the Z80 assembly language uses the",
        "of the 8080A instructions.  For example, where Intel uses the mnemonics MOV, MVI, LDA, STA, LXI, LDAX, STAX, LHLD, and SHLD for various data transfer instructions, the Z80 assembly language uses the mnemonic LD for all of them.  A similar case is the NEC V20 and V30 CPUs, enhanced copies of the Intel 8086 and 8088, respectively.  Like Zilog with the Z80, NEC invented new mnemonics for all of the 8086 and 8088 instructions, to avoid accusations of infringement of Intel's copyright.  (It is questionable whether such copyrights can be valid, and later CPU companies such as AMD and Cyrix republished Intel's x86/IA-32 instruction mnemonics exactly with neither permission nor legal penalty.)  It is doubtful whether in practice many people who programmed the V20 and V30 actually wrote in NEC's",
        "Intel's x86/IA-32 instruction mnemonics exactly with neither permission nor legal penalty.)  It is doubtful whether in practice many people who programmed the V20 and V30 actually wrote in NEC's assembly language rather than Intel's; since any two assembly languages for the same instruction set architecture are isomorphic (somewhat like English and Pig Latin), there is no requirement to use a manufacturer's own published assembly language with that manufacturer's products.",
        "Language design\n\nBasic elements\nThere is a large degree of diversity in the way the authors of assemblers categorize statements and in the nomenclature that they use. In particular, some describe anything other than a machine mnemonic or extended mnemonic as a pseudo-operation (pseudo-op). A typical assembly language consists of 3 types of instruction statements that are used to define program operations:\n\n Opcode mnemonics\n Data definitions\n Assembly directives",
        "Opcode mnemonics and extended mnemonics",
        "Instructions (statements) in assembly language are generally very simple, unlike those in high-level languages. Generally, a mnemonic is a symbolic name for a single executable machine language instruction (an opcode), and there is at least one opcode mnemonic defined for each machine language instruction. Each instruction typically consists of an operation or opcode plus zero or more operands. Most instructions refer to a single value or a pair of values.  Operands can be immediate (value coded in the instruction itself), registers specified in the instruction or implied, or the addresses of data located elsewhere in storage. This is determined by the underlying processor architecture: the assembler merely reflects how this architecture works. Extended mnemonics are often used to specify",
        "data located elsewhere in storage. This is determined by the underlying processor architecture: the assembler merely reflects how this architecture works. Extended mnemonics are often used to specify a combination of an opcode with a specific operand, e.g., the System/360 assemblers use  as an extended mnemonic for  with a mask of 15 and  (\"NO OPeration\" – do nothing for one step) for  with a mask of 0.",
        "Extended mnemonics are often used to support specialized uses of instructions, often for purposes not obvious from the instruction name. For example, many CPU's do not have an explicit NOP instruction, but do have instructions that can be used for the purpose. In 8086 CPUs the instruction  is used for , with  being a pseudo-opcode to encode the instruction . Some disassemblers recognize this and will decode the  instruction as . Similarly, IBM assemblers for System/360 and System/370 use the extended mnemonics  and  for  and  with zero masks.  For the SPARC architecture, these are known as synthetic instructions.",
        "Some assemblers also support simple built-in macro-instructions that generate two or more machine instructions. For instance, with some Z80 assemblers the instruction  is recognized to generate  followed by . These are sometimes known as pseudo-opcodes.\n\nMnemonics are arbitrary symbols; in 1985 the IEEE published Standard 694 for a uniform set of mnemonics to be used by all assemblers. The standard has since been withdrawn.",
        "Mnemonics are arbitrary symbols; in 1985 the IEEE published Standard 694 for a uniform set of mnemonics to be used by all assemblers. The standard has since been withdrawn.\n\nData directives\nThere are instructions used to define data elements to hold data and variables.  They define the type of data, the length and the alignment of data. These instructions can also define whether the data is available to outside programs (programs assembled separately) or only to the program in which the data section is defined. Some assemblers classify these as pseudo-ops.",
        "Assembly directives\nAssembly directives, also called pseudo-opcodes, pseudo-operations or pseudo-ops, are commands given to an assembler \"directing it to perform operations other than assembling instructions\". Directives affect how the assembler operates and \"may affect the object code, the symbol table, the listing file, and the values of internal assembler parameters\". Sometimes the term pseudo-opcode is reserved for directives that generate object code, such as those that generate data.",
        "The names of pseudo-ops often start with a dot to distinguish them from machine instructions.  Pseudo-ops can make the assembly of the program dependent on parameters input by a programmer, so that one program can be assembled in different ways, perhaps for different applications. Or, a pseudo-op can be used to manipulate presentation of a program to make it easier to read and maintain. Another common use of pseudo-ops is to reserve storage areas for run-time data and optionally initialize their contents to known values.",
        "Symbolic assemblers let programmers associate arbitrary names (labels or symbols) with memory locations and various constants. Usually, every constant and variable is given a name so instructions can reference those locations by name, thus promoting self-documenting code. In executable code, the name of each subroutine is associated with its entry point, so any calls to a subroutine can use its name. Inside subroutines, GOTO destinations are given labels. Some assemblers support local symbols which are often lexically distinct from normal symbols (e.g., the use of \"10$\" as a GOTO destination).",
        "Some assemblers, such as NASM, provide flexible symbol management, letting programmers manage different namespaces, automatically calculate offsets within data structures, and assign labels that refer to literal values or the result of simple computations performed by the assembler. Labels can also be used to initialize constants and variables with relocatable addresses.",
        "Assembly languages, like most other computer languages, allow comments to be added to program source code that will be ignored during assembly. Judicious commenting is essential in assembly language programs, as the meaning and purpose of a sequence of binary machine instructions can be difficult to determine. The \"raw\" (uncommented) assembly language generated by compilers or disassemblers is quite difficult to read when changes must be made.",
        "Macros",
        "Many assemblers support predefined macros, and others support programmer-defined (and repeatedly re-definable) macros involving sequences of text lines in which variables and constants are embedded. The macro definition is most commonly a mixture of assembler statements, e.g., directives, symbolic machine instructions, and templates for assembler statements. This sequence of text lines may include opcodes or directives. Once a macro has been defined its name may be used in place of a mnemonic. When the assembler processes such a statement, it replaces the statement with the text lines associated with that macro, then processes them as if they existed in the source code file (including, in some assemblers, expansion of any macros existing in the replacement text). Macros in this sense date",
        "with that macro, then processes them as if they existed in the source code file (including, in some assemblers, expansion of any macros existing in the replacement text). Macros in this sense date to IBM autocoders of the 1950s.",
        "Macro assemblers typically have directives to, e.g., define macros, define variables, set variables to the result of an arithmetic, logical or string expression, iterate, conditionally generate code. Some of those directives may be restricted to use within a macro definition, e.g., MEXIT in HLASM, while others may be permitted within open code (outside macro definitions), e.g., AIF and COPY in HLASM.",
        "In assembly language, the term \"macro\" represents a more comprehensive concept than it does in some other contexts, such as the pre-processor in the C programming language, where its #define directive typically is used to create short single line macros. Assembler macro instructions, like macros in PL/I and some other languages, can be lengthy \"programs\" by themselves, executed by interpretation by the assembler during assembly.",
        "Since macros can have 'short' names but expand to several or indeed many lines of code, they can be used to make assembly language programs appear to be far shorter, requiring fewer lines of source code, as with higher level languages. They can also be used to add higher levels of structure to assembly programs, optionally introduce embedded debugging code via parameters and other similar features.",
        "Macro assemblers often allow macros to take parameters. Some assemblers include quite sophisticated macro languages, incorporating such high-level language elements as optional parameters, symbolic variables, conditionals, string manipulation, and arithmetic operations, all usable during the execution of a given macro, and allowing macros to save context or exchange information. Thus a macro might generate numerous assembly language instructions or data definitions, based on the macro arguments. This could be used to generate record-style data structures or \"unrolled\" loops, for example, or could generate entire algorithms based on complex parameters. For instance, a \"sort\" macro could accept the specification of a complex sort key and generate code crafted for that specific key, not",
        "or could generate entire algorithms based on complex parameters. For instance, a \"sort\" macro could accept the specification of a complex sort key and generate code crafted for that specific key, not needing the run-time tests that would be required for a general procedure interpreting the specification. An organization using assembly language that has been heavily extended using such a macro suite can be considered to be working in a higher-level language since such programmers are not working with a computer's lowest-level conceptual elements. Underlining this point, macros were used to implement an early virtual machine in SNOBOL4 (1967), which was written in the SNOBOL Implementation Language (SIL), an assembly language for a virtual machine. The target machine would translate this to",
        "an early virtual machine in SNOBOL4 (1967), which was written in the SNOBOL Implementation Language (SIL), an assembly language for a virtual machine. The target machine would translate this to its native code using a macro assembler. This allowed a high degree of portability for the time.",
        "Macros were used to customize large scale software systems for specific customers in the mainframe era and were also used by customer personnel to satisfy their employers' needs by making specific versions of manufacturer operating systems. This was done, for example, by systems programmers working with IBM's Conversational Monitor System / Virtual Machine (VM/CMS) and with IBM's \"real time transaction processing\" add-ons, Customer Information Control System CICS, and ACP/TPF, the airline/financial system that began in the 1970s and still runs many large computer reservation systems (CRS) and credit card systems today.",
        "It is also possible to use solely the macro processing abilities of an assembler to generate code written in completely different languages, for example, to generate a version of a program in COBOL using a pure macro assembler program containing lines of COBOL code inside assembly time operators instructing the assembler to generate arbitrary code. IBM OS/360 uses macros to perform system generation. The user specifies options by coding a series of assembler macros.  Assembling these macros generates a job stream to build the system, including job control language and utility control statements.",
        "This is because, as was realized in the 1960s, the concept of \"macro processing\" is independent of the concept of \"assembly\", the former being in modern terms more word processing, text processing, than generating object code. The concept of macro processing appeared, and appears, in the C programming language, which supports \"preprocessor instructions\" to set variables, and make conditional tests on their values. Unlike certain previous macro processors inside assemblers, the C preprocessor is not Turing-complete because it lacks the ability to either loop or \"go to\", the latter allowing programs to loop.\n\nDespite the power of macro processing, it fell into disuse in many high level languages (major exceptions being C, C++ and PL/I) while remaining a perennial for assemblers.",
        "Despite the power of macro processing, it fell into disuse in many high level languages (major exceptions being C, C++ and PL/I) while remaining a perennial for assemblers.\n\nMacro parameter substitution is strictly by name: at macro processing time, the value of a parameter is textually substituted for its name. The most famous class of bugs resulting was the use of a parameter that itself was an expression and not a simple name when the macro writer expected a name. In the macro:\n\n foo: macro a\n load a*b",
        "foo: macro a\n load a*b\n\nthe intention was that the caller would provide the name of a variable, and the \"global\" variable or constant b would be used to multiply \"a\". If foo is called with the parameter a-c, the macro expansion of load a-c*b occurs.  To avoid any possible ambiguity, users of macro processors can parenthesize formal parameters inside macro definitions, or callers can parenthesize the input parameters.\n\nSupport for structured programming",
        "Support for structured programming\n\nPackages of macros have been written providing structured programming elements to encode execution flow. The earliest example of this approach was in the Concept-14 macro set, originally proposed by Harlan Mills (March 1970), and implemented by Marvin Kessler at IBM's Federal Systems Division, which provided IF/ELSE/ENDIF and similar control flow blocks for OS/360 assembler programs. This was a way to reduce or eliminate the use of GOTO operations in assembly code, one of the main factors causing spaghetti code in assembly language. This approach was widely accepted in the early 1980s (the latter days of large-scale assembly language use). IBM's High Level Assembler Toolkit includes such a macro package.",
        "A curious design was A-Natural, a \"stream-oriented\" assembler for 8080/Z80, processors from Whitesmiths Ltd. (developers of the Unix-like Idris operating system, and what was reported to be the first commercial C compiler). The language was classified as an assembler because it worked with raw machine elements such as opcodes, registers, and memory references; but it incorporated an expression syntax to indicate execution order. Parentheses and other special symbols, along with block-oriented structured programming constructs, controlled the sequence of the generated instructions. A-natural was built as the object language of a C compiler, rather than for hand-coding, but its logical syntax won some fans.",
        "There has been little apparent demand for more sophisticated assemblers since the decline of large-scale assembly language development. In spite of that, they are still being developed and applied in cases where resource constraints or peculiarities in the target system's architecture prevent the effective use of higher-level languages.\n\nAssemblers with a strong macro engine allow structured programming via macros, such as the switch macro provided with the Masm32 package (this code is a complete program):\n\ninclude \\masm32\\include\\masm32rt.inc\t; use the Masm32 library",
        "include \\masm32\\include\\masm32rt.inc\t; use the Masm32 library\n\n.code\ndemomain:\n  REPEAT 20\n\tswitch rv(nrandom, 9)\t; generate a number between 0 and 8\n\tmov ecx, 7\n\tcase 0\n\t\tprint \"case 0\"\n\tcase ecx\t\t\t\t; in contrast to most other programming languages,\n\t\tprint \"case 7\"\t\t; the Masm32 switch allows \"variable cases\"\n\tcase 1 .. 3\n\t\t.if eax==1\n\t\t\tprint \"case 1\"\n\t\t.elseif eax==2\n\t\t\tprint \"case 2\"\n\t\t.else\n\t\t\tprint \"cases 1 to 3: other\"\n\t\t.endif\n\tcase 4, 6, 8\n\t\tprint \"cases 4, 6 or 8\"\n\tdefault\n\t\tmov ebx, 19\t\t     ; print 20 stars\n\t\t.Repeat\n\t\t\tprint \"*\"\n\t\t\tdec ebx\n\t\t.Until Sign?\t\t ; loop until the sign flag is set\n\tendsw\n\tprint chr$(13, 10)\n  ENDM\n  exit\nend demomain\n\nUse of assembly language",
        "Use of assembly language\n\nHistorical perspective\nAssembly languages were not available at the time when the stored-program computer was introduced. Kathleen Booth \"is credited with inventing assembly language\" based on theoretical work she began in 1947, while working on the ARC2 at Birkbeck, University of London following consultation by Andrew Booth (later her husband) with mathematician John von Neumann and physicist Herman Goldstine at the Institute for Advanced Study.",
        "In late 1948, the Electronic Delay Storage Automatic Calculator (EDSAC) had an assembler (named \"initial orders\") integrated into its bootstrap program. It used one-letter mnemonics developed by David Wheeler, who is credited by the IEEE Computer Society as the creator of the first \"assembler\". Reports on the EDSAC introduced the term \"assembly\" for the process of combining fields into an instruction word. SOAP (Symbolic Optimal Assembly Program) was an assembly language for the IBM 650 computer written by Stan Poley in 1955.",
        "Assembly languages eliminate much of the error-prone, tedious, and time-consuming first-generation programming needed with the earliest computers, freeing programmers from tedium such as remembering numeric codes and calculating addresses. They were once widely used for all sorts of programming. However, by the late 1950s, their use had largely been supplanted by higher-level languages, in the search for improved programming productivity. Today, assembly language is still used for direct hardware manipulation, access to specialized processor instructions, or to address critical performance issues. Typical uses are device drivers, low-level embedded systems, and real-time systems (see ).",
        "Numerous programs have been written entirely in assembly language. The Burroughs MCP (1961) was the first computer for which an operating system was not developed entirely in assembly language; it was written in Executive Systems Problem Oriented Language (ESPOL), an Algol dialect. Many commercial applications were written in assembly language as well, including a large amount of the IBM mainframe software written by large corporations. COBOL, FORTRAN and some PL/I eventually displaced much of this work, although a number of large organizations retained assembly-language application infrastructures well into the 1990s.",
        "Assembly language has long been the primary development language for 8-bit home computers such Atari 8-bit family, Apple II, MSX, ZX Spectrum, and Commodore 64. Interpreted BASIC dialects on these systems offer insufficient execution speed and insufficient facilities to take full advantage of the available hardware. These systems have severe resource constraints,  idiosyncratic memory and display architectures, and provide limited system services. There are also few high-level language compilers suitable for microcomputer use. Similarly, assembly language is the default choice for 8-bit consoles such as the Atari 2600 and Nintendo Entertainment System.",
        "Key software for IBM PC compatibles was written in assembly language, such as MS-DOS, Turbo Pascal, and the Lotus 1-2-3 spreadsheet. As computer speed grew exponentially, assembly language became a tool for speeding up parts of programs, such as the rendering of Doom, rather than a dominant development language. In the 1990s, assembly language was used to get performance out of systems such as the Sega Saturn  and as the primary language for arcade hardware based on the TMS34010 integrated CPU/GPU such as Mortal Kombat and NBA Jam.\n\nCurrent usage\nThere has been debate over the usefulness and performance of assembly language relative to high-level languages.\n\nAlthough assembly language has specific niche uses where it is important (see below), there are other tools for optimization.",
        ", the TIOBE index of programming language popularity ranks assembly language at 11, ahead of Visual Basic, for example. Assembler can be used to optimize for speed or optimize for size. In the case of speed optimization, modern optimizing compilers are claimed to render high-level languages into code that can run as fast as hand-written assembly, despite the counter-examples that can be found. The complexity of modern processors and memory sub-systems makes effective optimization increasingly difficult for compilers, as well as for assembly programmers. Moreover, increasing processor performance has meant that most CPUs sit idle most of the time, with delays caused by predictable bottlenecks such as cache misses, I/O operations and paging. This has made raw code execution speed a",
        "performance has meant that most CPUs sit idle most of the time, with delays caused by predictable bottlenecks such as cache misses, I/O operations and paging. This has made raw code execution speed a non-issue for many programmers.",
        "There are some situations in which developers might choose to use assembly language:",
        "Writing code for systems with  that have limited high-level language options such as the Atari 2600, Commodore 64, and graphing calculators. Programs for these computers of the 1970s and 1980s are often written in the context of demoscene or retrogaming subcultures.\n Code that must interact directly with the hardware, for example in device drivers and interrupt handlers.\n In an embedded processor or DSP, high-repetition interrupts require the shortest number of cycles per interrupt, such as an interrupt that occurs 1000 or 10000 times a second.",
        "In an embedded processor or DSP, high-repetition interrupts require the shortest number of cycles per interrupt, such as an interrupt that occurs 1000 or 10000 times a second.\n Programs that need to use processor-specific instructions not implemented in a compiler. A common example is the bitwise rotation instruction at the core of many encryption algorithms, as well as querying the parity of a byte or the 4-bit carry of an addition.\n A stand-alone executable of compact size is required that must execute without recourse to the run-time components or libraries associated with a high-level language. Examples have included firmware for telephones, automobile fuel and ignition systems, air-conditioning control systems, security systems, and sensors.",
        "Programs with performance-sensitive inner loops, where assembly language provides optimization opportunities that are difficult to achieve in a high-level language. For example, linear algebra with BLAS or discrete cosine transformation (e.g. SIMD assembly version from x264).\n Programs that create vectorized functions for programs in higher-level languages such as C.  In the higher-level language this is sometimes aided by compiler intrinsic functions which map directly to SIMD mnemonics, but nevertheless result in a one-to-one assembly conversion specific for the given vector processor.",
        "Real-time programs such as simulations, flight navigation systems, and medical equipment. For example, in a fly-by-wire system, telemetry must be interpreted and acted upon within strict time constraints. Such systems must eliminate sources of unpredictable delays, which may be created by (some) interpreted languages, automatic garbage collection, paging operations, or preemptive multitasking. However, some higher-level languages incorporate run-time components and operating system interfaces that can introduce such delays. Choosing assembly or lower-level languages for such systems gives programmers greater visibility and control over processing details.\n Cryptographic algorithms that must always take strictly the same time to execute, preventing timing attacks.",
        "Cryptographic algorithms that must always take strictly the same time to execute, preventing timing attacks.\n Video encoders and decoders such as rav1e (an encoder for AV1) and dav1d (the reference decoder for AV1) contain assembly to leverage AVX2 and ARM Neon instructions when available.\n Modify and extend legacy code written for IBM mainframe computers.\n Situations where complete control over the environment is required, in extremely high-security situations where nothing can be taken for granted.\n Computer viruses, bootloaders, certain device drivers, or other items very close to the hardware or low-level operating system.\n Instruction set simulators for monitoring, tracing and debugging where additional overhead is kept to a minimum.",
        "Instruction set simulators for monitoring, tracing and debugging where additional overhead is kept to a minimum.\n Situations where no high-level language exists, on a new or specialized processor for which no cross compiler is available.\n Reverse engineering and modifying program files such as:\n existing binaries that may or may not have originally been written in a high-level language, for example when trying to recreate programs for which source code is not available or has been lost, or cracking copy protection of proprietary software.\n Video games (also termed ROM hacking), which is possible via several methods. The most widely employed method is altering program code at the assembly language level.",
        "Assembly language is still taught in most computer science and electronic engineering programs. Although few programmers today regularly work with assembly language as a tool, the underlying concepts remain important. Such fundamental topics as binary arithmetic, memory allocation, stack processing, character set encoding, interrupt processing, and compiler design would be hard to study in detail without a grasp of how a computer operates at the hardware level. Since a computer's behavior is fundamentally defined by its instruction set, the logical way to learn such concepts is to study an assembly language. Most modern computers have similar instruction sets. Therefore, studying a single assembly language is sufficient to learn: I) the basic concepts; II) to recognize situations where",
        "assembly language. Most modern computers have similar instruction sets. Therefore, studying a single assembly language is sufficient to learn: I) the basic concepts; II) to recognize situations where the use of assembly language might be appropriate; and III) to see how efficient executable code can be created from high-level languages.",
        "Typical applications\n Assembly language is typically used in a system's boot code, the low-level code that initializes and tests the system hardware prior to booting the operating system and is often stored in ROM. (BIOS on IBM-compatible PC systems and CP/M is an example.)\n Assembly language is often used for low-level code, for instance for operating system kernels, which cannot rely on the availability of pre-existing system calls and must indeed implement them for the particular processor architecture on which the system will be running.\n Some compilers translate high-level languages into assembly first before fully compiling, allowing the assembly code to be viewed for debugging and optimization purposes.",
        "Some compilers translate high-level languages into assembly first before fully compiling, allowing the assembly code to be viewed for debugging and optimization purposes.\n Some compilers for relatively low-level languages, such as Pascal or C, allow the programmer to embed assembly language directly in the source code (so called inline assembly). Programs using such facilities can then construct abstractions using different assembly language on each hardware platform. The system's portable code can then use these processor-specific components through a uniform interface.",
        "Assembly language is useful in reverse engineering. Many programs are distributed only in machine code form which is straightforward to translate into assembly language by a disassembler, but more difficult to translate into a higher-level language through a decompiler. Tools such as the Interactive Disassembler make extensive use of disassembly for such a purpose. This technique is used by hackers to crack commercial software, and competitors to produce software with similar results from competing companies.\n Assembly language is used to enhance speed of execution, especially in early personal computers with limited processing power and RAM.",
        "Assembly language is used to enhance speed of execution, especially in early personal computers with limited processing power and RAM.\n Assemblers can be used to generate blocks of data, with no high-level language overhead, from formatted and commented source code, to be used by other code.",
        "See also\n\n Compiler\n Comparison of assemblers\n Disassembler\n Hexadecimal\n Instruction set architecture\n Little man computer – an educational computer model with a base-10 assembly language\n Nibble\n Typed assembly language\n\nNotes\n\nReferences\n\nFurther reading\n  \n \n  (2+xiv+270+6 pages)\n \n \n \n \n \n \n \n \n \n \n  (\"An online book full of helpful ASM info, tutorials and code examples\" by the ASM Community, archived at the internet archive.)\n\nExternal links\n\n \n Unix Assembly Language Programming \n Linux Assembly\n PPR: Learning Assembly Language\n NASM – The Netwide Assembler BSD-licensed x86 assembler\n Assembly Language Programming Examples\n Authoring Windows Applications In Assembly Language\n Assembly Optimization Tips by Mark Larson",
        "Assembly language\nComputer-related introductions in 1949\nEmbedded systems\nLow-level programming languages\nProgramming language implementation\nProgramming languages created in 1949"
    ],
    [
        "Assembly line\nAn assembly line is a manufacturing process (often called a progressive assembly) in which parts (usually interchangeable parts) are added as the semi-finished assembly moves from workstation to workstation where the parts are added in sequence until the final assembly is produced. By mechanically moving the parts to the assembly work and moving the semi-finished assembly from work station to work station, a finished product can be assembled faster and with less labor than by having workers carry parts to a stationary piece for assembly.\n\nAssembly lines are common methods of assembling complex items such as automobiles and other transportation equipment, household appliances and electronic goods.\n\nWorkers in charge of the works of assembly line are called assemblers.",
        "Workers in charge of the works of assembly line are called assemblers.\n\nConcepts\n\nAssembly lines are designed for the sequential organization of workers, tools or machines, and parts. The motion of workers is minimized to the extent possible. All parts or assemblies are handled either by conveyors or motorized vehicles such as forklifts, or gravity, with no manual trucking. Heavy lifting is done by machines such as overhead cranes or forklifts. Each worker typically performs one simple operation unless job rotation strategies are applied.\n\nAccording to Henry Ford:",
        "Designing assembly lines is a well-established mathematical challenge, referred to as an assembly line balancing problem. In the simple assembly line balancing problem the aim is to assign a set of tasks that need to be performed on the workpiece to a sequence of workstations. Each task requires a given task duration for completion. The assignment of tasks to stations is typically limited by two constraints: (1) a precedence graph which indicates what other tasks need to be completed before a particular task can be initiated (e.g. not putting in a screw before drilling the hole) and (2) a cycle time which restricts the sum of task processing times which can be completed at each workstation before the work-piece is moved to the next station by the conveyor belt. Major planning problems for",
        "time which restricts the sum of task processing times which can be completed at each workstation before the work-piece is moved to the next station by the conveyor belt. Major planning problems for operating assembly lines include supply chain integration, inventory control and production scheduling.",
        "Simple example\n\nConsider the assembly of a car: assume that certain steps in the assembly line are to install the engine, install the hood, and install the wheels (in that order, with arbitrary interstitial steps); only one of these steps can be done at a time. In traditional production, only one car would be assembled at a time. If engine installation takes 20 minutes, hood installation takes five minutes, and wheels installation takes 10 minutes, then a car can be produced every 35 minutes.\n\nIn an assembly line, car assembly is split between several stations, all working simultaneously. When a station is finished with a car, it passes it on to the next. By having three stations, three cars can be operated on at the same time, each at a different stage of assembly.",
        "After finishing its work on the first car, the engine installation crew can begin working on the second car. While the engine installation crew works on the second car, the first car can be moved to the hood station and fitted with a hood, then to the wheels station and be fitted with wheels. After the engine has been installed on the second car, the second car moves to the hood assembly. At the same time, the third car moves to the engine assembly. When the third car's engine has been mounted, it then can be moved to the hood station; meanwhile, subsequent cars (if any) can be moved to the engine installation station.",
        "Assuming no loss of time when moving a car from one station to another, the longest stage on the assembly line determines the throughput (20 minutes for the engine installation) so a car can be produced every 20 minutes, once the first car taking 35 minutes has been produced.\n\nHistory\nBefore the Industrial Revolution, most manufactured products were made individually by hand. A single craftsman or team of craftsmen would create each part of a product. They would use their skills and tools such as files and knives to create the individual parts. They would then assemble them into the final product, making cut-and-try changes in the parts until they fit and could work together (craft production).",
        "Division of labor was practiced by Ancient Greeks, Chinese and other ancient civilizations. In Ancient Greece it was discussed by Plato and Xenophon. Adam Smith discussed the division of labour in the manufacture of pins at length in his book The Wealth of Nations (published in 1776).",
        "The Venetian Arsenal, dating to about 1104, operated similar to a production line. Ships moved down a canal and were fitted by the various shops they passed. At the peak of its efficiency in the early 16th century, the Arsenal employed some 16,000 people who could apparently produce nearly one ship each day and could fit out, arm, and provision a newly built galley with standardized parts on an assembly-line basis. Although the Arsenal lasted until the early Industrial Revolution, production line methods did not become common even then.\n\nIndustrial Revolution",
        "Industrial Revolution\n\nThe Industrial Revolution led to a proliferation of manufacturing and invention. Many industries, notably textiles, firearms, clocks and watches, horse-drawn vehicles, railway locomotives, sewing machines, and bicycles, saw expeditious improvement in materials handling, machining, and assembly during the 19th century, although modern concepts such as industrial engineering and logistics had not yet been named.\n\nThe automatic flour mill built by Oliver Evans in 1785 was called the beginning of modern bulk material handling by Roe (1916). Evans's mill used a leather belt bucket elevator, screw conveyors, canvas belt conveyors, and other mechanical devices to completely automate the process of making flour. The innovation spread to other mills and breweries.",
        "Probably the earliest industrial example of a linear and continuous assembly process is the Portsmouth Block Mills, built between 1801 and 1803. Marc Isambard Brunel (father of Isambard Kingdom Brunel), with the help of Henry Maudslay and others, designed 22 types of machine tools to make the parts for the rigging blocks used by the Royal Navy. This factory was so successful that it remained in use until the 1960s, with the workshop still visible at HM Dockyard in Portsmouth, and still containing some of the original machinery.",
        "One of the earliest examples of an almost modern factory layout, designed for easy material handling, was the Bridgewater Foundry. The factory grounds were bordered by the Bridgewater Canal and the Liverpool and Manchester Railway. The buildings were arranged in a line with a railway for carrying the work going through the buildings. Cranes were used for lifting the heavy work, which sometimes weighed in the tens of tons. The work passed sequentially through to erection of framework and final assembly.",
        "The first flow assembly line was initiated at the factory of Richard Garrett & Sons, Leiston Works in Leiston in the English county of Suffolk for the manufacture of portable steam engines. The assembly line area was called 'The Long Shop' on account of its length and was fully operational by early 1853. The boiler was brought up from the foundry and put at the start of the line, and as it progressed through the building it would stop at various stages where new parts would be added. From the upper level, where other parts were made, the lighter parts would be lowered over a balcony and then fixed onto the machine on the ground level. When the machine reached the end of the shop, it would be completed.\n\nInterchangeable parts",
        "Interchangeable parts\n\nDuring the early 19th century, the development of machine tools such as the screw-cutting lathe, metal planer, and milling machine, and of toolpath control via jigs and fixtures, provided the prerequisites for the modern assembly line by making interchangeable parts a practical reality.\n\nLate 19th-century steam and electric conveyors\n\nSteam-powered conveyor lifts began being used for loading and unloading ships some time in the last quarter of the 19th century. Hounshell (1984) shows a  sketch of an electric-powered conveyor moving cans through a filling line in a canning factory.",
        "The meatpacking industry of Chicago is believed to be one of the first industrial assembly lines (or disassembly lines) to be utilized in the United States starting in 1867. Workers would stand at fixed stations and a pulley system would bring the meat to each worker and they would complete one task. Henry Ford and others have written about the influence of this slaughterhouse practice on the later developments at Ford Motor Company.\n\n20th century\n\nAccording to Domm, the implementation of mass production of an automobile via an assembly line may be credited to Ransom Olds, who used it to build the first mass-produced automobile, the Oldsmobile Curved Dash. Olds patented the assembly line concept, which he put to work in his Olds Motor Vehicle Company factory in 1901.",
        "At Ford Motor Company, the assembly line was introduced by William \"Pa\" Klann upon his return from visiting Swift & Company's slaughterhouse in Chicago and viewing what was referred to as the \"disassembly line\", where carcasses were butchered as they moved along a conveyor. The efficiency of one person removing the same piece over and over without moving to another station caught his attention. He reported the idea to Peter E. Martin, soon to be head of Ford production, who was doubtful at the time but encouraged him to proceed. Others at Ford have claimed to have put the idea forth to Henry Ford, but Pa Klann's slaughterhouse revelation is well documented in the archives at the Henry Ford Museum and elsewhere, making him an important contributor to the modern automated assembly line",
        "Ford, but Pa Klann's slaughterhouse revelation is well documented in the archives at the Henry Ford Museum and elsewhere, making him an important contributor to the modern automated assembly line concept. Ford was appreciative, having visited the highly automated 40-acre Sears mail order handling facility around 1906. At Ford, the process was an evolution by trial and error of a team consisting primarily of Peter E. Martin, the factory superintendent; Charles E. Sorensen, Martin's assistant; Clarence W. Avery; C. Harold Wills, draftsman and toolmaker; Charles Ebender; and József Galamb. Some of the groundwork for such development had recently been laid by the intelligent layout of machine tool placement that Walter Flanders had been doing at Ford up to 1908.",
        "The moving assembly line was developed for the Ford Model T and began operation on October 7, 1913, at the Highland Park Ford Plant, and continued to evolve after that, using time and motion study. The assembly line, driven by conveyor belts, reduced production time for a Model T to just 93 minutes by dividing the process into 45 steps. Producing cars quicker than paint of the day could dry, it had an immense influence on the world.\n\nIn 1922, Ford (through his ghostwriter Crowther) said of his 1913 assembly line:\n\nCharles E. Sorensen, in his 1956 memoir My Forty Years with Ford, presented a different version of development that was not so much about individual \"inventors\" as a gradual, logical development of industrial engineering:",
        "As a result of these developments in method, Ford's cars came off the line in three-minute intervals or six feet per minute. This was much faster than previous methods, increasing production by eight to one (requiring 12.5 man-hours before, 1 hour 33 minutes after), while using less manpower. It was so successful, paint became a bottleneck. Only japan black would dry fast enough, forcing the company to drop the variety of colours available before 1914, until fast-drying Duco lacquer was developed in 1926.",
        "The assembly line technique was an integral part of the diffusion of the automobile into American society. Decreased costs of production allowed the cost of the Model T to fall within the budget of the American middle class. In 1908, the price of a Model T was around $825, and by 1912 it had decreased to around $575. This price reduction is comparable to a reduction from $15,000 to $10,000 in dollar terms from the year 2000. In 1914, an assembly line worker could buy a Model T with four months' pay.",
        "Ford's complex safety procedures—especially assigning each worker to a specific location instead of allowing them to roam about—dramatically reduced the rate of injury. The combination of high wages and high efficiency is called \"Fordism\", and was copied by most major industries. The efficiency gains from the assembly line also coincided with the take-off of the United States. The assembly line forced workers to work at a certain pace with very repetitive motions which led to more output per worker while other countries were using less productive methods.",
        "In the automotive industry, its success was dominating, and quickly spread worldwide. Ford France and Ford Britain in 1911, Ford Denmark 1923, Ford Germany and Ford Japan 1925; in 1919, Vulcan (Southport, Lancashire) was the first native European manufacturer to adopt it. Soon, companies had to have assembly lines, or risk going broke by not being able to compete; by 1930, 250 companies which did not had disappeared.",
        "The massive demand for military hardware in World War II prompted assembly-line techniques in shipbuilding and aircraft production. Thousands of Liberty ships were built making extensive use of prefabrication, enabling ship assembly to be completed in weeks or even days. After having produced fewer than 3,000 planes for the United States Military in 1939, American aircraft manufacturers built over 300,000 planes in World War II. Vultee pioneered the use of the powered assembly line for aircraft manufacturing. Other companies quickly followed. As William S. Knudsen (having worked at Ford, GM and the National Defense Advisory Commission) observed, \"We won because we smothered the enemy in an avalanche of production, the like of which he had never seen, nor dreamed possible.\"",
        "Improved working conditions\nIn his 1922 autobiography, Henry Ford mentions several benefits of the assembly line including:\n Workers do not do any heavy lifting.\n No stooping or bending over.\n No special training was required.\n There are jobs that almost anyone can do.\n Provided employment to immigrants.",
        "The gains in productivity allowed Ford to increase worker pay from $1.50 per day to $5.00 per day once employees reached three years of service on the assembly line. Ford continued on to reduce the hourly work week while continuously lowering the Model T price. These goals appear altruistic; however, it has been argued that they were implemented by Ford in order to reduce high employee turnover: when the assembly line was introduced in 1913, it was discovered that \"every time the company wanted to add 100 men to its factory personnel, it was necessary to hire 963\" in order to counteract the natural distaste the assembly line seems to have inspired.",
        "Sociological problems\nSociological work has explored the social alienation and boredom that many workers feel because of the repetition of doing the same specialized task all day long.",
        "Karl Marx expressed in his theory of alienation the belief that, in order to achieve job satisfaction, workers need to see themselves in the objects they have created, that products should be \"mirrors in which workers see their reflected essential nature\". Marx viewed labour as a chance for people to externalize facets of their personalities. Marxists argue that performing repetitive, specialized tasks causes a feeling of disconnection between what a worker does all day, who they really are, and what they would ideally be able to contribute to society. Furthermore, Marx views these specialised jobs as insecure, since the worker is expendable as soon as costs rise and technology can replace more expensive human labour.",
        "Since workers have to stand in the same place for hours and repeat the same motion hundreds of times per day, repetitive stress injuries are a possible pathology of occupational safety. Industrial noise also proved dangerous. When it was not too high, workers were often prohibited from talking. Charles Piaget, a skilled worker at the LIP factory, recalled that besides being prohibited from speaking, the semi-skilled workers had only 25 centimeters in which to move. Industrial ergonomics later tried to minimize physical trauma.\n\nSee also",
        "See also\n\n Modern Times, a 1936 film featuring the Tramp character (played by Charlie Chaplin) struggling to adapt to assembly line work\nFinal Offer, a documentary film about the 1984 UAW/CAW contract negotiations shows working life on the floor of the GM Oshawa Ontario Car Assembly Plant (Watch Online)\nReconfigurable and flexible manufacturing systems, involving Post-Fordism and lean manufacturing-influenced production\n\nReferences\n\nFootnotes\n\nWorks cited\n\nExternal links\n\n Homepage for assembly line optimization research\n Assembly line optimization problems\n History of the assembly line and its widespread effects\n Cars Assembly Line",
        "Works cited\n\nExternal links\n\n Homepage for assembly line optimization research\n Assembly line optimization problems\n History of the assembly line and its widespread effects\n Cars Assembly Line\n\nIndustrial processes\nMass production\nManufacturing buildings and structures\nAmerican inventions\nCulture of Detroit\nHistory of science and technology in the United States\nArticles containing video clips\nTypes of production\n\nca:Producció en cadena"
    ],
    [
        "Association for Computing Machinery\nThe Association for Computing Machinery (ACM) is a US-based international learned society for computing. It was founded in 1947 and is the world's largest scientific and educational computing society. The ACM is a non-profit professional membership group, reporting nearly 110,000 student and professional members . Its headquarters are in New York City.\n\nThe ACM is an umbrella organization for academic and scholarly interests in computer science (informatics). Its motto is \"Advancing Computing as a Science & Profession\".",
        "History\nIn 1947, a notice was sent to various people:\nOn January 10, 1947, at the Symposium on Large-Scale Digital Calculating Machinery at the Harvard computation Laboratory, Professor Samuel H. Caldwell of Massachusetts Institute of Technology spoke of the need for an association of those interested in computing machinery, and of the need for communication between them.\n[...]\nAfter making some inquiries during May and June, we believe there is ample interest to start an informal association of many of those interested in the new machinery for computing and reasoning. Since there has to be a beginning, we are acting as a temporary committee to start such an association:",
        "E. C. Berkeley, Prudential Insurance Co. of America, Newark, N. J.\nR. V. D. Campbell, Raytheon Manufacturing Co., Waltham, Mass.\n, Bureau of Standards, Washington, D.C.\nH. E. Goheen, Office of Naval Research, Boston, Mass.\nJ. W. Mauchly, Electronic Control Co., Philadelphia, Pa.\nT. K. Sharpless, Moore School of Elec. Eng., Philadelphia, Pa.\nR. Taylor, Mass. Inst. of Tech., Cambridge, Mass.\nC. B. Tompkins, Engineering Research Associates, Washington, D.C.",
        "The committee (except for Curtiss) had gained experience with computers during World War II: Berkeley, Campbell, and Goheen helped build Harvard Mark I under Howard H. Aiken, Mauchly and Sharpless were involved in building ENIAC, Tompkins had used \"the secret Navy code-breaking machines\", and Taylor had worked on Bush's Differential analyzers.\n\nThe ACM was then founded in 1947 under the name Eastern Association for Computing Machinery, which was changed the following year to the Association for Computing Machinery. The ACM History Committee since 2016 has published the A.M.Turing Oral History project, the ACM Key Award Winners Video Series, and the India Industry Leaders Video project.\n\nActivities",
        "Activities\n\nACM is organized into over 246 local professional chapters and 38 Special Interest Groups (SIGs), through which it conducts most of its activities. Additionally, there are over 833 college and university chapters. The first student chapter was founded in 1961 at the University of Louisiana at Lafayette.\n\nMany of the SIGs, such as SIGGRAPH, SIGDA, SIGPLAN, SIGCSE and SIGCOMM, sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters.",
        "ACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer.\n\nServices\n\nPublications",
        "ACM publishes over 50 journals including the prestigious Journal of the ACM, and two general magazines for computer professionals, Communications of the ACM (also known as Communications or CACM) and Queue. Other publications of the ACM include:\nACM XRDS, formerly \"Crossroads\", was redesigned in 2010 and is the most popular student computing magazine in the US.\nACM Interactions, an interdisciplinary HCI publication focused on the connections between experiences, people and technology, and the third largest ACM publication.\nACM Computing Surveys (CSUR)\nComputers in Entertainment (CIE)\nACM Journal on Emerging Technologies in Computing Systems (JETC) \nACM Special Interest Group: Computers and Society (SIGCAS)",
        "ACM Computing Surveys (CSUR)\nComputers in Entertainment (CIE)\nACM Journal on Emerging Technologies in Computing Systems (JETC) \nACM Special Interest Group: Computers and Society (SIGCAS)\nA number of journals, specific to subfields of computer science, titled ACM Transactions. Some of the more notable transactions include:\nACM Transactions on Algorithms (TALG)\nACM Transactions on Embedded Computing Systems (TECS)\nACM Transactions on Computer Systems (TOCS)\nIEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)\nACM Transactions on Computational Logic (TOCL)\nACM Transactions on Computer-Human Interaction (TOCHI)\nACM Transactions on Database Systems (TODS)\nACM Transactions on Graphics (TOG)\nACM Transactions on Mathematical Software (TOMS)",
        "ACM Transactions on Computer-Human Interaction (TOCHI)\nACM Transactions on Database Systems (TODS)\nACM Transactions on Graphics (TOG)\nACM Transactions on Mathematical Software (TOMS)\nACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)\nIEEE/ACM Transactions on Networking (TON)\nACM Transactions on Programming Languages and Systems (TOPLAS)",
        "Although Communications no longer publishes primary research, and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages.\n\nACM has made almost all of its publications available to paid subscribers online at its Digital Library and also has a Guide to Computing Literature. ACM also offers insurance, online courses, and other services to its members.\n\nIn 1997, ACM Press published Wizards and Their Wonders: Portraits in Computing (), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry.",
        "Portal and Digital Library\nThe ACM Portal is an online service of the ACM. Its core are two main sections: ACM Digital Library and the ACM Guide to Computing Literature.",
        "The ACM Digital Library was launched in October 1997. It is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries.\nThe ACM Digital Library contains a comprehensive archive starting in the 1950s of the organization's journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying bibliographic database containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature.",
        "ACM adopted a hybrid Open Access (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement.\n\nACM was a \"green\" publisher before the term was invented. Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Library's permanently maintained Version of Record.\n\nAll metadata in the Digital Library is open to the world, including abstracts, linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription.",
        "There is also a mounting challenge to the ACM's publication practices coming from the open access movement. Some authors see a subscription business model as less relevant and publish on their home pages or on unreviewed sites like arXiv. Other organizations have sprung up which do their peer review entirely free and online, such as Journal of Artificial Intelligence Research, Journal of Machine Learning Research and the Journal of Research and Practice in Information Technology.\n\nACM has made its publications from 1951 to 2000  open access through its digital library on 7 April 2022 as part of its 75th anniversary of the organisation.\n\nMembership grades",
        "ACM has made its publications from 1951 to 2000  open access through its digital library on 7 April 2022 as part of its 75th anniversary of the organisation.\n\nMembership grades\n\nIn addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and \"demonstrated performance that sets them apart from their peers\".\n\nThe number of Fellows, Distinguished Members, and Senior Members cannot exceed 1%, 10%, and 25% of the total number of professional members, respectively.\n\nFellows",
        "The number of Fellows, Distinguished Members, and Senior Members cannot exceed 1%, 10%, and 25% of the total number of professional members, respectively.\n\nFellows\n\nThe ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 \"to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM.\" There are 1,310 Fellows  out of about 100,000 members.",
        "Distinguished Members\nIn 2006, ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and \"have made a significant impact on the computing field\". In 2006 when the Distinguished Members first came out, one of the three levels was called \"Distinguished Member\" and was changed about two years later to \"Distinguished Educator\". Those who already had the Distinguished Member title had their titles changed to one of the other three titles.\n\nList of Distinguished Members of the Association for Computing Machinery",
        "List of Distinguished Members of the Association for Computing Machinery\n\nSenior Members\nAlso in 2006, ACM began recognizing Senior Members. According to the ACM, \"The Senior Members Grade recognizes those ACM members with at least 10 years of professional experience and 5 years of continuous Professional Membership who have demonstrated performance through technical leadership, and technical or professional contributions\". Senior membership also requires 3 letters of reference",
        "Distinguished Speakers\nWhile not technically a membership grade, the ACM recognizes distinguished speakers on topics in computer science. A distinguished speaker is appointed for a three-year period. There are usually about 125 current distinguished speakers. The ACM website describes these people as 'Renowned International Thought Leaders'. The distinguished speakers program (DSP) has been in existence for over 20 years and serves as an outreach program that brings renowned experts from Academia, Industry and Government to present on the topic of their expertise.  The DSP is overseen by a committee\n\nChapters\nACM has three kinds of chapters: Special Interest Groups, Professional Chapters, and Student Chapters.\n\n, ACM has professional & SIG Chapters in 56 countries.",
        "Chapters\nACM has three kinds of chapters: Special Interest Groups, Professional Chapters, and Student Chapters.\n\n, ACM has professional & SIG Chapters in 56 countries.\n\n, there exist ACM student chapters in 41 countries.\n\nSpecial Interest Groups",
        "SIGACCESS: Accessible Computing\n SIGACT: Algorithms and Computation Theory\n SIGAda: Ada Programming Language\n SIGAI: Artificial Intelligence\n SIGAPP: Applied Computing\n SIGARCH: Computer Architecture\n SIGBED: Embedded Systems\n SIGBio: Bioinformatics \n SIGCAS: Computers and Society\n SIGCHI: Computer–Human Interaction\n SIGCOMM: Data Communication\n SIGCSE: Computer Science Education\n SIGDA: Design Automation\n SIGDOC: Design of Communication\n SIGecom: Electronic Commerce\n SIGEVO: Genetic and Evolutionary Computation\n SIGGRAPH: Computer Graphics and Interactive Techniques\n SIGHPC: High Performance Computing\n SIGIR: Information Retrieval\n SIGITE: Information Technology Education\n SIGKDD: Knowledge Discovery and Data Mining\n SIGLOG: Logic and Computation\n SIGMETRICS: Measurement and Evaluation",
        "SIGIR: Information Retrieval\n SIGITE: Information Technology Education\n SIGKDD: Knowledge Discovery and Data Mining\n SIGLOG: Logic and Computation\n SIGMETRICS: Measurement and Evaluation\n SIGMICRO: Microarchitecture\n SIGMIS: Management Information Systems\n SIGMM: Multimedia\n SIGMOBILE: Mobility of Systems, Users, Data and Computing\n SIGMOD: Management of Data\n SIGOPS: Operating Systems\n SIGPLAN: Programming Languages\n SIGSAC: Security, Audit, and Control\n SIGSAM: Symbolic and Algebraic Manipulation\n SIGSIM: Simulation and Modeling\n SIGSOFT: Software Engineering\n SIGSPATIAL: Spatial Information\n SIGUCCS: University and College Computing Services\n SIGWEB: Hypertext, Hypermedia, and Web",
        "Conferences\n\nACM and its Special Interest Groups (SIGs) sponsors numerous conferences with 170 hosted worldwide in 2017. ACM Conferences page has an up-to-date complete list while a partial list is shown below. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example, SIGGRAPH 2007 attracted about 30000 attendees, while CIKM 2005 and RecSys 2022 had paper acceptance rates of only accepted 15% and 17% respectively.",
        "AIES: Conference on AI, Ethics, and Society\n ASPLOS: International Conference on Architectural Support for Programming Languages and Operating Systems\n CHI: Conference on Human Factors in Computing Systems\n CIKM: Conference on Information and Knowledge Management\n COMPASS: International Conference on Computing and Sustainable Societies\n DAC: Design Automation Conference\n DEBS: Distributed Event Based Systems\n FAccT: Conference on Fairness, Accountability, and Transparency\n FCRC: Federated Computing Research Conference\n GECCO: Genetic and Evolutionary Computation Conference\n HT: Hypertext: Conference on Hypertext and Hypermedia\n JCDL: Joint Conference on Digital Libraries\n MobiHoc: International Symposium on Mobile Ad Hoc Networking and Computing\n SC: Supercomputing Conference",
        "JCDL: Joint Conference on Digital Libraries\n MobiHoc: International Symposium on Mobile Ad Hoc Networking and Computing\n SC: Supercomputing Conference\n SIGCOMM: ACM SIGCOMM Conference\n SIGCSE: SIGCSE Technical Symposium on Computer Science Education\n SIGGRAPH: International Conference on Computer Graphics and Interactive Techniques\n RecSys: ACM Conference on Recommender Systems\n TAPIA: Richard Tapia Celebration of Diversity in Computing Conference",
        "The ACM is a co–presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the Anita Borg Institute for Women and Technology.\n\nSome conferences are hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM. In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended.\n\nFor additional non-ACM conferences, see this list of computer science conferences.\n\nAwards\nThe ACM presents or co–presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology.",
        "ACM A. M. Turing Award\n ACM – AAAI Allen Newell Award\n ACM Athena Lecturer Award\n ACM/CSTA Cutler-Bell Prize in High School Computing\n ACM Distinguished Service Award\n ACM Doctoral Dissertation Award\n ACM Eugene L. Lawler Award\n ACM Fellowship, awarded annually since 1993\n ACM Gordon Bell Prize\n ACM Grace Murray Hopper Award\n ACM – IEEE CS George Michael Memorial HPC Fellowships\n ACM – IEEE CS Ken Kennedy Award\n ACM – IEEE Eckert-Mauchly Award\n ACM India Doctoral Dissertation Award\n ACM Karl V. Karlstrom Outstanding Educator Award\n ACM Paris Kanellakis Theory and Practice Award\n ACM Policy Award\n ACM Presidential Award\n ACM Prize in Computing (formerly: ACM – Infosys Foundation Award in the Computing Sciences)\n ACM Programming Systems and Languages Paper Award",
        "ACM Policy Award\n ACM Presidential Award\n ACM Prize in Computing (formerly: ACM – Infosys Foundation Award in the Computing Sciences)\n ACM Programming Systems and Languages Paper Award\n ACM Student Research Competition\n ACM Software System Award\n International Science and Engineering Fair\n Outstanding Contribution to ACM Award\n SIAM/ACM Prize in Computational Science and Engineering",
        "Over 30 of ACM's Special Interest Groups also award individuals for their contributions with a few listed below.\n\n ACM Alan D. Berenbaum Distinguished Service Award\n ACM Maurice Wilkes Award\n ISCA Influential Paper Award\n\nLeadership",
        "The President of ACM for 2022–2024 is Yannis Ioannidis, Professor at the National and Kapodistrian University of Athens. He is successor of Gabriele Kotsis (2020–2022), Professor at the Johannes Kepler University Linz; Cherri M. Pancake (2018–2020), Professor Emeritus at Oregon State University and Director of the Northwest Alliance for Computational Science and Engineering (NACSE); Vicki L. Hanson (2016–2018), Distinguished Professor at the Rochester Institute of Technology and visiting professor at the University of Dundee; Alexander L. Wolf (2014–2016), Dean of the Jack Baskin School of Engineering at the University of California, Santa Cruz; Vint Cerf (2012–2014), American computer scientist and Internet pioneer; Alain Chesnais (2010–2012); and Dame Wendy Hall of the University of",
        "Engineering at the University of California, Santa Cruz; Vint Cerf (2012–2014), American computer scientist and Internet pioneer; Alain Chesnais (2010–2012); and Dame Wendy Hall of the University of Southampton, UK (2008–2010).",
        "ACM is led by a council consisting of the president, vice-president, treasurer, past president, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members-At-Large. This institution is often referred to simply as \"Council\" in Communications of the ACM.\n\nInfrastructure\nACM has numerous boards, committees, and task forces which run the organization:\n\n ACM Council\n ACM Executive Committee\n Digital Library Board\n Education Board\n Practitioner Board\n Publications Board\n SIG Governing Board\n DEI Council\n ACM Technology Policy Council\n ACM Representatives to Other Organizations\n Computer Science Teachers Association\n\nACM Council on Women in Computing",
        "ACM-W, the ACM council on women in computing, supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM–W's main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively.  ACM-W collaborates with organizations such as the Anita Borg Institute, the National Center for Women & Information Technology (NCWIT), and Committee on the Status of Women in Computing Research (CRA-W).",
        "The ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science. This program began in 2006. Speakers are nominated by SIG officers.",
        "Partner organizations\nACM's primary partner has been the IEEE Computer Society (IEEE-CS), which is the largest subgroup of the Institute of Electrical and Electronics Engineers (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACM's agenda. They have many joint activities including conferences, publications and awards. ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE. Eckert-Mauchly Award and Ken Kennedy Award, both major awards in computer science, are given jointly by ACM and the IEEE-CS. They occasionally cooperate on projects like developing computing curricula.",
        "ACM has also jointly sponsored on events with other professional organizations like the Society for Industrial and Applied Mathematics (SIAM).\n\nCriticism",
        "Criticism\n\nIn December 2019, the ACM co-signed a letter with over one hundred other publishers to President Donald Trump saying that an open access mandate would increase costs to taxpayers or researchers and hurt intellectual property. This was in response to rumors that he was considering issuing an executive order that would require federally funded research be made freely available online immediately after being published. It is unclear how these rumors started. Many ACM members opposed the letter, leading ACM to issue a statement clarifying that they remained committed to open access, and they wanted to see communication with stakeholders about the potential mandate. The statement did not significantly assuage criticism from ACM members.",
        "The SoCG conference, while originally an ACM conference, parted ways with ACM in 2014 because of problems when organizing conferences abroad.\n\nSee also\n\n ACM Classification Scheme\n Franz Alt, former president\n\n Edmund Berkeley, co-founder\n Computer science\n Computing\n Bernard Galler, former president\n Fellows of the ACM (by year)\n Fellows of the ACM (category)\n Grace Murray Hopper Award\n\n Presidents of the Association for Computing Machinery\n Timeline of computing hardware before 1950\n Turing Award\n List of academic databases and search engines\n\nReferences\n\nExternal links\n\nACM portal for publications\nACM Digital Library\nAssociation for Computing Machinery Records, 1947-2009, Charles Babbage Institute, University of Minnesota.\n ACM Upsilon Phi Epsilon  honor society",
        "ACM portal for publications\nACM Digital Library\nAssociation for Computing Machinery Records, 1947-2009, Charles Babbage Institute, University of Minnesota.\n ACM Upsilon Phi Epsilon  honor society\n\n \n1947 establishments in the United States\nComputer science-related professional associations\nInternational learned societies\nOrganizations established in 1947\n501(c)(3) organizations"
    ],
    [
        "Asteroids (video game)\nAsteroids is a space-themed multidirectional shooter arcade video game designed by Lyle Rains and Ed Logg released in November 1979 by Atari, Inc. The player controls a single spaceship in an asteroid field which is periodically traversed by flying saucers. The object of the game is to shoot and destroy the asteroids and saucers, while not colliding with either, or being hit by the saucers' counter-fire. The game becomes harder as the number of asteroids increases.",
        "Asteroids was conceived during a meeting between Logg and Rains, who decided to use hardware developed by Howard Delman previously used for Lunar Lander. Asteroids was based on an unfinished game titled Cosmos; its physics model, control scheme, and gameplay elements were derived from Spacewar!, Computer Space, and Space Invaders and refined through trial and error.  The game is rendered on a vector display in a two-dimensional view that wraps around both screen axes.",
        "Asteroids was one of the first major hits of the golden age of arcade games; the game sold  47,840 upright cabinets and 8,725 cocktail cabinets arcade cabinets and proved both popular with players and influential with developers. In the 1980s it was ported to Atari's home systems, and the Atari VCS version sold over three million copies. The game was widely imitated, and it directly influenced Defender, Gravitar, and many other video games.\n\nGameplay",
        "Gameplay\n\nThe objective of Asteroids is to destroy asteroids and saucers. The player controls a triangular ship that can rotate left and right, fire shots straight forward, and thrust forward. Once the ship begins moving in a direction, it will continue in that direction for a time without player intervention unless the player applies thrust in a different direction. The ship eventually comes to a stop when not thrusting. The player can also send the ship into hyperspace, causing it to disappear and reappear in a random location on the screen, at the risk of self-destructing or appearing on top of an asteroid.",
        "Each level starts with a few large asteroids drifting in various directions on the screen. Objects wrap around screen edges – for instance, an asteroid that drifts off the top edge of the screen reappears at the bottom and continues moving in the same direction. As the player shoots asteroids, they break into smaller asteroids that move faster and are more difficult to hit. Smaller asteroids are also worth more points. Two flying saucers appear periodically on the screen; the \"big saucer\" shoots randomly and poorly, while the \"small saucer\" fires frequently at the ship. After reaching a score of 40,000, only the small saucer appears. As the player's score increases, the angle range of the shots from the small saucer diminishes until the saucer fires extremely accurately. Once the screen",
        "of 40,000, only the small saucer appears. As the player's score increases, the angle range of the shots from the small saucer diminishes until the saucer fires extremely accurately. Once the screen has been cleared of all asteroids and flying saucers, a new set of large asteroids appears, thus starting the next level. The game gets harder as the number of asteroids increases until after the score reaches a range between 40,000 and 60,000. The player starts with 3–5 lives upon game start and gains an extra life per 10,000 points. Play continues to the last ship lost, which ends the game. The machine \"turns over\" at 99,990 points, which is the maximum high score that can be achieved.",
        "Lurking exploit",
        "In the original game design, saucers were supposed to begin shooting as soon as they appeared, but this was changed.  Additionally, saucers can only aim at the player's ship on-screen; they are not capable of aiming across a screen boundary. These behaviors allow a \"lurking\" strategy, in which the player stays near the edge of the screen opposite the saucer. By keeping just one or two rocks in play, a player can shoot across the boundary and destroy saucers to accumulate points indefinitely with little risk of being destroyed.  Arcade operators began to complain about losing revenue due to this exploit. In response, Atari issued a patched EPROM and, due to the impact of this exploit, Atari (and other companies) changed their development and testing policies to try to prevent future games",
        "exploit. In response, Atari issued a patched EPROM and, due to the impact of this exploit, Atari (and other companies) changed their development and testing policies to try to prevent future games from having such exploits.",
        "Development",
        "Concept",
        "Asteroids was conceived by Lyle Rains and programmed by Ed Logg with collaborations from other Atari staff. Logg was impressed with the Atari Video Computer System (later called the Atari 2600), and he joined Atari's coin-op division to work on Dirt Bike, which was never released due to an unsuccessful field test. Paul Mancuso joined the development team as Asteroids technician and engineer Howard Delman contributed to the hardware. During a meeting in April 1979, Rains discussed Planet Grab, a multiplayer arcade game later renamed to Cosmos. Logg did not know the name of the game, thinking Computer Space as \"the inspiration for the two-dimensional approach\". Rains conceived of Asteroids as a mixture of Computer Space and Space Invaders, combining the two-dimensional approach of Computer",
        "Space as \"the inspiration for the two-dimensional approach\". Rains conceived of Asteroids as a mixture of Computer Space and Space Invaders, combining the two-dimensional approach of Computer Space with Space Invaders addictive gameplay of \"completion\" and \"eliminate all threats\". The unfinished game featured a giant, indestructible asteroid, so Rains asked Logg: \"Well, why don’t we have a game where you shoot the rocks and blow them up?\" In response, Logg described a similar concept where the player selectively shoots at rocks that break into smaller pieces. Both agreed on the concept.",
        "Hardware\nAsteroids was implemented on hardware developed by Delman and is a vector game, in which the graphics are composed of lines drawn on a vector monitor. Rains initially wanted the game done in raster graphics, but Logg, experienced in vector graphics, suggested an XY monitor because the high image quality would permit precise aiming. The hardware is chiefly a MOS 6502 executing the game program, and QuadraScan, a high-resolution vector graphics processor developed by Atari and referred to as an \"XY display system\" and the \"Digital Vector Generator (DVG)\".",
        "The original design concepts for QuadraScan came out of Cyan Engineering, Atari's off-campus research lab in Grass Valley, California, in 1978. Cyan gave it to Delman, who finished the design and first used it for Lunar Lander. Logg received Delman's modified board with five buttons, 13 sound effects, and additional RAM, and he used it to develop Asteroids. The size of the board was 4 by 4 inches, and it was \"linked up\" to a monitor.",
        "Implementation\nLogg modeled the player's ship, the five-button control scheme, and the game physics after Spacewar!, which he had played as a student at the University of California, Berkeley, but made several changes to improve playability. The ship was programmed into the hardware and rendered by the monitor, and it was configured to move with thrust and inertia. The hyperspace button was not placed near Logg's right thumb, which he was dissatisfied with, as he had a problem \"tak[ing] his hand off the thrust button\". Drawings of asteroids in various shapes were incorporated into the game. Logg copied the idea of a high score table with initials from Exidy's Star Fire.",
        "The two saucers were formulated to be different from each other. A steadily decreasing timer shortens intervals between saucer attacks to keep the player from not shooting asteroids and saucers. A \"heartbeat\" soundtrack quickens as the game progresses. The game does not have a sound chip. Delman created a hardware circuit for 13 sound effects by hand which was wired onto the board.",
        "A prototype of Asteroids was well received by several Atari staff and engineers, who \"wander[ed] between labs, passing comment and stopping to play as they went\". Logg was often asked when he would be leaving by employees eager to play the prototype, so he created a second prototype for staff to play. Atari tested the game in arcades in Sacramento, California, and also observed players during focus group sessions at Atari. Players used to Spacewar! struggled to maintain grip on the thrust button and requested a joystick; players accustomed to Space Invaders noted they get no break in the game. Logg and other engineers observed proceedings and documented comments in four pages.",
        "Asteroids slows down as the player gains 50–100 lives, because there is no limit to the number of lives displayed. The player can \"lose\" the game after more than 250 lives are collected.\n\nPorts\nAsteroids was released for the Atari VCS (later renamed the Atari 2600) and Atari 8-bit family in 1981, then the Atari 7800 in 1986. A port for the Atari 5200, identical to the Atari 8-bit computer version, was in development in 1982, but was not published. The Atari 7800 version was a launch title and includes cooperative play; the asteroids have colorful textures and the \"heartbeat\" sound effect remains intact.",
        "Programmers Brad Stewart and Bob Smith were unable to fit the Atari VCS port into a 4 KB cartridge. It became the first game for the console to use bank switching, a technique that increases ROM size from 4 KB to 8 KB.",
        "Reception",
        "Asteroids was immediately successful upon release. It displaced Space Invaders by popularity in the United States and became Atari's best selling arcade game of all time, with over 70,000 units sold. Atari earned an estimated $150 million in sales from the game, and arcade operators earned a further $500 million from coin drops. Atari had been in the process of manufacturing another vector game, Lunar Lander, but demand for Asteroids was so high \"that several hundred Asteroids games were shipped in Lunar Lander cabinets\". Asteroids was so popular that some video arcade operators had to install large boxes to hold the number of coins spent by players. It replaced Space Invaders at the top of the US RePlay amusement arcade charts in April 1980, though Space Invaders remained the top game at",
        "large boxes to hold the number of coins spent by players. It replaced Space Invaders at the top of the US RePlay amusement arcade charts in April 1980, though Space Invaders remained the top game at street locations. Asteroids went on to become the highest-grossing arcade video game of 1980 in the United States, dethroning Space Invaders. It shipped 70,000 arcade units worldwide in 1980, including over 60,000 sold in the United States that year, and grossed about  worldwide ( adjusted for inflation) by 1980. The game remained at the top of the US RePlay charts through March 1981. However, the game did not perform as well overseas in Europe and Asia. It sold 30,000 arcade units overseas, for a total of 100,000 arcade units sold worldwide. Atari manufactured 76,312 units from its US and",
        "the game did not perform as well overseas in Europe and Asia. It sold 30,000 arcade units overseas, for a total of 100,000 arcade units sold worldwide. Atari manufactured 76,312 units from its US and Ireland plants, including 21,394 Asteroids Deluxe units. It was a commercial failure in Japan when it released there in 1980, partly due to its complex controls and partly due to the Japanese market beginning to lose interest in space shoot 'em ups at the time.",
        "Asteroids received positive reviews from video game critics and has been regarded as Logg's magnum opus. Richard A. Edwards reviewed the 1981 Asteroids home cartridge in The Space Gamer No. 46. Edwards commented that \"this home cartridge is a virtual duplicate of the ever-popular Atari arcade game. [...] If blasting asteroids is the thing you want to do then this is the game, but at this price I can't wholeheartedly recommend it\". Video Games Player magazine reviewed the Atari VCS version, rating the graphics and sound a B, while giving the game an overall B+ rating. Electronic Fun with Computers & Games magazine gave the Atari VCS version an A rating.",
        "William Cassidy, writing for GameSpy's \"Classic Gaming\", noticed its innovations, including being one of the first video games to track initials and allow players to enter their initials for appearing in the top 10 high scores, and commented, \"the vector graphics fit the futuristic outer space theme very well\". In 1995, Flux magazine ranked the arcade version 11th on their \"Top 100 Video Games.\" In 1996, Next Generation listed it as number 39 on their \"Top 100 Games of All Time\", particularly lauding the control dynamics which require \"the constant juggling of speed, positioning, and direction\". In 1999, Next Generation listed Asteroids as number 29 on their \"Top 50 Games of All Time\", commenting that \"Asteroids was a classic the day it was released, and it has never lost any of its",
        "In 1999, Next Generation listed Asteroids as number 29 on their \"Top 50 Games of All Time\", commenting that \"Asteroids was a classic the day it was released, and it has never lost any of its appeal\". Asteroids was ranked fourth on Retro Gamers list of \"Top 25 Arcade Games\"; the Retro Gamer staff cited its simplicity and the lack of a proper ending as allowances of revisiting the game. In 2012, Asteroids was listed on Time All-Time 100 greatest video games list. Entertainment Weekly named Asteroids one of the top ten games for the Atari 2600 in 2013. It was added to the Museum of Modern Art's collection of video games. In 2021, The Guardian listed Asteroids as the second greatest video game of the 1970s, just below Galaxian (1979). By contrast, in March 1983 the Atari 8-bit port of",
        "Art's collection of video games. In 2021, The Guardian listed Asteroids as the second greatest video game of the 1970s, just below Galaxian (1979). By contrast, in March 1983 the Atari 8-bit port of Asteroids won sixth place in Softlines Dog of the Year awards \"for badness in computer games\", Atari division, based on reader submissions.",
        "Usage of the names of Saturday Night Live characters \"Mr. Bill\" and \"Sluggo\" to refer to the saucers in an Esquire article about the game led to Logg receiving a cease and desist letter from a lawyer with the \"Mr. Bill Trademark\".\n\nLegacy",
        "Legacy\n\nArcade sequels\nReleased in 1981, Asteroids Deluxe was the first sequel to Asteroids. Dave Shepperd edited the code and made enhancements to the game without Logg's involvement. The onscreen objects are tinted blue, and hyperspace is replaced by a shield that depletes when used. The asteroids rotate, and new \"killer satellite\" enemies break into smaller ships that home in on the player's position. The arcade machine's monitor displays vector graphics overlaying a holographic backdrop. The game is more difficult than the original and enables saucers to shoot across the screen boundary, eliminating the lurking strategy for high scores in the original.\n\nSpace Duel, released in arcades in 1982, replaces the rocks with colorful geometric shapes and adds cooperative two-player gameplay.",
        "Space Duel, released in arcades in 1982, replaces the rocks with colorful geometric shapes and adds cooperative two-player gameplay.\n\n1987's Blasteroids includes \"power-ups, ship morphing, branching levels, bosses, and the ability to dock your ships in multiplayer for added firepower\". Blasteroids uses raster graphics instead of vectors.\n\nRe-releases\nThe game is half of the Atari Lynx pairing Super Asteroids & Missile Command, and included in the 1993  Microsoft Arcade compilation.\n\nActivision published an enhanced version of Asteroids for the PlayStation (1998), Nintendo 64 (1999), Microsoft Windows (1998), Game Boy Color (1999), and Mac (2000). The Atari Flashback series of dedicated video game consoles have included both the 2600 and the arcade versions of Asteroids.",
        "Published by Crave Entertainment on December 14, 1999, Asteroids Hyper 64 made the ship and asteroids 3D and added new weapons and a multiplayer mode.\n\nA technical demo of Asteroids was developed by iThink for the Atari Jaguar but was never released. Unofficially referred to as Asteroids 2000, it was demonstrated at E-JagFest 2000.\n\nIn 2001, Infogrames released Atari Anniversary Edition for the Dreamcast, PlayStation, and Microsoft Windows. Developed by Digital Eclipse, it includes emulated versions of Asteroids and other games. The arcade and Atari 2600 versions of Asteroids were included in Atari Anthology for both Xbox and PlayStation 2.",
        "Released on November 28, 2007, the Xbox Live Arcade port of Asteroids has revamped HD graphics along with an added intense \"throttle monkey\" mode. The arcade and 2600 versions were made available through Microsofts Game Room service in 2010. Glu Mobile released an enhanced mobile phone port.\n\nAsteroids is included on Atari Greatest Hits Volume 1 for the Nintendo DS.\n\nAn updated version of the game was announced in 2018 for the Intellivision Amico.\n\nBoth the Atari 2600 and Atari 7800 versions of the game was included on Atari Collection 1 and 2 in 2020 for the Evercade.\n\nA remake, Asteroids: Recharged, was released in December 2021 for the Nintendo Switch, PlayStation 4, PlayStation 5, Windows, Xbox One, and Xbox Series X/S. It was developed by Adamvision Studios and SneakyBox.",
        "Clones\nQuality Software's Asteroids in Space (1980) was one of the best selling games for the Apple II and voted one of the most popular software titles of 1978-80 by Softalk magazine.",
        "In December 1981, Byte reviewed eight Asteroids clones for home computers. Three clones for the Apple II were reviewed together in the 1982 Creative Computing Software Buyers Guide: The Asteroid Field, Asteron, and Apple-Oids. In the last of these, the asteroids are in the shape of apples. Two independent clones, Asteroid for the Apple II and Fasteroids for TRS-80, were renamed to Planetoids and sold by Adventure International. Others clones include Acornsoft's Meteors, Moons of Jupiter for the VIC-20, MineStorm for the Vectrex, and Quicksilva's Meteor Storm for the ZX Spectrum which uses speech synthesis.  A poorly implemented Asteroids clone for the VIC-20, published by Bug-Byte, motivated Jeff Minter to found Llamasoft.",
        "The Intellivision game Meteor! was cancelled to avoid a lawsuit for being too similar to Asteroids and was reworked as Astrosmash.  The game borrows elements from Asteroids and Space Invaders.\n\nProposed film adaptation\nOn July 2, 2009, Universal Pictures offered Emmerich the option to direct the film adaptation of Asteroids, with Matt Lopez writing the script and Lorenzo di Bonaventura will produce the film adaptation. On June 8, 2011, Universal offered Roland Emmerich the option to direct the film adaptation with Lopez and di Bonaventura still attached to write and produce the film adaptation, respectively, which Emmerich passed on directing, while Evan Spiliotopoulos and F. Scott Frazier were hired to rewrite the screenplay.",
        "World records\nOn February 6, 1982, Leo Daniels of Carolina Beach, North Carolina, set a world record score of 40,101,910 points.  On November 13 of the same year, 15-year-old Scott Safran of Cherry Hill, New Jersey, set a new record at 41,336,440 points.  In 1998, to congratulate Safran on his accomplishment, the Twin Galaxies Intergalactic Scoreboard searched for him for four years until 2002, when it was discovered that he had died in an accident in 1989. In a ceremony in Philadelphia on April 27, 2002, Walter Day of Twin Galaxies presented an award to the surviving members of Safran's family, commemorating his achievement. On April 5, 2010, John McAllister broke Safran's record with a high score of 41,838,740 in a 58-hour Internet livestream.\n\nReferences\n\nExternal links",
        "References\n\nExternal links\n\n  at Atari\n \n \n \n \n \n\n1979 video games\nArcade video games\nAtari 2600 games\nAtari 7800 games\nAtari 8-bit family games\nAtari arcade games\nAtari Lynx games\nCancelled Atari 5200 games\nCancelled Atari Jaguar games\nEd Logg games\nFiction about asteroids\nGame Boy games\nGame Boy Color games\nMultidirectional shooters\nMultiplayer and single-player video games\nScience fiction video games\nSega arcade games\nTaito arcade games\nXbox 360 games\nXbox 360 Live Arcade games\nVector arcade video games\nVideo games developed in the United States"
    ],
    [
        "Asynchronous communication\nIn telecommunications, asynchronous communication is transmission of data, generally without the use of an external clock signal, where data can be transmitted intermittently rather than in a steady stream. Any timing required to recover data from the communication symbols is encoded within the symbols.\n\nThe most significant aspect of asynchronous communications is that data is not transmitted at regular intervals, thus making possible variable bit rate, and that the transmitter and receiver clock generators do not have to be exactly synchronized all the time. In asynchronous transmission, data is sent one byte at a time and each byte is preceded by start and stop bits.\n\nPhysical layer",
        "Physical layer \n\nIn asynchronous serial communication in the physical protocol layer, the data blocks are code words of a certain word length, for example octets (bytes) or ASCII characters, delimited by start bits and stop bits. A variable length space can be inserted between the code words. No bit synchronization signal is required. This is sometimes called character oriented communication. Examples include MNP2 and modems older than V.2.",
        "Data link layer and higher \nAsynchronous communication at the data link layer or higher protocol layers is known as statistical multiplexing, for example Asynchronous Transfer Mode (ATM). In this case, the asynchronously transferred blocks are called data packets, for example ATM cells. The opposite is circuit switched communication, which provides constant bit rate, for example ISDN and SONET/SDH.",
        "The packets may be encapsulated in a data frame, with a frame synchronization bit sequence indicating the start of the frame, and sometimes also a bit synchronization bit sequence, typically 01010101, for identification of the bit transition times. Note that at the physical layer, this is considered as synchronous serial communication. Examples of packet mode data link protocols that can be/are transferred using synchronous serial communication are the HDLC, Ethernet, PPP and USB protocols.\n\nApplication layer",
        "Application layer \n\nAn asynchronous communication service or application does not require a constant bit rate. Examples are file transfer, email and the World Wide Web. An example of the opposite, a synchronous communication service, is realtime streaming media, for example IP telephony, IPTV and video conferencing.\n\nElectronically mediated communication",
        "Electronically mediated communication \n\nElectronically mediated communication often happens asynchronously in that the participants do not communicate concurrently. Examples include email\nand bulletin-board systems, where participants send or post messages at different times than they read them. The term \"asynchronous communication\" acquired currency in the field of online learning, where teachers and students often exchange information asynchronously instead of synchronously (that is, simultaneously), as they would in face-to-face or in telephone conversations.\n\nSee also\n Synchronization in telecommunications\n Asynchronous serial communication\n Asynchronous system\n Asynchronous circuit\n Anisochronous\n Baud rate\n Plesiochronous\n Plesiochronous Digital Hierarchy (PDH)\n\nReferences",
        "References\n\nSynchronization\nTelecommunications techniques"
    ],
    [
        "Asynchronous Transfer Mode\nAsynchronous Transfer Mode (ATM) is a telecommunications standard defined by the American National Standards Institute and ITU-T (formerly CCITT) for digital transmission of multiple types of traffic. ATM was developed to meet the needs of the Broadband Integrated Services Digital Network as defined in the late 1980s, and designed to integrate telecommunication networks. It can handle both traditional high-throughput data traffic and real-time, low-latency content such as telephony (voice) and video. ATM provides functionality that uses features of circuit switching and packet switching networks by using asynchronous time-division multiplexing.",
        "In the OSI reference model data link layer (layer 2), the basic transfer units are called frames. In ATM these frames are of a fixed length (53 octets) called cells. This differs from approaches such as Internet Protocol (IP) (OSI layer 3) or Ethernet (also layer 2) that use variable-sized packets or frames. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the data exchange begins. These virtual circuits may be either permanent (dedicated connections that are usually preconfigured by the service provider), or switched (set up on a per-call basis using signaling and disconnected when the call is terminated).",
        "The ATM network reference model approximately maps to the three lowest layers of the OSI model: physical layer, data link layer, and network layer. ATM is a core protocol used in the synchronous optical networking and synchronous digital hierarchy (SONET/SDH) backbone of the public switched telephone network and in the Integrated Services Digital Network (ISDN) but has largely been superseded in favor of next-generation networks based on IP technology. Wireless and mobile ATM never established a significant foothold.",
        "Protocol architecture\nTo minimize queuing delay and packet delay variation (PDV), all ATM cells are the same small size. Reduction of PDV is particularly important when carrying voice traffic, because the conversion of digitized voice into an analog audio signal is an inherently real-time process. The decoder needs an evenly spaced stream of data items.\n\nAt the time of the design of ATM, 155 Mbit/s synchronous digital hierarchy with 135 Mbit/s payload was considered a fast optical network link, and many plesiochronous digital hierarchy links in the digital network were considerably slower, ranging from 1.544 to 45 Mbit/s in the US, and 2 to 34 Mbit/s in Europe.",
        "At 155 Mbit/s, a typical full-length 1,500 byte Ethernet frame would take 77.42 µs to transmit. On a lower-speed 1.544 Mbit/s T1 line, the same packet would take up to 7.8 milliseconds. A queuing delay induced by several such data packets might exceed the figure of 7.8 ms several times over. This was considered unacceptable for speech traffic.",
        "The design of ATM aimed for a low-jitter network interface. Cells were introduced to provide short queuing delays while continuing to support datagram traffic. ATM broke up all packets, data, and voice streams into 48-byte chunks, adding a 5-byte routing header to each one so that they could be reassembled later. The choice of 48 bytes was political rather than technical. When the CCITT (now ITU-T) was standardizing ATM, parties from the United States wanted a 64-byte payload because this was felt to be a good compromise between larger payloads optimized for data transmission and shorter payloads optimized for real-time applications like voice. Parties from Europe wanted 32-byte payloads because the small size (and therefore short transmission times) improve performance for voice",
        "payloads optimized for real-time applications like voice. Parties from Europe wanted 32-byte payloads because the small size (and therefore short transmission times) improve performance for voice applications. Most of the European parties eventually came around to the arguments made by the Americans, but France and a few others held out for a shorter cell length. With 32 bytes, France would have been able to implement an ATM-based voice network with calls from one end of France to the other requiring no echo cancellation. 48 bytes (plus 5 header bytes = 53) was chosen as a compromise between the two sides. 5-byte headers were chosen because it was thought that 10% of the payload was the maximum price to pay for routing information. ATM multiplexed these 53-byte cells instead of packets",
        "the two sides. 5-byte headers were chosen because it was thought that 10% of the payload was the maximum price to pay for routing information. ATM multiplexed these 53-byte cells instead of packets which reduced worst-case cell contention jitter by a factor of almost 30, reducing the need for echo cancellers.",
        "Cell structure\nAn ATM cell consists of a 5-byte header and a 48-byte payload. ATM defines two different cell formats: user–network interface (UNI) and network–network interface (NNI). Most ATM links use UNI cell format.",
        "GFC\nThe generic flow control (GFC) field is a 4-bit field that was originally added to support the connection of ATM networks to shared access networks such as a distributed queue dual bus (DQDB) ring. The GFC field was designed to give the User-Network Interface (UNI) 4 bits in which to negotiate multiplexing and flow control among the cells of various ATM connections. However, the use and exact values of the GFC field have not been standardized, and the field is always set to 0000.\nVPI\nVirtual path identifier (8 bits UNI, or 12 bits NNI)\nVCI\nVirtual channel identifier (16 bits)\nPT\nPayload type (3 bits)\nBit 3 (msbit): Network management cell.  If 0, user data cell and the following apply:\nBit 2: Explicit forward congestion indication (EFCI); 1 = network congestion experienced",
        "Payload type (3 bits)\nBit 3 (msbit): Network management cell.  If 0, user data cell and the following apply:\nBit 2: Explicit forward congestion indication (EFCI); 1 = network congestion experienced\nBit 1 (lsbit): ATM user-to-user (AAU) bit.  Used by AAL5 to indicate packet boundaries.\nCLP\nCell loss priority (1-bit)\nHEC\nHeader error control (8-bit CRC, polynomial = X8 + X2 + X + 1)",
        "ATM uses the PT field to designate various special kinds of cells for operations, administration and management (OAM) purposes, and to delineate packet boundaries in some ATM adaptation layers (AAL).  If the most significant bit (MSB) of the PT field is 0, this is a user data cell, and the other two bits are used to indicate network congestion and as a general-purpose header bit available for ATM adaptation layers. If the MSB is 1, this is a management cell, and the other two bits indicate the type: network management segment, network management end-to-end, resource management, and reserved for future use.",
        "Several ATM link protocols use the HEC field to drive a CRC-based framing algorithm, which allows locating the ATM cells with no overhead beyond what is otherwise needed for header protection. The 8-bit CRC is used to correct single-bit header errors and detect multi-bit header errors. When multi-bit header errors are detected, the current and subsequent cells are dropped until a cell with no header errors is found.\n\nA UNI cell reserves the GFC field for a local flow control and sub-multiplexing system between users. This was intended to allow several terminals to share a single network connection in the same way that two ISDN phones can share a single basic rate ISDN connection. All four GFC bits must be zero by default.",
        "The NNI cell format replicates the UNI format almost exactly, except that the 4-bit GFC field is re-allocated to the VPI field, extending the VPI to 12 bits. Thus, a single NNI ATM interconnection is capable of addressing almost 212 VPs of up to almost 216 VCs each.\n\nService types\nATM supports different types of services via AALs. Standardized AALs include AAL1, AAL2, and AAL5, and the rarely used AAL3 and AAL4. AAL1 is used for constant bit rate (CBR) services and circuit emulation. Synchronization is also maintained at AAL1. AAL2 through AAL4 are used for variable bitrate (VBR) services, and AAL5 for data. Which AAL is in use for a given cell is not encoded in the cell. Instead, it is negotiated by or configured at the endpoints on a per-virtual-connection basis.",
        "Following the initial design of ATM, networks have become much faster. A 1500 byte (12000-bit) full-size Ethernet frame takes only 1.2 µs to transmit on a 10 Gbit/s network, reducing the motivation for small cells to reduce jitter due to contention. The increased link speeds by themselves do not eliminate jitter due to queuing.\n\nATM provides a useful ability to carry multiple logical circuits on a single physical or virtual medium, although other techniques exist, such as Multi-link PPP, Ethernet VLANs, VxLAN, MPLS, and multi-protocol support over SONET.",
        "Virtual circuits  \nAn ATM network must establish a connection before two parties can send cells to each other. This is called a virtual circuit (VC). It can be a permanent virtual circuit (PVC), which is created administratively on the end points, or a switched virtual circuit (SVC), which is created as needed by the communicating parties. SVC creation is managed by signaling, in which the requesting party indicates the address of the receiving party, the type of service requested, and whatever traffic parameters may be applicable to the selected service. Call admission is then performed by the network to confirm that the requested resources are available and that a route exists for the connection.",
        "Motivation \nATM operates as a channel-based transport layer, using VCs. This is encompassed in the concept of the virtual paths (VP) and virtual channels. Every ATM cell has an 8- or 12-bit virtual path identifier (VPI) and 16-bit virtual channel identifier (VCI) pair defined in its header. The VCI, together with the VPI, is used to identify the next destination of a cell as it passes through a series of ATM switches on its way to its destination. The length of the VPI varies according to whether the cell is sent on a user-network interface (at the edge of the network), or if it is sent on a network-network interface (inside the network).",
        "As these cells traverse an ATM network, switching takes place by changing the VPI/VCI values (label swapping). Although the VPI/VCI values are not necessarily consistent from one end of the connection to the other, the concept of a circuit is consistent (unlike IP, where any given packet could get to its destination by a different route than the others). ATM switches use the VPI/VCI fields to identify the virtual channel link (VCL) of the next network that a cell needs to transit on its way to its final destination. The function of the VCI is similar to that of the data link connection identifier (DLCI) in Frame Relay and the logical channel number and logical channel group number in X.25.",
        "Another advantage of the use of virtual circuits comes with the ability to use them as a multiplexing layer, allowing different services (such as voice, Frame Relay, IP). The VPI is useful for reducing the switching table of some virtual circuits which have common paths.\n\nTypes \nATM can build virtual circuits and virtual paths either statically or dynamically. Static circuits (permanent virtual circuits or PVCs) or paths (permanent virtual paths or PVPs) require that the circuit is composed of a series of segments, one for each pair of interfaces through which it passes.",
        "PVPs and PVCs, though conceptually simple, require significant effort in large networks. They also do not support the re-routing of service in the event of a failure. Dynamically built PVPs (soft PVPs or SPVPs) and PVCs (soft PVCs or SPVCs), in contrast, are built by specifying the characteristics of the circuit (the service contract) and the two endpoints.\n\nATM networks create and remove switched virtual circuits (SVCs) on demand when requested by an end station. One application for SVCs is to carry individual telephone calls when a network of telephone switches are interconnected using ATM. SVCs were also used in attempts to replace local area networks with ATM.",
        "Routing \nMost ATM networks supporting SPVPs, SPVCs, and SVCs use the Private Network-to-Network Interface (PNNI) protocol to share topology information between switches and select a route through a network. PNNI is a link-state routing protocol like OSPF and IS-IS. PNNI also includes a very powerful route summarization mechanism to allow construction of very large networks, as well as a call admission control (CAC) algorithm which determines the availability of sufficient bandwidth on a proposed route through a network in order to satisfy the service requirements of a VC or VP.",
        "Traffic engineering \nAnother key ATM concept involves the traffic contract. When an ATM circuit is set up each switch on the circuit is informed of the traffic class of the connection. ATM traffic contracts form part of the mechanism by which quality of service (QoS) is ensured. There are four basic types (and several variants) which each have a set of parameters describing the connection.\n CBR  Constant bit rate: a Peak Cell Rate (PCR) is specified, which is constant.\n VBR  Variable bit rate: an average or Sustainable Cell Rate (SCR) is specified, which can peak at a certain level, a PCR, for a maximum interval before being problematic.\n ABR  Available bit rate: a minimum guaranteed rate is specified.",
        "ABR  Available bit rate: a minimum guaranteed rate is specified.\n UBR  Unspecified bit rate: traffic is allocated to all remaining transmission capacity.",
        "VBR has real-time and non-real-time variants, and serves for bursty traffic. Non-real-time is sometimes abbreviated to vbr-nrt. Most traffic classes also introduce the concept of cell-delay variation tolerance (CDVT), which defines the clumping of cells in time.",
        "Traffic policing \nTo maintain network performance, networks may apply traffic policing to virtual circuits to limit them to their traffic contracts at the entry points to the network, i.e. the user–network interfaces (UNIs) and network-to-network interfaces (NNIs): usage/network parameter control (UPC and NPC). The reference model given by the ITU-T and ATM Forum for UPC and NPC is the generic cell rate algorithm (GCRA), which is a version of the leaky bucket algorithm. CBR traffic will normally be policed to a PCR and CDVt alone, whereas VBR traffic will normally be policed using a dual leaky bucket controller to a PCR and CDVt and an SCR and Maximum Burst Size (MBS). The MBS will normally be the packet (SAR-SDU) size for the VBR VC in cells.",
        "If the traffic on a virtual circuit is exceeding its traffic contract, as determined by the GCRA, the network can either drop the cells or mark the Cell Loss Priority (CLP) bit (to identify a cell as potentially redundant). Basic policing works on a cell by cell basis, but this is sub-optimal for encapsulated packet traffic (as discarding a single cell will invalidate the whole packet). As a result, schemes such as partial packet discard (PPD) and early packet discard (EPD) have been created that will discard a whole series of cells until the next packet starts. This reduces the number of useless cells in the network, saving bandwidth for full packets. EPD and PPD work with AAL5 connections as they use the end of packet marker: the ATM user-to-ATM user (AUU) indication bit in the",
        "of useless cells in the network, saving bandwidth for full packets. EPD and PPD work with AAL5 connections as they use the end of packet marker: the ATM user-to-ATM user (AUU) indication bit in the payload-type field of the header, which is set in the last cell of a SAR-SDU.",
        "Traffic shaping \nTraffic shaping usually takes place in the network interface card (NIC) in user equipment, and attempts to ensure that the cell flow on a VC will meet its traffic contract, i.e. cells will not be dropped or reduced in priority at the UNI. Since the reference model given for traffic policing in the network is the GCRA, this algorithm is normally used for shaping as well, and single and dual leaky bucket implementations may be used as appropriate.",
        "Reference model\nThe ATM network reference model approximately maps to the three lowest layers of the OSI reference model. It specifies the following layers:\nAt the physical network level, ATM specifies a layer that is equivalent to the OSI physical layer.\nThe ATM layer 2 roughly corresponds to the OSI data link layer.\nThe OSI network layer is implemented as the ATM adaptation layer (AAL).\n\nDeployment",
        "Deployment \n\nATM became popular with telephone companies and many computer makers in the 1990s. However, even by the end of the decade, the better price/performance of Internet Protocol-based products was competing with ATM technology for integrating real-time and bursty network traffic. Companies such as FORE Systems focused on ATM products, while other large vendors such as Cisco Systems provided ATM as an option. After the burst of the dot-com bubble, some still predicted that \"ATM is going to dominate\". However, in 2005 the ATM Forum, which had been the trade organization promoting the technology, merged with groups promoting other technologies, and eventually became the Broadband Forum.",
        "Wireless or mobile ATM",
        "Wireless ATM, or mobile ATM, consists of an ATM core network with a wireless access network. ATM cells are transmitted from base stations to mobile terminals. Mobility functions are performed at an ATM switch in the core network, known as \"crossover switch\", which is similar to the MSC (mobile switching center) of GSM networks. The advantage of wireless ATM is its high bandwidth and high speed handoffs done at layer 2. In the early 1990s, Bell Labs and NEC research labs worked actively in this field. Andy Hopper from the University of Cambridge Computer Laboratory also worked in this area. There was a wireless ATM forum formed to standardize the technology behind wireless ATM networks. The forum was supported by several telecommunication companies, including NEC, Fujitsu and AT&T. Mobile",
        "was a wireless ATM forum formed to standardize the technology behind wireless ATM networks. The forum was supported by several telecommunication companies, including NEC, Fujitsu and AT&T. Mobile ATM aimed to provide high speed multimedia communications technology, capable of delivering broadband mobile communications beyond that of GSM and WLANs.",
        "See also \n VoATM\n ATM25\n\nNotes\n\nReferences \n\n \n \n \n \n \n \n ATM Cell formats- Cisco Systems\n\nExternal links \n \n ATM Info and resources \n ATM ChipWeb - Chip and NIC database\n A tutorial from Juniper web site\n ATM Tutorial\n \n\n \nITU-T recommendations\nLink protocols\nNetworking standards"
    ],
    [
        "Atanasoff–Berry computer\nThe Atanasoff–Berry computer (ABC) was the first automatic electronic digital computer. Limited by the technology of the day, and execution, the device has remained somewhat obscure. The ABC's priority is debated among historians of computer technology, because it was neither programmable, nor Turing-complete. Conventionally, the ABC would be considered the first electronic ALU (arithmetic logic unit) which is integrated into every modern processor's design.",
        "Its unique contribution was to make computing faster by being the first to use vacuum tubes to do the arithmetic calculations. Prior to this, slower electro-mechanical methods were used by Konrad Zuse's Z1 computer, and the simultaneously developed Harvard Mark I. The first electronic, programmable, digital machine, the Colossus computer from 1943 to 1945, used similar tube-based technology as ABC.",
        "Overview\nConceived in 1937, the machine was built by Iowa State College mathematics and physics professor John Vincent Atanasoff with the help of graduate student Clifford Berry. It was designed only to solve systems of linear equations and was successfully tested in 1942. However, its intermediate result storage mechanism, a paper card writer/reader, was not perfected, and when John Vincent Atanasoff left Iowa State College for World War II assignments, work on the machine was discontinued. The ABC pioneered important elements of modern computing, including binary arithmetic and electronic switching elements,  but its special-purpose nature and lack of a changeable, stored program distinguish it from modern computers. The computer was designated an IEEE Milestone in 1990.",
        "Atanasoff and Berry's computer work was not widely known until it was rediscovered in the 1960s, amid patent disputes over the first instance of an electronic computer. At that time ENIAC, that had been created by John Mauchly and J. Presper Eckert, was considered to be the first computer in the modern sense, but in 1973 a U.S. District Court invalidated the ENIAC patent and concluded that the ENIAC inventors had derived the subject matter of the electronic digital computer from Atanasoff. When, in the mid-1970s, the secrecy surrounding the British World War II development of the Colossus computers that pre-dated ENIAC, was lifted and Colossus was described at a conference in Los Alamos, New Mexico, in June 1976, John Mauchly and Konrad Zuse were reported to have been astonished.",
        "Design and construction",
        "According to Atanasoff's account, several key principles of the Atanasoff–Berry computer were conceived in a sudden insight after a long nighttime drive to Rock Island, Illinois, during the winter of 1937–38. The ABC innovations included electronic computation, binary arithmetic, parallel processing, regenerative capacitor memory, and a separation of memory and computing functions. The mechanical and logic design was worked out by Atanasoff over the next year. A grant application to build a proof of concept prototype was submitted in March 1939 to the Agronomy department, which was also interested in speeding up computation for economic and research analysis. $5,000 of further funding () to complete the machine came from the nonprofit Research Corporation of New York City.",
        "The ABC was built by Atanasoff and Berry in the basement of the physics building at Iowa State College during 1939–1942. The initial funds were released in September, and the 11-tube prototype was first demonstrated in October 1939. A December demonstration prompted a grant for construction of the full-scale machine. The ABC was built and tested over the next two years. A January 15, 1941, story in the Des Moines Register announced the ABC as \"an electrical computing machine\" with more than 300 vacuum tubes that would \"compute complicated algebraic equations\" (but gave no precise technical description of the computer). The system weighed more than . It contained approximately  of wire, 280 dual-triode vacuum tubes, 31 thyratrons, and was about the size of a desk.",
        "It was not programmable, which distinguishes it from more general machines of the same era, such as Konrad Zuse's 1941 Z3 (or earlier iterations) and the Colossus computers of 1943–1945. Nor did it implement the stored-program architecture, first implemented in the Manchester Baby of 1948, required for fully general-purpose practical computing machines.\n\nThe machine was, however, the first to implement:\nUsing vacuum tubes, rather than wheels, ratchets, mechanical switches, or telephone relays, allowing for greater speed than previous computers\nUsing capacitors for memory, rather than mechanical components, allowing for greater speed and density",
        "The memory of the Atanasoff–Berry computer was a system called regenerative capacitor memory, which consisted of a pair of drums, each containing 1600 capacitors that rotated on a common shaft once per second. The capacitors on each drum were organized into 32 \"bands\" of 50 (30 active bands and two spares in case a capacitor failed), giving the machine a speed of 30 additions/subtractions per second. Data was represented as 50-bit binary fixed-point numbers. The electronics of the memory and arithmetic units could store and operate on 60 such numbers at a time (3000 bits).\n\nThe alternating current power-line frequency of 60 Hz was the primary clock rate for the lowest-level operations.",
        "The alternating current power-line frequency of 60 Hz was the primary clock rate for the lowest-level operations.\n\nThe arithmetic logic functions were fully electronic, implemented with vacuum tubes. The family of logic gates ranged from inverters to two- and three-input gates. The input and output levels and operating voltages were compatible between the different gates. Each gate consisted of one inverting vacuum-tube amplifier, preceded by a resistor divider input network that defined the logical function. The control logic functions, which only needed to operate once per drum rotation and therefore did not require electronic speed, were electromechanical, implemented with relays.",
        "The ALU operated on only one bit of each number at a time; it kept the carry/borrow bit in a capacitor for use in the next AC cycle.\n\nAlthough the Atanasoff–Berry computer was an important step up from earlier calculating machines, it was not able to run entirely automatically through an entire problem. An operator was needed to operate the control switches to set up its functions, much like the electro-mechanical calculators and unit record equipment of the time. Selection of the operation to be performed, reading, writing, converting to or from binary to decimal, or reducing a set of equations was made by front-panel switches and, in some cases, jumpers.",
        "There were two forms of input and output: primary user input and output and an intermediate results output and input. The intermediate results storage allowed operation on problems too large to be handled entirely within the electronic memory. (The largest problem that could be solved without the use of the intermediate output and input was two simultaneous equations, a trivial problem.)",
        "Intermediate results were binary, written onto paper sheets by electrostatically modifying the resistance at 1500 locations to represent 30 of the 50-bit numbers (one equation). Each sheet could be written or read in one second. The reliability of the system was limited to about 1 error in 100,000 calculations by these units, primarily attributed to lack of control of the sheets' material characteristics. In retrospect, a solution could have been to add a parity bit to each number as written. This problem was not solved by the time Atanasoff left the university for war-related work.\n\nPrimary user input was decimal, via standard IBM 80-column punched cards, and output was decimal, via a front-panel display.",
        "Primary user input was decimal, via standard IBM 80-column punched cards, and output was decimal, via a front-panel display.\n\nFunction\nThe ABC was designed for a specific purpose the solution of systems of simultaneous linear equations. It could handle systems with up to 29 equations, a difficult problem for the time. Problems of this scale were becoming common in physics, the department in which John Atanasoff worked. The machine could be fed two linear equations with up to 29 variables and a constant term and eliminate one of the variables. This process would be repeated manually for each of the equations, which would result in a system of equations with one fewer variable. Then the whole process would be repeated to eliminate another variable.",
        "George W. Snedecor, the head of Iowa State's Statistics Department, was very likely the first user of an electronic digital computer to solve real-world mathematics problems. He submitted many of these problems to Atanasoff.",
        "Patent dispute\nOn June 26, 1947, J. Presper Eckert and John Mauchly were the first to file for patent on a digital computing device (ENIAC), much to the surprise of Atanasoff. The ABC had been examined by John Mauchly in June 1941, and Isaac Auerbach, a former student of Mauchly's, alleged that it influenced his later work on ENIAC, although Mauchly denied this. The ENIAC patent did not issue until 1964, and by 1967 Honeywell sued Sperry Rand in an attempt to break the ENIAC patents, arguing that the ABC constituted prior art. The United States District Court for the District of Minnesota released its judgement on October 19, 1973, finding in Honeywell v. Sperry Rand that the ENIAC patent was a derivative of John Atanasoff's invention.\n\nCampbell-Kelly and Aspray conclude:",
        "Campbell-Kelly and Aspray conclude:\n\nThe case was legally resolved on October 19, 1973, when U.S. District Judge Earl R. Larson held the ENIAC patent invalid, ruling that the ENIAC derived many basic ideas from the Atanasoff–Berry computer. Judge Larson explicitly stated:\n\nHerman Goldstine, one of the original developers of ENIAC wrote:\n\nReplica\nThe original ABC was eventually dismantled in 1948, when the university converted the basement to classrooms, and all of its pieces except for one memory drum were discarded.",
        "Replica\nThe original ABC was eventually dismantled in 1948, when the university converted the basement to classrooms, and all of its pieces except for one memory drum were discarded.\n\nIn 1997, a team of researchers led by Dr. Delwyn Bluhm and John Gustafson from Ames Laboratory (located on the Iowa State University campus) finished building a working replica of the Atanasoff–Berry computer at a cost of $350,000 (equivalent to $ in ). The replica ABC was on display in the first floor lobby of the Durham Center for Computation and Communication at Iowa State University and was subsequently exhibited at the Computer History Museum.\n\nSee also\n History of computing hardware\n List of vacuum-tube computers\n Mikhail Kravchuk\n\nReferences\n\nBibliography",
        "See also\n History of computing hardware\n List of vacuum-tube computers\n Mikhail Kravchuk\n\nReferences\n\nBibliography\n\nExternal links\nThe Birth of the ABC\nReconstruction of the ABC, 1994-1997\nJohn Gustafson, Reconstruction of the Atanasoff-Berry Computer\nThe ENIAC patent trial\nHoneywell v. Sperry Rand Records, 1846-1973, Charles Babbage Institute, University of Minnesota.\nThe Atanasoff-Berry Computer In Operation (YouTube)\n\n1940s computers\nOne-of-a-kind computers\nVacuum tube computers\nComputer-related introductions in 1942\nEarly computers\nIowa State University\nSerial computers"
    ],
    [
        "Atari 2600\nThe Atari 2600 is a home video game console developed and produced by Atari, Inc. Released in September 1977, it popularized microprocessor-based hardware and games stored on swappable ROM cartridges, a format first used with the Fairchild Channel F in 1976. Branded as the Atari Video Computer System (Atari VCS) from its release until November 1982, the VCS was bundled with two joystick controllers, a conjoined pair of paddle controllers, and a game cartridgeinitially Combat and later Pac-Man.",
        "Atari was successful at creating arcade video games, but their development cost and limited lifespan drove CEO Nolan Bushnell to seek a programmable home system. The first inexpensive microprocessors from MOS Technology in late 1975 made this feasible. The console was prototyped as codename Stella by Atari subsidiary Cyan Engineering. Lacking funding to complete the project, Bushnell sold Atari to Warner Communications in 1976.",
        "The Atari VCS launched in 1977 with nine simple, low-resolution games in 2 KB cartridges. The system's first killer app was the home conversion of Taito's arcade game Space Invaders in 1980. The VCS became widely successful, leading to the founding of Activision and other third-party game developers and to competition from console manufacturers Mattel and Coleco. By the end of its primary lifecycle in 1983–84, games for the 2600 were using more than four times the storage size of the launch games with significantly more advanced visuals and gameplay than the system was designed for, such as Activision's Pitfall!",
        "By 1982, the Atari 2600 was the dominant game system in North America. However, it saw competition from other consoles such as the Intellivision and ColecoVision, and poor decisions by Atari management damaged both the system and company's reputation, most notably the release of two highly anticipated games for the 2600: a port of the arcade game Pac-Man and E.T. the Extra-Terrestrial. Pac-Man became the 2600's highest-selling game, but was panned for being inferior to the arcade version. E.T. was rushed to market for the holiday shopping season and was similarly panned and became a commercial failure. Both games, and a glut of third-party shovelware, were factors in ending Atari's relevance in the console market, contributing to the video game crash of 1983.",
        "Warner sold Atari's home division to former Commodore CEO Jack Tramiel in 1984. In 1986, the new Atari Corporation under Tramiel released a lower-cost version of the 2600 and the backward-compatible Atari 7800, but it was Nintendo that led the recovery of the industry with its 1985 launch of the Nintendo Entertainment System. Production of the Atari 2600 ended on January 1, 1992, with an estimated 30 million units sold across its lifetime.\n\nHistory",
        "Atari, Inc. was founded by Nolan Bushnell and Ted Dabney in 1972. Its first major product was Pong, released in 1972, the first successful coin-operated video game. While Atari continued to develop new arcade games in following years, Pong gave rise to a number of competitors to the growing arcade game market. The competition along with other missteps by Atari led to financial problems in 1974, though recovering by the end of the year. By 1975, Atari had released a Pong home console, competing against Magnavox, the only other major producer of home consoles at the time. Atari engineers recognized, however, the limitation of custom logic integrated onto the circuit board, permanently confining the whole console to only one game. The increasing competition increased the risk, as Atari had",
        "however, the limitation of custom logic integrated onto the circuit board, permanently confining the whole console to only one game. The increasing competition increased the risk, as Atari had found with past arcade games and again with dedicated home consoles. Both platforms are built from integrating discrete electro-mechanical components into circuits, rather than programmed as on a mainframe computer. Therefore, development of a console had cost at least  plus time to complete, but the final product only had about a three-month shelf life until becoming outdated by competition.",
        "By 1974, Atari had acquired Cyan Engineering, a Grass Valley electronics company founded by Steve Mayer and Larry Emmons, both former colleagues of Bushnell and Dabney from Ampex, who helped to develop new ideas for Atari's arcade games. Even prior to the release of the home version of Pong, Cyan's engineers, led by Mayer and Ron Milner, had envisioned a home console powered by new programmable microprocessors capable of playing Atari's current arcade offerings. The programmable microprocessors would make a console's design significantly simpler and more powerful than any dedicated single-game unit. However, the cost  of such chips was far outside the range that their market would tolerate. Atari had opened negotiations to use Motorola's new 6800 in future systems.",
        "MOS Technology 6502/6507\nIn September 1975, MOS Technology debuted the 6502 microprocessor for  at the Wescon trade show in San Francisco. Mayer and Milner attended, and met with the leader of the team that created the chip, Chuck Peddle. They proposed using the 6502 in a game console, and offered to discuss it further at Cyan's facilities after the show.",
        "Over two days, MOS and Cyan engineers sketched out a 6502-based console design by Meyer and Milner's specifications. Financial models showed that even at , the 6502 would be too expensive, and Peddle offered them a planned 6507 microprocessor, a cost-reduced version of the 6502, and MOS's RIOT chip for input/output. Cyan and MOS negotiated the 6507 and RIOT chips at  a pair. MOS also introduced Cyan to Microcomputer Associates, who had separately developed debugging software and hardware for MOS, and had developed the JOLT Computer for testing the 6502, which Peddle suggested would be useful for Atari and Cyan to use while developing their system. Milner was able to demonstrate a proof-of-concept for a programmable console by implementing Tank, an arcade game by Atari's subsidiary Kee",
        "for Atari and Cyan to use while developing their system. Milner was able to demonstrate a proof-of-concept for a programmable console by implementing Tank, an arcade game by Atari's subsidiary Kee Games, on the JOLT.",
        "As part of the deal, Atari wanted a second source of the chipset. Peddle and Paivinen suggested Synertek whose co-founder, Bob Schreiner, was a friend of Peddle. In October 1975, Atari informed the market that it was moving forward with MOS. The Motorola sales team had already told its management that the Atari deal was finalized, and Motorola management was livid. They announced a lawsuit against MOS the next week.\n\nBuilding the system",
        "Building the system\n\nBy December 1975, Atari hired Joe Decuir, a recent graduate from University of California, Berkeley who had been doing his own testing on the 6502. Decuir began debugging the first prototype designed by Mayer and Milner, which gained the codename \"Stella\" after the brand of Decuir's bicycle. This prototype included a breadboard-level design of the graphics interface to build upon. A second prototype was completed by March 1976 with the help of Jay Miner, who created a chip called the Television Interface Adaptor (TIA) to send graphics and audio to a television. The second prototype included a TIA, a 6507, and a ROM cartridge slot and adapter.",
        "As the TIA's design was refined, Al Alcorn brought in Atari's game developers to provide input on features. There are significant limitations in the 6507, the TIA, and other components, so the programmers creatively optimized their games to maximize the console. The console lacks a framebuffer and requires games to instruct the system to generate graphics in synchronization with the electron gun in the cathode-ray tube (CRT) as it scans across rows on the screen. The programmers found ways to \"race the beam\" to perform other functions while the electron gun scans outside of the visible screen.",
        "Alongside the electronics development, Bushnell brought in Gene Landrum, a consultant who had just prior consulted for Fairchild Camera and Instrument for its upcoming Channel F, to determine the consumer requirements for the console. In his final report, Landrum suggested a living room aesthetic, with a wood grain finish, and the cartridges must be \"idiot proof, child proof and effective in resisting potential static [electricity] problems in a living room environment\". Landrum recommended it include four to five dedicated games in addition to the cartridges, but this was dropped in the final designs. The cartridge design was done by James Asher and Douglas Hardy. Hardy had been an engineer for Fairchild and helped in the initial design of the Channel F cartridges, but he quit to join",
        "designs. The cartridge design was done by James Asher and Douglas Hardy. Hardy had been an engineer for Fairchild and helped in the initial design of the Channel F cartridges, but he quit to join Atari in 1976. The interior of the cartridge that Asher and Hardy designed was sufficiently different to avoid patent conflicts, but the exterior components were directly influenced by the Channel F to help work around the static electricity concerns.",
        "Atari was still recovering from its 1974 financial woes and needed additional capital to fully enter the home console market, though Bushnell was wary of being beholden to outside financial sources. Atari obtained smaller investments through 1975, but not at the scale it needed, and began considering a sale to a larger firm by early 1976. Atari was introduced to Warner Communications, which saw the potential for the growing video game industry to help offset declining profits from its film and music divisions. Negotiations took place during 1976, during which Atari cleared itself of liabilities, including settling a patent infringement lawsuit with Magnavox over Ralph H. Baer's patents that were the basis for the Magnavox Odyssey. In mid-1976, Fairchild announced the Channel F, planned",
        "including settling a patent infringement lawsuit with Magnavox over Ralph H. Baer's patents that were the basis for the Magnavox Odyssey. In mid-1976, Fairchild announced the Channel F, planned for release later that year, beating Atari to the market.",
        "By October 1976, Warner and Atari agreed to the purchase of Atari for . Warner provided an estimated  which was enough to fast-track Stella. By 1977, development had advanced enough to brand it the \"Atari Video Computer System\" (VCS) and start developing games.\n\nLaunch and success\n\nThe unit was showcased on June 4, 1977, at the Summer Consumer Electronics Show with plans for retail release in October. The announcement was purportedly delayed to wait out the terms of the Magnavox patent lawsuit settlement, which would have given Magnavox all technical information on any of Atari's products announced between June 1, 1976, and June 1, 1977. However, Atari encountered production problems during its first batch, and its testing was complicated by the use of cartridges.",
        "The Atari VCS was launched in September 1977 at , with two joysticks and a Combat cartridge; eight additional games were sold separately. Most of the launch games were based on arcade games developed by Atari or its subsidiary Kee Games: for example, Combat was based on Kee's Tank (1974) and Atari's Jet Fighter (1975). Atari sold between 350,000 and 400,000 Atari VCS units during 1977, attributed to the delay in shipping the units and consumers' unfamiliarity with a swappable-cartridge console that is not dedicated to only one game.",
        "In 1978, Atari sold only 550,000 of the 800,000 systems manufactured. This required further financial support from Warner to cover losses. Atari sold 1 million consoles in 1979, particularly during the holiday season, but there was new competition from the Mattel Electronics Intellivision and Magnavox Odyssey², which also use swappable ROM cartridges.  The 2019 book They Create Worlds has Atari selling about 600,000 VCS systems in 1979, bringing the installed base to a little over 1.3 million.",
        "Atari obtained a license from Taito to develop a VCS conversion of its 1978 arcade hit Space Invaders. This is the first officially licensed arcade conversion for a home console. Its release in March 1980 doubled the console's sales for the year to more than 2 million units, and was considered the Atari VCS' killer application. Sales then doubled again for the next two years. The book They Create Worlds has Atari selling 1.25 million Space Invaders cartridges and over 1 million VCS systems in 1980, nearly doubling the install base to over 2 million, and then an estimated 3.1 million VCS systems in 1981. By 1982, 10 million consoles had been sold in the United States, while its best-selling game was Pac-Man at over  copies sold by 1990. Pac-Man propelled worldwide Atari VCS sales to  units",
        "in 1981. By 1982, 10 million consoles had been sold in the United States, while its best-selling game was Pac-Man at over  copies sold by 1990. Pac-Man propelled worldwide Atari VCS sales to  units during 1982, according to a November 1983 article in InfoWorld magazine. An August 1984 InfoWorld magazine article says more than  Atari 2600 machines are sold by 1982. A March 1983 article in IEEE Spectrum magazine has about 3 million VCS sales in 1981, about 5.5 million in 1982, as well as a total of over 12 million VCS systems and estimated 120 million cartridges sold.",
        "In Europe, the Atari VCS sold 125,000 units in the United Kingdom during 1980, and 450,000 in West Germany by 1984. In France, where the VCS released in 1982, the system sold 600,000 units by 1989. The console was distributed by Epoch Co. in Japan in 1979 under the name \"Cassette TV Game\", but not sell as well as Epoch's own Cassette Vision system in 1981.\n\nIn 1982, Atari launched its second programmable console, the Atari 5200. To standardize naming, the VCS was renamed to the \"Atari 2600 Video Computer System\", or \"Atari 2600\", derived from the manufacture part number CX2600. By 1982, the 2600 cost Atari about  to make and was sold for an average of . The company spent .50 to  to manufacture each cartridge, plus  to  for advertising, wholesaling for .",
        "Third-party development\nActivision, formed by Crane, Whitehead, and Miller in 1979, started developing third-party VCS games using their knowledge of VCS design and programming tricks, and began releasing games in 1980. Kaboom! (1981) and Pitfall! (1982) are among the most successful with at least one and four million copies sold, respectively. In 1980, Atari attempted to block the sale of the Activision cartridges, accusing the four of intellectual property infringement. The two companies settled out of court, with Activision agreeing to pay Atari a licensing fee for their games. This made Activision the first third-party video game developer and established the licensing model that continues to be used by console manufacturers for game development.",
        "Activision's success led to the establishment of other third-party VCS game developers following Activision's model in the early 1980s, including U.S. Games, Telesys, Games by Apollo, Data Age, Zimag, Mystique, and CommaVid. The founding of Imagic included ex-Atari programmers. Mattel and Coleco, each already producing its own more advanced console, created simplified versions of their existing games for the 2600. Mattel used the M Network brand name for its cartridges. Third-party games accounted for half of VCS game sales by 1982.",
        "Decline and redesign",
        "In addition to third-party game development, Atari also received the first major threat to its hardware dominance from the Colecovision. Coleco had a license from Nintendo to develop a version of the arcade game Donkey Kong (1981), which was bundled with every Colecovision console. Coleco gained about 17% of the hardware market in 1982 compared to Atari's 58%. With third parties competing for market share, Atari worked to maintain dominance in the market by acquiring licenses for popular arcade games and other properties to make games from. Pac-Man has numerous technical and aesthetic flaws, but nevertheless more than 7 million copies were sold. Heading into the 1982 holiday shopping season, Atari had placed high sales expectations on E.T. the Extra-Terrestrial, a game programmed in about",
        "nevertheless more than 7 million copies were sold. Heading into the 1982 holiday shopping season, Atari had placed high sales expectations on E.T. the Extra-Terrestrial, a game programmed in about six weeks.  Atari produced an estimated four million cartridges, but the game was poorly reviewed, and only about 1.5 million units were sold.",
        "Warner Communications reported weaker results than expected in December 1982 to its shareholders, having expected a 50% year-to-year growth but only obtaining 10–15% due to declining sales at Atari. Coupled with the oversaturated home game market, Atari's weakened position led investors to start pulling funds out of video games, beginning a cascade of disastrous effects known as the video game crash of 1983. Many of the third-party developers formed prior to 1983 were closed, and Mattel and Coleco left the video game market by 1985.",
        "In September 1983, Atari sent 14 truckloads of unsold Atari 2600 cartridges and other equipment to a landfill in the New Mexico desert, later labeled the Atari video game burial. Long considered an urban legend that claimed the burial contained millions of unsold cartridges, the site was excavated in 2014, confirming reports from former Atari executives that only about 700,000 cartridges had actually been buried. Atari reported a  loss for 1983 as a whole, and continued to lose money into 1984, with a  loss reported in the second quarter. By mid-1984, software development for the 2600 had essentially stopped except that of Atari and Activision.",
        "Warner, wary of supporting its failing Atari division, started looking for buyers in 1984. Warner sold most of Atari to Jack Tramiel, the founder of Commodore International, in July 1984 for about , though Warner retained Atari's arcade business. Tramiel was a proponent of personal computers, and halted all new 2600 game development soon after the sale.",
        "The North American video game market did not recover until about 1986, after Nintendo's 1985 launch of the Nintendo Entertainment System in North America. Atari Corporation released a redesigned model of the 2600 in 1986, supported by an ad campaign touting a price of \"under 50 bucks\". With a large library of cartridges and a low price point, the 2600 continued to sell into the late 1980s. Atari released the last batch of games in 1989–90 including Secret Quest and Fatal Run. By 1986, over  Atari VCS units had been sold worldwide. The final Atari-licensed release is the PAL-only version of the arcade game KLAX in 1990.",
        "After more than 14 years on the market, the 2600 line was formally discontinued on January 1, 1992, along with the Atari 7800 and Atari 8-bit family of home computers. In Europe, last stocks of the 2600 were sold until Summer/Fall of 1995.\n\nHardware\n\nConsole",
        "Hardware\n\nConsole\n\nThe Atari 2600's CPU is the MOS Technology 6507, a version of the 6502, running at 1.19 MHz in the 2600. Though their internal silicon was identical, the 6507 was cheaper than the 6502 because its package included fewer memory-address pins—13 instead of 16. The designers of the Atari 2600 selected an inexpensive cartridge interface that has one fewer address pins than the 13 allowed by the 6507, further reducing the already limited addressable memory from 8 KB (213 = 8,192) to 4 KB (212 = 4,096). This was believed to be sufficient as Combat is itself only 2 KB. Later games circumvented this limitation with bank switching.\n\nThe console has 128 bytes of RAM for scratch space, the call stack, and the state of the game environment.",
        "The console has 128 bytes of RAM for scratch space, the call stack, and the state of the game environment.\n\nThe top bezel of the console originally had six switches: power, TV type selection (color or black-and-white), game selection, player difficulty, and game reset. The difficulty switches were moved to the back of the bezel in later versions of the console. The back bezel also included the controller ports, TV output, and power input.\n\nGraphics",
        "Graphics\n\nThe Atari 2600 was designed to be compatible with the cathode-ray tube television sets produced in the late 1970s and early 1980s, which commonly lack auxiliary video inputs to receive audio and video from another device. Therefore, to connect to a TV, the console generates a radio frequency signal compatible with the regional television standards (NTSC, PAL, or SECAM), using a special switch box to act as the television's antenna.",
        "Atari developed the Television Interface Adaptor (TIA) chip in the VCS to handle the graphics and conversion to a television signal. It provides a single-color, 20-bit background register that covers the left half of the screen (each bit represents 4 adjacent pixels) and is either repeated or reflected on the right side. There are 5 single-color sprites: two 8-pixel wide players; two 1 bit missiles, which share the same colors as the players; and a 1-pixel ball, which shares the background color. The 1-bit sprites all can be controlled to stretch to 1, 2, 4, or 8 pixels.",
        "The system was designed without a frame buffer to avoid the cost of the associated RAM. The background and sprites apply to a single scan line, and as the display is output to the television, the program can change colors, sprite positions, and background settings. The careful timing required to sync the code to the screen on the part of the programmer was labeled \"racing the beam\"; the actual game logic runs when the television beam is outside of the visible area of the screen. Early games for the system use the same visuals for pairs of scan lines, giving a lower vertical resolution, to allow more time for the next row of graphics to be prepared. Later games, such as Pitfall!, change the visuals for each scan line or extend the black areas around the screen to extend the game code's",
        "allow more time for the next row of graphics to be prepared. Later games, such as Pitfall!, change the visuals for each scan line or extend the black areas around the screen to extend the game code's processing time.",
        "Regional releases of the Atari 2600 use modified TIA chips for each region's television formats, which require games to be developed and published separately for each region. All modes are 160 pixels wide. NTSC mode provides 192 visible lines per screen, drawn at 60 Hz, with 16 colors, each at 8 levels of brightness. PAL mode provides more vertical scanlines, with 228 visible lines per screen, but drawn at 50 Hz and only 13 colors. SECAM mode, also a 50 Hz format, is limited to 8 colors, each with only a single brightness level.\n\nControllers",
        "The first VCS bundle has two types of controllers: a joystick (part number CX10) and pair of rotary paddle controllers (CX30). Driving controllers, which are similar to paddle controllers but can be continuously rotated, shipped with the Indy 500 launch game. After less than a year, the CX10 joystick was replaced with the CX40 model designed by James C. Asher. Because the Atari joystick port and CX40 joystick became industry standards, 2600 joysticks and some other peripherals work with later systems, including the MSX, Commodore 64, Amiga, Atari 8-bit family, and Atari ST. The CX40 joystick can be used with the Master System and Sega Genesis, but does not provide all the buttons of a native controller. Third-party controllers include Wico's Command Control joystick. Later, the CX42",
        "can be used with the Master System and Sega Genesis, but does not provide all the buttons of a native controller. Third-party controllers include Wico's Command Control joystick. Later, the CX42 Remote Control Joysticks, similar in appearance but using wireless technology, were released, together with a receiver whose wires could be inserted in the controller jacks.",
        "Atari introduced the CX50 Keyboard Controller in June 1978 along with two games that require it: Codebreaker and Hunt & Score. The similar, but simpler, CX23 Kid's Controller was released later for a series of games aimed at a younger audience. The CX22 Trak-Ball controller was announced in January 1983 and is compatible with the Atari 8-bit family.\n\nThere were two attempts to turn the Atari 2600 into a keyboard-equipped home computer: Atari's never-released CX3000 \"Graduate\" keyboard, and the CompuMate keyboard by Spectravideo which was released in 1983.\n\nConsole models",
        "Minor revisions",
        "The initial production of the VCS was made in Sunnyvale during 1977, using thick polystyrene plastic for the casing as to give the impression of weight from what was mostly an empty shell inside. The initial Sunnyvale batch had also included potential mounts for an internal speaker system on the casing, though the speakers were found to be too expensive to include and instead sound was routed through the TIA to the connected television. All six console switches on the front panel. Production of the unit was moved to Taiwan in 1978, where a less thick internal metal shielding was used and thinner plastic was used for the casing, reducing the system's weight. These two versions are commonly referred to as \"Heavy Sixers\" and \"Light Sixers\" respectively, referencing the six front switches.",
        "In 1980, the difficulty switches were moved to the back of the console, leaving four switches on the front. Otherwise, these four-switch consoles look nearly identical to the earlier six-switch models. In 1982 Atari rebranded the console as the \"Atari 2600\", a name first used on a version of the four-switch model without woodgrain, giving it an all-black appearance.\n\nSears Video Arcade \nAtari continued its OEM relationship with Sears under the latter's Tele-Games brand, which started in 1975 with the original Pong. This is unrelated to the company Telegames, which later produced 2600 cartridges. Sears released several models of the VCS as the Sears Video Arcade series starting in 1977. In 1983, the previously Japan-only Atari 2800 was rebranded as the Sears Video Arcade II.",
        "Sears released versions of Atari's games with Tele-Games branding, usually with different titles. Three games were produced by Atari for Sears as exclusive releases: Steeplechase, Stellar Track, and Submarine Commander.",
        "Atari 2800 \nThe Atari 2800 is the Japanese version of the 2600 released in October 1983. It is the first Japan-specific release of a 2600, though companies like Epoch had distributed the 2600 in Japan previously. The 2800 was released a short time after Nintendo's Family Computer (which became the dominant console in Japan), and it did not gain a significant share of the market. Sears previously released the 2800 in the US during late 1982 as the Sears Video Arcade II, which came packaged with two controllers and Space Invaders. Around 30 specially branded games were released for the 2800.",
        "Designed by engineer Joe Tilly, the 2800 has four controller ports instead of the two of the 2600. The controllers are an all-in one design using a combination of an 8-direction digital joystick and a 270-degree paddle, designed by John Amber. The 2800's case design departed from the 2600, using a wedge shape with non-protruding switches. The case style is the basis for the Atari 7800, which was redesigned for the 7800 by Barney Huang.",
        "Atari 2600 Jr.\nThe 1986 model has a smaller, cost-reduced form factor with an Atari 7800-like appearance. It was advertised as a budget gaming system (under ) with the ability to run a large collection of games. Released after the video game crash of 1983, and after the North American launch of the Nintendo Entertainment System, the 2600 was supported with new games and television commercials promoting \"The fun is back!\". Atari released several minor stylistic variations: the \"large rainbow\" (shown), \"short rainbow\", and an all-black version sold only in Ireland. Later European versions include a joypad.\n\nUnreleased prototypes\nThe Atari 2700 was a version of the 2600 with wireless controllers.",
        "Unreleased prototypes\nThe Atari 2700 was a version of the 2600 with wireless controllers.\n\nThe CX2000, with integrated joystick controllers, was a redesign based on human factor analysis by Henry Dreyfuss Associates. \n\nThe circa-1982 Atari 3200 was a  backward compatible 2600 successor.\n\nRelated hardware\nThe Atari 7800, announced in 1984 and released in 1986, is the official successor to the Atari 2600 and is backward compatible with 2600 cartridges.",
        "Multiple microconsoles are based on the Atari 2600:\nThe TV Boy includes 127 games in an enlarged joypad.\nThe Atari Classics 10-in-1 TV Game, manufactured by Jakks Pacific, emulates the 2600 with ten games inside a Atari-style joystick with composite-video output.\nThe Atari Flashback 2 (2005) contains 40 games, with four additional programs unlocked by a cheat code. It is compatible with original 2600 controllers and can be modified to play original 2600 cartridges.\nIn 2017, Hyperkin announced the RetroN 77, a clone of the Atari 2600 that plays original cartridges instead of preinstalled games.\nThe Atari VCS (2021 console) can download and emulate 2600 games via an online store.",
        "The Atari VCS (2021 console) can download and emulate 2600 games via an online store.\nAtari, Inc. plans to release the Atari 2600+, an 80% scale replica of the 1980 CX2600-A model, on November 17, 2023. The 2600+ includes support for original Atari 2600 and 7200 cartridges.",
        "Games\n\nIn 1977, nine games were released on cartridge to accompany the launch of the console: Air-Sea Battle, Basic Math, Blackjack, Combat, Indy 500, Star Ship, Street Racer, Surround, and Video Olympics. Indy 500 shipped with special \"driving controllers\", which are like paddles but rotate freely. Street Racer and Video Olympics use the standard paddle controllers. \nAtari, Inc. was the only developer for the first few years, releasing dozens of games.",
        "Atari determined that box art featuring only descriptions of the game and screenshots would not be sufficient to sell games in retail stores, since most games were based on abstract principles and screenshots give little information. Atari outsourced box art to Cliff Spohn, who created visually interesting artwork with implications of dynamic movement intended to engage the player's imagination while staying true to the gameplay. Spohn's style became a standard for Atari when bringing in assistant artists, including Susan Jaekel, Rick Guidice, John Enright, and Steve Hendricks. Spohn and Hendricks were the largest contributors to the covers in the Atari 2600 library. Ralph McQuarrie, a concept artist on the Star Wars series, was commissioned for one cover, the arcade conversion of",
        "Hendricks were the largest contributors to the covers in the Atari 2600 library. Ralph McQuarrie, a concept artist on the Star Wars series, was commissioned for one cover, the arcade conversion of Vanguard. These artists generally conferred with the programmer to learn about the game before drawing the art.",
        "An Atari VCS port of the Breakout arcade game appeared in 1978. The original is in black and white with a colored overlay, and the home version is in color. In 1980, Atari released Adventure, the first action-adventure game, and the first home game with a hidden Easter egg.\n\nRick Maurer's port of Taito's Space Invaders, released in 1980, is the first VCS game to have more than one million copies sold—eventually doubling that within a year and totaling more than  cartridges by 1983. It became the killer app to drive console sales. Versions of Atari's own Asteroids and Missile Command arcade games, released in 1981, were also major hits.",
        "Each early VCS game is in a 2K ROM. Later games, like Space Invaders, have 4K. The VCS port of Asteroids (1981) is the first game for the system to use 8K via a bank switching technique between two 4K segments. Some later releases, including Atari's ports of Dig Dug and Crystal Castles, are 16K cartridges. One of the final games, Fatal Run (1990), doubled this to 32K.\n\nTwo Atari-published games, both from the system's peak in 1982, E.T. the Extra-Terrestrial and Pac-Man, are cited as factors in the video game crash of 1983.",
        "Two Atari-published games, both from the system's peak in 1982, E.T. the Extra-Terrestrial and Pac-Man, are cited as factors in the video game crash of 1983.\n\nA company named American Multiple Industries produced a number of pornographic games for the 2600 under the Mystique Presents Swedish Erotica label. The most notorious, Custer's Revenge, was protested by women's and Native American groups because it depicted General George Armstrong Custer raping a bound Native American woman. Atari sued American Multiple Industries in court over the release of the game.\n\nLegacy",
        "Legacy\n\nThe 2600 was so successful in the late 1970s and early 1980s that \"Atari\" was a synonym for the console in mainstream media and for video games in general. Jay Miner directed the creation of the successors to the 2600's TIA chip—CTIA and ANTIC—which are central to the Atari 8-bit computers released in 1979 and later the Atari 5200 console.\n\nThe Atari 2600 was inducted into the National Toy Hall of Fame at The Strong in Rochester, New York, in 2007. In 2009, the Atari 2600 was named the number two console of all time by IGN, which cited its remarkable role behind both the first video game boom and the video game crash of 1983, and called it \"the console that our entire industry is built upon\".",
        "In November 2021, the current incarnation of Atari announced three 2600 games to be published under \"Atari XP\" label: Yars' Return, Aquaventure, and Saboteur. These were previously included in Atari Flashback consoles.\n\nNotes\n\nReferences\n\nCitations\n\nGeneral bibliography\n\nExternal links\n\n A history of the Atari VCS/2600\n Inside the Atari 2600\n Hardware and prototypes at the Atari Museum\n\n \n1970s toys\n1980s toys\n2600\nComputer-related introductions in 1977\nHome video game consoles\nProducts and services discontinued in 1992\nSecond-generation video game consoles\n65xx-based video game consoles\nDiscontinued video game consoles"
    ],
    [
        "Atari 5200\nThe Atari 5200 SuperSystem or simply Atari 5200 is a home video game console introduced in 1982 by Atari, Inc. as a higher-end complement for the popular Atari Video Computer System. The VCS was renamed to the Atari 2600 at the time of the 5200's launch. Created to compete with Mattel's Intellivision, the 5200 wound up a direct competitor of ColecoVision shortly after its release. While the Coleco system shipped with the first home version of Nintendo's Donkey Kong, the 5200 included the 1978 arcade game Super Breakout which had already appeared on the Atari 8-bit family and Atari VCS in 1979 and 1981 respectively.",
        "The CPU and the graphics and sound hardware are almost identical to that of the Atari 8-bit computers, although software is not directly compatible between the two systems. The 5200's controllers have an analog joystick and a numeric keypad along with start, pause, and reset buttons. The 360-degree non-centering joystick was touted as offering more control than the eight-way Atari CX40 joystick of the 2600, but was a focal point for criticism.\n\nOn May 21, 1984, during a press conference at which the Atari 7800 was introduced, company executives revealed that the 5200 had been discontinued after less than two years on the market. Total sales of the 5200 were reportedly in excess of 1 million units, far short of its predecessor's sales of over 30 million.\n\nHardware",
        "Hardware\n\nMuch of the technology in the Atari 8-bit family of home computer was originally developed as a second-generation games console intended to replace the Atari Video Computer System console. However, as the system was reaching completion, the personal computer revolution was starting with the release of machines like the Commodore PET, TRS-80, and Apple II. These machines had less advanced hardware than the new Atari technology, but sold for much higher prices with associated higher profit margins. Atari's management decided to enter this market, and the technology was repackaged into the Atari 400 and 800. The chipset used in these machines was created with the mindset that the VCS would likely be obsolete by 1980.",
        "Atari later decided to re-enter the games market with a design that closely matched their original 1978 specifications. In its prototype stage, the Atari 5200 was originally called the \"Atari Video System X – Advanced Video Computer System\", and was codenamed \"Pam\" after a female employee at Atari, Inc. It is also rumored that PAM actually stood for \"Personal Arcade Machine\", as the majority of games for the system ended up being arcade conversions. Actual working Atari Video System X machines, whose hardware is 100% identical to the Atari 5200 do exist, but are extremely rare.",
        "The initial 1982 release of the system featured four controller ports, where nearly all other systems of the day had only one or two ports. The 5200 also featured a new style of controller with an analog joystick, numeric keypad, two fire buttons on each side of the controller and game function keys for Start, Pause, and Reset. The 5200 also featured the innovation of the first automatic TV switchbox, allowing it to automatically switch from regular TV viewing to the game system signal when the system was activated. Previous RF adapters required the user to slide a switch on the adapter by hand. The RF box was also where the power supply connected in a unique dual power/television signal setup similar to the RCA Studio II's. A single cable coming out of the 5200 plugged into the switch",
        "hand. The RF box was also where the power supply connected in a unique dual power/television signal setup similar to the RCA Studio II's. A single cable coming out of the 5200 plugged into the switch box and carried both electricity and the television signal.",
        "The 1983 revision of the Atari 5200 has two controller ports instead of four, and a change back to the more conventional separate power supply and standard non-autoswitching RF switch. It also has changes in the cartridge port address lines to allow for the Atari 2600 adapter released that year. While the adapter was only made to work on the two-port version, modifications can be made to the four-port to make it line-compatible. In fact, towards the end of the four-port model's production run, there were a limited number of consoles produced which included these modifications. These consoles can be identified by an asterisk in their serial numbers.",
        "At one point following the 5200's release, Atari planned a smaller, cost-reduced version of the Atari 5200, which removed the controller storage bin. Code-named the \"Atari 5100\" (a.k.a. \"Atari 5200 Jr.\"), only a few fully working prototype 5100s were made before the project was canceled.\n\nControllers",
        "The controller prototypes used in the electrical development lab employed a yoke-and-gimbal mechanism that came from an RC airplane controller kit. The design of the analog joystick, which used a weak rubber boot rather than springs to provide centering, proved to be ungainly and unreliable. They quickly became the Achilles' heel of the system due to the combination of an overly complex mechanical design and a very low-cost internal flex circuit system. Another major flaw of the controllers was that the design did not translate into a linear acceleration from the center through the arc of the stick travel. The controllers did, however, include a pause button, a unique feature at the time. Various third-party replacement joysticks were also released, including those made by Wico.",
        "Atari Inc. released the Pro-Line Trak-Ball controller for the system, which was used primarily for gaming titles such as Centipede and Missile Command. A paddle controller and an updated self-centering version of the original controller were also in development, but never made it to market.\n\nGames were shipped with plastic card overlays that snapped in over the keypad. The card would indicate which game functions, such as changing the view or vehicle speed, were assigned to each key.\n\nThe primary controller was ranked the 10th worst video game controller by IGN editor Craig Harris. An editor for Next Generation said that their non-centering joysticks \"rendered many games nearly unplayable\".",
        "Internal differences from 8-bit computers\nDavid H. Ahl in 1983 described the Atari 5200 as \"a 400 computer in disguise\". Its internal design is a tweaked version of the Atari 8-bit family using the ANTIC, POKEY, and GTIA coprocessors. Software designed for one does not run on the other, but source code can be mechanically converted unless it uses computer-specific features. Antic magazine reported in 1984 that \"the similarities grossly outweigh the differences, so that a 5200 program can be developed and almost entirely debugged [on an Atari 8-bit computer] before testing on a 5200\". John J. Anderson of Creative Computing alluded to the incompatibility being intentional, caused by Atari's console division removing 8-bit compatibility to not lose control to the rival computer division.",
        "Besides the 5200's lack of a keyboard, the differences are:\n The Atari computer 10 KB operating system is replaced with a simpler 2 KB version, of which 1 KB is the built-in character set.\n Some hardware registers, such as those of the GTIA and POKEY chips, are at different memory locations.\n The purpose of some registers is slightly different on the 5200.\n The 5200's analog joysticks appear as pairs of paddles to the hardware, which requires different input handling from the digital joystick input on the Atari computers",
        "In 1987, Atari Corporation released the XE Game System console, which is a repackaged 65XE (from 1985) with a detachable keyboard that can run home computer titles directly, unlike the 5200. Anderson wrote in 1984 that Atari could have released a console compatible with computer software in 1981.\n\nReception",
        "Reception\n\nThe Atari 5200 did not fare well commercially compared to its predecessor, the Atari 2600. While it touted superior graphics to the 2600 and Mattel's Intellivision, the system was initially incompatible with the 2600's expansive library of games, and some market analysts have speculated that this hurt its sales, especially since an Atari 2600 cartridge adapter had been released for the Intellivision II. (A revised two-port model was released in 1983, along with a game adapter that allowed gamers to play all 2600 games.) This lack of new games was due in part to a lack of funding, with Atari continuing to develop most of its games for the saturated 2600 market.",
        "Many of the 5200's games appeared simply as updated versions of 2600 titles, which failed to excite consumers. Its pack-in game, Super Breakout, was criticized for not doing enough to demonstrate the system's capabilities. This gave the ColecoVision a significant advantage as its pack-in, Donkey Kong, delivered a more authentic arcade experience than any previous game cartridge. In its list of the top 25 game consoles of all time, IGN claimed that the main reason for the 5200's market failure was the technological superiority of its competitor, while other sources maintain that the two consoles are roughly equivalent in power.\n\nThe 5200 received much criticism for the \"sloppy\" design of its non-centering analog controllers. Anderson described the controllers as \"absolutely atrocious\".",
        "The 5200 received much criticism for the \"sloppy\" design of its non-centering analog controllers. Anderson described the controllers as \"absolutely atrocious\".\n\nDavid H. Ahl of Creative Computing Video & Arcade Games said in 1983 that the \"Atari 5200 is, dare I say it, Atari's answer to Intellivision, Colecovision, and the Astrocade\", describing the console as a \"true mass market\" version of the Atari 8-bit computers despite the software incompatibility. He criticized the joystick's imprecise control but said that \"it is at least as good as many other controllers\", and wondered why Super Breakout was the pack-in game when it did not use the 5200's improved graphics.\n\nTechnical specifications",
        "CPU: Custom MOS Technology 6502C @ 1.79 MHz (not a 65C02)\n Graphics chips: ANTIC and GTIA\n Support hardware: 3 custom VLSI chips\n Screen resolution: 14 modes: Six text modes (8×8, 4×8, and 8×10 character matrices supported), Eight graphics modes including 80 pixels per line (16 color), 160 pixels per line (4 color), 320 pixels per line (2 color), variable height and width up to overscan 384×240 pixels\n Color palette: 128 (16 hues, 8 luma) or 256 (16 hues, 16 luma)",
        "Color palette: 128 (16 hues, 8 luma) or 256 (16 hues, 16 luma)\n Colors on screen: 2 (320 pixels per line) to 16 (80 pixels per line). Up to 23 colors per line with player/missile and playfield priority control mixing.  Register values can be changed at every scanline using ANTIC display list interrupts, allowing up to 256 (16 hues, 16 luma) to be displayed at once, with up to 16 per scanline.\n Sprites: Four 8-pixel-wide sprites, four 2-pixel-wide sprites; height of each is either 128 or 256 pixels; 1 color per sprite",
        "Sprites: Four 8-pixel-wide sprites, four 2-pixel-wide sprites; height of each is either 128 or 256 pixels; 1 color per sprite\n Scrolling: Coarse and fine scrolling horizontally and vertically. (Horizontal coarse scroll 4, 8, or 16-pixel/color clock increments, and vertically by mode line height 2, 4, 8, or 16 scan lines.) (Or horizontal fine scroll 0 to 3, 7, or 15  single-pixel/color clock increments and then a 4, 8, or 16-pixel/color clock increment coarse scroll; and vertical fine scroll 0 to 1, 3, 7, or 15 scan line increments and then a 2, 4, 8, or 16 scan line increment coarse scroll)\n Sound: 4-channel PSG sound via POKEY sound chip, which also handles keyboard scanning, serial I/O, high resolution interrupt capable timers (single cycle accurate), and random number generation.",
        "Sound: 4-channel PSG sound via POKEY sound chip, which also handles keyboard scanning, serial I/O, high resolution interrupt capable timers (single cycle accurate), and random number generation.\n RAM: 16 KB\n ROM:\n 2 KB on-board BIOS for system startup and interrupt routing.\n 32 KB ROM window for standard game cartridges, expandable using bank switching techniques.\n Dimensions: 13\" × 15\" × 4.25\"",
        "Popular culture\nCritical to the plot of the 1984 film Cloak & Dagger is an Atari 5200 game cartridge called Cloak & Dagger. The arcade version appears in the movie; in actuality the Atari 5200 version was started but never completed. The game was under development with the title Agent X when the movie producers and Atari learned of each other's projects and decided to cooperate. This collaboration was part of a larger phenomenon, of films featuring video games as critical plot elements (as with Tron and The Last Starfighter) and of video game tie-ins to the same films (as with the Tron games for the Intellivision and other platforms).\n\nGames\n\nSee also\n List of Atari 5200 emulators\n Video game crash of 1983\n\nReferences\n\nExternal links",
        "Games\n\nSee also\n List of Atari 5200 emulators\n Video game crash of 1983\n\nReferences\n\nExternal links\n\n AtariAge – Comprehensive Atari 5200 database and information \n Atari Museum 5200 Super System section \n\n5200\nHome video game consoles\nSecond-generation video game consoles\nProducts introduced in 1982\n65xx-based video game consoles\nDiscontinued video game consoles"
    ],
    [
        "Atari 7800\nThe Atari 7800 ProSystem, or simply the Atari 7800, is a home video game console officially released by Atari Corporation in 1986 as the successor to both the Atari 2600 and Atari 5200. It can run almost all Atari 2600 cartridges, making it one of the first consoles with backward compatibility. It shipped with a different model of joystick from the 2600-standard CX40 and Pole Position II as the pack-in game. Most of the announced titles at launch were ports of 1981–1983 arcade video games.",
        "Designed by General Computer Corporation, the 7800 has significantly improved graphics hardware over Atari's previous consoles, but the same Television Interface Adaptor chip that launched with the 2600 in 1977 is used to generate audio. In an effort to prevent the flood of poor quality games that contributed to the video game crash of 1983, cartridges had to be digitally signed by Atari.\n\nThe Atari 7800 was first announced by Atari, Inc. on May 21, 1984, but a general release was shelved until May 1986 due to the sale of the company. Atari Corporation dropped support for the 7800, along with the 2600 and the Atari 8-bit family, on January 1, 1992.",
        "History \nAtari had been facing pressure from Coleco and its ColecoVision console, which supported graphics that more closely mirrored arcade games of the time than either the Atari 2600 or 5200. The Atari 5200 (released as a successor to the Atari 2600) was criticized for not being able to play 2600 games without an adapter.\n\nThe Atari 7800 ProSystem was the first console from Atari, Inc. designed by an outside company, General Computer Corporation. It was designed in 1983–84 with an intended mass market rollout in June 1984, but was canceled after the sale of the company to Tramel Technology Ltd on July 2, 1984. The project was originally called the Atari 3600.",
        "With a background in creating arcade games such as Food Fight, GCC designed the new system with a graphics architecture similar to arcade machines of the time. The CPU is a slightly customized 6502 processor, the Atari SALLY, running at 1.79 MHz. By some measures the 7800 is more powerful, and by others less, than the 1983 Nintendo Entertainment System. It uses the 2600's Television Interface Adaptor chip, with the same restrictions, for generating two-channels of audio.",
        "Launch \nThe 7800 was announced on May 21, 1984. Thirteen games were announced for the system's launch: Ms. Pac-Man, Pole Position II, Centipede, Joust, Dig Dug, Nile Flyer (eventually released as Desert Falcon), Robotron: 2084, Galaga, Food Fight, Ballblazer, Rescue on Fractalus! (later canceled), Track & Field, and Xevious.",
        "On July 2, 1984, Warner Communications sold Atari's Consumer Division to Jack Tramiel. All projects were halted during an initial evaluation period. GCC had not been paid for their development of the 7800, and Warner and Tramiel fought over who was accountable. In May 1985, Tramiel relented and paid GCC. This led to additional negotiations regarding the launch titles GCC had developed, then an effort to find someone to lead their new video game division, which was completed in November 1985. The original production run of the Atari 7800 languished in warehouses until it was introduced in January 1986.",
        "The console was released nationwide in May 1986 for $79.95. It launched with titles intended for the 7800's debut in 1984 and was aided by a marketing campaign with a budget in the \"low millions\" according to Atari Corporation officials. This was substantially less than the $9 million spent by Sega and the $16 million spent by Nintendo. The keyboard and high score cartridge planned by Warner were cancelled.",
        "In February 1987, Computer Entertainer reported that 100,000 Atari 7800 consoles had been sold in the United States, including those which had been warehoused since 1984. This was less than the Master System's 125,000 and the NES's 1.1 million. A complaint from owners in 1986 was the slow release of games. Galaga in August was followed by Xevious in November. By the end of 1986, the 7800 had 10 games, compared to Sega's 20 and Nintendo's 36. Atari would sell over 1 million 7800 consoles by June 1988.",
        "Discontinuation \nOn January 1, 1992, Atari Corporation announced the end of production and support for the  7800, 2600, and the 8-bit computer family including the Atari XEGS. At least one game, an unreleased port of Toki, was worked on past this date. By the time of the discontinuation, the Nintendo Entertainment System controlled 80% of the North American market while Atari had 12%. In Europe, last stocks of the 7800 were sold until summer/fall of 1995.\n\nRetro Gamer magazine issue 132 reported that according to Atari UK Marketing Manager Darryl Still, \"it was very well stocked by European retail; although it never got the consumer traction that the 2600 did, I remember we used to sell a lot of units through mail order catalogues and in the less affluent areas\".",
        "Technical specifications",
        "CPU: Atari SALLY (custom variant of the 6502)\n 1.79 MHz, which drops to 1.19 MHz when the Television Interface Adaptor or (6532 RAM-I/O-Timer) chips are accessed\n Unlike a standard 6502, SALLY can be halted in a known state with a single pin to let other devices control the bus.\n Sometimes referred to by Atari as \"6502C\", but not the same as the official MOS Technology 6502C.\n RAM: 4 KB (2 6116 2Kx8 RAM ICs)\n ROM: built in 4 KB BIOS ROM, 48 KB Cartridge ROM space without bank switching\n Graphics: MARIA custom chip\n Resolution: 160×240 (160×288 PAL) or 320×240 (320×288 PAL)\n Color palette: 256 (16 hues * 16 luma), different graphics modes restricted the number of usable colors and the number of colors per sprite\n Direct Memory Access (DMA)\n Graphics clock: 7.15 MHz",
        "Color palette: 256 (16 hues * 16 luma), different graphics modes restricted the number of usable colors and the number of colors per sprite\n Direct Memory Access (DMA)\n Graphics clock: 7.15 MHz\n Line buffer: 200 bytes (double buffering), 160 sprite pixels per scanline, up to 30 sprites per scanline (without background), up to 100 sprites on screen\n Sprite/zone sizes: 4 to 160 width, height of 4, 8 or 16 pixels\n Colors per sprite: 1 to 12 (1 to 8 visible colors, 1 to 4 transparency bits)\n I/O: Joystick and console switch IO handled by 6532 RIOT and TIA\n Ports\n2 joystick ports\ncartridge port\nexpansion connector\npower in\nRF output\n Sound: TIA as used in the 2600 for video and sound. In 7800 mode it is only used for sound.\n At least two games include a POKEY sound chip for improved audio.",
        "Graphics \nGraphics are generated by the custom MARIA chip, which uses an approach common in contemporary arcade system boards and is  different from other second and third generation consoles. Instead of a limited number of hardware sprites, MARIA treats everything as a sprite described in a series of display lists. Each display list contains pointers to graphics data and color and positioning information.\n\nMARIA supports a palette of 256 colors and graphics modes which are either 160 pixels wide or 320 pixels wide. While the 320 pixel modes theoretically enable the 7800 to create games at higher resolution than the 256 pixel wide graphics found in the Nintendo Entertainment System and Master System, the processing demands of MARIA result in most games using the 160 pixel mode.",
        "Each sprite can have from 1 to 12 colors, with 3 colors plus transparency being the most common. In this format, the sprite references one of 8 palettes, where each palette holds 3 colors. The background–visible when not covered by other objects–can also be assigned a color. In total, 25 colors can appear on a scan line.\n\nThe graphics resolution, color palettes, and background color can be adjusted between scan lines. This can be used to render high resolution text in one area of the screen, while displaying more colorful graphics at lower resolution in the gameplay area.",
        "Sound \nThe 7800 uses the TIA chip for two channel audio, the same chip used in the 1977 Atari VCS, and the sound is of the same quality as that system. To compensate, GCC's engineers allowed games to include a POKEY audio chip in the cartridge. Only Ballblazer and Commando do this.\n\nGCC planned to make a low-cost, high performance sound chip, GUMBY, which could also be placed in 7800 cartridges to enhance its sound capabilities further. This project was cancelled when Atari was sold to Jack Tramiel.",
        "Digitally signed cartridges \nFollowing the large number of low quality, third party games for the Atari 2600, Atari required that cartridges for the 7800 be digitally signed. When a cartridge is inserted into the system, the BIOS generates a signature of the cartridge ROM and compares it to the one stored on the cartridge. If they match, the console operates in 7800 mode, granting the game access to MARIA and other features, otherwise the console operates as a 2600. This digital signature code is not present in PAL 7800s, which use various heuristics to detect 2600 cartridges, due to export restrictions.",
        "Backward compatibility \nThe 7800's compatibility with the Atari 2600 is made possible by including many of the same chips used in the 2600. When playing an Atari 2600 game, the 7800 uses a Television Interface Adaptor chip to generate graphics and sound. The processor is slowed to 1.19 MHz, to mirror the performance of the 2600's 6507 chip. RAM is limited to 128 bytes and cartridge data is accessed in 4K blocks.",
        "When in 7800 mode (signified by the appearance of the full-screen Atari logo), the graphics are generated entirely by the MARIA graphics processing unit. All system RAM is available and cartridge data is accessed in larger 48K blocks. The system's SALLY 6502 runs at its normal 1.79 MHz. The 2600 chips are used to generate sound and to provide the interfaces to the controllers and console switches.\n\nSystem revisions",
        "System revisions\n\nInitial version: two joystick ports on lower front panel. Side expansion port for upgrades and add-ons. Bundled with two CX24 Pro-Line joysticks, AC adapter, switchbox, RCA connecting cable, and Pole Position II cartridge.\nSecond revision: Slightly revised motherboard. Expansion port connector removed from motherboard but is still etched. Shell has indentation of where expansion port was to be.\nThird revision: Same as above but with only a small blemish on the shell where the expansion port was.\n\nPeripherals",
        "Peripherals \n\nThe Atari 7800 came bundled with the Atari Pro-Line Joystick, a two-button controller with a joystick for movement. The Pro-Line was developed for the 2600 and advertised in 1983, but delayed until Atari proceeded with the 7800. The right fire button only works as a separate fire button for certain 7800 games; otherwise, it duplicates the left fire button, allowing either button to be used for 2600 games. While physically compatible, the 7800's controllers do not work with the Sega Master System, and Sega's controllers are unable to use the 7800's two-button mode.",
        "In response to criticism over ergonomic issues with the Pro-Line controllers, Atari later released a joypad controller with the European 7800. Similar in style to controllers found on Nintendo and Sega systems, it was not available in the United States.\n\nThe Atari XG-1 light gun, bundled with the Atari XEGS and also sold separately, is compatible with the 7800. Atari released five 7800 light gun games: Alien Brigade, Barnyard Blaster, Crossbow, Meltdown, and Sentinel.\n\nCancelled peripherals \nAfter the acquisition of the Atari Consumer Division by Jack Tramiel in 1984, several expansion options for the system were cancelled:",
        "Cancelled peripherals \nAfter the acquisition of the Atari Consumer Division by Jack Tramiel in 1984, several expansion options for the system were cancelled:\n\n The High Score Cartridge was designed to save high scores for up to 65 separate games. The cartridge was intended as a pass-through device, similar to the later Game Genie. Nine games were programmed to support the cartridge.\n The expansion port, to allow for the addition of a planned computer keyboard and connection to laserdisc players and other peripherals, was removed in the second and third revisions of the 7800.\n A dual joystick holder was designed for Robotron: 2084 and future games like Battlezone, but not produced.\n\nGames",
        "Games \n\nWhile the system can play the over 400 games for the Atari 2600, there were only 59 official releases  for the 7800. The lineup emphasized high-quality versions of games from the golden age of arcade video games. Pole Position II, Dig Dug, and Galaga, by the time of the 1986 launch, were three, four, and five years old, respectively. A raster graphics version of 1979's Asteroids was released in 1987. In 1988, Atari published a conversion of Nintendo's Donkey Kong, seven years after the original arcade game and five years after the Atari 8-bit family cartridge.  Atari also marketed a line of games called \"Super Games\" which were arcade and computer games previously not playable on a home console such as One-On-One Basketball and Impossible Mission.",
        "Eleven games were developed and sold by three third-party companies under their own labels (Absolute Entertainment, Activision, and Froggo) with the rest published by Atari Corporation. Most of the games from Atari were developed by outside companies under contract.\n\nSome NES games were developed by companies who had licensed their title from a different arcade manufacturer. While the creator of the NES version would be restricted from making a competitive version of an NES game, the original arcade copyright holder was not precluded from licensing out rights for a home version of an arcade game to multiple systems. Through this loophole, Atari 7800 conversions of Mario Bros., Double Dragon, Commando, Rampage, Xenophobe, Ikari Warriors, and Kung-Fu Master were licensed and developed.",
        "A final batch of games was released by Atari in 1990: Alien Brigade, Basketbrawl, Fatal Run, Meltdown, Midnight Mutants, MotorPsycho, Ninja Golf, Planet Smashers, and Scrapyard Dog. Scrapyard Dog was later released for the Atari Lynx.\n\nLegacy\n\nAtari Flashback\nIn 2004, the Infogrames-owned version of Atari released the Atari Flashback console. It resembles a miniature Atari 7800 and has five 7800 and fifteen 2600 games built-in. Built using the NES-On-A-Chip hardware instead of recreating the Atari 7800 hardware, it was criticized for failing to properly replicate the actual gaming experience. A subsequent 7800 project was cancelled after prototypes were made.",
        "Game development\nThe digital signature long prevented aftermarket games from being developed. The signing software was eventually found and released at Classic Gaming Expo in 2001.\nSeveral new Atari 7800 games such as Beef Drop, B*nQ, Combat 1990, CrazyBrix, Failsafe, and Santa Simon have been released..\n\nSource code\nThe source code for 13 games, the operating system, and the development tools which run on the Atari ST were discovered in a dumpster behind the Atari building in Sunnyvale, California. Commented assembly language source code was made available for Centipede, Commando, Crossbow, Desert Falcon, Dig Dug, Food Fight, Galaga, Hat Trick, Joust, Ms. Pac-Man, Super Stunt Cycle, Robotron: 2084, and Xevious.",
        "See also\n History of Atari\n List of Atari 7800 games\n List of Atari 2600 games\n\nReferences\n\nExternal links\n\n AtariAge – Comprehensive Atari 7800 database and information \n Atari 7800 Information & Resources\n Atari Museum – History of the Atari 7800 ProSystem \n Atari 7800 Development Wiki\n ProSystem emulator for Microsoft Windows\n\n7800\nHome video game consoles\nBackward-compatible video game consoles\nThird-generation video game consoles\n1986 in video gaming\nComputer-related introductions in 1986\nProducts introduced in 1986\nProducts and services discontinued in 1992\n1980s toys\n65xx-based video game consoles\nDiscontinued video game consoles"
    ],
    [
        "Atari Jaguar\nThe Atari Jaguar is a home video game console developed by Atari Corporation and  released in North America in November 1993. Part of the fifth generation of video game consoles, it competed with the 16-bit Sega Genesis, the Super NES and the 32-bit 3DO Interactive Multiplayer that launched the same year. Powered by two custom 32-bit Tom and in addition to a Motorola 68000, Atari marketed it as the world's first 64-bit game system, emphasizing its 64-bit bus used by the blitter.  The Jaguar launched with Cybermorph as the pack-in game, which received divisive reviews. The system's library ultimately comprised only 50 licensed games.",
        "Development of the Atari Jaguar started in the early 1990s by Flare Technology, which focused on the system after cancellation of the Atari Panther console. The Jaguar was an important system for Atari after the company shifted its focus from computers - having ceased development of its Atari ST - back to consoles. However, the multi-chip architecture, hardware bugs, and poor tools made writing games for the Jaguar difficult. Underwhelming sales further eroded the console's third-party support.",
        "Atari attempted to extend the lifespan of the system with the Atari Jaguar CD add-on, with an additional 13 games, and emphasizing the Jaguar's price of over  less than its competitors. With the release of the Sega Saturn and PlayStation in 1995, sales of the Jaguar continued to fall. It sold no more than 150,000 units before it was discontinued in 1996. The commercial failure of the Jaguar prompted Atari to leave the console market.\n\nAfter Hasbro Interactive acquired all Atari Corporation properties, the patents of the Jaguar were released into the public domain, with the console declared an open platform. Since its discontinuation, hobbyists have produced games for the system.\n\nHistory",
        "History\n\nDevelopment\nAtari Corporation's previous home video game console, the Atari 7800, was released in 1986. While it sold 3.77 million units in the U.S. in the period to 1990, it was considered an 'also-ran' and far behind rival Nintendo. Around 1989 work began on a new console leveraging technology from their Atari ST computers. Originally named the Super XE - following the Atari XE Game System - it eventually became the Atari Panther using either 16 or 32-bit architecture. A more advanced system codenamed Jaguar also began work.",
        "Both the Jaguar and Panther were developed by the members of Flare Technology, a company formed by Martin Brennan and John Mathieson. The team had claimed that they could not only make a console superior to the Genesis or the Super NES, but they could also be cost-effective. Impressed by their work on the Konix Multisystem, Atari persuaded them to close Flare and form a new company called Flare II, with Atari providing the funding.",
        "Work on the Jaguar design progressed faster than expected, so Atari canceled the Panther project in 1991 to focus on the more promising Jaguar, and rumors were already circulating of a 1992 launch and its 32-bit or even 64-bit architecture. By this time the Atari ST had long been surpassed in popularity by the Amiga, while both Atari and Commodore became victims of 'Wintel', which would become the dominant computer platform. Support for Atari's legacy 8-bit products were dropped to fully focus on developing the Jaguar console, while their line of ST computers were dropped during the Jaguar's release in 1993.\n\nThe Atari Jaguar was unveiled in at the Summer Consumer Electronics Show in June 1993, calling it a \"multi-media entertainment system\".",
        "Launch\nThe Jaguar was launched on November 23, 1993, at a price of $249.99, under a $500 million manufacturing deal with IBM. The system was initially available only in the test markets of New York City and San Francisco, with the slogan \"Get bit by Jaguar\", claiming superiority over competing 16-bit and 32-bit systems. During this test launch Atari sold all units hoping it would rally support for the system. A nationwide release followed six months later, in early 1994. The Jaguar struggled to attain a substantial user base. Atari reported that it had shipped 17,000 units as part of the system's initial test market in 1993. By the end of 1994, it reported that it had sold approximately 100,000 units.",
        "Computer Gaming World wrote in January 1994 that the Jaguar was \"a great machine in search of a developer/customer base\", as Atari had to \"overcome the stigma of its name (lack of marketing and customer support, as well as poor developer relations in the past)\". Atari had \"ventured late into third party software support\" for the Jaguar while competing console 3DO's \"18 month public relations blitz\" would result in \"an avalanche of software support\", the magazine reported. The small size and poor quality of the Jaguar's game library became the most commonly cited reason for the Jaguar's tepid adoption, as early releases like Trevor McFur in the Crescent Galaxy, Raiden, and Evolution: Dino Dudes also received poor reviews, the latter two for failing to take full advantage of the Jaguar's",
        "adoption, as early releases like Trevor McFur in the Crescent Galaxy, Raiden, and Evolution: Dino Dudes also received poor reviews, the latter two for failing to take full advantage of the Jaguar's hardware.  Jaguar did eventually earn praise with games such as Tempest 2000, Doom, and Wolfenstein 3D. The most successful title during the Jaguar's first year was Alien vs. Predator. However, these occasional successes were seen as insufficient while the Jaguar's competitors were receiving a continual stream of critically acclaimed software; GamePro concluded their rave review of Alien vs. Predator by remarking \"If Atari can turn out a dozen more games like AvP, Jaguar owners could truly rest easy and enjoy their purchase.\"  Next Generation commented that \"thus far, Atari has spectacularly",
        "by remarking \"If Atari can turn out a dozen more games like AvP, Jaguar owners could truly rest easy and enjoy their purchase.\"  Next Generation commented that \"thus far, Atari has spectacularly failed to deliver on the software side, leaving many to question the actual quality and capability of the hardware. With only one or two exceptions – Tempest 2000 is cited most frequently – there have just been no truly great games for the Jaguar up to now.\" They further noted that while Atari is well known by older gamers, the company had much less overall brand recognition than Sega, Sony, Nintendo, or even The 3DO Company. However, they argued that with its low price point, the Jaguar might still compete if Atari could improve the software situation.",
        "Bit count controversy",
        "Atari tried to downplay competing consoles by proclaiming the Jaguar was the only \"64-bit\" system; in its marketing in the American market the company used the tagline do the math!, in reference to the 64 number. This claim is questioned by some, because the Motorola 68000 CPU and the Tom and Jerry coprocessors execute 32-bit instruction sets. Atari's reasoning that the 32-bit Tom and Jerry chips work in tandem to add up to a 64-bit system was ridiculed in a mini-editorial by Electronic Gaming Monthly, which commented that \"If Sega did the math for the Sega Saturn the way Atari did the math for their 64-bit Jaguar system, the Sega Saturn would be a 112-bit monster of a machine.\" Next Generation, while giving a mostly negative review of the Jaguar, maintained that it is a true 64-bit",
        "for their 64-bit Jaguar system, the Sega Saturn would be a 112-bit monster of a machine.\" Next Generation, while giving a mostly negative review of the Jaguar, maintained that it is a true 64-bit system, since the data path from the DRAM to the CPU and Tom and Jerry chips is 64 bits wide.",
        "Arrival of Saturn and PlayStation\nIn early 1995, Atari announced that they had dropped the price of the Jaguar to $149.99, in order to be more competitive. Atari ran infomercials with enthusiastic salesmen touting the game system. These aired for most of 1995, but did not sell the remaining stock of Jaguar systems.",
        "In a 1995 interview with Next Generation, then-CEO Sam Tramiel declared that the Jaguar was as powerful, if not more powerful, than the newly launched Sega Saturn, and slightly weaker than the upcoming PlayStation. Next Generation received a deluge of letters in response to Tramiel's comments, particularly his threat to bring Sony to court for price dumping if the PlayStation entered the U.S. market at a retail price below $300. Many readers found this threat hollow and hypocritical, since Tramiel noted in the same interview that Atari was selling the Jaguar at a loss. The editor responded that price dumping does not have to do with a product being priced below cost, but its being priced much lower in one country than anotherwhich, as Tramiel said, is illegal. Tramiel and Next Generation",
        "price dumping does not have to do with a product being priced below cost, but its being priced much lower in one country than anotherwhich, as Tramiel said, is illegal. Tramiel and Next Generation agreed that the PlayStation's Japanese price converts to approximately $500. His remark, that the small number of third party Jaguar games was good for Atari's profitability, angered Jaguar owners who were already frustrated at how few games were coming out for the system.",
        "In Atari's 1995 annual report, it noted:  In addition, Atari had severely limited financial resources, and so could not create the level of marketing which has historically backed successful gaming consoles.\n\nDecline\nBy November 1995, mass layoffs and insider statements were fueling journalistic speculation that Atari had ceased both development and manufacturing for the Jaguar and was simply trying to sell off existing stock before exiting the video game industry. Although Atari continued to deny these theories going into 1996, core Jaguar developers such as High Voltage Software and Beyond Games stated that they were no longer receiving communications from Atari regarding future Jaguar projects.",
        "In its 10-K405 SEC Filing, filed April 12, 1996, Atari informed stockholders that its revenues had declined by more than half, from $38.7 million in 1994 to $14.6 million in 1995, then gave them the news on the truly dire nature of the Jaguar:\n\nThe filing confirmed that Atari had abandoned the Jaguar in November 1995 and in the subsequent months were concerned chiefly with liquidating its inventory of Jaguar products. On April 8, 1996, Atari Corporation agreed to merge with JTS, Inc. in a reverse takeover, thus forming JTS Corporation. The merger was finalized on July 30.",
        "After the merger, the bulk of Jaguar inventory remained unsold and would be finally moved out to Tiger Software, a private liquidator, on December 23, 1996. On March 13, 1998, JTS sold the Atari name and all of the Atari properties to Hasbro Interactive.\n\nTechnical specifications\nFrom the Jaguar Software Reference manual, page 1:",
        "Technical specifications\nFrom the Jaguar Software Reference manual, page 1:\n\nDesign specs for the console allude to the GPU or DSP being capable of acting as a CPU, leaving the Motorola 68000 to read controller inputs. Atari's Leonard Tramiel also specifically suggested that the 68000 not be used by developers. In practice, however, many developers use the Motorola 68000 to drive gameplay logic due to the greater developer familiarity of the 68000 and the adequacy of the 68000 for certain types of games. Most critically, a flaw in the memory controller means that certain obscure conventions must be followed for the RISC chips to be able to execute code from RAM.",
        "The system was notoriously difficult to program for, not only because of its two-processor design but development tools were released in an unfinished state and the hardware had crippling bugs.",
        "Processors\n Tom chip, 26.59 MHz\n Graphics processing unit (GPU) – 32-bit RISC architecture, 4 KB internal RAM, all graphical effects are software-based, with additional instructions intended for 3D operations\n Object Processor – 64-bit fixed-function video processor, converts display lists to video output at scan time.\n Blitter – 64-bit high speed logic operations, z-buffering and Gouraud shading, with 64-bit internal registers.\n DRAM controller, 8-, 16-, 32- and 64-bit memory management\n Jerry chip, 26.59 MHz\n Digital Signal Processor – 32-bit RISC architecture, 8 KB internal RAM\n Similar RISC core as the GPU, additional instructions intended for audio operations\n CD-quality sound (16-bit stereo)\n Number of sound channels limited by software",
        "Similar RISC core as the GPU, additional instructions intended for audio operations\n CD-quality sound (16-bit stereo)\n Number of sound channels limited by software\n Two DACs (stereo) convert digital data to analog sound signals\n Full stereo capabilities\n Wavetable synthesis and AM synthesis\n A clock control block, incorporating timers, and a UART\n Joystick control\n Motorola 68000 - system processor \"used as a manager\".\n General purpose 16-/32-bit control processor, 13.295 MHz",
        "Other features\n\n RAM: 2 MB on a 64-bit bus using 4 16-bit fast-page-mode DRAMs (80 ns)\n Storage: ROM cartridges – up to 6 MB\n DSP-port (JagLink)\n Monitor-port (composite/S-Video/RGB)\n Antenna-port (UHF/VHF) - fixed at 591 MHz in Europe; not present on French model\n Support for ComLynx I/O\n NTSC/PAL machines can be identified by their power LED colour, Red: NTSC; Green: PAL.\n\nCOJAG arcade games",
        "COJAG arcade games\n\nAtari Games licensed the Atari Jaguar's chipset for use in its arcade games. The system, named COJAG (for \"Coin-Op Jaguar\"), replaced the 68000 with a 68020 or MIPS R3000-based CPU (depending on the board version), added more RAM, a full 64-bit wide ROM bus (Jaguar ROM bus being 32-bit), and optionally a hard drive (some games such as Freeze are ROM only). It runs the lightgun games Area 51 and Maximum Force, which were released by Atari as dedicated cabinets or as the Area 51 and Maximum Force combo machine. Other games were developed but never released: 3 On 3 Basketball, Fishin' Frenzy, Freeze, and Vicious Circle.\n\nPeripherals",
        "Peripherals\n\nPrior to the launch of the console in November 1993, Atari had announced a variety of peripherals to be released over the console's lifespan. This included a CD-ROM-based console, a dial-up Internet link with support for online gaming, a virtual reality headset, and an MPEG-2 video card. However, due to the poor sales and eventual commercial failure of the Jaguar, most of the peripherals in development were canceled. The only peripherals and add-ons released by Atari for the Jaguar are a redesigned controller, an adapter for four players, a CD console add-on, and a link cable for local area network (LAN) gaming.",
        "The redesigned second controller, the ProController by Atari, added three more face buttons and two triggers. It was created in response to the criticism of the original controller, said to lack enough buttons for fighting games in particular. Sold independently, however, it was never bundled with the system. The Team Tap multitap adds 4-controller support, compatible only with the optionally bundled White Men Can't Jump and NBA Jam Tournament Edition. Eight player gameplay with two Team Taps is possible but unsupported by those games. For LAN multiplayer support, the Jaglink Interface links two Jaguar consoles through a modular extension and a UTP phone cable. It is compatible with three games: AirCars, BattleSphere, and Doom.",
        "In 1994 at the CES, Atari announced that it had partnered with Phylon, Inc. to create the Jaguar Voice/Data Communicator. The unit was delayed and an estimated 100 units were produced, but eventually in 1995 was canceled. The Jaguar Voice Modem or JVM utilizes a 19.9 kbit/s dial up modem to answer incoming phone calls and store up to 18 phone numbers. Players directly dial each other for online play, only compatible with Ultra Vortek which initializes the modem by entering 911 on the key pad at startup.\n\nJaguar CD",
        "Jaguar CD\n\nThe Jaguar CD is a CD-ROM peripheral for games. It was released in September 1995, two years after the Jaguar's launch. Thirteen CD games were released during its manufacturing lifetime, with more being made later by homebrew developers. Each Jaguar CD unit has a Virtual Light Machine, which displays light patterns corresponding to music, if the user inserts an audio CD into the console. It was developed by Jeff Minter, after experimenting with graphics during the development of Tempest 2000. The program was deemed a spiritual successor to the Atari Video Music, a visualizer released in 1976.",
        "The Memory Track is a cartridge accessory for the Jaguar CD, providing Jaguar CD games with 128 K EEPROM for persistent storage of data such as preferences and saved games. The Atari Jaguar Duo (codenamed Jaguar III) was a proposal to integrate the Jaguar CD to make a new console, a concept similar to the TurboDuo and Genesis CDX. A prototype, described by journalists as resembling a bathroom scale, was unveiled at the 1995 Winter Consumer Electronics Show, but the console was canceled before production.",
        "Jaguar VR",
        "A virtual reality headset compatible with the console, tentatively titled the Jaguar VR, was unveiled by Atari at the 1995 Winter Consumer Electronics Show. The development of the peripheral was a response to Nintendo's virtual reality console, the Virtual Boy, which had been announced the previous year. The headset was developed in cooperation with Virtuality, which had previously created many virtual reality arcade systems, and was already developing a similar headset for practical purposes, named Project Elysium, for IBM. The peripheral was targeted for a commercial release before Christmas 1995. However, the deal with Virtuality was abandoned in October 1995. After Atari's merger with JTS in 1996, all prototypes of the headset were allegedly destroyed. However, two working units, one",
        "1995. However, the deal with Virtuality was abandoned in October 1995. After Atari's merger with JTS in 1996, all prototypes of the headset were allegedly destroyed. However, two working units, one low-resolution prototype with red and grey-colored graphics and one high-resolution prototype with blue and grey-colored graphics, have since been recovered, and are regularly showcased at retrogaming-themed conventions and festivals. Only one game was developed for the Jaguar VR prototype: a 3D-rendered version of the 1980 arcade game Missile Command, titled Missile Command 3D, and a demo of Virtuality's Zone Hunter was created.",
        "Unlicensed peripherals",
        "An unofficial expansion peripheral for the Atari Jaguar dubbed the \"Catbox\" was released by the Rockford, Illinois company ICD. It was originally slated to be released early in the Jaguar's life, in the second quarter of 1994, but was not actually released until mid-1995. The ICD CatBox plugs directly into the AV/DSP connectors located in the rear of the Jaguar console and provides three main functions. These are audio, video, and communications. It features six output formats, three for audio (line level stereo, RGB monitor, headphone jack with volume control) and three for video (composite, S-Video, and RGB analog component video) making the Jaguar compatible with multiple high quality monitor systems and multiple monitors at the same time. It is capable of communications methods known",
        "S-Video, and RGB analog component video) making the Jaguar compatible with multiple high quality monitor systems and multiple monitors at the same time. It is capable of communications methods known as CatNet and RS-232 as well as DSP pass through, allowing the user to connect two or more Jaguars together for multiplayer games either directly or with modems. The ICD CatBox features a polished stainless steel casing and red LEDs in the jaguar's eyes on the logo that indicate communications activity. An IBM AT-type null modem cable may be used to connect two Jaguars together. The CatBox is also compatible with Atari's Jaglink Interface peripheral.",
        "An adaptor for the Jaguar that allows for WebTV access was revealed in 1998; one prototype is known to exist.\n\nGame library\n\nReception\n\nReviewing the Jaguar just a few weeks prior to its launch, GamePro gave it a \"thumbs sideways\". They praised the power of the hardware but criticized the controller, and were dubious of how the software lineup would turn out, commenting that Atari's failure to secure support from key third party publishers such as Capcom was a bad sign. They concluded that \"Like the 3DO, the Jaguar is a risky investment – just not quite as expensive.\"\n\nThe Jaguar won GameFan'''s \"Best New System\" award for 1993.",
        "The small size and poor quality of the Jaguar's game library became the most commonly cited reason for its failure in the marketplace. The pack-in game Cybermorph was one of the first polygon-based games for consoles, but was criticized for design flaws and a weak color palette, and compared unfavorably with the SNES's Star Fox. Other early releases like Trevor McFur in the Crescent Galaxy, Raiden, and Evolution: Dino Dudes also received poor reviews, the latter two for failing to take full advantage of the Jaguar's hardware.  Jaguar did eventually earn praise with games such as Tempest 2000, Doom, and Wolfenstein 3D. The most successful title during the Jaguar's first year was Alien vs. Predator. However, these occasional successes were seen as insufficient while the Jaguar's competitors",
        "Doom, and Wolfenstein 3D. The most successful title during the Jaguar's first year was Alien vs. Predator. However, these occasional successes were seen as insufficient while the Jaguar's competitors were receiving a continual stream of critically acclaimed software; GamePro concluded their rave review of Alien vs. Predator by remarking \"If Atari can turn out a dozen more games like AvP, Jaguar owners could truly rest easy and enjoy their purchase.\" In late 1995 reviews of the Jaguar, Game Players remarked, \"The Jaguar suffers from several problems, most importantly the lack of good software.\" and Next Generation likewise commented that \"thus far, Atari has spectacularly failed to deliver on the software side, leaving many to question the actual quality and capability of the hardware.",
        "and Next Generation likewise commented that \"thus far, Atari has spectacularly failed to deliver on the software side, leaving many to question the actual quality and capability of the hardware. With only one or two exceptions – Tempest 2000 is cited most frequently – there have just been no truly great games for the Jaguar up to now.\" They further noted that while Atari is well known by older gamers, the company had much less overall brand recognition than Sega, Sony, Nintendo, or even The 3DO Company. However, they argued that with its low price point, the Jaguar might still compete if Atari could improve the software situation. They gave the system two out of five stars. Game Players also stated the despite being 64-bit, the Jaguar is much less powerful than the 3DO, Saturn, and",
        "Atari could improve the software situation. They gave the system two out of five stars. Game Players also stated the despite being 64-bit, the Jaguar is much less powerful than the 3DO, Saturn, and PlayStation, even when supplemented with the Jaguar CD. With such a small library of games to challenge the incumbent 16-bit game consoles, Jaguar's appeal never grew beyond a small gaming audience. Digital Spy commented: \"Like many failed hardware ventures, it still maintains something of a cult following but can only be considered a misstep for Atari.\"",
        "In 2006 IGN editor Craig Harris rated the original Jaguar controller as the worst game controller ever, criticizing the unwarranted recycling of the 1980s \"phone keypad\" format and the small number of action buttons, which he found particularly unwise given that Atari was actively trying to court fighting game fans to the system. Ed Semrad of Electronic Gaming Monthly commented that many Jaguar games gratuitously used all of the controller's phone keypad buttons, making the controls much more difficult than they needed to be. GamePros The Watch Dog remarked, \"The controller usually doesn't use the keypad, and for games that use the keypad extensively (Alien vs. Predator, Doom), a keypad overlay is used to minimize confusion. But yes, it is a lot of buttons for nuttin'.\" Atari added more",
        "the keypad, and for games that use the keypad extensively (Alien vs. Predator, Doom), a keypad overlay is used to minimize confusion. But yes, it is a lot of buttons for nuttin'.\" Atari added more action buttons for its Pro Controller, to improve performance in fighting games in particular.",
        "Legacy\n\nTelegames continued to publish games for the Jaguar after it was discontinued, and for a time was the only company to do so. On May 14, 1999, Hasbro Interactive announced that it had released all patents to the Jaguar, declaring it an open platform; this opened the doors for extensive homebrew development. Following the announcement, Songbird Productions joined Telegames in releasing unfinished Jaguar games alongside new games to satisfy the cult following. Hasbro Interactive, along with all the Atari properties, was sold to Infogrames on January 29, 2001.",
        "In the United Kingdom in 2001, Telegames and retailer Game made a deal to bring the Jaguar to Game's retail outlets. It was initially sold for £29.99 new and software ranged between £9.99 for more common games such as Doom and Ruiner Pinball and £39.99 for rarer releases such as Defender 2000 and Checkered Flag. The machine had a presence in the stores until 2007, when remaining consoles were sold off for £9.99 and games were sold for as low as 97p.",
        "Molds\nIn 1997, Imagin Systems, a manufacturer of dental imaging equipment, purchased the Jaguar cartridge and console molds, including the molds for the CD add-on, from JTS. The console molds could, with minor modification, fit their HotRod camera, and the cartridge molds were reused to create an optional memory expansion card. In a retrospective, Imagin founder Steve Mortenson praised the design, but admitted that their device came at the time of the dental industry's transition to USB, and apart from a few prototypes, the molds went unused.",
        "In December 2014, the molds were purchased from Imagin Systems by Mike Kennedy, owner of the Kickstarter funded Retro Videogame Magazine'', to propose a new crowdfunded video game console, the Retro VGS, later rebranded the Coleco Chameleon after entering a licensing agreement with Coleco. The purchase of the molds was far cheaper than designing and manufacturing entirely new molds, and Kennedy described their acquisition as \"the entire reason [the Retro VGS] is possible\". However, the project was terminated in March 2016 following criticism of Kennedy and doubts regarding demand for the proposed console. Two \"prototypes\" were discovered to be fakes and Coleco withdrew from the project. After the project's termination, the molds were sold to Albert Yarusso, the founder of the AtariAge",
        "console. Two \"prototypes\" were discovered to be fakes and Coleco withdrew from the project. After the project's termination, the molds were sold to Albert Yarusso, the founder of the AtariAge website.",
        "See also\n Contiki, portable operating system, including a port for the Jaguar with GUI, TCP/IP, and web browser support.\n\nReferences\n\nExternal links\n\n \n Atari Jaguar review, 1994\n\nProducts introduced in 1993\nProducts and services discontinued in 1996\nJaguar duo\nHome video game consoles\nFifth-generation video game consoles\n1990s toys\n68k-based game consoles\nDiscontinued video game consoles\n Regionless game consoles"
    ],
    [
        "Atari Lynx\nThe Atari Lynx is a hybrid 8/16-bit fourth-generation hand-held game console released by Atari Corporation in September 1989 in North America and 1990 in Europe and Japan. It was the first handheld game console with a color liquid-crystal display. Powered by a 16 MHz 65C02 8-bit CPU and a custom 16-bit blitter, the Lynx was more advanced than Nintendo's monochrome Game Boy, released two months earlier. It also competed with Sega's Game Gear and NEC's TurboExpress, released the following year.",
        "The system was developed at Epyx by two former designers of the Amiga personal computers. The project was called the Handy Game or simply Handy. In 1991, Atari replaced the Lynx with a smaller model internally referred to as the Lynx II. Atari published a total of 73 games for the Lynx before it was discontinued in 1995.",
        "History",
        "The Lynx system was originally developed by Epyx as the Handy Game. In 1986, two former Amiga designers, R. J. Mical and Dave Needle, had been asked by a former manager at Amiga, David Morse, to design a portable gaming system. Morse now worked at Epyx, a game software company with a recent string of hit games. Morse's son had asked him if he could make a portable gaming system, prompting a meeting with Mical and Needle to discuss the idea. Morse convinced Mical and Needle and they were hired by Epyx to be a part of the design team. Planning and design of the console began in 1986 and was completed in 1987. Epyx first showed the Handy system at the Winter Consumer Electronics Show (CES) in January 1989. Facing financial difficulties, Epyx sought partners. Nintendo, Sega, and other",
        "was completed in 1987. Epyx first showed the Handy system at the Winter Consumer Electronics Show (CES) in January 1989. Facing financial difficulties, Epyx sought partners. Nintendo, Sega, and other companies declined, but Atari and Epyx eventually agreed that Atari would handle production and marketing, and Epyx would handle software development. Epyx declared bankruptcy by the end of the year, so Atari essentially owned the entire project. Both Atari and others had to purchase Amigas from Atari arch-rival Commodore in order to develop Lynx software.",
        "The Handy was designed to run games from the cartridge format, and the game data must be copied from ROM to RAM before it can be used. Thus, less RAM is then available and each game's initial loading is slow. There are trace remnants of a cassette tape interface physically capable of being programmed to read a tape. Lynx developers have noted that \"there is still reference of the tape and some hardware addresses\" and an updated vintage Epyx manual describes the bare existence of what could be utilized for tape support. A 2009 retrospective interview with Mical clarifies that there is no truth to some early reports claiming that games were loaded from tape, and elaborates, \"We did think about hard disk a little.\"",
        "The networking system was originally developed to run over infrared links and codenamed RedEye. This was changed to a cable-based networking system before the final release as the infrared beam was too easily interrupted when players walked through the beam, according to Peter Engelbrite. Engelbrite developed the first recordable eight-player co-op game, and the only eight-player game for the  Lynx, Todd's Adventures in Slime World.\n\nAtari changed the internal speaker and removed the thumb stick on the control pad. At Summer 1989 CES, Atari's press demonstration included the \"Portable Color Entertainment System\", which was changed to \"Lynx\" when distributed to resellers, initially retailing in the US at .",
        "Its launch was successful. Atari reported that it had sold 90% of the 50,000 units shipped in the launch month in the U.S. with a limited launch in New York. US sales in 1990 were approximately 500,000 units according to the Associated Press. In late 1991, it was reported that Atari sales estimates were about 800,000, which Atari claimed was within its expected projections. Lifetime sales by 1995 amount to fewer than 7 million units when combined with the Game Gear. In comparison, 16 million Game Boy units were sold by 1995 because of its ruggedness, half price, much longer battery life, bundling with the smash hit Tetris, and superior game library.",
        "As with the console units, the game cartridge design evolved over the first year of the console's release. The first generation of cartridges are flat, and designed to be stackable for ease of storage. However, this design proved to be very difficult to remove from the console and was replaced by a second design. This style, called \"tabbed\" or \"ridged\", adds two small tabs on the underside to aid in removal. The original flat style cartridges can be stacked on top of the newer cartridges, but the newer cartridges can not be easily stacked on each other, nor were they stored easily. Thus a third style, the \"curved lip\" style was produced, and all official and third-party cartridges during the console's lifespan were released (or re-released) using this style.",
        "In May 1991, Sega launched its Game Gear portable gaming handheld with a color screen. In comparison to the Lynx it had shorter battery life (3–4 hours as opposed to 4-5 for the Lynx), but it is slightly smaller, has significantly more games, and cost $30 less than the Lynx at launch.\n\nRetailers such as Game and Toys \"R\" Us continued to sell the Lynx well into the mid-1990s on the back of the Atari Jaguar launch, helped by magazines such as Ultimate Future Games which continued to cover the Lynx alongside the new generation of 32-bit and 64-bit consoles.\n\nLynx II",
        "Lynx II\n\nIn July 1991, Atari introduced a new version of the Lynx, internally called the \"Lynx II\", with a new marketing campaign, new packaging, slightly improved hardware, better battery life, and a sleeker look. It has rubber hand grips and a clearer backlit color screen with a power save option (which turns off the backlighting). The monaural headphone jack of the original Lynx was replaced with one wired for stereo. The Lynx II was available without any accessories, dropping the price to .\n\nDecline\n\nIn 1993, Atari started shifting its focus away from the Lynx in order to prepare for the launch of the Jaguar; a few games were released during that time, including Battlezone 2000. Support for the Lynx was formally discontinued in 1995.",
        "After the respective launches of the Sega Saturn and Sony PlayStation caused the commercial failure of the Jaguar, Atari ceased all game development and hardware manufacturing by early 1996 and would later merge with JTS, Inc. on July 30 of that year.\n\nFeatures\nThe Atari Lynx has a backlit color LCD display, switchable right- and left-handed (upside down) configuration, and the ability to network with other units via Comlynx cable. The maximum stable connection allowed is eight players. Each Lynx needs a copy of the game, and one cable can connect two machines. The cables can be connected into a chain.",
        "The Lynx was cited as the \"first gaming console with hardware support for zooming and distortion of sprites\". With a 4096 color palette and integrated math and graphics co-processors (including a sprite engine unit), its color graphics display was said to be the key defining feature in the system's competition against Nintendo's monochromatic Game Boy. The fast pseudo-3D graphics features were made possible on a minimal hardware system by co-designer Dave Needle having \"invented the technique for planar expansion/shrinking capability\" and using stretched, textured, triangles instead of full polygons.\n\nTechnical specifications",
        "Mikey (8-bit VLSI custom CMOS chip running at 16 MHz)\n VLSI 8-bit VL65NC02 processor (based on the MOS 6502) running at up to 4 MHz (3.6 MHz average)\n Sound engine\n 4 channel sound\n 8-bit DAC for each channel (4 channels × 8-bits/channel = 32 bits commonly quoted) these four sound channels can also switch in analogue sound mode to generate PSG sound.\n Video DMA driver for liquid-crystal display\n Custom built and designed by Jay Miner and David Morse\n 160×102 pixels resolution\n 4,096 color (12-bit) palette\n 16 simultaneous colors (4 bits) from palette per scanline\n Variable frame rate (up to 75 frames/second)\n Eight system timers (two reserved for LCD timing, one for UART)\n Interrupt controller\n UART (for Comlynx) (fixed format 8E1, up to 62500 Bd / TurboMode 1,000,000Bd)",
        "Eight system timers (two reserved for LCD timing, one for UART)\n Interrupt controller\n UART (for Comlynx) (fixed format 8E1, up to 62500 Bd / TurboMode 1,000,000Bd)\n 512 bytes of bootstrap and game-card loading ROM\n Suzy (16-bit VLSI custom CMOS chip running at )\n Unlimited number of blitter \"sprites\" with collision detection\n Hardware sprite scaling, distortion, and tilting effects\n Hardware decoding of compressed sprite data\n Hardware clipping and multi-directional scrolling\n Math engine\n Hardware 16-bit × 16-bit → 32-bit multiply with optional accumulation; 32-bit ÷ 16-bit → 16-bit divide\n Parallel processing of CPU\n RAM: 64 KB 120ns DRAM\n Cartridges: 128, 256, 512 KB and (with bank-switching) 1 MB\n Ports:\n Headphone port ( stereo; wired for mono on the original Lynx)",
        "Parallel processing of CPU\n RAM: 64 KB 120ns DRAM\n Cartridges: 128, 256, 512 KB and (with bank-switching) 1 MB\n Ports:\n Headphone port ( stereo; wired for mono on the original Lynx)\n ComLynx (multiple unit communications, serial)\n LCD Screen: 3.5\" diagonal\n Battery holder (six AA) 4–5 hours (Lynx I) 5–6 hours (Lynx II)",
        "Reception\n\nLynx was reviewed in 1990 in Dragon, which gave it 5 out of 5 stars. The review states that the Lynx \"throws the Game Boy into the prehistoric age\", and praises the built-in object scaling capabilities, the multiplayer feature of the ComLynx cable, and the strong set of launch games.\n\nLegacy\nTelegames released several games in the late 1990s, including a port of Raiden and a platformer called Fat Bobby in 1997, and an action sports game called Hyperdrome in 1999.",
        "On March 13, 1998, nearly three years after the Lynx's discontinuation, JTS Corporation sold all of the Atari assets to Hasbro Interactive for $5 million. On May 14, 1999, Hasbro, which held on to those properties until selling Hasbro Interactive to Infogrames in 2001, released into the public domain all rights to the Jaguar, opening up the platform for anyone to publish software on without Hasbro's interference. Internet theories say that the Lynx's rights may have been released to the public at the same time as the Jaguar, but this is clearly disputed. Nevertheless, since discontinuation, the Lynx, like the Jaguar, has continued to receive support from a grassroots community which would go on to produce many successful homebrew games such as T-Tris (the first Lynx game with a save-game",
        "Lynx, like the Jaguar, has continued to receive support from a grassroots community which would go on to produce many successful homebrew games such as T-Tris (the first Lynx game with a save-game feature), Alpine Games, and Zaku.",
        "In 2008, Atari was honored at the 59th Annual Technology & Engineering Emmy Awards for pioneering the development of handheld games with the Lynx.\n\nSee also \n List of Atari Lynx games\n History of Atari\n\nReferences\n\nExternal links\n\n AtariAge – Comprehensive Lynx Database and information\n Guide to Atari Lynx games at Retro Video Gamer\n Too Powerful for Its Own Good, Atari's Lynx Remains a Favorite 25 Years Later\n Atari Lynx review, 1990\n Atari Lynx Hardware Documentation\n Atari Lynx Development Wiki\n\nComputer-related introductions in 1989\nDiscontinued handheld game consoles\nHandheld game consoles\nFourth-generation video game consoles\nLynx\n1980s toys\n1990s toys\n65xx-based video game consoles\nPublic domain in the United States\nRegionless game consoles"
    ],
    [
        "Athlon\nAthlon is the brand name applied to a series of x86-compatible microprocessors designed and manufactured by AMD. The original Athlon (now called Athlon Classic) was the first seventh-generation x86 processor and the first desktop processor to reach speeds of one gigahertz (GHz). It made its debut as AMD's high-end processor brand on June 23, 1999. Over the years AMD has used the Athlon name with the 64-bit Athlon 64 architecture, the Athlon II, and Accelerated Processing Unit (APU) chips targeting the Socket AM1 desktop SoC architecture, and Socket AM4 Zen microarchitecture. The modern Zen-based Athlon with a Radeon Graphics processor was introduced in 2019 as AMD's highest-performance entry-level processor.",
        "Athlon comes from the Ancient Greek  (athlon), meaning \"(sport) contest\", or \"prize of a contest\", or \"place of a contest; arena\". With the Athlon name originally used for AMD's high-end processors, AMD currently uses Athlon for budget APUs with integrated graphics. AMD positions the Athlon against its rival, the Intel Pentium.\n\nBrand history",
        "K7 design and development",
        "The first Athlon processor was a result of AMD's development of K7 processors in the 1990s. AMD founder and then-CEO Jerry Sanders aggressively pursued strategic partnerships and engineering talent in the late 1990s, working to build on earlier successes in the PC market with the AMD K6 processor line. One major partnership announced in 1998 paired AMD with semiconductor giant Motorola to co-develop copper-based semiconductor technology, resulting in the K7 project being the first commercial processor to utilize copper fabrication technology. In the announcement, Sanders referred to the partnership as creating a \"virtual gorilla\" that would enable AMD to compete with Intel on fabrication capacity while limiting AMD's financial outlay for new facilities. The K7 design team was led by Dirk",
        "as creating a \"virtual gorilla\" that would enable AMD to compete with Intel on fabrication capacity while limiting AMD's financial outlay for new facilities. The K7 design team was led by Dirk Meyer, who had previously worked as a lead engineer at DEC on multiple Alpha microprocessors. When DEC was sold to Compaq in 1998 and discontinued Alpha processor development, Sanders brought most of the Alpha design team to the K7 project. This added to the previously acquired NexGen K6 team, which already included engineers such as Vinod Dham.",
        "Original release",
        "The AMD Athlon processor launched on June 23, 1999, with general availability by August 1999. Subsequently, from August 1999 until January 2002, this initial K7 processor was the fastest x86 chip in the world. Wrote the Los Angeles Times on October 5, 1999: \"AMD has historically trailed Intel’s fastest processors, but has overtaken the industry leader with the new Athlon. Analysts say the Athlon, which will be used by Compaq, IBM and other manufacturers in their most powerful PCs, is significantly faster than Intel’s flagship Pentium III, which runs at a top speed of 600MHz.\" A number of features helped the chips compete with Intel. By working with Motorola, AMD had been able to refine copper interconnect manufacturing about one year before Intel, with the revised process permitting",
        "features helped the chips compete with Intel. By working with Motorola, AMD had been able to refine copper interconnect manufacturing about one year before Intel, with the revised process permitting 180-nanometer processor production. The accompanying die-shrink resulted in lower power consumption, permitting AMD to increase Athlon clock speeds to the 1 GHz range. The Athlon architecture also used the EV6 bus licensed from DEC as its main system bus, allowing AMD to develop its own products without needing to license Intel's GTL+ bus. By the summer of 2000, AMD was shipping Athlons at high volume, and the chips were being used in systems by Gateway, Hewlett-Packard, and Fujitsu Siemens Computers among others.",
        "Later Athlon iterations\nThe second-generation Athlon, the Thunderbird, debuted in 2000. AMD released the Athlon XP the following year, and the Athlon XP's immediate successor, the Athlon 64, was an AMD64-architecture microprocessor released in 2003. After the 2007 launch of the Phenom processors, the Athlon name was also used for mid-range processors, positioned above brands such as Sempron. The Athlon 64 X2 was released in 2005 as the first native dual-core desktop CPU designed by AMD, and the Athlon X2 was a subsequent family based on the Athlon 64 X2.  Introduced in 2009, Athlon II was a dual-core family of Athlon chips.",
        "A USD$55 low-power Athlon 200GE with a Radeon graphics processor was introduced in September 2018, sitting under the Ryzen 3 2200G. This iteration of Athlon used AMD's Zen-based Raven Ridge core, which in turn had debuted in Ryzen with Radeon graphics processors. With the release, AMD began using the Athlon brand name to refer to \"low-cost, high-volume products\", in a situation similar to both Intel's Celeron and Pentium Gold. The modern Athlon 3000G was introduced in 2019 and was positioned as AMD's highest-performance entry-level processor. AMD positions the Athlon against its rival, the Intel Pentium. While CPU processing performance is in the same ballpark, the Athlon 3000G uses Radeon Vega graphics, which are rated as more powerful than the Pentium's Intel UHD Graphics.\n\nGenerations",
        "Generations\n\nAthlon Classic (1999)\n\nThe AMD Athlon processor launched on June 23, 1999, with general availability by August 1999. Subsequently, from August 1999 until January 2002, this initial K7 processor was the fastest x86 chip in the world. At launch it was, on average, 10% faster than the Pentium III at the same clock for business applications and 20% faster for gaming workloads. In commercial terms, the Athlon \"Classic\" was an enormous success.\n\nFeatures",
        "The Athlon Classic is a cartridge-based processor, named Slot A and similar to Intel's cartridge Slot 1 used for Pentium II and Pentium III. It used the same, commonly available, physical 242-pin connector used by Intel Slot 1 processors but rotated by 180 degrees to connect the processor to the motherboard. The cartridge assembly allowed the use of higher-speed cache memory modules than could be put on (or reasonably bundled with) motherboards at the time. Similar to the Pentium II and the Katmai-based Pentium III, the Athlon Classic contained 512 KB of L2 cache. This high-speed SRAM cache was run at a divisor of the processor clock and was accessed via its own 64-bit back-side bus, allowing the processor to service both front-side bus requests and cache accesses simultaneously, as",
        "was run at a divisor of the processor clock and was accessed via its own 64-bit back-side bus, allowing the processor to service both front-side bus requests and cache accesses simultaneously, as compared to pushing everything through the front-side bus.",
        "The Argon-based Athlon contained 22 million transistors and measured 184 mm2. It was fabricated by AMD in a version of their CS44E process, a 250 nm complementary metal–oxide–semiconductor (CMOS) process with six levels of aluminium interconnect. \"Pluto\" and \"Orion\" Athlons were fabricated in a 180 nm process.",
        "The Athlon's CPU cache consisted of the typical two levels. Athlon was the first x86 processor with a 128 KB split level-1 cache; a 2-way associative cache separated into 2×64 KB for data and instructions (a concept from Harvard architecture). SRAM cache designs at the time were incapable of keeping up with the Athlon's clock scalability, resulting in compromised CPU performance in some computers. With later Athlon models, AMD would integrate the L2 cache onto the processor itself, removing dependence on external cache chips. The Slot-A Athlons were the first multiplier-locked CPUs from AMD, preventing users from setting their own desired clock speed. This was done by AMD in part to hinder CPU remarking and overclocking by resellers, which could result in inconsistent performance.",
        "AMD, preventing users from setting their own desired clock speed. This was done by AMD in part to hinder CPU remarking and overclocking by resellers, which could result in inconsistent performance. Eventually a product called the \"Goldfingers device\" was created that could unlock the CPU.",
        "AMD designed the CPU with more robust x86 instruction decoding capabilities than that of K6, to enhance its ability to keep more data in-flight at once. The critical branch-predictor unit was enhanced compared to the K6. Deeper pipelining with more stages allowed higher clock speeds to be attained. Like the AMD K5 and K6, the Athlon dynamically buffered internal micro-instructions at runtime resulting from parallel x86 instruction decoding. The CPU is an out-of-order design, again like previous post-5x86 AMD CPUs. The Athlon utilizes the Alpha 21264's EV6 bus architecture with double data rate (DDR) technology.",
        "AMD ended its long-time handicap with floating point x87 performance by designing a super-pipelined, out-of-order, triple-issue floating-point unit (FPU). Each of its three units could independently calculate an optimal type of instructions with some redundancy, making it possible to operate on more than one floating-point instruction at once. This FPU was a huge step forward for AMD, helping compete with Intel's P6 FPU. The 3DNow! floating-point SIMD technology, again present, received some revisions and was renamed \"Enhanced 3DNow!\" Additions included DSP instructions and the extended MMX subset of Intel SSE.",
        "Specifications\n L1-cache: 64 + 64 KB (data + instructions)\n L2-cache: 512 KB, external chips on CPU module with 50%, 40% or 33% of CPU speed\n MMX, 3DNow!\n Slot A (EV6)\n Front-side bus: 200 MT/s (100 MHz double-pumped)\n Vcore: 1.6 V (K7), 1.6–1.8 V (K75)\n First release: June 23, 1999 (K7), November 29, 1999 (K75)\n Clock-rate: 500–700 MHz (K7), 550–1000 MHz (K75)\n\nAthlon Thunderbird (2000–2001)",
        "The second-generation Athlon, the Thunderbird or T-Bird, debuted on June 4, 2000. This version of the Athlon was available in a traditional pin-grid array (PGA) format that plugged into a socket (\"Socket A\") on the motherboard, or packaged as a Slot A cartridge. The major difference between it and the Athlon Classic was cache design, with AMD adding in 256 KB of on-chip, full-speed exclusive cache. In moving to an exclusive cache design, the L1 cache's contents were not duplicated in the L2, increasing total cache size and functionally creating a large L1 cache with a slower region (the L2) and a fast region (the L1), making the L2 cache into basically a victim cache. With the new cache design, need for high L2 performance and size was lessened, and the simpler L2 cache was less likely to",
        "a fast region (the L1), making the L2 cache into basically a victim cache. With the new cache design, need for high L2 performance and size was lessened, and the simpler L2 cache was less likely to cause clock scaling and yield issues. Thunderbird also moved to a 16-way associative layout.",
        "The Thunderbird was \"cherished by many for its overclockability\" and proved commercially successful, as AMD's most successful product since the Am386DX-40 ten years earlier. AMD's new fab facility in Dresden increased production for AMD overall and put out Thunderbirds at a fast rate, with the process technology improved by a switch to copper interconnects. After several versions were released in 2000 and 2001 of the Thunderbird, the last Athlon processor using the Thunderbird core was released in 2001 in the summer, at which point speeds were at 1.4 GHz.\n\nThe locked multipliers of Socket A Thunderbirds could often be disabled through adding conductive bridges on the surface on the chip, a practice widely known as \"the pencil trick\".",
        "The locked multipliers of Socket A Thunderbirds could often be disabled through adding conductive bridges on the surface on the chip, a practice widely known as \"the pencil trick\".\n\nSpecifications\n L1-cache: 64 + 64 KB (data + instructions)\n L2-cache: 256 KB, full speed\n MMX, 3DNow!\n Slot A & Socket A (EV6)\n Front-side bus: 100 MHz (Slot-A, B-models), 133 MHz (C-models) (200 MT/s, 266 MT/s)\n Vcore: 1.70–1.75 V\n First release: June 4, 2000\n Transistor count: 37 million\n Manufacturing process: /180 nm\n Clock rate:\n Slot A: 650–1000 MHz\n Socket A, 100 MHz FSB (B-models): 600–1400 MHz\n Socket A, 133 MHz FSB (C-models): 1000–1400 MHz\n\nAthlon XP (2001–2003)",
        "Athlon XP (2001–2003)\n\nOverall, there are four main variants of the Athlon XP desktop CPU: the Palomino, the Thoroughbred, the Thorton, and the Barton. A number of mobile processors were also released, including the Corvette models, and the Dublin model among others.\n\nPalomino",
        "On May 14, 2001, AMD released the Athlon XP processor. It debuted as the Mobile Athlon 4, a mobile version codenamed Corvette, with the desktop Athlon XP released in the fall. The third-generation Athlon, code-named Palomino, came out on October 9, 2001, as the Athlon XP, with the suffix signifying extreme performance and unofficially referencing Windows XP. Palomino's design used 180 nm fabrication process size. The Athlon XP was marketed using a performance rating (PR) system comparing it to the Thunderbird predecessor core. Among other changes, Palomino consumed 20% less power than the Thunderbird, comparatively reducing heat output, and was roughly 10% faster than Thunderbird. Palomino also had enhanced K7's TLB architecture and included a hardware data prefetch mechanism to take",
        "Thunderbird, comparatively reducing heat output, and was roughly 10% faster than Thunderbird. Palomino also had enhanced K7's TLB architecture and included a hardware data prefetch mechanism to take better advantage of memory bandwidth. Palomino was the first K7 core to include the full SSE instruction set from the Intel Pentium III, as well as AMD's 3DNow! Professional. Palomino was also the first socketed Athlon officially supporting dual processing, with chips certified for that purpose branded as the Athlon MP (multi processing), which had different specifications. According to HardwareZone, it was possible to modify the Athlon XP to function as an MP.",
        "Specifications\n L1-cache: 64 + 64 KB (data + instructions)\n L2-cache: 256 KB, full speed\n MMX, 3DNow!, SSE\n Socket A (EV6)\n Front-side bus: 133 MHz (266 MT/s)\n Vcore: 1.50 to 1.75 V\n Power consumption: 68 W\n First release: October 9, 2001\n Clock-rate:\n Athlon 4: 850–1400 MHz\n Athlon XP: 1333–1733 MHz (1500+ to 2100+)\n Athlon MP: 1000–1733 MHz\n\nThoroughbred",
        "Thoroughbred\n\nThe fourth-generation of Athlon was introduced with the Thoroughbred core, or T-Bred, on April 17, 2002. The Thoroughbred core marked AMD's first production 130 nm silicon, with smaller die size than its predecessor. There came to be two steppings (revisions) of this core commonly referred to as Tbred-A and Tbred-B. Introduced in June 2002, the initial A version was mostly a direct die shrink of the preceding Palomino core, but did not significantly increase clock speeds over the Palomino. A revised Thoroughbred core, Thoroughbred-B, added a ninth \"metal layer\" to the eight-layered Thoroughbred-A, offering improvement in headroom over the A and making it popular for overclocking.",
        "Specifications\n L1-cache: 64 + 64 KB (data + instructions)\n L2-cache: 256 KB, full speed\n MMX, 3DNow!, SSE\n Socket A (EV6)\n Front-side bus: 133/166 MHz (266/333 MT/s)\n Vcore: 1.50–1.65 V\n First release: June 10, 2002 (A), August 21, 2002 (B)\n Clock-rate:\n Thoroughbred \"A\": 1400–1800 MHz (1600+ to 2200+)\n Thoroughbred \"B\": 1400–2250 MHz (1600+ to 2800+)\n 133 MHz FSB: 1400–2133 MHz (1600+ to 2600+)\n 166 MHz FSB: 2083–2250 MHz (2600+ to 2800+)\n\nBarton / Thorton",
        "Fifth-generation Athlon Barton-core processors were released in early 2003. While not operating at higher clock rates than Thoroughbred-core processors, they featured an increased L2 cache, and later models had an increased 200 MHz (400 MT/s) front side bus. The Thorton core, a blend of thoroughbred and Barton, was a later variant of the Barton with half of the L2 cache disabled. The Barton was used to officially introduce a higher 400 MT/s bus clock for the Socket A platform, which was used to gain some Barton models more efficiency. By this point with the Barton, the four-year-old Athlon EV6 bus architecture had scaled to its limit and required a redesign to exceed the performance of newer Intel processors. By 2003, the Pentium 4 had become more than competitive with AMD's processors,",
        "architecture had scaled to its limit and required a redesign to exceed the performance of newer Intel processors. By 2003, the Pentium 4 had become more than competitive with AMD's processors, and Barton only saw a small performance increase over the Thoroughbred-B it derived from, insufficient to outperform the Pentium 4. The K7-derived Athlons such as Barton were replaced in September 2003 by the Athlon 64 family, which featured an on-chip memory controller and a new HyperTransport bus.",
        "Notably, the 2500+ Barton with 11× multiplier was effectively identical to the 3200+ part other than the FSB speed it was binned for, meaning that seamless overclocking was possible more often than not. Early Thortons could be restored to the full Barton specification with the enabling of the other half of the L2 cache from a slight CPU surface modification, but the result was not always reliable.",
        "Specifications\nBarton (130 nm)\n L1-cache: 64 + 64 KB (data + instructions)\n L2-cache: 512 KB, full speed\n MMX, 3DNow!, SSE\n Socket A (EV6)\n Front-side bus: 166/200 MHz (333/400 MT/s)\n Vcore: 1.65 V\n First release: February 10, 2003\n Clock rate: 1833–2333 MHz (2500+ to 3200+)\n 133 MHz FSB: 1867–2133 MHz (2500+ to 2800+); uncommon\n 166 MHz FSB: 1833–2333 MHz (2500+ to 3200+)\n 200 MHz FSB: 2100, 2200 MHz (3000+, 3200+)",
        "Thorton (130 nm)\n L1-cache: 64 + 64 KB (Data + Instructions)\n L2-cache: 256 KB, full speed\n MMX, 3DNow!, SSE\n Socket A (EV6)\n Front-side bus: 133/166/200 MHz (266/333/400 MT/s)\n Vcore: 1.50–1.65 V\n First release: September 2003\n Clock rate: 1667–2200 MHz (2000+ to 3100+)\n 133 MHz FSB: 1600–2133 MHz (2000+ to 2600+)\n 166 MHz FSB: 2083 MHz (2600+)\n 200 MHz FSB: 2200 MHz (3100+)\n\nMobile Athlon XP",
        "The Palomino core debuted in the mobile market before the PC market in May 2001, where it was branded as Mobile Athlon 4 with the codename \"Corvette\". It distinctively used a ceramic interposer much like the Thunderbird instead of the organic pin grid array package used on all later Palomino processors. In November 2001, AMD released a 1.2 GHz Athlon 4 and a 950 MHz Duron. The Mobile Athlon 4 processors included the PowerNow! function, which controlled a laptop's \"level of processor performance by dynamically adjusting its operating frequency and voltage according to the task at hand\", thus extending \"battery life by reducing processor power when it isn't needed by applications\". Duron chips also included PowerNow! In 2002, AMD released a version of PowerNow! called Cool'n'Quiet,",
        "thus extending \"battery life by reducing processor power when it isn't needed by applications\". Duron chips also included PowerNow! In 2002, AMD released a version of PowerNow! called Cool'n'Quiet, implemented on the Athlon XP but only adjusting clock speed instead of voltage.",
        "In 2002 the Athlon XP-M (Mobile Athlon XP) replaced the Mobile Athlon 4 using the newer Thoroughbred core, with Barton cores for full-size notebooks. The Athlon XP-M was also offered in a compact microPGA socket 563 version. Mobile XPs were not multiplier-locked, making them popular with desktop overclockers.\n\nAthlon 64 (2003–2009)\n\nThe immediate successor to the Athlon XP, the Athlon 64 is an AMD64-architecture microprocessor produced by AMD, released on September 23, 2003. A number of variations, all named after cities, were released with 90 nm architecture in 2004 and 2005. Versions released in 2007 and 2009 utilized 65 nm architecture.\n\nAthlon 64 X2 (2005–2009)",
        "Athlon 64 X2 (2005–2009)\n\nThe Athlon 64 X2 was released in 2005 as the first native dual-core desktop CPU designed by AMD using an Athlon 64. The Athlon X2 was a subsequent family of microprocessors based on the Athlon 64 X2. The original Brisbane Athlon X2 models used 65 nm architecture and were released in 2007.\n\nAthlon II (2009–2012) \nAthlon II is a family of central processing units. Initially a dual-core version of the Athlon II, the K-10-based Regor was released in June 2009 with 45-nanometer architecture. This was followed by a single-core version Sargas, followed by the quad-core Propus, the triple-core Rana in November 2009, and the Llano 32 nm version released in 2011.\n\nBristol-Ridge-based Athlon X4 (2017)",
        "Bristol-Ridge-based Athlon X4 (2017)\n\nMain Article: Athlon X4 \"Bristol Ridge\" (2017, 28 nm) \nThe Bristol Ridge Athlon X4 lineup was released in 2017. It was based on the Excavator microarchitecture and used 2 Excavator modules tolalling 4 \"cores\". It had a dual-channel DDR4-2400 memory controller with clockspeeds up to 4.0GHz. It ran on the new Socket AM4 platform that was being used for Zen1-3 CPU's.\n\nZen-based Athlon (2018–present)\nThe Zen-based Athlon with Radeon graphics processors was launched in September 2018 with the Athlon 200GE. Based on AMD's Raven Ridge core previously used in variants of the Ryzen 3 and Ryzen 5, the Athlon 200GE had half of the cores but left SMT enabled. It also kept the same 4 MiB L3 cache, but the L2 cache was halved to 1 MiB.",
        "In addition, the number of graphics compute units was limited to 3 in the Athlon 200GE, and the chip was multiplier-locked. Despite its limitations, the Athlon 200GE performed competitively against the 5000-series Intel Pentium-G, displaying similar CPU performance but an advantage in GPU performance.\n\nOn November 19, 2019, AMD released the Athlon 3000G, with a higher 3.5 GHz core clock and 1100 MHz graphics clock compared to the Athlon 200GE, also with two cores. The main functional difference between the 200GE was the Athlon 3000G's unlocked multiplier, allowing the latter to be overclocked on B450 and X470 motherboards.",
        "Specifications\nRaven Ridge (14 nm), Picasso (12 nm) (see the list article for more details)\n L1 cache: 192 KiB (2×64 KiB + 2×32 KiB)\n L2 cache: 1 MiB (2×512 KiB)\n L3 cache: 4 MiB\n Memory: dual-channel DDR4-2666, 64 GiB max.\n Socket AM4\n TDP: 35 W\n First release: September 6, 2018\n CPU clock rate: 3.2 to 3.5 GHz\n GPU clock rate: 1000 to 1100 MHz",
        "Supercomputers\nA number of supercomputers have been built using Athlon chips, largely at universities. Among them:\n In 2000, several American students claimed to have built the world's least expensive supercomputer by clustering 64 AMD Athlon chips together, also marking the first time Athlons had been clustered in a supercomputer.\n The PRESTO III, a Beowulf cluster of 78 AMD Athlon processors, was built in 2001 by the Tokyo Institute of Technology. That year it ranked 439 on the TOP500 list of supercomputers.\n In 2002, a \"128-Node 256-Processor AMD Athlon Supercomputer Cluster\" was installed at the Ohio Supercomputer Center at the University of Toledo.\n Rutgers University, Department of Physics & Astronomy. Machine: NOW Cluster—AMD Athlon. CPU: 512 AthlonMP (1.65 GHz). Rmax: 794 GFLOPS.",
        "See also\n List of AMD Athlon processors\n List of AMD Duron processors\n List of AMD Phenom processors\n List of AMD Opteron processors\n List of AMD Sempron processors\n\nReferences\n\nExternal links\n\nWebsite\n\nComputer-related introductions in 1999\nAMD x86 microprocessors\nAMD microarchitectures\nSuperscalar microprocessors\nX86 microarchitectures"
    ],
    [
        "Atlas Autocode\nAtlas Autocode (AA) is a programming language developed around 1963 at the University of Manchester. A variant of the language ALGOL, it was developed by Tony Brooker and Derrick Morris for the Atlas computer. The initial AA and AB compilers were written by Jeff Rohl and Tony Brooker using the Brooker-Morris Compiler-compiler, with a later hand-coded non-CC implementation (ABC) by Jeff Rohl.\n\nThe word Autocode was basically an early term for programming language. Different autocodes could vary greatly.",
        "The word Autocode was basically an early term for programming language. Different autocodes could vary greatly.\n\nFeatures\nAA was a block structured language that featured explicitly typed variables, subroutines, and functions. It omitted some ALGOL features such as passing parameters by name, which in ALGOL 60 means passing the memory address of a short subroutine (a thunk) to recalculate a parameter each time it is mentioned.\n\nThe AA compiler could generate range-checking for array accesses, and allowed an array to have dimensions that were determined at runtime, i.e., an array could be declared as integer array Thing (i:j), where i and j were calculated values.",
        "AA high-level routines could include machine code, either to make an inner loop more efficient or to effect some operation which otherwise cannot be done easily.\n\nAA included a complex data type to represent complex numbers, partly because of pressure from the electrical engineering department, as complex numbers are used to represent the behavior of alternating current. The imaginary unit square root of -1 was represented by i, which was treated as a fixed complex constant = i.\n\nThe complex data type was dropped when Atlas Autocode later evolved into the language Edinburgh IMP. IMP was an extension of AA and was used to write the Edinburgh Multiple Access System (EMAS) operating system.",
        "AA's second-greatest claim to fame (after being the progenitor of IMP and EMAS) was that it had many of the features of the original Compiler Compiler. A variant of the AA compiler included run-time support for a top-down recursive descent parser.  The style of parser used in the Compiler Compiler was in use continuously at Edinburgh from the 60's until almost the year 2000.\n\nOther Autocodes were developed for the Titan computer, a prototype Atlas 2 at Cambridge, and the Ferranti Mercury.",
        "Other Autocodes were developed for the Titan computer, a prototype Atlas 2 at Cambridge, and the Ferranti Mercury.\n\nSyntax\nAtlas Autocode's syntax was largely similar to ALGOL, though it was influenced by the output device which the author had available, a Friden Flexowriter. Thus, it allowed symbols like ½ for .5 and the superscript 2 for to the power of 2.  The Flexowriter supported overstriking and thus, AA did also: up to three characters could be overstruck as a single symbol. For example, the character set had no ↑ symbol, so exponentiation was an overstrike of | and *. The aforementioned underlining of reserved words (keywords) could also be done using overstriking. The language is described in detail in the Atlas Autocode Reference Manual.",
        "Other Flexowriter characters that were found a use in AA were: α in floating-point numbers, e.g., 3.56α-7 for modern 3.56e-7 ; β to mean the second half of a 48-bit Atlas memory word; π for the mathematical constant pi.\n\nWhen AA was ported to the English Electric KDF9 computer, the character set was changed to International Organization for Standardization (ISO). That compiler has been recovered from an old paper tape by the Edinburgh Computer History Project and is available online, as is a high-quality scan of the original Edinburgh version of the Atlas Autocode manual.",
        "Keywords in AA were distinguishable from other text by being underlined, which was implemented via overstrike in the Flexowriter (compare to bold in ALGOL). There were also two stropping regimes. First, there was an \"uppercasedelimiters\" mode where all uppercase letters (outside strings) were treated as underlined lowercase. Second, in some versions (but not in the original Atlas version), it was possible to strop keywords by placing a \"%\" sign in front of them, for example the keyword endofprogramme could be typed as %end %of %programme or %endofprogramme. This significantly reduced typing, due to only needing one character, rather than overstriking the whole keyword. As in ALGOL, there were no reserved words in the language as keywords were identified by underlining (or stropping), not",
        "due to only needing one character, rather than overstriking the whole keyword. As in ALGOL, there were no reserved words in the language as keywords were identified by underlining (or stropping), not by recognising reserved character sequences. In the statement if token=if then result = token, there is both a keyword if and a variable named if.",
        "As in ALGOL, AA allowed spaces in variable names, such as integer previous value. Spaces were not significant and were removed before parsing in a trivial pre-lexing stage called \"line reconstruction\". What the compiler would see in the above example would be \"iftoken=ifthenresult=token\". Spaces were possible due partly to keywords being distinguished in other ways, and partly because the source was processed by scannerless parsing, without a separate lexing phase, which allowed the lexical syntax to be context-sensitive.",
        "The syntax for expressions let the multiplication operator be omitted, e.g., 3a was treated as 3*a, and a(i+j) was treated as a*(i+j) if a was not an array. In ambiguous uses, the longest possible name was taken (maximal munch), for example ab was not treated as a*b, whether or not a and b had been declared.\n\nReferences\n\nExternal links\n The main features of Atlas Autocode, By R. A. Brooker, J. S. Rohl, and S. R. Clark\n The Atlas Autocode Mini-Manual by W. F. Lunnon, G. Riding (July 1965)\n Atlas Autocode Reference Manual by R.A. Brooker, J.S.Rohl (March 1965)\n Mercury Autocode, Atlas Autocode and some Associated Matters. by Vic Forrington (Jan 2014)\n Flowcharts for Atlas Autocode compiler on KDF9.\n\nFerranti\nHistory of computing in the United Kingdom\nStructured programming languages"
    ],
    [
        "ATM\nATM or atm often refers to:\n\n Atmosphere (unit) or atm, a unit of atmospheric pressure\n Automated teller machine, a cash dispenser or cash machine\n\nATM or atm may also refer to:\n\nComputing \n ATM (computer), a ZX Spectrum clone developed in Moscow in 1991\n Adobe Type Manager, a computer program for managing fonts\n Accelerated Turing machine, or Zeno machine, a model of computation used in theoretical computer science\n Alternating Turing machine, a model of computation used in theoretical computer science\n Asynchronous Transfer Mode, a telecommunications protocol used in networking\n ATM adaptation layer\n ATM Adaptation Layer 5",
        "Media \n Amateur Telescope Making, a series of books by Albert Graham Ingalls\n ATM (2012 film), an American film\n ATM: Er Rak Error, a 2012 Thai film\n Azhagiya Tamil Magan, a 2007 Indian film\n \"ATM\" (song), a 2018 song by J. Cole from KOD\n\nPeople and organizations \n Abiding Truth Ministries, anti-LGBT organization in Springfield, Massachusetts, US\n Association of Teachers of Mathematics, UK \n Acrylic Tank Manufacturing, US aquarium manufacturer, televised in Tanked\n ATM FA, a football club in Malaysia\n A. T. M. Wilson (1906–1978), British psychiatrist\nAfrican Transformation Movement, South African political party founded in 2018\nThe a2 Milk Company (NZX ticker symbol ATM)",
        "Science \n Apollo Telescope Mount, a solar observatory\n ATM serine/threonine kinase, a serine/threonine kinase activated by DNA damage\n The Airborne Topographic Mapper, a laser altimeter among the instruments used by NASA's Operation IceBridge\n\nTransportation \n Active traffic management, a motorway scheme on the M42 in England\n Air traffic management, a concept in aviation\n Altamira Airport, in Brazil (IATA code ATM)\n Azienda Trasporti Milanesi, the municipal public transport company of Milan\n Airlines of Tasmania (ICAO code ATM)",
        "Catalonia, Spain \n Autoritat del Transport Metropolità (ATM Àrea de Barcelona), in the Barcelona metropolitan area\n Autoritat Territorial de la Mobilitat del Camp de Tarragona (ATM Camp de Tarragona), in the Camp de Tarragona area\n Autoritat Territorial de la Mobilitat de l'Àrea de Girona (ATM Àrea de Girona), in the Girona area\n Autoritat Territorial de la Mobilitat de l'Àrea de Lleida (ATM Àrea de Lleida), in the Lleida area",
        "Other uses \n Actun Tunichil Muknal, a cave in Belize\n Anti-tank missile, a missile designed to destroy tanks\n Ass to mouth, a sexual act\n At the money, moneyness where the strike price is the same as the current spot price\n At-the-market offering, a type of follow-on offering of stock\n Automatenmarken, a variable value stamp\nContracted form of Atlético Madrid, football club in Spain\nCommon abbreviation in SMS language for \"at the moment\""
    ],
    [
        "Atomic semantics\nAtomic semantics is a type of guarantee provided by a data register shared by several processors in a parallel machine or in a network of computers working together.\nAtomic semantics are very strong. An atomic register provides strong guarantees even when there is concurrency and failures.\n\nA read/write register R stores a value and is accessed by two basic operations: read and write(v). A read  returns the value stored in R and write(v) changes the value stored in R to v.\nA register is called atomic if it satisfies the two following properties:\n\n1) Each invocation op of a read or write operation:\n\n•Must appear as if it were executed at a single point τ(op) in time.",
        "1) Each invocation op of a read or write operation:\n\n•Must appear as if it were executed at a single point τ(op) in time.\n\n•τ (op) works as follow:\nτb(op) ≤ τ (op) ≤ τe(op): where τb(op) and τe(op) indicate the time when the operation op begins and ends.\n\n•If op1 ≠ op2, then τ (op1)≠τ (op2)\n\n2) Each read operation returns the value written by the last write operation before the read, in the sequence where all operations are ordered by their τ values.\n\nAtomic/Linearizable register:\n\nTermination: when a node is correct, sooner or later each read and write operation will complete.\n\nSafety Property (Linearization points for read and write and failed operations):\n\nRead operation:It appears as if happened at all nodes at some times between the invocation and response time.",
        "Safety Property (Linearization points for read and write and failed operations):\n\nRead operation:It appears as if happened at all nodes at some times between the invocation and response time.\n\nWrite operation: Similar to read operation, it appears as if happened at all nodes at some times between the invocation and response time.\n\nFailed operation(The atomic term comes from this notion):It appears as if it is completed at every single node or it never happened at any node.\n\nExample : We know that an atomic register is one that is linearizable to a sequential safe register.\n\nThe following picture shows where we should put the linearization point for each operation:",
        "Example : We know that an atomic register is one that is linearizable to a sequential safe register.\n\nThe following picture shows where we should put the linearization point for each operation:\n\n \nAn atomic register could be defined for a variable with a single writer but multi- readers (SWMR), single-writer/single-reader (SWSR), or multi-writer/multi-reader (MWMR). Here is an example of a multi-reader multi-writer atomic register which is accessed by three processes (P1, P2, P3). Note that R. read() → v means that the corresponding read operation returns v, which is the value of the register. Therefore, the following execution of the register R could satisfies the definition of the atomic registers:\nR.write(1), R.read()→1, R.write(3), R.write(2), R.read()→2, R.read()→2.",
        "See also\nRegular semantics\nSafe semantics\n\nReferences\n Atomic semantics are defined formally in Lamport's \"On Interprocess Communication\" Distributed Computing 1, 2 (1986), 77-101. (Also appeared as SRC Research Report 8).\n\nConcurrency control"
    ],
    [
        "Au file format\nThe Au file format is a simple audio file format introduced by Sun Microsystems. The format was common on NeXT systems and on early Web pages. Originally it was headerless, being simply 8-bit μ-law-encoded data at an 8000 Hz sample rate. Hardware from other vendors often used sample rates as high as 8192 Hz, often integer multiples of video clock signal frequencies. Newer files have a header that consists of six unsigned 32-bit words, an optional information chunk which is always of non-zero size, and then the data (in big-endian format).",
        "Although the format now supports many audio encoding formats, it remains associated with the μ-law logarithmic encoding. This encoding was native to the SPARCstation 1 hardware, where SunOS exposed the encoding to application programs through the /dev/audio device file interface. This encoding and interface became a de facto standard for Unix sound.\n\nNew format\nAll fields are stored in big-endian format, including the sample data.",
        "The type of encoding depends on the value of the \"encoding\" field (word 3 of the header). Formats 2 through 7 are uncompressed linear PCM, therefore technically lossless (although not necessarily free of quantization error, especially in 8-bit form). Formats 1 and 27 are μ-law and A-law, respectively, both companding logarithmic representations of PCM, and arguably lossy as they pack what would otherwise be almost 16 bits of dynamic range into 8 bits of encoded data, even though this is achieved by an altered dynamic response and no data is actually \"thrown away\". Formats 23 through 26 are ADPCM, which is an early form of lossy compression, usually but not always with 4 bits of encoded data per audio sample (for 4:1 efficiency with 16-bit input, or 2:1 with 8-bit; equivalent to e.g.",
        "ADPCM, which is an early form of lossy compression, usually but not always with 4 bits of encoded data per audio sample (for 4:1 efficiency with 16-bit input, or 2:1 with 8-bit; equivalent to e.g. encoding CD quality MP3 at a 352kbit rate using a low quality encoder). Several of the others (number 8 through 22) are DSP commands or data, designed to be processed by the NeXT Music Kit software.",
        "Note: PCM formats are encoded as signed data (as opposed to unsigned).\n\nThe current format supports only a single audio data segment per file. The variable-length annotation field is currently ignored by most audio applications.\n\nReferences\n\nExternal links \nOracle man pages: audio(7i) - generic audio device interface (for information on the /dev/audio interface)\n\nComputer file formats\nDigital container formats\nAudio codecs"
    ],
    [
        "Au\nAu, AU,  au or a.u. may refer to:\n\nScience and technology\n\nComputing\n .au, the internet country code for Australia\n Au file format, Sun Microsystems' audio format\n Audio Units, a system level plug-in architecture from Apple Computer\n Adobe Audition, a sound editor program\n Windows Update or Automatic Updates, in Microsoft Windows\n Windows 10 Anniversary Update, of August 2016a",
        "Physics and chemistry\n Gold, symbol Au (from Latin ), a chemical element\n Absorbance unit, a reporting unit in spectroscopy\n Atomic units, a system of units convenient for atomic physics and other fields\n Ångström unit, a unit of length equal to 10−10 m or 0.1 nanometre. \n Astronomical unit, a unit of length often used in planetary systems astronomy, an approximation for the average distance between the Earth and the Sun\n Arbitrary unit, a relative placeholder unit for when the actual value of a measurement is unknown or unimportant (\"a.u.\" is deprecated, use  \"arb. unit\" instead)\n\nArts and entertainment",
        "Arts and entertainment\n\nMusic\n AU (band), an experimental pop group headed by Luke Wyland\n Au, a 2010 release by Scottish rock band Donaldson, Moir and Paterson\n Au a track on Some Time in New York City by an album by John Lennon & Yoko Ono and Elephant's Memory\n\nMagazines\n Alternative Ulster, a Northern Irish music magazine, now called AU\n A&U: America's AIDS Magazine, sponsor of the Christopher Hewitt Award\n\nLiterature\nAlternative universe (fan fiction), fiction by fan authors that deliberately alters facts of the canonical universe written about.\n\nOther media\n Au Co, a fairy in Vietnamese mythology\n Age of Ultron, a 2013 series published by Marvel Comics\n A.U, a Chinese media franchise and brand",
        "Other media\n Au Co, a fairy in Vietnamese mythology\n Age of Ultron, a 2013 series published by Marvel Comics\n A.U, a Chinese media franchise and brand\n\nOrganizations\n au (mobile phone company), a mobile phone operator in Japan\n African Union, a continental union\n Americans United for Separation of Church and State\n Athletic Union, the union of sports clubs in a British university\n Austral Líneas Aéreas (IATA code AU)\n Auxiliary Units, specially trained, highly secret units created by the United Kingdom government during the Second World War\n AGROunia, an agrarian-socialist political party in Poland\n\nUniversities",
        "Universities\n\nAsia\n Ajou University  in Suwon, Gyeonggi, South Korea\n Abasyn University in Peshawar, Khyber Pakhtunkhwa, Pakistan\n Andhra University in Visakhapatnam, AP, India\n Anhui University in Hefei, Anhui, China \n Aletheia University in New Taipei City, Taiwan\n Allahabad University in Allahabad, Uttar Pradesh, India\n Arellano University in Philippines\n Assumption University (Thailand) in Thailand\n Abhilashi University in Himachal Pradesh, India\n Adesh University in Bathinda, Punjab, India.\n\nEurope\n Aarhus University in Aarhus, Denmark\n Aberystwyth University in Aberystwyth, Wales, United Kingdom\n Akademia Umiejętności in Kraków, Poland\n Arden University in Coventry, England\n\nOceania\n Auckland University in New Zealand",
        "North America\n Adelphi University in Garden City, New York\n Alfred University in Alfred, New York\n Algoma University in Sault Ste. Marie, Ontario, Canada\n American University in Washington, D.C.\n Anaheim University in Anaheim, California\n Anderson University (Indiana) in Anderson, Indiana\n Anderson University (South Carolina) in Anderson, South Carolina\n Andrews University in Berrien Springs, Michigan\n Antioch University in Culver City, California\n Apollos University in Huntington Beach, California\n Arcadia University in Glenside, Pennsylvania\n Argosy University in Alameda, California\n Arizona University in Tucson, Arizona\n Ashland University in Ashland, Ohio\n Athabasca University in Athabasca, Alberta, Canada\n Auburn University in Auburn, Alabama",
        "Arizona University in Tucson, Arizona\n Ashland University in Ashland, Ohio\n Athabasca University in Athabasca, Alberta, Canada\n Auburn University in Auburn, Alabama\n Augsburg University in Minneapolis, Minnesota\n Aurora University in Aurora, Illinois",
        "Other\n Air University (disambiguation), various Air Force universities\n\nPlaces\n Aue (toponymy), a frequent element in Germanic toponymy\n Australia (ISO 3166 country code)\n Au, Guinea, Kankan Region\n\nAustria\n Austria (informal two-letter country code) \n Au, Vorarlberg, Bregenz, Austria\n Au am Leithaberge, Austria\n Au im Bregenzerwald, Austria\n\nGermany\n Au (Munich), Munich, Germany\n Au (Schwarzwald), Baden-Württemberg, Germany\n Au (squat), a building and cultural center in Frankfurt, Germany\n Au am Rhein, Germany\n Au in der Hallertau, Germany\n\nSwitzerland\n Au, St. Gallen\n Au, Zürich\n Au peninsula\n Schloss Au, a château in Wädenswil\n\nVehicles\n Ford Falcon (AU), a family car made in Australia\n Vought AU, a post-World War II US Marine Corps variant of the F4U Corsair aircraft",
        "Vehicles\n Ford Falcon (AU), a family car made in Australia\n Vought AU, a post-World War II US Marine Corps variant of the F4U Corsair aircraft\n\nOther uses\n Aú, a cartwheel in the Brazilian martial art of Capoeira\n Au (surname), a Chinese family name\n Au language\n Ab urbe condita (sometimes abbreviated as a.u.), Latin for \"from the founding of the City\" (Rome)\n a'u, the Hawaiian name for the Pacific blue marlin"
    ],
    [
        "AutoCAD DXF\nAutoCAD DXF (Drawing Interchange Format, or Drawing Exchange Format) is a CAD data file format developed by Autodesk for enabling data interoperability between AutoCAD and other programs.\n\nDXF was introduced in December 1982 as part of AutoCAD 1.0, and was intended to provide an exact representation of the data in the AutoCAD native file format, DWG (Drawing).  For many years, Autodesk did not publish specifications, making correct creation of DXF files difficult. Autodesk now publishes the incomplete DXF specifications online.\n\nVersions of AutoCAD from Release 10 (October 1988) and up support both ASCII and binary forms of DXF.  Earlier versions support only ASCII.",
        "Versions of AutoCAD from Release 10 (October 1988) and up support both ASCII and binary forms of DXF.  Earlier versions support only ASCII.\n\nAs AutoCAD has become more powerful, supporting more complex object types, DXF has become less useful. Certain object types, including ACIS solids and regions, are not documented. Other object types, including AutoCAD 2006's dynamic blocks, and all of the objects specific to the vertical market versions of AutoCAD, are partially documented, but not well enough to allow other developers to support them. For these reasons many CAD applications use the DWG format which can be licensed from Autodesk or non-natively from the Open Design Alliance. DXF files do not specify the units of measurement used for its coordinates and dimensions.",
        "Most CAD systems and many vector graphics packages support the import and export of DXF files, notably Adobe products, Inkscape & Blender. Some CAD systems use DXF as their native format, notably QCAD and LibreCAD.",
        "File structure \nASCII versions of DXF can be read with any text editor. The basic organization of a DXF file is as follows:\n  section\n General information about the drawing. Each parameter has a variable name and an associated value.\n  section\n Holds the information for application-defined classes whose instances appear in the , , and  sections of the database. Generally does not provide sufficient information to allow interoperability with other programs.\n  section\n This section contains definitions of named items.\n Application ID () table\n Block Record () table\n Dimension Style () table\n Layer () table\n Linetype () table\n Text style () table\n User Coordinate System () table\n View () table\n Viewport configuration () table",
        "section \n This section contains Block Definition entities describing the entities comprising each Block in the drawing.\n  section\n This section contains the drawing entities, including any Block References.\n  section \n Contains the data that apply to nongraphical objects, used by AutoLISP, and ObjectARX applications.\n  section \n Contains the preview image for the DXF file.",
        "The data format of a DXF is called a \"tagged data\" format, which \"means that each data element in the file is preceded by an integer number that is called a group code. A group code's value indicates what type of data element follows. This value also indicates the meaning of a data element for a given object (or record) type. Virtually all user-specified information in a drawing file can be represented in DXF format.\"",
        "Criticism \nBecause DXF is only partially and poorly documented, missing documentation of key functionality like blocks & layouts, consideration is often given to alternative open formats like SVG (an open format defined by the W3C), DWF (an open format defined by Autodesk) or even EPS (ISO/IEC standard 29112:2018). DXF (as well as DWG) is however still a prefered format for CAD files for use by the ISO.\n\nSee also \n Design Web Format (DWF)\n Open Design Alliance (originally called OpenDWG)\n\nReferences\n\nExternal links \n\n AutoCAD DXF Reference (from Release 14, 1998) (PDF version from 2012)\n AutoCAD DXF File Format Summary.\n Annotated example DXF file \n AutoDesk Online DXF File Viewer.\n\n1982 introductions\nDXF\nAutodesk products\nDXF"
    ],
    [
        "AutoCAD\nAutoCAD is a 2D and \n3D computer-aided design (CAD) software application for desktop, web, and mobile developed by Autodesk. It was first released in December 1982 for the CP/M and IBM PC platforms as a desktop app running on microcomputers with internal graphics controllers. Initially a DOS application, subsequent versions were later released for other platforms including Classic Mac OS (1992), Microsoft Windows (1992), web browsers (2010), iOS (2010), macOS (2010), and Android (2011).",
        "AutoCAD is a general drafting and design application used in industry by architects, project managers, engineers, graphic designers, city planners and other professionals to prepare technical drawings. After discontinuing the sale of perpetual licenses in January 2016, commercial versions of AutoCAD are licensed through a term-based subscription.\n\nHistory\nBefore AutoCAD was introduced, most commercial CAD programs ran on mainframe computers or minicomputers, with each CAD operator (user) working at a separate graphics terminal.\n\nOrigins",
        "History\nBefore AutoCAD was introduced, most commercial CAD programs ran on mainframe computers or minicomputers, with each CAD operator (user) working at a separate graphics terminal.\n\nOrigins\n\nAutoCAD was derived from a program that began in 1977, and then released in 1979  called Interact CAD, also referred to in early Autodesk documents as MicroCAD, which was written prior to Autodesk's (then Marinchip Software Partners) formation by Autodesk cofounder Michael Riddle.",
        "The first version by Autodesk was demonstrated at the 1982 Comdex and released that December. AutoCAD supported CP/M-80 computers. As Autodesk's flagship product, by March 1986 AutoCAD had become the most ubiquitous CAD program worldwide. The 2022 release marked the 36th major release of AutoCAD for Windows and the 12th consecutive year of AutoCAD for Mac. The native file format of AutoCAD is .dwg. This and, to a lesser extent, its interchange file format DXF, have become de facto, if proprietary, standards for CAD data interoperability, particularly for 2D drawing exchange. AutoCAD has included support for .dwf, a format developed and promoted by Autodesk, for publishing CAD data.\n\nFeatures",
        "Features\n\nCompatibility with other software \nESRI ArcMap 10 permits export as AutoCAD drawing files. Civil 3D permits export as AutoCAD objects and as LandXML. Third-party file converters exist for specific formats such as Bentley MX GENIO Extension, PISTE Extension (France), ISYBAU (Germany), OKSTRA and Microdrainage (UK); also, conversion of .pdf files is feasible, however, the accuracy of the results may be unpredictable or distorted. For example, jagged edges may appear. Several vendors provide online conversions for free such as Cometdocs.",
        "Language\nAutoCAD and AutoCAD LT are available for English, German, French, Italian, Spanish, Japanese, Korean, Chinese Simplified, Chinese Traditional, Brazilian Portuguese, Russian, Czech, Polish and Hungarian (also through additional language packs). The extent of localization varies from full translation of the product to documentation only. The AutoCAD command set is localized as a part of the software localization.",
        "Extensions\nAutoCAD supports a number of APIs for customization and automation. These include AutoLISP, Visual LISP, VBA, .NET and ObjectARX. ObjectARX is a C++ class library, which was also the base for:\n products extending AutoCAD functionality to specific fields\n creating products such as AutoCAD Architecture, AutoCAD Electrical, AutoCAD Civil 3D\n third-party AutoCAD-based application\nThere are a large number of AutoCAD plugins (add-on applications) available on the application store Autodesk Exchange Apps.\nAutoCAD's DXF, drawing exchange format, allows importing and exporting drawing information.\n\nVertical integration\nAutodesk has also developed a few vertical programs for discipline-specific enhancements such as:",
        "Vertical integration\nAutodesk has also developed a few vertical programs for discipline-specific enhancements such as:\n\n Advance Steel\n AutoCAD Architecture\n AutoCAD Electrical\n AutoCAD Map 3D\n AutoCAD Mechanical\n AutoCAD MEP\n AutoCAD Plant 3D\n Autodesk Civil 3D\n\nSince AutoCAD 2019 several verticals are included with AutoCAD subscription as Industry-Specific Toolset.",
        "Since AutoCAD 2019 several verticals are included with AutoCAD subscription as Industry-Specific Toolset.\n\nFor example, AutoCAD Architecture (formerly Architectural Desktop) permits architectural designers to draw 3D objects, such as walls, doors, and windows, with more intelligent data associated with them rather than simple objects, such as lines and circles. The data can be programmed to represent specific architectural products sold in the construction industry, or extracted into a data file for pricing, materials estimation, and other values related to the objects represented.",
        "Additional tools generate standard 2D drawings, such as elevations and sections, from a 3D architectural model. Similarly, Civil Design, Civil Design 3D, and Civil Design Professional support data-specific objects facilitating easy standard civil engineering calculations and representations.\n\nSoftdesk Civil was developed as an AutoCAD add-on by a company in New Hampshire called Softdesk (originally DCA).  Softdesk was acquired by Autodesk, and Civil became Land Development Desktop (LDD), later renamed Land Desktop. Civil 3D was later developed and Land Desktop was retired.\n\nFile formats\nAutoCAD's native file formats are denoted either by a .dwg, .dwt, .dws, or .dxf filename extension.",
        "File formats\nAutoCAD's native file formats are denoted either by a .dwg, .dwt, .dws, or .dxf filename extension.\n\nThe primary file format for 2D and 3D drawing files created with AutoCAD is .dwg. While other third-party CAD software applications can create .dwg files, AutoCAD uniquely creates RealDWG files.",
        "Using AutoCAD, any .dwg file may be saved to a derivative format. These derivative formats include:\n Drawing Template Files .dwt: New .dwg are created from a .dwt file. Although the default template file is acad.dwt for AutoCAD and acadlt.dwt for AutoCAD LT, custom .dwt files may be created to include foundational configurations such as drawing units and layers.\n Drawing Standards File .dws: Using the CAD Standards feature of AutoCAD, a Drawing Standards File may be associated to any .dwg or .dwt file to enforce graphical standards.\n Drawing Interchange Format .dxf: The .dxf format is an ASCII representation of a .dwg file, and is used to transfer data between various applications.\n\nVariants",
        "Variants\n\nAutoCAD LT\nAutoCAD LT is the lower-cost version of AutoCAD, with reduced capabilities, first released in November 1993. Autodesk developed AutoCAD LT to have an entry-level CAD package to compete in the lower price level. Priced at $495, it became the first AutoCAD product priced below $1000. It was sold directly by Autodesk and in computer stores unlike the full version of AutoCAD, which must be purchased from official Autodesk dealers. AutoCAD LT 2015 introduced Desktop Subscription service from $360 per year; as of 2018, three subscription plans were available, from $50 a month to a 3-year, $1170 license.\n\nWhile there are hundreds of small differences between the full AutoCAD package and AutoCAD LT, there are a few recognized major differences in the software's features:",
        "While there are hundreds of small differences between the full AutoCAD package and AutoCAD LT, there are a few recognized major differences in the software's features:\n\n 3D capabilities: AutoCAD LT lacks the ability to create, visualize and render 3D models as well as 3D printing.\n Network licensing: AutoCAD LT cannot be used on multiple machines over a network.\n Customization: AutoCAD LT does not support customization with LISP, ARX, .NET and VBA.\n Management and automation capabilities with Sheet Set Manager and Action Recorder.\n CAD standards management tools.",
        "AutoCAD Mobile and AutoCAD Web\nAutoCAD Mobile and AutoCAD Web (formerly AutoCAD WS and AutoCAD 360) is an account-based mobile and web application enabling registered users to view, edit, and share AutoCAD files via mobile device and web using a limited AutoCAD feature set — and using cloud-stored drawing files.  The program, which is an evolution and combination of previous products, uses a freemium business model with a free plan and two paid levels, including various amounts of storage, tools, and online access to drawings. 360 includes new features such as a \"Smart Pen\" mode and linking to third-party cloud-based storage such as Dropbox. Having evolved from Flash-based software, AutoCAD Web uses HTML5 browser technology available in newer browsers including Firefox and Google Chrome.",
        "AutoCAD WS began with a version for the iPhone and subsequently expanded to include versions for the iPod Touch, iPad, Android phones, and Android tablets. Autodesk released the iOS version in September 2010, following with the Android version on April 20, 2011. The program is available via download at no cost from the App Store (iOS), Google Play (Android) and Amazon Appstore (Android).",
        "In its initial iOS version, AutoCAD WS supported drawing of lines, circles, and other shapes; creation of text and comment boxes; and management of color, layer, and measurements — in both landscape and portrait modes. Version 1.3, released August 17, 2011, added support for unit typing, layer visibility, area measurement and file management. The Android variant includes the iOS feature set along with such unique features as the ability to insert text or captions by voice command as well as manually. Both Android and iOS versions allow the user to save files on-line — or off-line in the absence of an Internet connection.\n\nIn 2011, Autodesk announced plans to migrate the majority of its software to \"the cloud\", starting with the AutoCAD WS mobile application.",
        "In 2011, Autodesk announced plans to migrate the majority of its software to \"the cloud\", starting with the AutoCAD WS mobile application.\n\nAccording to a 2013 interview with Ilai Rotbaein, an AutoCAD WS product manager for Autodesk, the name AutoCAD WS had no definitive meaning, and was interpreted variously as Autodesk Web Service, White Sheet or Work Space. In 2013, AutoCAD WS was renamed to AutoCAD 360. Later, it was renamed to AutoCAD Web App.",
        "Student versions",
        "AutoCAD is licensed, for free, to students, educators, and educational institutions, with a 12-month renewable license available. Licenses acquired before March 25, 2020 were a 36-month license, with its last renovation on March 24, 2020. The student version of AutoCAD is functionally identical to the full commercial version, with one exception: DWG files created or edited by a student version have an internal bit-flag set (the \"educational flag\"). When such a DWG file is printed by any version of AutoCAD (commercial or student) older than AutoCAD 2014 SP1 or AutoCAD 2019 and newer, the output includes a plot stamp/banner on all four sides. Objects created in the Student Version cannot be used for commercial use. Student Version objects \"infect\" a commercial version DWG file if they are",
        "includes a plot stamp/banner on all four sides. Objects created in the Student Version cannot be used for commercial use. Student Version objects \"infect\" a commercial version DWG file if they are imported in versions older than AutoCAD 2015 or newer than AutoCAD 2018.",
        "Ports\n\nWindows\n\nAutoCAD Release 12 in 1992 was the first version of the software to support the Windows platform - in that case Windows 3.1. After Release 14 in 1997, support for MS-DOS, Unix and Macintosh were dropped, and AutoCAD was exclusively Windows supported. In general any new AutoCAD version supports the current Windows version and some older ones. AutoCAD 2016 to 2020 support Windows 7 up to Windows 10.",
        "Mac",
        "Autodesk stopped supporting Apple's Macintosh computers in 1994. Over the next several years, no compatible versions for the Mac were released. In 2010 Autodesk announced that it would once again support Apple's Mac OS X software in the future. Most of the features found in the 2012 Windows version can be found in the 2012 Mac version. The main difference is the user interface and layout of the program. The interface is designed so that users who are already familiar with Apple's macOS software will find it similar to other Mac applications. Autodesk has also built-in various features in order to take full advantage of Apple's Trackpad capabilities as well as the full-screen mode in Apple's OS X Lion. AutoCAD 2012 for Mac supports both the editing and saving of files in DWG formatting",
        "to take full advantage of Apple's Trackpad capabilities as well as the full-screen mode in Apple's OS X Lion. AutoCAD 2012 for Mac supports both the editing and saving of files in DWG formatting that will allow the file to be compatible with other platforms besides macOS. AutoCAD 2019 for Mac requires OS X El Capitan or later.",
        "AutoCAD LT 2013 was available through the Mac App Store for $899.99. The full-featured version of AutoCAD 2013 for Mac, however, wasn't available through the Mac App Store due to the price limit of $999 set by Apple. AutoCAD 2014 for Mac was available for purchase from Autodesk's web site for $4,195 and AutoCAD LT 2014 for Mac for $1,200, or from an Autodesk authorized reseller. The latest version available for Mac is AutoCAD 2022 as of January 2022.\n\nVersion history\n\nSee also\n Autodesk 3ds Max\n Autodesk Maya\n Autodesk Revit\n AutoShade\n AutoSketch\n Comparison of computer-aided design software\n Design Web Format\nOpen source CAD software:\n LibreCAD\n FreeCAD\n BRL-CAD\n\nReferences\n\nFurther reading\n\nExternal links\n\n \n Engineering and Design Firm",
        "References\n\nFurther reading\n\nExternal links\n\n \n Engineering and Design Firm\n\nAutodesk products\n \n1982 software\nIRIX software\nComputer-aided design software\nIOS software\nClassic Mac OS software\nAndroid (operating system) software\nMacOS computer-aided design software\nSoftware that uses Qt"
    ],
    [
        "Autocorrelation\nAutocorrelation, sometimes known as serial correlation in the discrete time case, is the correlation of a signal with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations of a random variable as a function of the time lag between them. The analysis of autocorrelation is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency  in a signal implied by its harmonic frequencies. It is often used in signal processing for analyzing functions or series of values, such as time domain signals.",
        "Different fields of study define autocorrelation differently, and not all of these definitions are equivalent. In some fields, the term is used interchangeably with autocovariance.\n\nUnit root processes, trend-stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation.",
        "Unit root processes, trend-stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation.\n\nAuto-correlation of stochastic processes \nIn statistics, the autocorrelation of a real or complex random process is the Pearson correlation between values of the process at different times, as a function of the two times or of the time lag. Let  be a random process, and  be any point in time ( may be an integer for a discrete-time process or a real number for a continuous-time process). Then  is the value (or realization) produced by a given run of the process at time . Suppose that the process has mean  and variance  at time , for each . Then the definition of the auto-correlation function between times  and  is",
        "where  is the expected value operator and the bar represents complex conjugation. Note that the expectation may not be well defined.\n\nSubtracting the mean before multiplication yields the auto-covariance function between times  and :\n\nNote that this expression is not well defined for all time series or processes, because the mean may not exist, or the variance may be zero (for a constant process) or infinite (for processes with distribution lacking well-behaved moments, such as certain types of power law).",
        "Definition for wide-sense stationary stochastic process \nIf  is a wide-sense stationary process then the mean  and the variance  are time-independent, and further the autocovariance function depends only on the lag between  and : the autocovariance depends only on the time-distance between the pair of values but not on their position in time. This further implies that the autocovariance and auto-correlation can be expressed as a function of the time-lag, and that this would be an even function of the lag  . This gives the more familiar forms for the auto-correlation function\n\nand the auto-covariance function:\n\nIn particular, note that",
        "and the auto-covariance function:\n\nIn particular, note that\n\nNormalization \nIt is common practice in some disciplines (e.g. statistics and time series analysis) to normalize the autocovariance function to get a time-dependent Pearson correlation coefficient. However, in other disciplines (e.g. engineering) the normalization is usually dropped and the terms \"autocorrelation\" and \"autocovariance\" are used interchangeably.\n\nThe definition of the auto-correlation coefficient of a stochastic process is\n\nIf the function  is well defined, its value must lie in the range , with 1 indicating perfect correlation and −1 indicating perfect anti-correlation.\n\nFor a wide-sense stationary (WSS) process, the definition is\n\n.",
        "For a wide-sense stationary (WSS) process, the definition is\n\n.\n\nThe normalization is important both because the interpretation of the autocorrelation as a correlation provides a scale-free measure of the strength of statistical dependence, and because the normalization has an effect on the statistical properties of the estimated autocorrelations.\n\nProperties\n\nSymmetry property\nThe fact that the auto-correlation function  is an even function can be stated as\n\nrespectively for a WSS process:\n\nMaximum at zero\nFor a WSS process:\n\nNotice that  is always real.\n\nCauchy–Schwarz inequality\nThe Cauchy–Schwarz inequality, inequality for stochastic processes:",
        "respectively for a WSS process:\n\nMaximum at zero\nFor a WSS process:\n\nNotice that  is always real.\n\nCauchy–Schwarz inequality\nThe Cauchy–Schwarz inequality, inequality for stochastic processes:\n\nAutocorrelation of white noise\nThe autocorrelation of a continuous-time white noise signal will have a strong peak (represented by a Dirac delta function) at  and will be exactly  for all other .\n\nWiener–Khinchin theorem\nThe Wiener–Khinchin theorem relates the autocorrelation function  to the power spectral density  via the Fourier transform:\n\nFor real-valued functions, the symmetric autocorrelation function has a real symmetric transform, so the Wiener–Khinchin theorem can be re-expressed in terms of real cosines only:\n\nAuto-correlation of random vectors",
        "Auto-correlation of random vectors\n\nThe (potentially time-dependent) auto-correlation matrix (also called second moment) of a (potentially time-dependent) random vector  is an  matrix containing as elements the autocorrelations of all pairs of elements of the random vector . The autocorrelation matrix is used in various digital signal processing algorithms.\n\nFor a random vector  containing random elements whose expected value and variance exist, the auto-correlation matrix is defined by\n\nwhere  denotes the transposed matrix of dimensions .\n\nWritten component-wise:\n\nIf  is a complex random vector, the autocorrelation matrix is instead defined by\n\nHere  denotes Hermitian transpose.\n\nFor example, if  is a random vector, then  is a  matrix whose -th entry is .",
        "If  is a complex random vector, the autocorrelation matrix is instead defined by\n\nHere  denotes Hermitian transpose.\n\nFor example, if  is a random vector, then  is a  matrix whose -th entry is .\n\nProperties of the autocorrelation matrix\n The autocorrelation matrix is a Hermitian matrix for complex random vectors and a symmetric matrix for real random vectors.\n The autocorrelation matrix is a positive semidefinite matrix, i.e.  for a real random vector, and respectively  in case of a complex random vector.\n All eigenvalues of the autocorrelation matrix are real and non-negative.\n The auto-covariance matrix is related to the autocorrelation matrix as follows:Respectively for complex random vectors:",
        "Auto-correlation of deterministic signals \nIn signal processing, the above definition is often used without the normalization, that is, without subtracting the mean and dividing by the variance. When the autocorrelation function is normalized by mean and variance, it is sometimes referred to as the autocorrelation coefficient or autocovariance function.\n\nAuto-correlation of continuous-time signal \nGiven a signal , the continuous autocorrelation  is most often defined as the continuous cross-correlation integral of  with itself, at lag .\n\nwhere  represents the complex conjugate of . Note that the parameter  in the integral is a dummy variable and is only necessary to calculate the integral. It has no specific meaning.",
        "where  represents the complex conjugate of . Note that the parameter  in the integral is a dummy variable and is only necessary to calculate the integral. It has no specific meaning.\n\nAuto-correlation of discrete-time signal \nThe discrete autocorrelation  at lag  for a discrete-time signal  is\n\nThe above definitions work for signals that are square integrable, or square summable, that is, of finite energy. Signals that \"last forever\" are treated instead as random processes, in which case different definitions are needed, based on expected values. For wide-sense-stationary random processes, the autocorrelations are defined as\n\nFor processes that are not stationary, these will also be functions of , or .",
        "For processes that are not stationary, these will also be functions of , or .\n\nFor processes that are also ergodic, the expectation can be replaced by the limit of a time average. The autocorrelation of an ergodic process is sometimes defined as or equated to\n\nThese definitions have the advantage that they give sensible well-defined single-parameter results for periodic functions, even when those functions are not the output of stationary ergodic processes.\n\nAlternatively, signals that last forever can be treated by a short-time autocorrelation function analysis, using finite time integrals. (See short-time Fourier transform for a related process.)",
        "Alternatively, signals that last forever can be treated by a short-time autocorrelation function analysis, using finite time integrals. (See short-time Fourier transform for a related process.)\n\nDefinition for periodic signals\nIf  is a continuous periodic function of period , the integration from  to  is replaced by integration over any interval  of length :\n\nwhich is equivalent to\n\nProperties\nIn the following, we will describe properties of one-dimensional autocorrelations only, since most properties are easily transferred from the one-dimensional case to the multi-dimensional cases. These properties hold for wide-sense stationary processes.",
        "A fundamental property of the autocorrelation is symmetry, , which is easy to prove from the definition. In the continuous case,\n the autocorrelation is an even function  when  is a real function, and\n the autocorrelation is a Hermitian function  when  is a complex function.\n The continuous autocorrelation function reaches its peak at the origin, where it takes a real value, i.e. for any delay , . This is a consequence of the rearrangement inequality. The same result holds in the discrete case.\n The autocorrelation of a periodic function is, itself, periodic with the same period.\n The autocorrelation of the sum of two completely uncorrelated functions (the cross-correlation is zero for all ) is the sum of the autocorrelations of each function separately.",
        "The autocorrelation of the sum of two completely uncorrelated functions (the cross-correlation is zero for all ) is the sum of the autocorrelations of each function separately.\n Since autocorrelation is a specific type of cross-correlation, it maintains all the properties of cross-correlation.\n By using the symbol  to represent convolution and  is a function which manipulates the function  and is defined as , the definition for  may be written as:",
        "Multi-dimensional autocorrelation\nMulti-dimensional autocorrelation is defined similarly. For example, in three dimensions the autocorrelation of a square-summable discrete signal would be\n\nWhen mean values are subtracted from signals before computing an autocorrelation function, the resulting function is usually called an auto-covariance function.",
        "When mean values are subtracted from signals before computing an autocorrelation function, the resulting function is usually called an auto-covariance function.\n\nEfficient computation\nFor data expressed as a discrete sequence, it is frequently necessary to compute the autocorrelation with high computational efficiency. A brute force method based on the signal processing definition  can be used when the signal size is small. For example, to calculate the autocorrelation of the real signal sequence  (i.e. , and  for all other values of ) by hand, we first recognize that the definition just given is the same as the \"usual\" multiplication, but with right shifts, where each vertical addition gives the autocorrelation for particular lag values:",
        "Thus the required autocorrelation sequence is , where   and  the autocorrelation for other lag values being zero. In this calculation we do not perform the carry-over operation during addition as is usual in normal multiplication. Note that we can halve the number of operations required by exploiting the inherent symmetry of the autocorrelation. If the signal happens to be periodic, i.e.  then we get a circular autocorrelation (similar to circular convolution) where the left and right tails of the previous autocorrelation sequence will overlap and give  which has the same period as the signal sequence  The procedure can be regarded as an application of the convolution property of Z-transform of a discrete signal.",
        "While the brute force algorithm is order , several efficient algorithms exist which can compute the autocorrelation in order . For example, the Wiener–Khinchin theorem allows computing the autocorrelation from the raw data  with two fast Fourier transforms (FFT):\n\nwhere IFFT denotes the inverse fast Fourier transform. The asterisk denotes complex conjugate.\n\nAlternatively, a multiple  correlation can be performed by using brute force calculation for low  values, and then progressively binning the  data with a logarithmic density to compute higher values, resulting in the same  efficiency, but with lower memory requirements.\n\nEstimation\nFor a discrete process with known mean and variance for which we observe  observations , an estimate of the autocorrelation coefficient may be obtained as",
        "for any positive integer . When the true mean  and variance  are known, this estimate is unbiased. If the true mean and variance of the process are not known there are several possibilities:\n If  and  are replaced by the standard formulae for sample mean and sample variance, then this is a biased estimate.\n A periodogram-based estimate replaces  in the above formula with . This estimate is always biased; however, it usually has a smaller mean squared error.\n Other possibilities derive from treating the two portions of data  and  separately and calculating separate sample means and/or sample variances for use in defining the estimate.",
        "The advantage of estimates of the last type is that the set of estimated autocorrelations, as a function of , then form a function which is a valid autocorrelation in the sense that it is possible to define a theoretical process having exactly that autocorrelation. Other estimates can suffer from the problem that, if they are used to calculate the variance of a linear combination of the 's, the variance calculated may turn out to be negative.\n\nRegression analysis",
        "Regression analysis\n\nIn regression analysis using time series data, autocorrelation in a variable of interest is typically modeled either with an autoregressive model (AR), a moving average model (MA), their combination as an autoregressive-moving-average model (ARMA), or an extension of the latter called an autoregressive integrated moving average model (ARIMA). With multiple interrelated data series, vector autoregression (VAR) or its extensions are used.",
        "In ordinary least squares (OLS), the adequacy of a model specification can be checked in part by establishing whether there is autocorrelation of the regression residuals. Problematic autocorrelation of the errors, which themselves are unobserved, can generally be detected because it produces autocorrelation in the observable residuals. (Errors are also known as \"error terms\" in econometrics.) Autocorrelation of the errors violates the ordinary least squares assumption that the error terms are uncorrelated, meaning that the Gauss Markov theorem does not apply, and that OLS estimators are no longer the Best Linear Unbiased Estimators (BLUE). While it does not bias the OLS coefficient estimates, the standard errors tend to be underestimated (and the t-scores overestimated) when the",
        "are no longer the Best Linear Unbiased Estimators (BLUE). While it does not bias the OLS coefficient estimates, the standard errors tend to be underestimated (and the t-scores overestimated) when the autocorrelations of the errors at low lags are positive.",
        "The traditional test for the presence of first-order autocorrelation is the Durbin–Watson statistic or, if the explanatory variables include a lagged dependent variable, Durbin's h statistic. The Durbin-Watson can be linearly mapped however to the Pearson correlation between values and their lags.  A more flexible test, covering autocorrelation of higher orders and applicable whether or not the regressors include lags of the dependent variable, is the Breusch–Godfrey test. This involves an auxiliary regression, wherein the residuals obtained from estimating the model of interest are regressed on (a) the original regressors and (b) k lags of the residuals, where 'k' is the order of the test. The simplest version of the test statistic from this auxiliary regression is TR2, where T is the",
        "on (a) the original regressors and (b) k lags of the residuals, where 'k' is the order of the test. The simplest version of the test statistic from this auxiliary regression is TR2, where T is the sample size and R2 is the coefficient of determination. Under the null hypothesis of no autocorrelation, this statistic is asymptotically distributed as  with k degrees of freedom.",
        "Responses to nonzero autocorrelation include generalized least squares and the Newey–West HAC estimator (Heteroskedasticity and Autocorrelation Consistent).\n\nIn the estimation of a moving average model (MA), the autocorrelation function is used to determine the appropriate number of lagged error terms to be included. This is based on the fact that for an MA process of order q, we have , for , and , for .\n\nApplications",
        "Autocorrelation analysis is used heavily in fluorescence correlation spectroscopy to provide quantitative insight into molecular-level diffusion and chemical reactions.\n Another application of autocorrelation is the measurement of optical spectra and the measurement of very-short-duration light pulses produced by lasers, both using optical autocorrelators.",
        "Another application of autocorrelation is the measurement of optical spectra and the measurement of very-short-duration light pulses produced by lasers, both using optical autocorrelators.\n Autocorrelation is used to analyze dynamic light scattering data, which notably enables determination of the particle size distributions of nanometer-sized particles or micelles suspended in a fluid. A laser shining into the mixture produces a speckle pattern that results from the motion of the particles. Autocorrelation of the signal can be analyzed in terms of the diffusion of the particles. From this, knowing the viscosity of the fluid, the sizes of the particles can be calculated.",
        "Utilized in the GPS system to correct for the propagation delay, or time shift, between the point of time at the transmission of the carrier signal at the satellites, and the point of time at the receiver on the ground.  This is done by the receiver generating a replica signal of the 1,023-bit C/A (Coarse/Acquisition) code, and generating lines of code chips [-1,1] in packets of ten at a time, or 10,230 chips (1,023 × 10), shifting slightly as it goes along in order to accommodate for the doppler shift in the incoming satellite signal, until the receiver replica signal and the satellite signal codes match up.\n The small-angle X-ray scattering intensity of a nanostructured system is the Fourier transform of the spatial autocorrelation function of the electron density.",
        "The small-angle X-ray scattering intensity of a nanostructured system is the Fourier transform of the spatial autocorrelation function of the electron density.\nIn surface science and scanning probe microscopy, autocorrelation is used to establish a link between surface morphology and functional characteristics.\n In optics, normalized autocorrelations and cross-correlations give the degree of coherence of an electromagnetic field.\n In signal processing, autocorrelation can give information about repeating events like musical beats (for example, to determine tempo) or pulsar frequencies, though it cannot tell the position in time of the beat. It can also be used to estimate the pitch of a musical tone.",
        "In music recording, autocorrelation is used as a pitch detection algorithm prior to vocal processing, as a distortion effect or to eliminate undesired mistakes and inaccuracies.\n Autocorrelation in space rather than time, via the Patterson function, is used by X-ray diffractionists to help recover the \"Fourier phase information\" on atom positions not available through diffraction alone.\n In statistics, spatial autocorrelation between sample locations also helps one estimate mean value uncertainties when sampling a heterogeneous population.\n The SEQUEST algorithm for analyzing mass spectra makes use of autocorrelation in conjunction with cross-correlation to score the similarity of an observed spectrum to an idealized spectrum representing a peptide.",
        "In astrophysics, autocorrelation is used to study and characterize the spatial distribution of galaxies in the universe and in multi-wavelength observations of low mass X-ray binaries.\n In panel data, spatial autocorrelation refers to correlation of a variable with itself through space.\n In analysis of Markov chain Monte Carlo data, autocorrelation must be taken into account for correct error determination.\n In geosciences (specifically in geophysics) it can be used to compute an autocorrelation seismic attribute, out of a 3D seismic survey of the underground.\n In medical ultrasound imaging, autocorrelation is used to visualize blood flow.",
        "In medical ultrasound imaging, autocorrelation is used to visualize blood flow.\n In intertemporal portfolio choice, the presence or absence of autocorrelation in an asset's rate of return can affect the optimal portion of the portfolio to hold in that asset.\n Autocorrelation has been used to accurately measure power system frequency in numerical relays.",
        "Serial dependence \nSerial dependence is closely linked to the notion of autocorrelation, but represents a distinct concept (see Correlation and dependence). In particular, it is possible to have serial dependence but no (linear) correlation. In some fields however, the two terms are used as synonyms.\n\nA  time series of a random variable has serial dependence if the value at some time  in the series is statistically dependent on the value at another time . A series is serially independent if there is no dependence between any pair.\n\nIf a time series  is stationary, then statistical dependence between the pair  would imply that there is statistical dependence between all pairs of values at the same lag .\n\nSee also",
        "If a time series  is stationary, then statistical dependence between the pair  would imply that there is statistical dependence between all pairs of values at the same lag .\n\nSee also\n\n Autocorrelation matrix\n Autocorrelation of a formal word\n Autocorrelation technique\n Autocorrelator\n Cochrane–Orcutt estimation (transformation for autocorrelated error terms)\n Correlation function\n Correlogram\n Cross-correlation\n CUSUM\n Fluorescence correlation spectroscopy\n Optical autocorrelation\n Partial autocorrelation function\n Phylogenetic autocorrelation (Galton's problem}\n Pitch detection algorithm\n Prais–Winsten transformation\n Scaled correlation\n Triple correlation\n Unbiased estimation of standard deviation\n\nReferences",
        "References\n\nFurther reading\n \n \n Mojtaba Soltanalian, and Petre Stoica. \"Computational design of sequences with good correlation properties.\" IEEE Transactions on Signal Processing, 60.5 (2012): 2180–2193.\n Solomon W. Golomb, and Guang Gong. Signal design for good correlation: for wireless communication, cryptography, and radar. Cambridge University Press, 2005.\n Klapetek, Petr (2018). Quantitative Data Processing in Scanning Probe Microscopy: SPM Applications for Nanometrology (Second ed.). Elsevier. pp. 108–112  .\n \n\n \nSignal processing\nTime domain analysis"
    ],
    [
        "Automated theorem proving\nAutomated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs.  Automated reasoning over mathematical proof was a major impetus for the development of computer science.",
        "Logical foundations",
        "While the roots of formalised logic go back to Aristotle, the end of the 19th and early 20th centuries saw the development of modern logic and formalised mathematics. Frege's Begriffsschrift (1879) introduced both a complete propositional calculus and what is essentially modern predicate logic.  His Foundations of Arithmetic, published in 1884, expressed (parts of) mathematics in formal logic. This approach was continued by Russell and Whitehead in their influential Principia Mathematica, first published 1910–1913, and with a revised second edition in 1927. Russell and Whitehead thought they could derive all mathematical truth using axioms and inference rules of formal logic, in principle opening up the process to automatisation. In 1920, Thoralf Skolem simplified a previous result by",
        "could derive all mathematical truth using axioms and inference rules of formal logic, in principle opening up the process to automatisation. In 1920, Thoralf Skolem simplified a previous result by Leopold Löwenheim, leading to the Löwenheim–Skolem theorem and, in 1930, to the notion of a Herbrand universe and a Herbrand interpretation that allowed (un)satisfiability of first-order formulas (and hence the validity of a theorem) to be reduced to (potentially infinitely many) propositional satisfiability problems.",
        "In 1929, Mojżesz Presburger showed that the first-order theory of the natural numbers with addition and equality (now called Presburger arithmetic in his honor) is decidable and gave an algorithm that could determine if a given sentence in the language was true or false.\nHowever, shortly after this positive result, Kurt Gödel published On Formally Undecidable Propositions of Principia Mathematica and Related Systems (1931), showing that in any sufficiently strong axiomatic system there are true statements that cannot be proved in the system. This topic was further developed in the 1930s by Alonzo Church and Alan Turing, who on the one hand gave two independent but equivalent definitions of computability, and on the other gave concrete examples of undecidable questions.",
        "First implementations",
        "Shortly after World War II, the first general-purpose computers became available. In 1954, Martin Davis programmed Presburger's algorithm for a JOHNNIAC vacuum-tube computer at the Institute for Advanced Study in Princeton, New Jersey. According to Davis, \"Its great triumph was to prove that the sum of two even numbers is even\". More ambitious was the Logic Theorist in 1956, a deduction system for the propositional logic of the Principia Mathematica, developed by Allen Newell, Herbert A. Simon and J. C. Shaw. Also running on a JOHNNIAC, the Logic Theorist constructed proofs from a small set of propositional axioms and three deduction rules: modus ponens, (propositional) variable substitution, and the replacement of formulas by their definition. The system used heuristic guidance, and",
        "of propositional axioms and three deduction rules: modus ponens, (propositional) variable substitution, and the replacement of formulas by their definition. The system used heuristic guidance, and managed to prove 38 of the first 52 theorems of the Principia.",
        "The \"heuristic\" approach of the Logic Theorist tried to emulate human mathematicians, and could not guarantee that a proof could be found for every valid theorem even in principle.  In contrast, other, more systematic algorithms achieved, at least theoretically, completeness for first-order logic. Initial approaches relied on the results of Herbrand and Skolem to convert a first-order formula into successively larger sets of propositional formulae by instantiating variables with terms from the Herbrand universe. The propositional formulas could then be checked for unsatisfiability using a number of methods. Gilmore's program used conversion to disjunctive normal form, a form in which the satisfiability of a formula is obvious.\n\nDecidability of the problem",
        "Decidability of the problem \n\nDepending on the underlying logic, the problem of deciding the validity of a formula varies from trivial to impossible. For the common case of propositional logic, the problem is decidable but co-NP-complete, and hence only exponential-time algorithms are believed to exist for general proof tasks. For a first-order predicate calculus, Gödel's completeness theorem states that the theorems (provable statements) are exactly the semantically valid well-formed formulas, so the valid formulas are computably enumerable: given unbounded resources, any valid formula can eventually be proven. However, invalid formulas (those that are not entailed by a given theory), cannot always be recognized.",
        "The above applies to first-order theories, such as Peano arithmetic. However, for a specific model that may be described by a first-order theory, some statements may be true but undecidable in the theory used to describe the model. For example, by Gödel's incompleteness theorem, we know that any consistent theory whose axioms are true for the natural numbers cannot prove all first-order statements true for the natural numbers, even if the list of axioms is allowed to be infinite enumerable. It follows that an automated theorem prover will fail to terminate while searching for a proof precisely when the statement being investigated is undecidable in the theory being used, even if it is true in the model of interest. Despite this theoretical limit, in practice, theorem provers can solve",
        "when the statement being investigated is undecidable in the theory being used, even if it is true in the model of interest. Despite this theoretical limit, in practice, theorem provers can solve many hard problems, even in models that are not fully described by any first-order theory (such as the integers).",
        "Related problems \n\nA simpler, but related, problem is proof verification, where an existing proof for a theorem is certified valid. For this, it is generally required that each individual proof step can be verified by a primitive recursive function or program, and hence the problem is always decidable.\n\nSince the proofs generated by automated theorem provers are typically very large, the problem of proof compression is crucial, and various techniques aiming at making the prover's output smaller, and consequently more easily understandable and checkable, have been developed.",
        "Proof assistants require a human user to give hints to the system. Depending on the degree of automation, the prover can essentially be reduced to a proof checker, with the user providing the proof in a formal way, or significant proof tasks can be performed automatically. Interactive provers are used for a variety of tasks, but even fully automatic systems have proved a number of interesting and hard theorems, including at least one that has eluded human mathematicians for a long time, namely the Robbins conjecture. However, these successes are sporadic, and work on hard problems usually requires a proficient user.",
        "Another distinction is sometimes drawn between theorem proving and other techniques, where a process is considered to be theorem proving if it consists of a traditional proof, starting with axioms and producing new inference steps using rules of inference.  Other techniques would include model checking, which, in the simplest case, involves brute-force enumeration of many possible states (although the actual implementation of model checkers requires much cleverness, and does not simply reduce to brute force).",
        "There are hybrid theorem proving systems that use model checking as an inference rule. There are also programs that were written to prove a particular theorem, with a (usually informal) proof that if the program finishes with a certain result, then the theorem is true. A good example of this was the machine-aided proof of the four color theorem, which was very controversial as the first claimed mathematical proof that was essentially impossible to verify by humans due to the enormous size of the program's calculation (such proofs are called non-surveyable proofs).  Another example of a program-assisted proof is the one that shows that the game of Connect Four can always be won by the first player.\n\nIndustrial uses",
        "Industrial uses \n\nCommercial use of automated theorem proving is mostly concentrated in integrated circuit design and verification.  Since the Pentium FDIV bug, the complicated floating point units of modern microprocessors have been designed with extra scrutiny. AMD, Intel and others use automated theorem proving to verify that division and other operations are correctly implemented in their processors.\n\nFirst-order theorem proving",
        "In the late 1960s agencies funding research in automated deduction began to emphasize the need for practical applications. One of the first fruitful areas was that of program verification whereby first-order theorem provers were applied to the problem of verifying the correctness of computer programs in languages such as Pascal, Ada, etc. Notable among early program verification systems was the Stanford Pascal Verifier developed by David Luckham at Stanford University. This was based on the Stanford Resolution Prover also developed at Stanford using John Alan Robinson's resolution principle. This was the first automated deduction system to demonstrate an ability to solve mathematical problems that were announced in the Notices of the American Mathematical Society before solutions were",
        "This was the first automated deduction system to demonstrate an ability to solve mathematical problems that were announced in the Notices of the American Mathematical Society before solutions were formally published.",
        "First-order theorem proving is one of the most mature subfields of automated theorem proving. The logic is expressive enough to allow the specification of arbitrary problems, often in a reasonably natural and intuitive way. On the other hand, it is still semi-decidable, and a number of sound and complete calculi have been developed, enabling fully automated systems. More expressive logics, such as higher-order logics, allow the convenient expression of a wider range of problems than first-order logic, but theorem proving for these logics is less well developed.",
        "Benchmarks, competitions, and sources \nThe quality of implemented systems has benefited from the existence of a large library of standard benchmark examples—the Thousands of Problems for Theorem Provers (TPTP) Problem Library—as well as from the CADE ATP System Competition (CASC), a yearly competition of first-order systems for many important classes of first-order problems.",
        "Some important systems (all have won at least one CASC competition division) are listed below.\n E is a high-performance prover for full first-order logic, but built on a purely equational calculus, originally developed in the automated reasoning group of Technical University of Munich under the direction of Wolfgang Bibel, and now at Baden-Württemberg Cooperative State University in Stuttgart.\n Otter, developed at the Argonne National Laboratory, is based on first-order resolution and paramodulation. Otter has since been replaced by Prover9, which is paired with Mace4.",
        "Otter, developed at the Argonne National Laboratory, is based on first-order resolution and paramodulation. Otter has since been replaced by Prover9, which is paired with Mace4.\n SETHEO is a high-performance system based on the goal-directed model elimination calculus, originally developed by a team under direction of Wolfgang Bibel. E and SETHEO have been combined (with other systems) in the composite theorem prover E-SETHEO.\n Vampire was originally developed and implemented at Manchester University by Andrei Voronkov and Kryštof Hoder. It is now developed by a growing international team. It has won the FOF division (among other divisions) at the CADE ATP System Competition regularly since 2001.",
        "Waldmeister is a specialized system for unit-equational first-order logic developed by Arnim Buch and Thomas Hillenbrand. It won the CASC UEQ division for fourteen consecutive years (1997–2010).\n SPASS is a first-order logic theorem prover with equality. This is developed by the research group Automation of Logic, Max Planck Institute for Computer Science.",
        "The Theorem Prover Museum is an initiative to conserve the sources of theorem prover systems for future analysis, since they are important cultural/scientific artefacts. It has the sources of many of the systems mentioned above.\n\nPopular techniques \n\nFirst-order resolution with unification\nModel elimination\nMethod of analytic tableaux\nSuperposition and term rewriting\nModel checking\nMathematical induction\nBinary decision diagrams\nDPLL\nHigher-order unification\nQuantifier elimination\n\nSoftware systems\n\nFree software \n Alt-Ergo\n Automath\n CVC\n E\n IsaPlanner\n LCF\n Mizar\n NuPRL\n Paradox\n Prover9\n PVS\n SPARK (programming language)\n Twelf\n Z3 Theorem Prover\n\nProprietary software \n CARINE\n Wolfram Mathematica\n ResearchCyc\n\nSee also",
        "Proprietary software \n CARINE\n Wolfram Mathematica\n ResearchCyc\n\nSee also \n\n Curry–Howard correspondence\n Symbolic computation\n Ramanujan machine\n Computer-aided proof\n Formal verification\n Logic programming\n Proof checking\n Model checking\n Proof complexity\n Computer algebra system\n Program analysis (computer science)\n General Problem Solver\n Metamath language for formalized mathematics\n\nNotes\n\nReferences \n\n \n \n\n \n \n \n  II .\n\nExternal links\n A list of theorem proving tools\n\n \nFormal methods"
    ],
    [
        "Automatic number announcement circuit\nAn automatic number announcement circuit (ANAC) is a component of a central office of a telephone company that provides a service to installation and service technicians to determine the telephone number of a telephone line. The facility has a telephone number that may be called to listen to an automatic announcement that includes the caller's telephone number. The ANAC facility is useful primarily during the installation of landline telephones to quickly identify one of multiple wire pairs in a bundle or at a termination point.",
        "Operation \nBy connecting a test telephone set, a technician calls the local telephone number of the automatic number announcement service. This call is connected to equipment at the central office that uses automatic equipment to announce the telephone number of the line calling in. The main purpose of this system is to allow telephone company technicians to identify the telephone line they are connected to.",
        "Automatic number announcement systems are based on automatic number identification. They are intended for use by phone company technicians, the ANAC system bypasses customer features, such as unlisted numbers, caller ID blocking, and outgoing call blocking. Installers of multi-line business services where outgoing calls from all lines display the company's main number on call display can use ANAC to identify a specific line in the system, even if CID displays every line as \"line one\".\n\nMost ANAC systems are provider-specific in each wire center, while others are regional or state-/province- or area-code-wide. No official lists of ANAC numbers are published, as telephone companies guard against abuse that would interfere with availability for installers.",
        "Exchange prefixes for testing\nThe North American Numbering Plan reserves the exchange (central office) prefixes 958 and 959 for plant testing purposes. Code 959 with three or four additional digits is dedicated for access to office test lines in local exchange carrier and interoffice carrier central offices. The specifications define several test features for line conditions, such as quiet line and busy line, and test tones transmitted to callers. Telephone numbers are assigned for ring back to test the ringer when installing telephone sets, milliwatt tone (a number simply answers with a continuous test tone) and a loop around (which connects a call to another inbound call to the same or another test number).",
        "ANAC services are typically installed in the 958 range, which is intended for communications between central offices. In some area codes, multiple additional prefixes may be reserved for test purposes. Many area codes reserved 999; 320 was also formerly reserved in Bell Canada territory.\n\nOther carrier-specific North American test numbers include 555-XXXX numbers (such as 555-0311 on Rogers Communications in Canada) or vertical service codes, such as *99 on Cablevision/Optimum Voice in the United States.",
        "Telephone numbers\nPlant testing telephone numbers are carrier-specific, there is no comprehensive list of telephone numbers for ANAC services. In some communities, test numbers change relatively often.  In others, a major incumbent carrier might assign a single number which provides test functions on its network  across an entire numbering plan area, throughout an entire province or state, or system-wide.",
        "Some telecommunication carriers maintain toll-free numbers for ANAC facilities. Some national toll-free numbers provide automatic number identification by speaking the telephone number of the caller, but these are not intended for use in identifying the customer's own phone number. They are used for the agent in a call center to confirm the telephone a customer is calling from, so that the customer's account information can be displayed as a \"screen pop\" for the next available customer service representative.\n\nSee also\n Plant test number\n Ringback number\n\nReferences\n\nTelephone numbers\nTelephony signals"
    ],
    [
        "Auxiliary Power's Demolition Derby and Figure 8 Race\nAuxiliary Power's Demolition Derby and Figure 8 Race is a PC demolition derby and figure 8 racing game. The game was developed by John C. Ardussi with the help of real demolition derby drivers and officials.\n\nGameplay\nIn this game, there are 4 difficulty levels and 9 different original tracks. There are 11 original body styles and over 50 original AI(computer) drivers. Engines smoke, stall, steam, and backfire. Vehicles get realistic damage. Just like real demolition derby, there is a heat, and then a final. You can get black flagged for sandbagging, or avoiding hits on purpose. Players start at a local level, then move on to regional then national level.\n\nReferences\n\nExternal links\n Auxiliary Power official website",
        "References\n\nExternal links\n Auxiliary Power official website\n\n2001 video games\nRacing video games\nVehicular combat games\nVideo games developed in the United States\nWindows games\nWindows-only games\nMultiplayer and single-player video games"
    ],
    [
        "Avionics\nAvionics (a blend of aviation and electronics) are the electronic systems used on aircraft. Avionic systems include communications, navigation, the display and management of multiple systems, and the hundreds of systems that are fitted to aircraft to perform individual functions. These can be as simple as a searchlight for a police helicopter or as complicated as the tactical system for an airborne early warning platform.\n\nHistory\nThe term \"avionics\" was coined in 1949 by  Philip J. Klass, senior editor at Aviation Week & Space Technology magazine as a portmanteau of \"aviation electronics\".",
        "Radio communication was first used in aircraft just prior to World War I.   The first airborne radios were in zeppelins, but the military sparked development of light radio sets that could be carried by heavier-than-air craft, so that aerial reconnaissance biplanes could report their observations immediately in case they were shot down. The first experimental radio transmission from an airplane was conducted by the U.S. Navy in August 1910. The first aircraft radios transmitted by radiotelegraphy, so they required two-seat aircraft with a second crewman to tap on a telegraph key to spell out messages by Morse code. During World War I, AM voice two way radio sets were made possible in 1917 by the development of the triode vacuum tube, which were simple enough that the pilot in a single",
        "messages by Morse code. During World War I, AM voice two way radio sets were made possible in 1917 by the development of the triode vacuum tube, which were simple enough that the pilot in a single seat aircraft could use it while flying.",
        "Radar, the central technology used today in aircraft navigation and air traffic control, was developed by several nations, mainly in secret, as an air defense system in the 1930s during the runup to World War II. Many modern avionics have their origins in World War II wartime developments. For example, autopilot systems that are commonplace today began as specialized systems to help bomber planes fly steadily enough to hit precision targets from high altitudes. Britain's 1940 decision to share its radar technology with its U.S. ally, particularly the magnetron vacuum tube, in the famous Tizard Mission, significantly shortened the war. Modern avionics is a substantial portion of military aircraft spending. Aircraft like the F-15E and the now retired F-14 have roughly 20 percent of their",
        "Mission, significantly shortened the war. Modern avionics is a substantial portion of military aircraft spending. Aircraft like the F-15E and the now retired F-14 have roughly 20 percent of their budget spent on avionics. Most modern helicopters now have budget splits of 60/40 in favour of avionics.",
        "The civilian market has also seen a growth in cost of avionics. Flight control systems (fly-by-wire) and new navigation needs brought on by tighter airspaces, have pushed up development costs. The major change has been the recent boom in consumer flying. As more people begin to use planes as their primary method of transportation, more elaborate methods of controlling aircraft safely in these high restrictive airspaces have been invented.",
        "Modern avionics\nAvionics plays a heavy role in modernization initiatives like the Federal Aviation Administration's (FAA) Next Generation Air Transportation System project in the United States and the Single European Sky ATM Research (SESAR) initiative in Europe. The Joint Planning and Development Office put forth a roadmap for avionics in six areas:\n Published Routes and Procedures – Improved navigation and routing\n Negotiated Trajectories – Adding data communications to create preferred routes dynamically\n Delegated Separation – Enhanced situational awareness in the air and on the ground\n LowVisibility/CeilingApproach/Departure – Allowing operations with weather constraints with less ground infrastructure\n Surface Operations – To increase safety in approach and departure",
        "LowVisibility/CeilingApproach/Departure – Allowing operations with weather constraints with less ground infrastructure\n Surface Operations – To increase safety in approach and departure\n ATM Efficiencies – Improving the air traffic management (ATM) process",
        "Market\n\nThe Aircraft Electronics Association reports $1.73 billion avionics sales for the first three quarters of 2017 in business and general aviation, a 4.1% yearly improvement: 73.5% came from North America, forward-fit represented 42.3% while 57.7% were retrofits as the U.S. deadline of January 1, 2020 for mandatory ADS-B out approach.",
        "Aircraft avionics",
        "The cockpit of an aircraft is a typical location for avionic equipment, including control, monitoring, communication, navigation, weather, and anti-collision systems. The majority of aircraft power their avionics using 14- or 28‑volt DC electrical systems; however, larger, more sophisticated aircraft (such as airliners or military combat aircraft) have AC systems operating at 115 volts 400 Hz, AC. There are several major vendors of flight avionics, including The Boeing Company, Panasonic Avionics Corporation, Honeywell (which now owns Bendix/King), Universal Avionics Systems Corporation, Rockwell Collins (now Collins Aerospace), Thales Group, GE Aviation Systems, Garmin, Raytheon, Parker Hannifin, UTC Aerospace Systems (now Collins Aerospace), Selex ES (now Leonardo S.p.A.), Shadin",
        "Rockwell Collins (now Collins Aerospace), Thales Group, GE Aviation Systems, Garmin, Raytheon, Parker Hannifin, UTC Aerospace Systems (now Collins Aerospace), Selex ES (now Leonardo S.p.A.), Shadin Avionics, and Avidyne Corporation.",
        "International standards for avionics equipment are prepared by the Airlines Electronic Engineering Committee (AEEC) and published by ARINC.\n\nCommunications\n\nCommunications connect the flight deck to the ground and the flight deck to the passengers. On‑board communications are provided by public-address systems and aircraft intercoms.",
        "Communications\n\nCommunications connect the flight deck to the ground and the flight deck to the passengers. On‑board communications are provided by public-address systems and aircraft intercoms.\n\nThe VHF aviation communication system works on the airband of 118.000 MHz to 136.975 MHz. Each channel is spaced from the adjacent ones by 8.33 kHz in Europe, 25 kHz elsewhere. VHF is also used for line of sight communication such as aircraft-to-aircraft and aircraft-to-ATC. Amplitude modulation (AM) is used, and the conversation is performed in simplex mode. Aircraft communication can also take place using HF (especially for trans-oceanic flights) or satellite communication.\n\nNavigation",
        "Navigation\n\nAir navigation is the determination of position and direction on or above the surface of the Earth. Avionics can use satellite navigation systems (such as GPS and WAAS), inertial navigation system (INS), ground-based radio navigation systems (such as VOR or LORAN), or any combination thereof. Some navigation systems such as GPS calculate the position automatically and display it to the flight crew on moving map displays. Older ground-based Navigation systems such as VOR or LORAN requires a pilot or navigator to plot the intersection of signals on a paper map to determine an aircraft's location; modern systems calculate the position automatically and display it to the flight crew on moving map displays.\n\nMonitoring",
        "The first hints of glass cockpits emerged in the 1970s when flight-worthy cathode ray tube (CRT) screens began to replace electromechanical displays, gauges and instruments. A \"glass\" cockpit refers to the use of computer monitors instead of gauges and other analog displays. Aircraft were getting progressively more displays, dials and information dashboards that eventually competed for space and pilot attention. In the 1970s, the average aircraft had more than 100 cockpit instruments and controls.",
        "Glass cockpits started to come into being with the Gulfstream G‑IV private jet in 1985. One of the key challenges in glass cockpits is to balance how much control is automated and how much the pilot should do manually. Generally they try to automate flight operations while keeping the pilot constantly informed.",
        "Aircraft flight-control system\n\nAircraft have means of automatically controlling flight. Autopilot was first invented by Lawrence Sperry during World War I to fly bomber planes steady enough to hit accurate targets from 25,000 feet. When it was first adopted by the U.S. military, a Honeywell engineer sat in the back seat with bolt cutters to disconnect the autopilot in case of emergency. Nowadays most commercial planes are equipped with aircraft flight control systems in order to reduce pilot error and workload at landing or takeoff.",
        "The first simple commercial auto-pilots were used to control heading and altitude and had limited authority on things like thrust and flight control surfaces. In helicopters, auto-stabilization was used in a similar way. The first systems were electromechanical. The advent of fly-by-wire and electro-actuated flight surfaces (rather than the traditional hydraulic) has increased safety. As with displays and instruments, critical devices that were electro-mechanical had a finite life. With safety critical systems, the software is very strictly tested.",
        "Fuel Systems\nFuel Quantity Indication System (FQIS) monitors the amount of fuel aboard. Using various sensors, such as capacitance tubes, temperature sensors, densitometers & level sensors, the FQIS computer calculates the mass of fuel remaining on board.\n\nFuel Control and Monitoring System (FCMS) reports fuel remaining on board in a similar manner, but, by controlling pumps & valves, also manages fuel transfers around various tanks.",
        "Fuel Control and Monitoring System (FCMS) reports fuel remaining on board in a similar manner, but, by controlling pumps & valves, also manages fuel transfers around various tanks.\n\n Refuelling control to upload to a certain total mass of fuel and distribute it automatically.\n Transfers during flight to the tanks that feed the engines. E.G. from fuselage to wing tanks\n Centre of gravity control transfers from the tail (trim) tanks forward to the wings as fuel is expended\n Maintaining fuel in the wing tips (to help stop the wings bending due to lift in flight) & transferring to the main tanks after landing\n Controlling fuel jettison during an emergency to reduce the aircraft weight.\n\nCollision-avoidance systems",
        "Collision-avoidance systems\n\nTo supplement air traffic control, most large transport aircraft and many smaller ones use a traffic alert and collision avoidance system (TCAS), which can detect the location of nearby aircraft, and provide instructions for avoiding a midair collision. Smaller aircraft may use simpler traffic alerting systems such as TPAS, which are passive (they do not actively interrogate the transponders of other aircraft) and do not provide advisories for conflict resolution.",
        "To help avoid controlled flight into terrain (CFIT), aircraft use systems such as ground-proximity warning systems (GPWS), which use radar altimeters as a key element. One of the major weaknesses of GPWS is the lack of \"look-ahead\" information, because it only provides altitude above terrain \"look-down\". In order to overcome this weakness, modern aircraft use a terrain awareness warning system (TAWS).\n\nFlight recorders\n\nCommercial aircraft cockpit data recorders, commonly known as \"black boxes\", store flight information and audio from the cockpit. They are often recovered from an aircraft after a crash to determine control settings and other parameters during the incident.\n\nWeather systems",
        "Weather systems\n\nWeather systems such as weather radar (typically Arinc 708 on commercial aircraft) and lightning detectors are important for aircraft flying at night or in instrument meteorological conditions, where it is not possible for pilots to see the weather ahead. Heavy precipitation (as sensed by radar) or severe turbulence (as sensed by lightning activity) are both indications of strong convective activity and severe turbulence, and weather systems allow pilots to deviate around these areas.",
        "Lightning detectors like the Stormscope or Strikefinder have become inexpensive enough that they are practical for light aircraft. In addition to radar and lightning detection, observations and extended radar pictures (such as NEXRAD) are now available through satellite data connections, allowing pilots to see weather conditions far beyond the range of their own in-flight systems. Modern displays allow weather information to be integrated with moving maps, terrain, and traffic onto a single screen, greatly simplifying navigation.",
        "Modern weather systems also include wind shear and turbulence detection and terrain and traffic warning systems. In‑plane weather avionics are especially popular in Africa, India, and other countries where air-travel is a growing market, but ground support is not as well developed.\n\nAircraft management systems\nThere has been a progression towards centralized control of the multiple complex systems fitted to aircraft, including engine monitoring and management. Health and usage monitoring systems (HUMS) are integrated with aircraft management computers to give maintainers early warnings of parts that will need replacement.",
        "The integrated modular avionics concept proposes an integrated architecture with application software portable across an assembly of common hardware modules. It has been used in fourth generation jet fighters and the latest generation of airliners.\n\nMission or tactical avionics\nMilitary aircraft have been designed either to deliver a weapon or to be the eyes and ears of other weapon systems. The vast array of sensors available to the military is used for whatever tactical means required. As with aircraft management, the bigger sensor platforms (like the E‑3D, JSTARS, ASTOR, Nimrod MRA4, Merlin HM Mk 1) have mission-management computers.\n\nPolice and EMS aircraft also carry sophisticated tactical sensors.\n\nMilitary communications",
        "Police and EMS aircraft also carry sophisticated tactical sensors.\n\nMilitary communications\n\nWhile aircraft communications provide the backbone for safe flight, the tactical systems are designed to withstand the rigors of the battle field. UHF, VHF Tactical (30–88 MHz) and SatCom systems combined with ECCM methods, and cryptography secure the communications. Data links such as Link 11, 16, 22 and BOWMAN, JTRS and even TETRA provide the means of transmitting data (such as images, targeting information etc.).\n\nRadar",
        "Radar\n\nAirborne radar was one of the first tactical sensors. The benefit of altitude providing range has meant a significant focus on airborne radar technologies. Radars include airborne early warning (AEW), anti-submarine warfare (ASW), and even weather radar (Arinc 708) and ground tracking/proximity radar.\n\nThe military uses radar in fast jets to help pilots fly at low levels. While the civil market has had weather radar for a while, there are strict rules about using it to navigate the aircraft.\n\nSonar",
        "Sonar\n\nDipping sonar fitted to a range of military helicopters allows the helicopter to protect shipping assets from submarines or surface threats. Maritime support aircraft can drop active and passive sonar devices (sonobuoys) and these are also used to determine the location of enemy submarines.\n\nElectro-optics\nElectro-optic systems include devices such as the head-up display (HUD), forward looking infrared (FLIR), infrared search and track and other passive infrared devices (Passive infrared sensor). These are all used to provide imagery and information to the flight crew. This imagery is used for everything from search and rescue to navigational aids and target acquisition.\n\nESM/DAS",
        "ESM/DAS\n\nElectronic support measures and defensive aids systems are used extensively to gather information about threats or possible threats. They can be used to launch devices (in some cases automatically) to counter direct threats against the aircraft. They are also used to determine the state of a threat and identify it.",
        "Aircraft networks\nThe avionics systems in military, commercial and advanced models of civilian aircraft are interconnected using an avionics databus. Common avionics databus protocols, with their primary application, include:\n Aircraft Data Network (ADN): Ethernet derivative for Commercial Aircraft\n Avionics Full-Duplex Switched Ethernet (AFDX): Specific implementation of ARINC 664 (ADN) for Commercial Aircraft\n ARINC 429: Generic Medium-Speed Data Sharing for Private and Commercial Aircraft\n ARINC 664: See ADN above\n ARINC 629: Commercial Aircraft (Boeing 777)\n ARINC 708: Weather Radar for Commercial Aircraft\n ARINC 717: Flight Data Recorder for Commercial Aircraft\n ARINC 825: CAN bus for commercial aircraft (for example Boeing 787 and Airbus A350)\n Commercial Standard Digital Bus",
        "ARINC 717: Flight Data Recorder for Commercial Aircraft\n ARINC 825: CAN bus for commercial aircraft (for example Boeing 787 and Airbus A350)\n Commercial Standard Digital Bus\n IEEE 1394b: Military Aircraft\n MIL-STD-1553: Military Aircraft\n MIL-STD-1760: Military Aircraft\n TTP – Time-Triggered Protocol: Boeing 787, Airbus A380, Fly-By-Wire Actuation Platforms from Parker Aerospace",
        "See also\n Astrionics, similar, for spacecraft\n\nAcronyms and abbreviations in avionics\n\nAvionics software\n\nEmergency locator beacon\nEmergency position-indicating radiobeacon station\n\nIntegrated modular avionics\n\nNotes\n\nFurther reading\nAvionics: Development and Implementation by Cary R. Spitzer (Hardcover – December 15, 2006)\nPrinciples of Avionics, 4th Edition by Albert Helfrick, Len Buckwalter, and Avionics Communications Inc. (Paperback – July 1, 2007)\nAvionics Training: Systems, Installation, and Troubleshooting by Len Buckwalter (Paperback – June 30, 2005)\nAvionics Made Simple, by Mouhamed Abdulla, Ph.D.; Jaroslav V. Svoboda, Ph.D. and Luis Rodrigues, Ph.D. (Coursepack – Dec. 2005 - ).\n\nExternal links",
        "External links\n\nAvionics in Commercial Aircraft\nAircraft Electronics Association (AEA)\nPilot's Guide to Avionics\nThe Avionic Systems Standardisation Committee\nSpace Shuttle Avionics\nAviation Today Avionics magazine\nRAES Avionics homepage\n\n \nAircraft instruments\nSpacecraft components\nElectronic engineering"
    ],
    [
        "AVL tree\nIn computer science, an AVL tree (named after inventors Adelson-Velsky and Landis) is a self-balancing binary search tree. In an AVL tree, the heights of the two child subtrees of any node differ by at most one; if at any time they differ by more than one, rebalancing is done to restore this property. Lookup, insertion, and deletion all take  time in both the average and worst cases, where  is the number of nodes in the tree prior to the operation. Insertions and deletions may require the tree to be rebalanced by one or more tree rotations.",
        "The AVL tree is named after its two Soviet inventors, Georgy Adelson-Velsky and Evgenii Landis, who published it in their 1962 paper \"An algorithm for the organization of information\".  It is the oldest self-balancing binary search tree data structure to be invented. \n\nAVL trees are often compared with red–black trees because both support the same set of operations and take  time for the basic operations. For lookup-intensive applications, AVL trees are faster than red–black trees because they are more strictly balanced. Similar to red–black trees, AVL trees are height-balanced. Both are, in general, neither weight-balanced nor -balanced for any ; that is, sibling nodes can have hugely differing numbers of descendants.\n\nDefinition",
        "Definition\n\nBalance factor\nIn a binary tree the balance factor of a node X is defined to be the height difference\n\nof its two child sub-trees rooted by node X. A binary tree is defined to be an AVL tree if the invariant\n\nholds for every node X in the tree.\n\nA node X with  is called \"left-heavy\", one with  is called \"right-heavy\", and one with  is sometimes simply called \"balanced\".\n\nProperties\nBalance factors can be kept up-to-date by knowing the previous balance factors and the change in height – it is not necessary to know the absolute height. For holding the AVL balance information, two bits per node are sufficient.",
        "The height  (counted as the maximal number of levels) of an AVL tree with  nodes lies in the interval:\n \nwhere   is the golden ratio and \nThis is because an AVL tree of height  contains at least  nodes where  is the Fibonacci sequence with the seed values\n\nOperations\nRead-only operations of an AVL tree involve carrying out the same actions as would be carried out on an unbalanced binary search tree, but modifications have to observe and restore the height balance of the sub-trees.",
        "Searching\nSearching for a specific key in an AVL tree can be done the same way as that of any balanced or unbalanced binary search tree. In order for search to work effectively it has to employ a comparison function which establishes a total order (or at least a total preorder) on the set of keys. The number of comparisons required for successful search is limited by the height  and for unsuccessful search is very close to , so both are in .\n\nTraversal\nAs a read-only operation the traversal of an AVL tree functions the same way as on any other binary tree. Exploring all  nodes of the tree visits each link exactly twice: one downward visit to enter the subtree rooted by that node, another visit upward to leave that node's subtree after having explored it.",
        "Once a node has been found in an AVL tree, the next or previous node can be accessed in amortized constant time. Some instances of exploring these \"nearby\" nodes require traversing up to  links (particularly when navigating from the rightmost leaf of the root's left subtree to the root or from the root to the leftmost leaf of the root's right subtree; in the AVL tree of figure 1, navigating from node P to the next-to-the-right node Q takes 3 steps). Since there are  links in any tree, the amortized cost is , or approximately 2.",
        "Insert\nWhen inserting a node into an AVL tree, you initially follow the same process as inserting into a Binary Search Tree. If the tree is empty, then the node is inserted as the root of the tree. If the tree is not empty, then we go down the root, and recursively go down the tree searching for the location to insert the new node. This traversal is guided by the comparison function. In this case, the node always replaces a NULL reference (left or right) of an external node in the tree i.e., the node is either made a left-child or a right-child of the external node.",
        "After this insertion, if a tree becomes unbalanced, only ancestors of the newly inserted node are unbalanced. This is because only those nodes have their sub-trees altered. So it is necessary to check each of the node's ancestors for consistency with the invariants of AVL trees: this is called \"retracing\". This is achieved by considering the balance factor of each node.",
        "Since with a single insertion the height of an AVL subtree cannot increase by more than one, the temporary balance factor of a node after an insertion will be in the range  For each node checked, if the temporary balance factor remains in the range from –1 to +1 then only an update of the balance factor and no rotation is necessary. However, if the temporary balance factor is ±2, the subtree rooted at this node is AVL unbalanced, and a rotation is needed. With insertion as the code below shows, the adequate rotation immediately perfectly rebalances the tree.\n\nIn figure 1, by inserting the new node Z as a child of node X the height of that subtree Z increases from 0 to 1.",
        "In figure 1, by inserting the new node Z as a child of node X the height of that subtree Z increases from 0 to 1.\n\nInvariant of the retracing loop for an insertion\nThe height of the subtree rooted by Z has increased by 1. It is already in AVL shape.",
        "for (X = parent(Z); X != null; X = parent(Z)) { // Loop (possibly up to the root)\n    // BF(X) has to be updated:\n    if (Z == right_child(X)) { // The right subtree increases\n        if (BF(X) > 0) { // X is right-heavy\n            // ==> the temporary BF(X) == +2\n            // ==> rebalancing is required.\n            G = parent(X); // Save parent of X around rotations\n            if (BF(Z) < 0)                  // Right Left Case  (see figure 3)\n                N = rotate_RightLeft(X, Z); // Double rotation: Right(Z) then Left(X)\n            else                            // Right Right Case (see figure 2)\n                N = rotate_Left(X, Z);      // Single rotation Left(X)\n            // After rotation adapt parent link\n        } else {\n            if (BF(X) < 0) {",
        "N = rotate_Left(X, Z);      // Single rotation Left(X)\n            // After rotation adapt parent link\n        } else {\n            if (BF(X) < 0) {\n                BF(X) = 0; // Z’s height increase is absorbed at X.\n                break; // Leave the loop\n            }\n            BF(X) = +1;\n            Z = X; // Height(Z) increases by 1\n            continue;\n        }\n    } else { // Z == left_child(X): the left subtree increases\n        if (BF(X) < 0) { // X is left-heavy\n            // ==> the temporary BF(X) == -2\n            // ==> rebalancing is required.\n            G = parent(X); // Save parent of X around rotations\n            if (BF(Z) > 0)                  // Left Right Case\n                N = rotate_LeftRight(X, Z); // Double rotation: Left(Z) then Right(X)",
        "if (BF(Z) > 0)                  // Left Right Case\n                N = rotate_LeftRight(X, Z); // Double rotation: Left(Z) then Right(X)\n            else                            // Left Left Case\n                N = rotate_Right(X, Z);     // Single rotation Right(X)\n            // After rotation adapt parent link\n        } else {\n            if (BF(X) > 0) {\n                BF(X) = 0; // Z’s height increase is absorbed at X.\n                break; // Leave the loop\n            }\n            BF(X) = -1;\n            Z = X; // Height(Z) increases by 1\n            continue;\n        }\n    }\n    // After a rotation adapt parent link:\n    // N is the new root of the rotated subtree\n    // Height does not change: Height(N) == old Height(X)\n    parent(N) = G;\n    if (G != null) {",
        "}\n    // After a rotation adapt parent link:\n    // N is the new root of the rotated subtree\n    // Height does not change: Height(N) == old Height(X)\n    parent(N) = G;\n    if (G != null) {\n        if (X == left_child(G))\n            left_child(G) = N;\n        else\n            right_child(G) = N;\n    } else\n        tree->root = N; // N is the new root of the total tree\n    break;\n    // There is no fall thru, only break; or continue;\n}\n// Unless loop is left via break, the height of the total tree increases by 1.",
        "In order to update the balance factors of all nodes, first observe that all nodes requiring correction lie from child to parent along the path of the inserted leaf. If the above procedure is applied to nodes along this path, starting from the leaf, then every node in the tree will again have a balance factor of −1, 0, or 1.\n\nThe retracing can stop if the balance factor becomes 0 implying that the height of that subtree remains unchanged.\n\nIf the balance factor becomes ±1 then the height of the subtree increases by one and the retracing needs to continue.\n\nIf the balance factor temporarily becomes ±2, this has to be repaired by an appropriate rotation after which the subtree has the same height as before (and its root the balance factor 0).",
        "If the balance factor temporarily becomes ±2, this has to be repaired by an appropriate rotation after which the subtree has the same height as before (and its root the balance factor 0).\n\nThe time required is  for lookup, plus a maximum of  retracing levels ( on average) on the way back to the root, so the operation can be completed in  time.\n\nDelete\nThe preliminary steps for deleting a node are described in section Binary search tree#Deletion.\nThere, the effective deletion of the subject node or the replacement node decreases the height of the corresponding child tree either from 1 to 0 or from 2 to 1, if that node had a child.\n\nStarting at this subtree, it is necessary to check each of the ancestors for consistency with the invariants of AVL trees. This is called \"retracing\".",
        "Since with a single deletion the height of an AVL subtree cannot decrease by more than one, the temporary balance factor of a node will be in the range from −2 to +2.\nIf the balance factor remains in the range from −1 to +1 it can be adjusted in accord with the AVL rules. If it becomes ±2 then the subtree is unbalanced and needs to be rotated. (Unlike insertion where a rotation always balances the tree, after delete, there may be BF(Z) ≠ 0 (see figures 2 and 3), so that after the appropriate single or double rotation the height of the rebalanced subtree decreases by one meaning that the tree has to be rebalanced again on the next higher level.) The various cases of rotations are described in section Rebalancing.",
        "Invariant of the retracing loop for a deletion\nThe height of the subtree rooted by N has decreased by 1. It is already in AVL shape.",
        "for (X = parent(N); X != null; X = G) { // Loop (possibly up to the root)\n    G = parent(X); // Save parent of X around rotations\n    // BF(X) has not yet been updated!\n    if (N == left_child(X)) { // the left subtree decreases\n        if (BF(X) > 0) { // X is right-heavy\n            // ==> the temporary BF(X) == +2\n            // ==> rebalancing is required.\n            Z = right_child(X); // Sibling of N (higher by 2)\n            b = BF(Z);\n            if (b < 0)                      // Right Left Case  (see figure 3)\n                N = rotate_RightLeft(X, Z); // Double rotation: Right(Z) then Left(X)\n            else                            // Right Right Case (see figure 2)\n                N = rotate_Left(X, Z);      // Single rotation Left(X)",
        "else                            // Right Right Case (see figure 2)\n                N = rotate_Left(X, Z);      // Single rotation Left(X)\n            // After rotation adapt parent link\n        } else {\n            if (BF(X) == 0) {\n                BF(X) = +1; // N’s height decrease is absorbed at X.\n                break; // Leave the loop\n            }\n            N = X;\n            BF(N) = 0; // Height(N) decreases by 1\n            continue;\n        }\n    } else { // (N == right_child(X)): The right subtree decreases\n        if (BF(X) < 0) { // X is left-heavy\n            // ==> the temporary BF(X) == -2\n            // ==> rebalancing is required.\n            Z = left_child(X); // Sibling of N (higher by 2)\n            b = BF(Z);",
        "// ==> the temporary BF(X) == -2\n            // ==> rebalancing is required.\n            Z = left_child(X); // Sibling of N (higher by 2)\n            b = BF(Z);\n            if (b > 0)                      // Left Right Case\n                N = rotate_LeftRight(X, Z); // Double rotation: Left(Z) then Right(X)\n            else                            // Left Left Case\n                N = rotate_Right(X, Z);     // Single rotation Right(X)\n            // After rotation adapt parent link\n        } else {\n            if (BF(X) == 0) {\n                BF(X) = -1; // N’s height decrease is absorbed at X.\n                break; // Leave the loop\n            }\n            N = X;\n            BF(N) = 0; // Height(N) decreases by 1\n            continue;\n        }\n    }",
        "break; // Leave the loop\n            }\n            N = X;\n            BF(N) = 0; // Height(N) decreases by 1\n            continue;\n        }\n    }\n    // After a rotation adapt parent link:\n    // N is the new root of the rotated subtree\n    parent(N) = G;\n    if (G != null) {\n        if (X == left_child(G))\n            left_child(G) = N;\n        else\n            right_child(G) = N;\n    } else\n        tree->root = N; // N is the new root of the total tree\n \n    if (b == 0)\n        break; // Height does not change: Leave the loop\n \n    // Height(N) decreases by 1 (== old Height(X)-1)\n}\n// If (b != 0) the height of the total tree decreases by 1.",
        "The retracing can stop if the balance factor becomes ±1 (it must have been 0) meaning that the height of that subtree remains unchanged.\n\nIf the balance factor becomes 0 (it must have been ±1) then the height of the subtree decreases by one and the retracing needs to continue.\n\nIf the balance factor temporarily becomes ±2, this has to be repaired by an appropriate rotation. It depends on the balance factor of the sibling Z (the higher child tree in figure 2) whether the height of the subtree decreases by one –and the retracing needs to continue– or does not change (if Z has the balance factor 0) and the whole tree is in AVL-shape.\n\nThe time required is  for lookup, plus a maximum of  retracing levels ( on average) on the way back to the root, so the operation can be completed in  time.",
        "The time required is  for lookup, plus a maximum of  retracing levels ( on average) on the way back to the root, so the operation can be completed in  time.\n\nSet operations and bulk operations\nIn addition to the single-element insert, delete and lookup operations, several set operations have been defined on AVL trees: union, intersection and set difference. Then fast bulk operations on insertions or deletions can be implemented based on these set functions. These set operations rely on two helper operations, Split and Join. With the new operations, the implementation of AVL trees can be more efficient and highly-parallelizable.",
        "The function Join on two AVL trees  and  and a key  will return a tree containing all elements in ,  as well as . It requires  to be greater than all keys in  and smaller than all keys in . If the two trees differ by height at most one, Join simply create a new node with left subtree , root  and right subtree . Otherwise, suppose that  is higher than  for more than one (the other case is symmetric). Join follows the right spine of  until a node  which is balanced with . At this point a new node with left child , root  and right child  is created to replace c. The new node satisfies the AVL invariant, and its height is one greater than . The increase in height can increase the height of its ancestors, possibly invalidating the AVL invariant of those nodes. This can be fixed either with a",
        "and its height is one greater than . The increase in height can increase the height of its ancestors, possibly invalidating the AVL invariant of those nodes. This can be fixed either with a double rotation if invalid at the parent or a single left rotation if invalid higher in the tree, in both cases restoring the height for any further ancestor nodes. Join will therefore require at most two rotations. The cost of this function is the difference of the heights between the two input trees.",
        "function JoinRightAVL(TL, k, TR)\n     (l, k', c) = expose(TL)\n     if (Height(c) <= Height(TR)+1)\n        T' = Node(c, k, TR)\n        if (Height(T') <= Height(l)+1) then return Node(l, k', T')\n        else return rotateLeft(Node(l, k', rotateRight(T')))\n     else \n         T' = JoinRightAVL(c, k, TR)\n         T'' = Node(l, k', T')\n         if (Height(T') <= Height(l)+1) return T''\n         else return rotateLeft(T'')\n\n function JoinLeftAVL(TL, k, TR)\n   /* symmetric to JoinRightAVL */",
        "function JoinLeftAVL(TL, k, TR)\n   /* symmetric to JoinRightAVL */\n\n function Join(TL, k, TR)\n     if (Height(TL)>Height(TR)+1) return JoinRightAVL(TL, k, TR)\n     if (Height(TR)>Height(TL)+1) return JoinLeftAVL(TL, k, TR)\n     return Node(TL, k, TR)\nHere Height(v) is the height of a subtree (node) . (l,k,r) = expose(v) extracts 's left child , the key  of 's root, and the right child . Node(l,k,r) means to create a node of left child , key , and right child .",
        "To split an AVL tree into two smaller trees, those smaller than key , and those larger than key , first draw a path from the root by inserting  into the AVL. After this insertion, all values less than  will be found on the left of the path, and all values greater than  will be found on the right. By applying Join, all the subtrees on the left side are merged bottom-up using keys on the path as intermediate nodes from bottom to top to form the left tree, and the right part is asymmetric. The cost of Split is , order of the height of the tree.",
        "function Split(T, k)\n     if (T = nil) return (nil, false, nil)\n     (L,m,R) = expose(T)\n     if (k = m) return (L, true, R)\n     if (k<m) \n        (L',b,R') = Split(L,k)\n        return (L', b, Join(R', m, R))\n     if (k>m) \n        (L',b,R') = Split(R, k)\n        return (Join(L, m, L'), b, R'))\n\nThe union of two AVL trees  and  representing sets  and , is an AVL  that represents .\n\n function Union(t1, t2):\n     if t1 = nil:\n         return t2\n     if t2 = nil:\n         return t1\n     (t<, b, t>) = Split(t2, t1.root)\n     return Join(Union(left(t1), t<), t1.root, Union(right(t1), t>))\n\nHere, Split is presumed to return two trees: one holding the keys less its input key, one holding the greater keys. (The algorithm is non-destructive, but an in-place destructive version exists as well.)",
        "The algorithm for intersection or difference is similar, but requires the Join2 helper routine that is the same as Join but without the middle key. Based on the new functions for union, intersection or difference, either one key or multiple keys can be inserted to or deleted from the AVL tree. Since Split calls Join but does not deal with the balancing criteria of AVL trees directly, such an implementation is usually called the \"join-based\" implementation.",
        "The complexity of each of union, intersection and difference is  for AVL trees of sizes  and . More importantly, since the recursive calls to union, intersection or difference are independent of each other, they can be executed in parallel with a parallel depth . When , the join-based implementation has the same computational DAG as single-element insertion and deletion.",
        "Rebalancing\nIf during a modifying operation the height difference between two child subtrees changes, this may, as long as it is < 2, be reflected by an adaption of the balance information at the parent. During insert and delete operations a (temporary) height difference of 2 may arise, which means that the parent subtree has to be \"rebalanced\". The given repair tools are the so-called tree rotations, because they move the keys only \"vertically\", so that the (\"horizontal\") in-order sequence of the keys is fully preserved (which is essential for a binary-search tree).",
        "Let X be the node that has a (temporary) balance factor of −2 or +2. Its left or right subtree was modified. Let Z be the higher child (see figures 2 and 3). Note that both children are in AVL shape by induction hypothesis.\n\nIn case of insertion this insertion has happened to one of Z's children in a way that Z's height has increased.\nIn case of deletion this deletion has happened to the sibling t1 of Z in a way so that t1's height being already lower has decreased. (This is the only case where Z's balance factor may also be 0.)\n\nThere are four possible variants of the violation:\n\nAnd the rebalancing is performed differently:",
        "There are four possible variants of the violation:\n\nAnd the rebalancing is performed differently:\n\nThereby, the situations are denoted as  where C (= child direction) and B (= balance) come from the set } with  The balance violation of case  is repaired by a simple rotation  whereas the case  is repaired by a double rotation \n\nThe cost of a rotation, either simple or double, is constant.",
        "The cost of a rotation, either simple or double, is constant.\n\nSimple rotation\nFigure 2 shows a Right Right situation. In its upper half, node X has two child trees with a balance factor of +2. Moreover, the inner child t23 of Z (i.e., left child when Z is right child, or right child when Z is left child) is not higher than its sibling t4. This can happen by a height increase of subtree t4 or by a height decrease of subtree t1. In the latter case, also the pale situation where t23 has the same height as t4 may occur.\n\nThe result of the left rotation is shown in the lower half of the figure. Three links (thick edges in figure 2) and two balance factors are to be updated.",
        "The result of the left rotation is shown in the lower half of the figure. Three links (thick edges in figure 2) and two balance factors are to be updated.\n\nAs the figure shows, before an insertion, the leaf layer was at level h+1, temporarily at level h+2 and after the rotation again at level h+1. In case of a deletion, the leaf layer was at level h+2, where it is again, when t23 and t4 were of same height. Otherwise the leaf layer reaches level h+1, so that the height of the rotated tree decreases.\n\nCode snippet of a simple left rotation",
        "Code snippet of a simple left rotation\n\nnode *rotate_Left(node *X, node *Z) {\n    // Z is by 2 higher than its sibling\n    t23 = left_child(Z); // Inner child of Z\n    right_child(X) = t23;\n    if (t23 != null)\n        parent(t23) = X;\n    left_child(Z) = X;\n    parent(X) = Z;\n    // 1st case, BF(Z) == 0,\n    //   only happens with deletion, not insertion:\n    if (BF(Z) == 0) { // t23 has been of same height as t4\n        BF(X) = +1;   // t23 now higher\n        BF(Z) = –1;   // t4 now lower than X\n    } else\n    { // 2nd case happens with insertion or deletion:\n        BF(X) = 0;\n        BF(Z) = 0;\n    }\n    return Z; // return new root of rotated subtree\n}",
        "Double rotation\nFigure 3 shows a Right Left situation. In its upper third, node X has two child trees with a balance factor of +2. But unlike figure 2, the inner child Y of Z is higher than its sibling t4. This can happen by the insertion of Y itself or a height increase of one of its subtrees t2 or t3 (with the consequence that they are of different height) or by a height decrease of subtree t1. In the latter case, it may also occur that t2 and t3 are of the same height.",
        "The result of the first, the right, rotation is shown in the middle third of the figure. (With respect to the balance factors, this rotation is not of the same kind as the other AVL single rotations, because the height difference between Y and t4 is only 1.) The result of the final left rotation is shown in the lower third of the figure. Five links (thick edges in figure 3) and three balance factors are to be updated.\n\nAs the figure shows, before an insertion, the leaf layer was at level h+1, temporarily at level h+2 and after the double rotation again at level h+1. In case of a deletion, the leaf layer was at level h+2 and after the double rotation it is at level h+1, so that the height of the rotated tree decreases.\n\nCode snippet of a right-left double rotation",
        "node *rotate_RightLeft(node *X, node *Z) {\n    // Z is by 2 higher than its sibling\n    Y = left_child(Z); // Inner child of Z\n    // Y is by 1 higher than sibling\n    t3 = right_child(Y);\n    left_child(Z) = t3;\n    if (t3 != null)\n        parent(t3) = Z;\n    right_child(Y) = Z;\n    parent(Z) = Y;\n    t2 = left_child(Y);\n    right_child(X) = t2;\n    if (t2 != null)\n        parent(t2) = X;\n    left_child(Y) = X;\n    parent(X) = Y;\n    // 1st case, BF(Y) == 0,\n    //   only happens with deletion, not insertion:\n    if (BF(Y) == 0) {\n        BF(X) = 0;\n        BF(Z) = 0;\n    } else\n    // other cases happen with insertion or deletion:\n        if (BF(Y) > 0) { // t3 was higher\n            BF(X) = –1;  // t1 now higher\n            BF(Z) = 0;\n        } else {\n            // t2 was higher",
        "if (BF(Y) > 0) { // t3 was higher\n            BF(X) = –1;  // t1 now higher\n            BF(Z) = 0;\n        } else {\n            // t2 was higher\n            BF(X) = 0;\n            BF(Z) = +1;  // t4 now higher\n        }\n    BF(Y) = 0;\n    return Y; // return new root of rotated subtree\n}",
        "Comparison to other structures",
        "Both AVL trees and red–black (RB) trees are self-balancing binary search trees and they are related mathematically. Indeed, every AVL tree can be colored red–black, but there are RB trees which are not AVL balanced. For maintaining the AVL (or RB) tree's invariants, rotations play an important role. In the worst case, even without rotations, AVL or RB insertions or deletions require  inspections and/or updates to AVL balance factors (or RB colors). RB insertions and deletions and AVL insertions require from zero to three tail-recursive rotations and run in amortized  time,  thus equally constant on average. AVL deletions requiring  rotations in the worst case are also  on average. RB trees require storing one bit of information (the color) in each node, while AVL trees mostly use two bits",
        "on average. AVL deletions requiring  rotations in the worst case are also  on average. RB trees require storing one bit of information (the color) in each node, while AVL trees mostly use two bits for the balance factor, although, when stored at the children, one bit with meaning «lower than sibling» suffices. The bigger difference between the two data structures is their height limit.",
        "For a tree of size \nan AVL tree's height is at most\n\nwhere   the golden ratio,     and  .\na RB tree's height is at most \n .\n\nAVL trees are more rigidly balanced than RB trees with an asymptotic relation AVL/RB ≈0.720 of the maximal heights. For insertions and deletions, Ben Pfaff shows in 79 measurements a relation of AVL/RB between 0.677 and 1.077 with median ≈0.947 and geometric mean ≈0.910.\n\nSee also\nWAVL tree\nSplay tree\nScapegoat tree\nB-tree\nT-tree\nList of data structures\n\nReferences\n\nFurther reading\n Donald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Third Edition. Addison-Wesley, 1997. . Pages 458–475 of section 6.2.3: Balanced Trees.\n .\n\nExternal links",
        "External links\n\n1962 in computing\nArticles with example pseudocode\nBinary trees\nSoviet inventions\nSearch trees\nAmortized data structures"
    ],
    [
        "AWK\nAWK (awk ) is a domain-specific language designed for text processing and typically used as a data extraction and reporting tool. Like sed and grep, it is a filter, and is a standard feature of most Unix-like operating systems.",
        "The AWK language is a data-driven scripting language consisting of a set of actions to be taken against streams of textual data – either run directly on files or used as part of a pipeline – for purposes of extracting or transforming text, such as producing formatted reports. The language extensively uses the string datatype, associative arrays (that is, arrays indexed by key strings), and regular expressions. While AWK has a limited intended application domain and was especially designed to support one-liner programs, the language is Turing-complete, and even the early Bell Labs users of AWK often wrote well-structured large AWK programs.",
        "AWK was created at Bell Labs in the 1970s, and its name is derived from the surnames of its authors: Alfred Aho, Peter Weinberger, and Brian Kernighan. The acronym is pronounced the same as the name of the bird species auk, which is illustrated on the cover of The AWK Programming Language.  When written in all lowercase letters, as awk, it refers to the Unix or Plan 9 program that runs scripts written in the AWK programming language.",
        "History\nAWK was initially developed in 1977 by Alfred Aho (author of egrep), Peter J. Weinberger (who worked on tiny relational databases), and Brian Kernighan. AWK takes its name from their respective initials. According to Kernighan, one of the goals of AWK was to have a tool that would easily manipulate both numbers and strings.\nAWK was also inspired by Marc Rochkind's programming language that was used to search for patterns in input data, and was implemented using yacc.",
        "As one of the early tools to appear in Version 7 Unix, AWK added computational features to a Unix pipeline besides the Bourne shell, the only scripting language available in a standard Unix environment. It is one of the mandatory utilities of the Single UNIX Specification, and is required by the Linux Standard Base specification.",
        "AWK was significantly revised and expanded in 1985–88, resulting in the GNU AWK implementation written by Paul Rubin, Jay Fenlason, and Richard Stallman, released in 1988. GNU AWK may be the most widely deployed version because it is included with GNU-based Linux packages. GNU AWK has been maintained solely by Arnold Robbins since 1994. Brian Kernighan's nawk (New AWK) source was first released in 1993 unpublicized, and publicly since the late 1990s; many BSD systems use it to avoid the GPL license.",
        "AWK was preceded by sed (1974). Both were designed for text processing. They share the line-oriented, data-driven paradigm, and are particularly suited to writing one-liner programs, due to the implicit main loop and current line variables. The power and terseness of early AWK programs – notably the powerful regular expression handling and conciseness due to implicit variables, which facilitate one-liners – together with the limitations of AWK at the time, were important inspirations for the Perl language (1987). In the 1990s, Perl became very popular, competing with AWK in the niche of Unix text-processing languages.\n\nStructure of AWK programs \n\nAn AWK program is a series of pattern action pairs, written as:\n\ncondition { action }\ncondition { action }\n...",
        "Structure of AWK programs \n\nAn AWK program is a series of pattern action pairs, written as:\n\ncondition { action }\ncondition { action }\n...\n\nwhere condition is typically an expression and action is a series of commands. The input is split into records, where by default records are separated by newline characters so that the input is split into lines. The program tests each record against each of the conditions in turn, and executes the action for each expression that is true. Either the condition or the action may be omitted. The condition defaults to matching every record. The default action is to print the record. This is the same pattern-action structure as sed.",
        "In addition to a simple AWK expression, such as foo == 1 or /^foo/, the condition can be BEGIN or END causing the action to be executed before or after all records have been read, or pattern1, pattern2 which matches the range of records starting with a record that matches pattern1 up to and including the record that matches pattern2 before again trying to match against pattern1 on subsequent lines.",
        "In addition to normal arithmetic and logical operators, AWK expressions include the tilde operator, ~, which matches a regular expression against a string. As handy syntactic sugar, /regexp/ without using the tilde operator matches against the current record; this syntax derives from sed, which in turn inherited it from the ed editor,  where / is used for searching. This syntax of using slashes as delimiters for regular expressions was subsequently adopted by Perl and ECMAScript, and is now common. The tilde operator was also adopted by Perl.\n\nCommands",
        "Commands \n\nAWK commands are the statements that are substituted for action in the examples above. AWK commands can include function calls, variable assignments, calculations, or any combination thereof. AWK contains built-in support for many functions; many more are provided by the various flavors of AWK. Also, some flavors support the inclusion of dynamically linked libraries, which can also provide more functions.\n\nThe print command \n\nThe print command is used to output text. The output text is always terminated with a predefined string called the output record separator (ORS) whose default value is a newline. The simplest form of this command is:",
        "print\nThis displays the contents of the current record. In AWK, records are broken down into fields, and these can be displayed separately:\n print $1\n Displays the first field of the current record\n print $1, $3\n Displays the first and third fields of the current record, separated by a predefined string called the output field separator (OFS) whose default value is a single space character\n\nAlthough these fields ($X) may bear resemblance to variables (the $ symbol indicates variables in Perl), they actually refer to the fields of the current record.  A special case, $0, refers to the entire record.  In fact, the commands \"print\" and \"print $0\" are identical in functionality.",
        "The print command can also display the results of calculations and/or function calls:\n/regex_pattern/ {\n    # Actions to perform in the event the record (line) matches the above regex_pattern\n    print 3+2\n    print foobar(3)\n    print foobar(variable)\n    print sin(3-2)\n}\n\nOutput may be sent to a file:\n/regex_pattern/ {\n    # Actions to perform in the event the record (line) matches the above regex_pattern\n    print \"expression\" > \"file name\"\n}\n\nor through a pipe:\n/regex_pattern/ {\n    # Actions to perform in the event the record (line) matches the above regex_pattern\n    print \"expression\" | \"command\"\n}\n\nBuilt-in variables",
        "or through a pipe:\n/regex_pattern/ {\n    # Actions to perform in the event the record (line) matches the above regex_pattern\n    print \"expression\" | \"command\"\n}\n\nBuilt-in variables \n\nAwk's built-in variables include the field variables: $1, $2, $3, and so on ($0 represents the entire record). They hold the text or values in the individual text-fields in a record.",
        "Other variables include:\n NR: Number of Records. Keeps a current count of the number of input records read so far from all data files. It starts at zero, but is never automatically reset to zero.\n FNR: File Number of Records. Keeps a current count of the number of input records read so far in the current file. This variable is automatically reset to zero each time a new file is started.\n NF: Number of Fields. Contains the number of fields in the current input record. The last field in the input record can be designated by $NF, the 2nd-to-last field by $(NF-1), the 3rd-to-last field by $(NF-2), etc.\n FILENAME: Contains the name of the current input-file.",
        "FILENAME: Contains the name of the current input-file.\n FS: Field Separator. Contains the \"field separator\" used to divide fields in the input record. The default, \"white space\", allows any sequence of space and tab characters. FS can be reassigned with another character or character sequence to change the field separator.\n RS: Record Separator. Stores the current \"record separator\" character. Since, by default, an input line is the input record, the default record separator character is a \"newline\".\n OFS: Output Field Separator. Stores the \"output field separator\", which separates the fields when Awk prints them. The default is a \"space\" character.",
        "OFS: Output Field Separator. Stores the \"output field separator\", which separates the fields when Awk prints them. The default is a \"space\" character.\n ORS: Output Record Separator. Stores the \"output record separator\", which separates the output records when Awk prints them. The default is a \"newline\" character.\n OFMT: Output Format. Stores the format for numeric output. The default format is \"%.6g\".",
        "Variables and syntax \n\nVariable names can use any of the characters [A-Za-z0-9_], with the exception of language keywords. The operators + - * / represent addition, subtraction, multiplication, and division, respectively. For string concatenation, simply place two variables (or string constants) next to each other. It is optional to use a space in between if string constants are involved, but two variable names placed adjacent to each other require a space in between. Double quotes delimit string constants. Statements need not end with semicolons. Finally, comments can be added to programs by using # as the first character on a line.\n\nUser-defined functions",
        "User-defined functions \n\nIn a format similar to C, function definitions consist of the keyword function, the function name, argument names and the function body.  Here is an example of a function.\nfunction add_three (number) {\n    return number + 3\n}\n\nThis statement can be invoked as follows:\n(pattern) {\n   print add_three(36)     # Outputs '''39'''\n}\n\nFunctions can have variables that are in the local scope. The names of these are added to the end of the argument list, though values for these should be omitted when calling the function. It is convention to add some whitespace in the argument list before the local variables, to indicate where the parameters end and the local variables begin.\n\nExamples\n\nHello World",
        "Examples\n\nHello World \n\nHere is the customary \"Hello, world\" program written in AWK:\nBEGIN {\n        print \"Hello, world!\"\n        exit\n}\n\nPrint lines longer than 80 characters \n\nPrint all lines longer than 80 characters. Note that the default action is to print the current line.\nlength($0) > 80\n\nCount words \n\nCount words in the input and print the number of lines, words, and characters (like wc):\n{\n    words += NF\n    chars += length + 1 # add one to account for the newline character at the end of each record (line)\n}\nEND { print NR, words, chars }\n\nAs there is no pattern for the first line of the program, every line of input matches by default, so the increment actions are executed for every line. Note that words += NF is shorthand for words = words + NF.\n\nSum last word",
        "Sum last word \n\n{ s += $NF }\nEND { print s + 0 }\n\ns is incremented by the numeric value of $NF, which is the last word on the line as defined by AWK's field separator (by default, white-space). NF is the number of fields in the current line, e.g. 4. Since $4 is the value of the fourth field, $NF is the value of the last field in the line regardless of how many fields this line has, or whether it has more or fewer fields than surrounding lines. $ is actually a unary operator with the highest operator precedence. (If the line has no fields, then NF is 0, $0 is the whole line, which in this case is empty apart from possible white-space, and so has the numeric value 0.)",
        "At the end of the input the END pattern matches, so s is printed. However, since there may have been no lines of input at all, in which case no value has ever been assigned to s, it will by default be an empty string. Adding zero to a variable is an AWK idiom for coercing it from a string to a numeric value. (Concatenating an empty string is to coerce from a number to a string, e.g. s \"\".  Note, there's no operator to concatenate strings, they're just placed adjacently.) With the coercion the program prints \"0\" on an empty input, without it, an empty line is printed.",
        "Match a range of input lines \nNR % 4 == 1, NR % 4 == 3 { printf \"%6d  %s\\n\", NR, $0 }",
        "NR % 4 == 1, NR % 4 == 3 { printf \"%6d  %s\\n\", NR, $0 }\nThe action statement prints each line numbered. The printf function emulates the standard C printf and works similarly to the print command described above. The pattern to match, however, works as follows: NR is the number of records, typically lines of input, AWK has so far read, i.e. the current line number, starting at 1 for the first line of input. % is the modulo operator. NR % 4 == 1 is true for the 1st, 5th, 9th, etc., lines of input. Likewise, NR % 4 == 3 is true for the 3rd, 7th, 11th, etc., lines of input. The range pattern is false until the first part matches, on line 1, and then remains true up to and including when the second part matches, on line 3. It then stays false until the first part matches again on line 5.",
        "Thus, the program prints lines 1,2,3, skips line 4, and then 5,6,7, and so on. For each line, it prints the line number (on a 6 character-wide field) and then the line contents. For example, when executed on this input:\n Rome\n Florence\n Milan\n Naples\n Turin\n Venice\n\nThe previous program prints:\n      1 Rome\n      2 Florence\n      3 Milan\n      5 Turin\n      6 Venice",
        "The previous program prints:\n      1 Rome\n      2 Florence\n      3 Milan\n      5 Turin\n      6 Venice\n\nPrinting the initial or the final part of a file \nAs a special case, when the first part of a range pattern is constantly true, e.g. 1, the range will start at the beginning of the input. Similarly, if the second part is constantly false, e.g. 0, the range will continue until the end of input. For example,\n /^--cut here--$/, 0\nprints lines of input from the first line matching the regular expression ^--cut here--$, that is, a line containing only the phrase \"--cut here--\", to the end.\n\nCalculate word frequencies",
        "Calculate word frequencies \n\nWord frequency using associative arrays:\nBEGIN {\n    FS=\"[^a-zA-Z]+\"\n}\n{\n    for (i=1; i<=NF; i++)\n        words[tolower($i)]++\n}\nEND {\n    for (i in words)\n        print i, words[i]\n}",
        "The BEGIN block sets the field separator to any sequence of non-alphabetic characters. Note that separators can be regular expressions. After that, we get to a bare action, which performs the action on every input line. In this case, for every field on the line, we add one to the number of times that word, first converted to lowercase, appears. Finally, in the END block, we print the words with their frequencies. The line\n for (i in words)\ncreates a loop that goes through the array words, setting i to each subscript of the array. This is different from most languages, where such a loop goes through each value in the array. The loop thus prints out each word followed by its frequency count. tolower was an addition to the One True awk (see below) made after the book was published.",
        "Match pattern from command line \n\nThis program can be represented in several ways. The first one uses the Bourne shell to make a shell script that does everything. It is the shortest of these methods:\n#!/bin/sh\n\npattern=\"$1\"\nshift\nawk '/'\"$pattern\"'/ { print FILENAME \":\" $0 }' \"$@\"\n\nThe $pattern in the awk command is not protected by single quotes so that the shell does expand the variable but it needs to be put in double quotes to properly handle patterns containing spaces.  A pattern by itself in the usual way checks to see if the whole line ($0) matches. FILENAME contains the current filename. awk has no explicit concatenation operator; two adjacent strings concatenate them. $0 expands to the original unchanged input line.",
        "There are alternate ways of writing this. This shell script accesses the environment directly from within awk:\n#!/bin/sh\n\nexport pattern=\"$1\"\nshift\nawk '$0 ~ ENVIRON[\"pattern\"] { print FILENAME \":\" $0 }' \"$@\"\n\nThis is a shell script that uses ENVIRON, an array introduced in a newer version of the One True awk after the book was published. The subscript of ENVIRON is the name of an environment variable; its result is the variable's value. This is like the getenv function in various standard libraries and POSIX. The shell script makes an environment variable pattern containing the first argument, then drops that argument and has awk look for the pattern in each file.",
        "~ checks to see if its left operand matches its right operand; !~ is its inverse. Note that a regular expression is just a string and can be stored in variables.\n\nThe next way uses command-line variable assignment, in which an argument to awk can be seen as an assignment to a variable:\n#!/bin/sh\n\npattern=\"$1\"\nshift\nawk '$0 ~ pattern { print FILENAME \":\" $0 }' \"pattern=$pattern\" \"$@\"\n\nOr You can use the -v var=value command line option (e.g. awk -v pattern=\"$pattern\" ...).",
        "pattern=\"$1\"\nshift\nawk '$0 ~ pattern { print FILENAME \":\" $0 }' \"pattern=$pattern\" \"$@\"\n\nOr You can use the -v var=value command line option (e.g. awk -v pattern=\"$pattern\" ...).\n\nFinally, this is written in pure awk, without help from a shell or without the need to know too much about the implementation of the awk script (as the variable assignment on command line one does), but is a bit lengthy:\nBEGIN {\n    pattern = ARGV[1]\n    for (i = 1; i < ARGC; i++) # remove first argument\n        ARGV[i] = ARGV[i + 1]\n    ARGC--\n    if (ARGC == 1) { # the pattern was the only thing, so force read from standard input (used by book)\n        ARGC = 2\n        ARGV[1] = \"-\"\n    }\n}\n$0 ~ pattern { print FILENAME \":\" $0 }",
        "The BEGIN is necessary not only to extract the first argument, but also to prevent it from being interpreted as a filename after the BEGIN block ends. ARGC, the number of arguments, is always guaranteed to be ≥1, as ARGV[0] is the name of the command that executed the script, most often the string \"awk\". Also note that ARGV[ARGC] is the empty string, \"\". # initiates a comment that expands to the end of the line.",
        "Note the if block. awk only checks to see if it should read from standard input before it runs the command. This means that\n awk 'prog'\nonly works because the fact that there are no filenames is only checked before prog is run! If you explicitly set ARGC to 1 so that there are no arguments, awk will simply quit because it feels there are no more input files. Therefore, you need to explicitly say to read from standard input with the special filename -.\n\nSelf-contained AWK scripts \n\nOn Unix-like operating systems self-contained AWK scripts can be constructed using the shebang syntax.\n\nFor example, a script that prints the content of a given file may be built by creating a file named print.awk with the following content:\n#!/usr/bin/awk -f\n{ print $0 }",
        "For example, a script that prints the content of a given file may be built by creating a file named print.awk with the following content:\n#!/usr/bin/awk -f\n{ print $0 }\n\nIt can be invoked with: ./print.awk <filename>\n\nThe -f tells AWK that the argument that follows is the file to read the AWK program from, which is the same flag that is used in sed. Since they are often used for one-liners, both these programs default to executing a program given as a command-line argument, rather than a separate file.\n\nVersions and implementations \n\nAWK was originally written in 1977 and distributed with Version 7 Unix.",
        "Versions and implementations \n\nAWK was originally written in 1977 and distributed with Version 7 Unix.\n\nIn 1985 its authors started expanding the language, most significantly by adding user-defined functions. The language is described in the book The AWK Programming Language, published 1988, and its implementation was made available in releases of UNIX System V. To avoid confusion with the incompatible older version, this version was sometimes called \"new awk\" or nawk. This implementation was released under a free software license in 1996 and is still maintained by Brian Kernighan (see external links below).\n                           \nOld versions of Unix, such as UNIX/32V, included awkcc, which converted AWK to C. Kernighan wrote a program to turn awk into C++; its state is not known.",
        "BWK awk, also known as nawk, refers to the version by Brian Kernighan. It has been dubbed the \"One True AWK\" because of the use of the term in association with the book that originally described the language and the fact that Kernighan was one of the original authors of AWK. FreeBSD refers to this version as one-true-awk. This version also has features not in the book, such as tolower and ENVIRON that are explained above; see the FIXES file in the source archive for details. This version is used by, for example, Android, FreeBSD, NetBSD, OpenBSD, macOS, and illumos. Brian Kernighan and Arnold Robbins are the main contributors to a source repository for nawk: .",
        "gawk (GNU awk) is another free-software implementation and the only implementation that makes serious progress implementing internationalization and localization and TCP/IP networking. It was written before the original implementation became freely available. It includes its own debugger, and its profiler enables the user to make measured performance enhancements to a script. It also enables the user to extend functionality with shared libraries. Some Linux distributions include gawk as their default AWK implementation.  As of version 5.2 (September 2022) gawk includes a persistent memory feature that can remember script-defined variables and functions from one invocation of a script to the next and pass data between unrelated scripts, as described in the Persistent-Memory gawk User",
        "feature that can remember script-defined variables and functions from one invocation of a script to the next and pass data between unrelated scripts, as described in the Persistent-Memory gawk User Manual: .",
        "gawk-csv. The CSV extension of gawk provides facilities for handling input and output CSV formatted data.\n mawk is a very fast AWK implementation by Mike Brennan based on a bytecode interpreter.\n libmawk is a fork of mawk, allowing applications to embed multiple parallel instances of awk interpreters.\n awka (whose front end is written atop the mawk program) is another translator of AWK scripts into C code. When compiled, statically including the author's libawka.a, the resulting executables are considerably sped up and, according to the author's tests, compare very well with other versions of AWK, Perl, or Tcl. Small scripts will turn into programs of 160–170 kB.",
        "tawk (Thompson AWK) is an AWK compiler for Solaris, DOS, OS/2, and Windows, previously sold by Thompson Automation Software (which has ceased its activities).\n Jawk is a project to implement AWK in Java, hosted on SourceForge. Extensions to the language are added to provide access to Java features within AWK scripts (i.e., Java threads, sockets, collections, etc.).\n xgawk is a fork of gawk  that  extends gawk with dynamically loadable libraries. The XMLgawk extension was integrated into the official GNU Awk release 4.1.0.\n QSEAWK is an embedded AWK interpreter implementation included in the QSE library that provides embedding application programming interface (API) for C and C++.\n libfawk is a very small, function-only, reentrant, embeddable interpreter written in C",
        "libfawk is a very small, function-only, reentrant, embeddable interpreter written in C\n BusyBox includes an AWK implementation written by Dmitry Zakharov. This is a very small implementation suitable for embedded systems.\n CLAWK by Michael Parker provides an AWK implementation in Common Lisp, based upon the regular expression library of the same author.",
        "Books\n\nSee also \n\n Data transformation\n Event-driven programming\n List of Unix commands\n sed\n\nReferences\n\nFurther reading \n \n  –  Interview with Alfred V. Aho on AWK\n \n \n \n AWK  –  Become an expert in 60 minutes\n\nExternal links \n\n The Amazing Awk Assembler by Henry Spencer.\n \n awklang.org The site for things related to the awk language\n\n1977 software\nCross-platform software\nDomain-specific programming languages\nFree compilers and interpreters\nPattern matching programming languages\nPlan 9 commands\nProgramming languages created in 1977\nScripting languages\nStandard Unix programs\nText-oriented programming languages\nUnix SUS2008 utilities\nUnix text processing utilities"
    ],
    [
        "B-tree\nIn computer science, a B-tree is a self-balancing tree data structure that maintains sorted data and allows searches, sequential access, insertions, and deletions in logarithmic time. The B-tree generalizes the binary search tree, allowing for nodes with more than two children. Unlike other self-balancing binary search trees, the B-tree is well suited for storage systems that read and write relatively large blocks of data, such as databases and file systems.",
        "History \nB-trees were invented by Rudolf Bayer and Edward M. McCreight while working at Boeing Research Labs, for the purpose of efficiently managing index pages for large random-access files. The basic assumption was that indices would be so voluminous that only small chunks of the tree could fit in main memory. Bayer and McCreight's paper, Organization and maintenance of large ordered indices, was first circulated in July 1970 and later published in Acta Informatica.\n\nBayer and McCreight never explained what, if anything, the B stands for: Boeing, balanced, between, broad, bushy, and Bayer have been suggested. McCreight has said that \"the more you think about what the B in B-trees means, the better you understand B-trees.\"",
        "In 2011 Google developed the C++ B-Tree, reporting a 50-80% reduction in memory use for small data types and improved performance for large data sets when compared to a Red-Black tree.\n\nDefinition \nAccording to Knuth's definition, a B-tree of order m is a tree which satisfies the following properties:\n\n Every node has at most m children.\n Every internal node has at least ⌈m/2⌉ children.\n The root node has at least two children unless it is a leaf.\n All leaves appear on the same level.\n A non-leaf node with k children contains k−1 keys.",
        "Each internal node's keys act as separation values which divide its subtrees. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys: a1 and a2. All values in the leftmost subtree will be less than a1, all values in the middle subtree will be between a1 and a2, and all values in the rightmost subtree will be greater than a2.",
        "Internal nodes",
        "Internal nodes (also known as inner nodes) are all nodes except for leaf nodes and the root node. They are usually represented as an ordered set of elements and child pointers. Every internal node contains a maximum of U children and a minimum of L children. Thus, the number of elements is always 1 less than the number of child pointers (the number of elements is between L−1 and U−1). U must be either 2L or 2L−1; therefore each internal node is at least half full. The relationship between U and L implies that two half-full nodes can be joined to make a legal node, and one full node can be split into two legal nodes (if there's room to push one element up into the parent). These properties make it possible to delete and insert new values into a B-tree and adjust the tree to preserve the",
        "into two legal nodes (if there's room to push one element up into the parent). These properties make it possible to delete and insert new values into a B-tree and adjust the tree to preserve the B-tree properties.",
        "The root node\n The root node's number of children has the same upper limit as internal nodes, but has no lower limit. For example, when there are fewer than L−1 elements in the entire tree, the root will be the only node in the tree with no children at all.",
        "Leaf nodes\n In Knuth's terminology, the \"leaf\" nodes are the actual data objects / chunks. The internal nodes that are one level above these leaves are what would be called the \"leaves\" by other authors: these nodes only store keys (at most m-1, and at least m/2-1 if they are not the root) and pointers (one for each key) to nodes carrying the data objects / chunks.\n\nA B-tree of depth n+1 can hold about U times as many items as a B-tree of depth n, but the cost of search, insert, and delete operations grows with the depth of the tree. As with any balanced tree, the cost grows much more slowly than the number of elements.",
        "Some balanced trees store values only at leaf nodes, and use different kinds of nodes for leaf nodes and internal nodes. B-trees keep values in every node in the tree except leaf nodes.\n\nDifferences in terminology\nThe literature on B-trees is not uniform in its terminology.\n\nBayer and McCreight (1972), Comer (1979), and others define the order of B-tree as the minimum number of keys in a non-root node.  Folk and Zoellick  points out that terminology is ambiguous because the maximum number of keys is not clear. An order 3 B-tree might hold a maximum of 6 keys or a maximum of 7 keys. Knuth (1998) avoids the problem by defining the order to be the maximum number of children (which is one more than the maximum number of keys).",
        "The term leaf is also inconsistent. Bayer and McCreight (1972) considered the leaf level to be the lowest level of keys, but Knuth considered the leaf level to be one level below the lowest keys. There are many possible implementation choices. In some designs, the leaves may hold the entire data record; in other designs, the leaves may only hold pointers to the data record. Those choices are not fundamental to the idea of a B-tree.\n\nFor simplicity, most authors assume there are a fixed number of keys that fit in a node. The basic assumption is the key size is fixed and the node size is fixed. In practice, variable length keys may be employed.\n\nInformal description",
        "Informal description\n\nNode structure \nAs with other trees, B-trees can be represented as a collection of three types of nodes: root, internal (a.k.a. interior), and leaf. \n\nNote the following variable definitions:\n\n : Maximum number of potential search keys for each node in a B-tree. (this value is constant over the entire tree).\n : The pointer to a child node which starts a sub-tree.\n : The pointer to a record which stores the data.\n : The search key at the zero-based node index .\n\nIn B-trees, the following properties are maintained for these nodes:\n\n If  exists in any node in a B+ tree, then  exists in that node where .\n All leaf nodes have the same number of ancestors (i.e., they are all at the same depth).\n\nEach internal node in a B-tree has the following format:",
        "Each internal node in a B-tree has the following format:\n\nEach leaf node in a B-tree has the following format:\n\nThe node bounds are summarized in the table below:\n\nInsertion and deletion \nIn order to maintain the pre-defined range of child nodes, internal nodes may be joined or split.\n\nUsually, the number of keys is chosen to vary between  and , where  is the minimum number of keys, and  is the minimum degree or branching factor of the tree. The factor of 2 will guarantee that nodes can be split or combined.",
        "If an internal node has  keys, then adding a key to that node can be accomplished by splitting the hypothetical  key node into two  key nodes and moving the key that would have been in the middle to the parent node. Each split node has the required minimum number of keys. Similarly, if an internal node and its neighbor each have  keys, then a key may be deleted from the internal node by combining it with its neighbor. Deleting the key would make the internal node have  keys; joining the neighbor would add  keys plus one more key brought down from the neighbor's parent. The result is an entirely full node of  keys.",
        "A B-tree is kept balanced after insertion by splitting a would-be overfilled node, of  keys, into two -key siblings and inserting the mid-value key into the parent. Depth only increases when the root is split, maintaining balance. Similarly, a B-tree is kept balanced after deletion by merging or redistributing keys among siblings to maintain the -key minimum for non-root nodes. A merger reduces the number of keys in the parent potentially forcing it to merge or redistribute keys with its siblings, and so on. The only change in depth occurs when the root has two children, of  and (transitionally)  keys, in which case the two siblings and parent are merged, reducing the depth by one.",
        "This depth will increase slowly as elements are added to the tree, but an increase in the overall depth is infrequent, and results in all leaf nodes being one more node farther away from the root.\n\nComparison to other trees \nBecause a range of child nodes is permitted, B-trees do not need re-balancing as frequently as other self-balancing search trees, but may waste some space, since nodes are not entirely full.",
        "B-trees have substantial advantages over alternative implementations when the time to access the data of a node greatly exceeds the time spent processing that data, because then the cost of accessing the node may be amortized over multiple operations within the node. This usually occurs when the node data are in secondary storage such as disk drives. By maximizing the number of keys within each internal node, the height of the tree decreases and the number of expensive node accesses is reduced. In addition, rebalancing of the tree occurs less often. The maximum number of child nodes depends on the information that must be stored for each child node and the size of a full disk block or an analogous size in secondary storage. While 2–3 B-trees are easier to explain, practical B-trees using",
        "the information that must be stored for each child node and the size of a full disk block or an analogous size in secondary storage. While 2–3 B-trees are easier to explain, practical B-trees using secondary storage need a large number of child nodes to improve performance.",
        "Variants\nThe term B-tree may refer to a specific design or it may refer to a general class of designs. In the narrow sense, a B-tree stores keys in its internal nodes but need not store those keys in the records at the leaves. The general class includes variations such as the B+ tree, the B* tree and the B*+ tree.\n In the B+ tree, the internal nodes do not store any pointers to records, thus all pointers to records are stored in the leaf nodes. In addition, a leaf node may include a pointer to the next leaf node to speed sequential access. Because B+ tree internal nodes have fewer pointers, each node can hold more keys, causing the tree to be shallower, and thus, faster to search.",
        "The B* tree balances more neighboring internal nodes to keep the internal nodes more densely packed. This variant ensures non-root nodes are at least 2/3 full instead of 1/2. As the most costly part of operation of inserting the node in B-tree is splitting the node, B*-trees are created to postpone splitting operation as long as they can. To maintain this, instead of immediately splitting up a node when it gets full, its keys are shared with a node next to it. This spill operation is less costly to do than split, because it requires only shifting the keys between existing nodes, not allocating memory for a new one. For inserting, first it is checked whether the node has some free space in it, and if so, the new key is just inserted in the node. However, if the node is full (it has  keys,",
        "memory for a new one. For inserting, first it is checked whether the node has some free space in it, and if so, the new key is just inserted in the node. However, if the node is full (it has  keys, where  is the order of the tree as maximum number of pointers to subtrees from one node), it needs to be checked whether the right sibling exists and has some free space. If the right sibling has  keys, then keys are redistributed between the two sibling nodes as evenly as possible. For this purpose,  keys from the current node, the new key inserted, one key from the parent node and  keys from the sibling node are seen as an ordered array of  keys. The array becomes split by half, so that  lowest keys stay in the current node, the next (middle) key is inserted in the parent and the rest go to",
        "sibling node are seen as an ordered array of  keys. The array becomes split by half, so that  lowest keys stay in the current node, the next (middle) key is inserted in the parent and the rest go to the right sibling. (The newly inserted key might end up in any of the three places.) The situation when right sibling is full, and left isn't is analogous. When both the sibling nodes are full, then the two nodes (current node and a sibling) are split into three and one more key is shifted up the tree, to the parent node. If the parent is full, then spill/split operation propagates towards the root node. Deleting nodes is somewhat more complex than inserting however.",
        "The B*+ tree combines the main B+ tree and B* tree features together.\n B-trees can be turned into order statistic trees to allow rapid searches for the Nth record in key order, or counting the number of records between any two records, and various other related operations.",
        "B-tree usage in databases\n\nTime to search a sorted file\nUsually, sorting and searching algorithms have been characterized by the number of comparison operations that must be performed using order notation. A binary search of a sorted table with  records, for example, can be done in roughly  comparisons. If the table had 1,000,000 records, then a specific record could be located with at most 20 comparisons: .",
        "Large databases have historically been kept on disk drives. The time to read a record on a disk drive far exceeds the time needed to compare keys once the record is available. The time to read a record from a disk drive involves a seek time and a rotational delay. The seek time may be 0 to 20 or more milliseconds, and the rotational delay averages about half the rotation period. For a 7200 RPM drive, the rotation period is 8.33 milliseconds. For a drive such as the Seagate ST3500320NS, the track-to-track seek time is 0.8 milliseconds and the average reading seek time is 8.5 milliseconds. For simplicity, assume reading from disk takes about 10 milliseconds.",
        "Naively, then, the time to locate one record out of a million would take 20 disk reads times 10 milliseconds per disk read, which is 0.2 seconds.\n\nThe time won't be that bad because individual records are grouped together in a disk block. A disk block might be 16 kilobytes. If each record is 160 bytes, then 100 records could be stored in each block. The disk read time above was actually for an entire block. Once the disk head is in position, one or more disk blocks can be read with little delay. With 100 records per block, the last 6 or so comparisons don't need to do any disk reads—the comparisons are all within the last disk block read.\n\nTo speed the search further, the first 13 to 14 comparisons (which each required a disk access) must be sped up.",
        "To speed the search further, the first 13 to 14 comparisons (which each required a disk access) must be sped up.\n\nAn index speeds the search\nA significant improvement in performance can be made with a B-tree index. A B-tree index creates a multi-level tree structure that breaks a database down into fixed-size blocks or pages. Each level of this tree can be used to link those pages via an address location, allowing one page (known as a node, or internal page) to refer to another with leaf pages at the lowest level. One page is typically the starting point of the tree, or the \"root\". This is where the search for a particular key would begin, traversing a path that terminates in a leaf. Most pages in this structure will be leaf pages which ultimately refer to specific table rows.",
        "Because each node (or internal page) can have more than two children, a B-tree index will usually have a shorter height (the distance from the root to the farthest leaf) than a Binary Search Tree. In the example above, initial disk reads narrowed the search range by a factor of two. That can be improved substantially by creating an auxiliary index that contains the first record in each disk block (sometimes called a sparse index). This auxiliary index would be 1% of the size of the original database, but it can be searched more quickly. Finding an entry in the auxiliary index would tell us which block to search in the main database; after searching the auxiliary index, we would have to search only that one block of the main database—at a cost of one more disk read. The index would hold",
        "which block to search in the main database; after searching the auxiliary index, we would have to search only that one block of the main database—at a cost of one more disk read. The index would hold 10,000 entries, so it would take at most 14 comparisons. Like the main database, the last six or so comparisons in the auxiliary index would be on the same disk block. The index could be searched in about eight disk reads, and the desired record could be accessed in 9 disk reads.",
        "The trick of creating an auxiliary index can be repeated to make an auxiliary index to the auxiliary index. That would make an aux-aux index that would need only 100 entries and would fit in one disk block.",
        "Instead of reading 14 disk blocks to find the desired record, we only need to read 3 blocks. This blocking is the core idea behind the creation of the B-tree, where the disk blocks fill-out a hierarchy of levels to make up the index. Reading and searching the first (and only) block of the aux-aux index which is the root of the tree identifies the relevant block in aux-index in the level below. Reading and searching that aux-index block identifies the relevant block to read, until the final level, known as the leaf level, identifies a record in the main database. Instead of 150 milliseconds, we need only 30 milliseconds to get the record.",
        "The auxiliary indices have turned the search problem from a binary search requiring roughly  disk reads to one requiring only  disk reads where  is the blocking factor (the number of entries per block:  entries per block in our example;  reads).\n\nIn practice, if the main database is being frequently searched, the aux-aux index and much of the aux index may reside in a disk cache, so they would not incur a disk read. The B-tree remains the standard index implementation in almost all relational databases, and many nonrelational databases use them too.\n\nInsertions and deletions\nIf the database does not change, then compiling the index is simple to do, and the index need never be changed. If there are changes, then managing the database and its index becomes more complicated.",
        "Deleting records from a database is relatively easy. The index can stay the same, and the record can just be marked as deleted. The database remains in sorted order. If there are a large number of lazy deletions, then searching and storage become less efficient.\n\nInsertions can be very slow in a sorted sequential file because room for the inserted record must be made. Inserting a record before the first record requires shifting all of the records down one. Such an operation is just too expensive to be practical. One solution is to leave some spaces. Instead of densely packing all the records in a block, the block can have some free space to allow for subsequent insertions. Those spaces would be marked as if they were \"deleted\" records.",
        "Both insertions and deletions are fast as long as space is available on a block. If an insertion won't fit on the block, then some free space on some nearby block must be found and the auxiliary indices adjusted. The hope is that enough space is available nearby, such that a lot of blocks do not need to be reorganized. Alternatively, some out-of-sequence disk blocks may be used.\n\nAdvantages of B-tree usage for databases\nThe B-tree uses all of the ideas described above. In particular, a B-tree:\n keeps keys in sorted order for sequential traversing\n uses a hierarchical index to minimize the number of disk reads\n uses partially full blocks to speed up insertions and deletions\n keeps the index balanced with a recursive algorithm",
        "In addition, a B-tree minimizes waste by making sure the interior nodes are at least half full. A B-tree can handle an arbitrary number of insertions and deletions.\n\nBest case and worst case heights\nLet  be the height of the classic B-tree (see  for the tree height definition). Let  be the number of entries in the tree. Let m be the maximum number of children a node can have. Each node can have at most  keys.\n\nIt can be shown (by induction for example) that a B-tree of height h with all its nodes completely filled has  entries. Hence, the best case height (i.e. the minimum height) of a B-tree is:\n \n\nLet  be the minimum number of children an internal (non-root) node must have. For an ordinary B-tree,",
        "Let  be the minimum number of children an internal (non-root) node must have. For an ordinary B-tree, \n\nComer (1979) and Cormen et al. (2001) give the worst case height (the maximum height) of a B-tree as\n\nAlgorithms\n\nSearch\nSearching is similar to searching a binary search tree. Starting at the root, the tree is recursively traversed from top to bottom. At each level, the search reduces its field of view to the child pointer (subtree) whose range includes the search value. A subtree's range is defined by the values, or keys, contained in its parent node. These limiting values are also known as separation values.\n\nBinary search is typically (but not necessarily) used within nodes to find the separation values and child tree of interest.\n\nInsertion",
        "Binary search is typically (but not necessarily) used within nodes to find the separation values and child tree of interest.\n\nInsertion\n\nAll insertions start at a leaf node. To insert a new element, search the tree to find the leaf node where the new element should be added. Insert the new element into that node with the following steps:",
        "If the node contains fewer than the maximum allowed number of elements, then there is room for the new element. Insert the new element in the node, keeping the node's elements ordered.\n Otherwise the node is full, evenly split it into two nodes so:\n A single median is chosen from among the leaf's elements and the new element that is being inserted.\n Values less than the median are put in the new left node and values greater than the median are put in the new right node, with the median acting as a separation value.\n The separation value is inserted in the node's parent, which may cause it to be split, and so on. If the node has no parent (i.e., the node was the root), create a new root above this node (increasing the height of the tree).",
        "If the splitting goes all the way up to the root, it creates a new root with a single separator value and two children, which is why the lower bound on the size of internal nodes does not apply to the root. The maximum number of elements per node is U−1. When a node is split, one element moves to the parent, but one element is added. So, it must be possible to divide the maximum number U−1 of elements into two legal nodes. If this number is odd, then U=2L and one of the new nodes contains (U−2)/2 = L−1 elements, and hence is a legal node, and the other contains one more element, and hence it is legal too. If U−1 is even, then U=2L−1, so there are 2L−2 elements in the node. Half of this number is L−1, which is the minimum number of elements allowed per node.",
        "An alternative algorithm supports a single pass down the tree from the root to the node where the insertion will take place, splitting any full nodes encountered on the way pre-emptively. This prevents the need to recall the parent nodes into memory, which may be expensive if the nodes are on secondary storage. However, to use this algorithm, we must be able to send one element to the parent and split the remaining U−2 elements into two legal nodes, without adding a new element. This requires U = 2L rather than U = 2L−1, which accounts for why some textbooks impose this requirement in defining B-trees.\n\nDeletion\nThere are two popular strategies for deletion from a B-tree.",
        "Deletion\nThere are two popular strategies for deletion from a B-tree.\n\n Locate and delete the item, then restructure the tree to retain its invariants, OR\n Do a single pass down the tree, but before entering (visiting) a node, restructure the tree so that once the key to be deleted is encountered, it can be deleted without triggering the need for any further restructuring\n\nThe algorithm below uses the former strategy.\n\nThere are two special cases to consider when deleting an element:\n\n The element in an internal node is a separator for its child nodes\n Deleting an element may put its node under the minimum number of elements and children\n\nThe procedures for these cases are in order below.",
        "The procedures for these cases are in order below.\n\nDeletion from a leaf node\n Search for the value to delete.\n If the value is in a leaf node, simply delete it from the node.\n If underflow happens, rebalance the tree as described in section \"Rebalancing after deletion\" below.\n\nDeletion from an internal node\nEach element in an internal node acts as a separation value for two subtrees, therefore we need to find a replacement for separation. Note that the largest element in the left subtree is still less than the separator. Likewise, the smallest element in the right subtree is still greater than the separator. Both of those elements are in leaf nodes, and either one can be the new separator for the two subtrees. Algorithmically described below:",
        "Choose a new separator (either the largest element in the left subtree or the smallest element in the right subtree), remove it from the leaf node it is in, and replace the element to be deleted with the new separator.\n The previous step deleted an element (the new separator) from a leaf node. If that leaf node is now deficient (has fewer than the required number of nodes), then rebalance the tree starting from the leaf node.",
        "Rebalancing after deletion",
        "Rebalancing starts from a leaf and proceeds toward the root until the tree is balanced. If deleting an element from a node has brought it under the minimum size, then some elements must be redistributed to bring all nodes up to the minimum. Usually, the redistribution involves moving an element from a sibling node that has more than the minimum number of nodes. That redistribution operation is called a rotation. If no sibling can spare an element, then the deficient node must be merged with a sibling. The merge causes the parent to lose a separator element, so the parent may become deficient and need rebalancing. The merging and rebalancing may continue all the way to the root. Since the minimum element count doesn't apply to the root, making the root be the only deficient node is not a",
        "and need rebalancing. The merging and rebalancing may continue all the way to the root. Since the minimum element count doesn't apply to the root, making the root be the only deficient node is not a problem. The algorithm to rebalance the tree is as follows:",
        "If the deficient node's right sibling exists and has more than the minimum number of elements, then rotate left\n Copy the separator from the parent to the end of the deficient node (the separator moves down; the deficient node now has the minimum number of elements)\n Replace the separator in the parent with the first element of the right sibling (right sibling loses one node but still has at least the minimum number of elements)\n The tree is now balanced\n Otherwise, if the deficient node's left sibling exists and has more than the minimum number of elements, then rotate right\n Copy the separator from the parent to the start of the deficient node (the separator moves down; deficient node now has the minimum number of elements)",
        "Copy the separator from the parent to the start of the deficient node (the separator moves down; deficient node now has the minimum number of elements)\n Replace the separator in the parent with the last element of the left sibling (left sibling loses one node but still has at least the minimum number of elements)\n The tree is now balanced\n Otherwise, if both immediate siblings have only the minimum number of elements, then merge with a sibling sandwiching their separator taken off from their parent\n Copy the separator to the end of the left node (the left node may be the deficient node or it may be the sibling with the minimum number of elements)\n Move all elements from the right node to the left node (the left node now has the maximum number of elements, and the right node – empty)",
        "Move all elements from the right node to the left node (the left node now has the maximum number of elements, and the right node – empty)\n Remove the separator from the parent along with its empty right child (the parent loses an element)\n If the parent is the root and now has no elements, then free it and make the merged node the new root (tree becomes shallower)\n Otherwise, if the parent has fewer than the required number of elements, then rebalance the parent",
        "Note: The rebalancing operations are different for B+ trees (e.g., rotation is different because parent has copy of the key) and B*-tree (e.g., three siblings are merged into two siblings).\n\nSequential access\nWhile freshly loaded databases tend to have good sequential behaviour, this behaviour becomes increasingly difficult to maintain as a database grows, resulting in more random I/O and performance challenges.\n\nInitial construction",
        "Initial construction\n\nA common special case is adding a large amount of pre-sorted data into an initially empty B-tree. While it is quite possible to simply perform a series of successive inserts, inserting sorted data results in a tree composed almost entirely of half-full nodes. Instead, a special \"bulk loading\" algorithm can be used to produce a more efficient tree with a higher branching factor.",
        "When the input is sorted, all insertions are at the rightmost edge of the tree, and in particular any time a node is split, we are guaranteed that no more insertions will take place in the left half. When bulk loading, we take advantage of this, and instead of splitting overfull nodes evenly, split them as unevenly as possible: leave the left node completely full and create a right node with zero keys and one child (in violation of the usual B-tree rules).",
        "At the end of bulk loading, the tree is composed almost entirely of completely full nodes; only the rightmost node on each level may be less than full. Because those nodes may also be less than half full, to re-establish the normal B-tree rules, combine such nodes with their (guaranteed full) left siblings and divide the keys to produce two nodes at least half full. The only node which lacks a full left sibling is the root, which is permitted to be less than half full.\n\nIn filesystems\nIn addition to its use in databases, the B-tree (or ) is also used in filesystems to allow quick random access to an arbitrary block in a particular file. The basic problem is turning the file block  address into a disk block address.",
        "Some operating systems require the user to allocate the maximum size of the file when the file is created. The file can then be allocated as contiguous disk blocks. In that case, to convert the file block address  into a disk block address, the operating system simply adds the file block address  to the address of the first disk block constituting the file. The scheme is simple, but the file cannot exceed its created size.\n\nOther operating systems allow a file to grow. The resulting disk blocks may not be contiguous, so mapping logical blocks to physical blocks is more involved.",
        "MS-DOS, for example, used a simple File Allocation Table (FAT). The FAT has an entry for each disk block, and that entry identifies whether its block is used by a file and if so, which block (if any) is the next disk block of the same file. So, the allocation of each file is represented as a linked list in the table. In order to find the disk address of file block , the operating system (or disk utility) must sequentially follow the file's linked list in the FAT. Worse, to find a free disk block, it must sequentially scan the FAT. For MS-DOS, that was not a huge penalty because the disks and files were small and the FAT had few entries and relatively short file chains. In the FAT12 filesystem (used on floppy disks and early hard disks), there were no more than 4,080  entries, and the FAT",
        "files were small and the FAT had few entries and relatively short file chains. In the FAT12 filesystem (used on floppy disks and early hard disks), there were no more than 4,080  entries, and the FAT would usually be resident in memory. As disks got bigger, the FAT architecture began to confront penalties. On a large disk using FAT, it may be necessary to perform disk reads to learn the disk location of a file block to be read or written.",
        "TOPS-20 (and possibly TENEX) used a 0 to 2 level tree that has similarities to a B-tree. A disk block was 512 36-bit words. If the file fit in a 512 (29) word block, then the file directory would point to that physical disk block. If the file fit in 218 words, then the directory would point to an aux index; the 512 words of that index would either be NULL (the block isn't allocated) or point to the physical address of the block. If the file fit in 227 words, then the directory would point to a block holding an aux-aux index; each entry would either be NULL or point to an aux index. Consequently, the physical disk block for a 227 word file could be located in two disk reads and read on the third.",
        "Apple's filesystem HFS+ and APFS, Microsoft's NTFS, AIX (jfs2) and some Linux filesystems, such as Btrfs and ext4, use B-trees.\n\nB*-trees are used in the HFS and Reiser4 file systems.\n\nDragonFly BSD's HAMMER file system uses a modified B+-tree.\n\nPerformance \n\nA B-tree grows slower with growing data amount, than the linearity of a linked list. Compared to a skip list, both structures have the same performance, but the B-tree scales better for growing n. A T-tree, for main memory database systems, is similar but more compact.\n\nVariations",
        "Variations\n\nAccess concurrency\nLehman and Yao showed that all the read locks could be avoided (and thus concurrent access greatly improved) by linking the tree blocks at each level together with a \"next\" pointer. This results in a tree structure where both insertion and search operations descend from the root to the leaf. Write locks are only required as a tree block is modified. This maximizes access concurrency by multiple users, an important consideration for databases and/or other B-tree-based ISAM storage methods. The cost associated with this improvement is that empty pages cannot be removed from the btree during normal operations. (However, see  for various strategies to implement node merging, and source code at.)",
        "United States Patent 5283894, granted in 1994, appears to show a way to use a 'Meta Access Method'  to allow concurrent B+ tree access and modification without locks. The technique accesses the tree 'upwards' for both searches and updates by means of additional in-memory indexes that point at the blocks in each level in the block cache. No reorganization for deletes is needed and there are no 'next' pointers in each block as in Lehman and Yao.\n\nParallel algorithms\nSince B-trees are similar in structure to red-black trees, parallel algorithms for red-black trees can be applied to B-trees as well.\n\nMaple tree\nA Maple tree is a B-tree developed for use in the Linux kernel to reduce lock contention in virtual memory management.\n\nSee also\n B+ tree\n R-tree\n Red–black tree\n 2–3 tree\n 2–3–4 tree",
        "Maple tree\nA Maple tree is a B-tree developed for use in the Linux kernel to reduce lock contention in virtual memory management.\n\nSee also\n B+ tree\n R-tree\n Red–black tree\n 2–3 tree\n 2–3–4 tree\n\nNotes\n\nReferences\n\nSources\n \n .\n . Chapter 18: B-Trees.\n \n . Section 6.2.4: Multiway Trees, pp. 481–491. Also, pp. 476–477 of section 6.2.3 (Balanced Trees) discusses 2–3 trees.\n\nOriginal papers\n .\n .\n\nExternal links",
        "Sources\n \n .\n . Chapter 18: B-Trees.\n \n . Section 6.2.4: Multiway Trees, pp. 481–491. Also, pp. 476–477 of section 6.2.3 (Balanced Trees) discusses 2–3 trees.\n\nOriginal papers\n .\n .\n\nExternal links\n\nB-tree lecture by David Scot Taylor, SJSU\nB-Tree visualisation (click \"init\")\nAnimated B-Tree visualization\nB-tree and UB-tree on Scholarpedia Curator: Dr Rudolf Bayer\nB-Trees: Balanced Tree Data Structures\nNIST's Dictionary of Algorithms and Data Structures: B-tree\nB-Tree Tutorial\nThe InfinityDB BTree implementation\nCache Oblivious B(+)-trees\nDictionary of Algorithms and Data Structures entry for B*-tree\nOpen Data Structures - Section 14.2 - B-Trees, Pat Morin\nCounted B-Trees\nB-Tree .Net, a modern, virtualized RAM & Disk implementation\n\nBulk loading",
        "Bulk loading\n\n \n\n \nComputer-related introductions in 1971\nDatabase index techniques"
    ],
    [
        "Badtrans\nBadTrans is a malicious Microsoft Windows computer worm distributed by e-mail. Because of a known vulnerability in older versions of Internet Explorer, some email programs, such as Microsoft's Outlook Express and Microsoft Outlook programs, may install and execute the worm as soon as the e-mail message is viewed.\n\nOnce executed, the worm replicates by sending copies of itself to other e-mail addresses found on the host's machine, and installs a keystroke logger, which then captures everything typed on the affected computer.  Badtrans then transmits the data to one of several e-mail addresses. \n\nAmong the e-mail addresses that received the keyloggers were free addresses at Excite, Yahoo, and IJustGotFired.com.",
        "Among the e-mail addresses that received the keyloggers were free addresses at Excite, Yahoo, and IJustGotFired.com.\n\nThe target address at IJustGotFired began receiving emails at 3:23pm on November 24, 2001.  Once the account exceeded its quotas, it was automatically disabled, but the messages were still saved as they arrived.  The address received over 100,000 keylogs in the first day alone.",
        "In mid-December, the FBI contacted Rudy Rucker, Jr., owner of MonkeyBrains, and requested a copy of the keylogged data. All of that data was stolen from the victims of the worm; it includes no information about the creator of Badtrans.\nInstead of complying with the FBI request, MonkeyBrains published a database website, https://web.archive.org/web/20070621140432/https://badtrans.monkeybrains.net/ for the public to determine if a given address has been compromised.  The database does not reveal the actual passwords or keylogged data.\n\nReferences\n\nEmail worms"
    ],
    [
        "Bastard Operator From Hell\nThe Bastard Operator From Hell (BOFH) is  a fictional rogue computer operator created by Simon Travaglia, who takes out his anger on users (who are \"lusers\" to him) and others who pester him with their computer problems, uses his expertise against his enemies and manipulates his employer.\n\nSeveral people have written stories about BOFHs, but only those by Simon Travaglia are considered canonical. \nThe BOFH stories were originally posted in 1992 to Usenet by Travaglia, with some being reprinted in Datamation. Since 2000 they have been published regularly in The Register (UK). Several collections of the stories have been published as books.\n\nBy extension, the term is also used to refer to any system administrator who displays the qualities of the original.",
        "By extension, the term is also used to refer to any system administrator who displays the qualities of the original.\n\nThe early accounts of the BOFH took place in a university; later the scenes were set in an office workplace. In 2000 (BOFH 2k), the BOFH and his pimply-faced youth (PFY) assistant moved to a new company.",
        "Other characters \n The PFY (Pimply-Faced Youth, the assistant to the BOFH. Real name is Stephen) Possesses a temperament similar to the BOFH, and often either teams up with or plots against him.\n The Boss (often portrayed as having no IT knowledge but believing otherwise; identity changes as successive bosses are sacked, leave, are committed, or have nasty \"accidents\")\n CEO of the company – The PFY's uncle Brian from 1996 until 2000, when the BOFH and PFY moved to a new company.\n The help desk operators, referred to as the \"Helldesk\" and often scolded for giving out the BOFH's personal number.\n The Boss's secretary, Sharon.\n The security department\n George, the cleaner (an invaluable source of information to the BOFH and PFY)\n\nBooks\n\nGames",
        "Books\n\nGames \n\nBOFH is a text adventure game written by Howard A. Sherman and published in 2002. It is available via Malinche.\n\nReferences in other media \n\nThe protagonist in Charles Stross's The Laundry Files series of novels named himself Bob Oliver Francis Howard in reference to the BOFH. As Bob Howard is a self-chosen pseudonym, and Bob is a network manager when not working as a computational demonologist, the name is all too appropriate. In the novella Pimpf, he acquires a pimply-faced young assistant by the name of Peter-Fred Young.\n\nAuthorship",
        "Authorship\n\nSimon Travaglia (born 1964) graduated from the University of Waikato, New Zealand in 1985. He worked as the IT infrastructure manager (2004–2008) and computer operator (1985–1992) at the University of Waikato  and the infrastructure manager at the Waikato Innovation Park, Hamilton, New Zealand (since 2008). Since 1999 he is a freelance writer for The Register. He lives in Hautapu, New Zealand.\n\nReferences\n\nFurther reading\n\nExternal links \n \n \n \n\nComputer humor\nInternet culture\nInternet slang\nSystem administration\nFictional characters introduced in 1992\nFictional people in information technology"
    ],
    [
        "Baudot code\nThe Baudot code () is an early character encoding for telegraphy invented by Émile Baudot in the 1870s. It was the predecessor to the International Telegraph Alphabet No. 2 (ITA2), the most common teleprinter code in use before ASCII. Each character in the alphabet is represented by a series of five bits, sent over a communication channel such as a telegraph wire or a radio signal by asynchronous serial communication. The symbol rate measurement is known as baud, and is derived from the same name.\n\nHistory\n\nBaudot code (ITA1)\n\nIn the below table, Columns I, II, III, IV, and V show the code; the Let. and Fig. columns show the letters and numbers for the Continental and UK versions; and the sort keys present the table in the order: alphabetical, Gray and UK",
        "Baudot developed his first multiplexed telegraph in 1872 and patented it in 1874. In 1876, he changed from a six-bit code to a five-bit code, as suggested by Carl Friedrich Gauss and Wilhelm Weber in 1834,\nwith equal on and off intervals, which allowed for transmission of the Roman alphabet, and included punctuation and control signals. The code itself was not patented (only the machine) because French patent law does not allow concepts to be patented.",
        "Baudot's 5-bit code was adapted to be sent from a manual keyboard, and no teleprinter equipment was ever constructed that used it in its original form. The code was entered on a keyboard which had just five piano-type keys and was operated using two fingers of the left hand and three fingers of the right hand. Once the keys had been pressed, they were locked down until mechanical contacts in a distributor unit passed over the sector connected to that particular keyboard, at which time the keyboard was unlocked ready for the next character to be entered, with an audible click (known as the \"cadence signal\") to warn the operator. Operators had to maintain a steady rhythm, and the usual speed of operation was 30 words per minute.",
        "The table \"shows the allocation of the Baudot code which was employed in the British Post Office for continental and inland services. A number of characters in the continental code are replaced by fractionals in the inland code. Code elements 1, 2 and 3 are transmitted by keys 1, 2 and 3, and these are operated by the first three fingers of the right hand. Code elements 4 and 5 are transmitted by keys 4 and 5, and these are operated by the first two fingers of the left hand.\"\n\nBaudot's code became known as the International Telegraph Alphabet No. 1 (ITA1). It is no longer used.\n\nMurray code",
        "In 1901, Baudot's code was modified by Donald Murray (1865–1945), prompted by his development of a typewriter-like keyboard. The Murray system employed an intermediate step; a keyboard perforator, which allowed an operator to punch a paper tape, and a tape transmitter for sending the message from the punched tape. At the receiving end of the line, a printing mechanism would print on a paper tape, and/or a reperforator could be used to make a perforated copy of the message. As there was no longer a connection between the operator's hand movement and the bits transmitted, there was no concern about arranging the code to minimize operator fatigue, and instead Murray designed the code to minimize wear on the machinery, assigning the code combinations with the fewest punched holes to the most",
        "arranging the code to minimize operator fatigue, and instead Murray designed the code to minimize wear on the machinery, assigning the code combinations with the fewest punched holes to the most frequently used characters.",
        "For example, the one-hole letters are E and T. The ten two-hole letters are AOINSHRDLZ, very similar to the \"Etaoin shrdlu\" order used in Linotype machines. Ten more letters, BCGFJMPUWY, have three holes each, and the four-hole letters are VXKQ.",
        "The Murray code also introduced what became known as \"format effectors\" or \"control characters\" the CR (Carriage Return) and LF (Line Feed) codes. A few of Baudot's codes moved to the positions where they have stayed ever since: the NULL or BLANK and the DEL code. NULL/BLANK was used as an idle code for when no messages were being sent, but the same code was used to encode the space separation between words. Sequences of DEL codes (fully punched columns) were used at start or end of messages or between them, allowing easy separation of distinct messages. (BELL codes could be inserted in those sequences to signal to the remote operator that a new message was coming or that transmission of a message was terminated).\n\nEarly British Creed machines also used the Murray system.\n\nWestern Union",
        "Early British Creed machines also used the Murray system.\n\nWestern Union\n\nMurray's code was adopted by Western Union which used it until the 1950s, with a few changes that consisted of omitting some characters and adding more control codes. An explicit SPC (space) character was introduced, in place of the BLANK/NULL, and a new BEL code rang a bell or otherwise produced an audible signal at the receiver. Additionally, the WRU or \"Who aRe yoU?\" code was introduced, which caused a receiving machine to send an identification stream back to the sender.\n\nITA2",
        "ITA2\n\nIn 1924, the CCITT introduced the International Telegraph Alphabet No. 2 (ITA2) code as an international standard, which was based on the Western Union code with some minor changes. The US standardized on a version of ITA2 called the American Teletypewriter code (US TTY) which was the basis for 5-bit teletypewriter codes until the debut of 7-bit ASCII in 1963.\n\nSome code points (marked blue in the table) were reserved for national-specific usage.",
        "The code position assigned to Null was in fact used only for the idle state of teleprinters. During long periods of idle time, the impulse rate was not synchronized between both devices (which could even be powered off or not permanently interconnected on commuted phone lines). To start a message it was first necessary to calibrate the impulse rate, a sequence of regularly timed \"mark\" pulses (1), by a group of five pulses, which could also be detected by simple passive electronic devices to turn on the teleprinter. This sequence of pulses generated a series of Erasure/Delete characters while also initializing the state of the receiver to the Letters shift mode. However, the first pulse could be lost, so this power on procedure could then be terminated by a single Null immediately",
        "while also initializing the state of the receiver to the Letters shift mode. However, the first pulse could be lost, so this power on procedure could then be terminated by a single Null immediately followed by an Erasure/Delete character. To preserve the synchronization between devices, the Null code could not be used arbitrarily in the middle of messages (this was an improvement to the initial Baudot system where spaces were not explicitly differentiated, so it was difficult to maintain the pulse counters for repeating spaces on teleprinters). But it was then possible to resynchronize devices at any time by sending a Null in the middle of a message (immediately followed by an Erasure/Delete/LS control if followed by a letter, or by a FS control if followed by a figure). Sending Null",
        "at any time by sending a Null in the middle of a message (immediately followed by an Erasure/Delete/LS control if followed by a letter, or by a FS control if followed by a figure). Sending Null controls also did not cause the paper band to advance to the next row (as nothing was punched), so this saved precious lengths of punchable paper band. On the other hand, the Erasure/Delete/LS control code was always punched and always shifted to the (initial) letters mode. According to some sources, the Null code point was reserved for country-internal usage only.",
        "The Shift to Letters code (LS) is also usable as a way to cancel/delete text from a punched tape after it has been read, allowing the safe destruction of a message before discarding the punched band. Functionally, it can also play the same filler role as the Delete code in ASCII (or other 7-bit and 8-bit encodings, including EBCDIC for punched cards). After codes in a fragment of text have been replaced by an arbitrary number of LS codes, what follows is still preserved and decodable. It can also be used as an initiator to make sure that the decoding of the first code will not give a digit or another symbol from the figures page (because the Null code can be arbitrarily inserted near the end or beginning of a punch band, and has to be ignored, whereas the Space code is significant in",
        "or another symbol from the figures page (because the Null code can be arbitrarily inserted near the end or beginning of a punch band, and has to be ignored, whereas the Space code is significant in text).",
        "The cells marked as reserved for extensions (which use the LS code again a second time—just after the first LS code—to shift from the figures page to the letters shift page) has been defined to shift into a new mode. In this new mode, the letters page contains only lowercase letters, but retains access to a third code page for uppercase letters, either by encoding for a single letter (by sending LS before that letter), or locking (with FS+LS) for an unlimited number of capital letters or digits before then unlocking (with a single LS) to return to lowercase mode. The cell marked as \"Reserved\" is also usable (using the FS code from the figures shift page) to switch the page of figures (which normally contains digits and national lowercase letters or symbols) to a fourth page (where",
        "is also usable (using the FS code from the figures shift page) to switch the page of figures (which normally contains digits and national lowercase letters or symbols) to a fourth page (where national letters are uppercase and other symbols may be encoded).",
        "ITA2 is still used in telecommunications devices for the deaf (TDD), Telex, and some amateur radio applications, such as radioteletype (\"RTTY\"). ITA2 is also used in Enhanced Broadcast Solution, an early 21st-century financial protocol specified by Deutsche Börse, to reduce the character encoding footprint.\n\nNomenclature\nNearly all 20th-century teleprinter equipment used Western Union's code, ITA2, or variants thereof. Radio amateurs casually call ITA2 and variants \"Baudot\" incorrectly, and even the American Radio Relay League's Amateur Radio Handbook does so, though in more recent editions the tables of codes correctly identifies it as ITA2.\n\nCharacter set\nThe values shown in each cell are the Unicode codepoints, given for comparison.\n\nOriginal Baudot variants",
        "Character set\nThe values shown in each cell are the Unicode codepoints, given for comparison.\n\nOriginal Baudot variants\n\nOriginal Baudot, domestic UK\n\nOriginal Baudot, Continental European\n\nOriginal Baudot, ITA 1\n\nBaudot–Murray variants\n\nMurray Code\n\nITA 2 and US-TTY\n\nWeather code\nMeteorologists used a variant of ITA2 with the figures-case symbols, except for the ten digits, BEL and a few other characters, replaced by weather symbols:",
        "ITA 2 and US-TTY\n\nWeather code\nMeteorologists used a variant of ITA2 with the figures-case symbols, except for the ten digits, BEL and a few other characters, replaced by weather symbols:\n\nDetails\nNote: This table presumes the space called \"1\" by Baudot and Murray is rightmost, and least significant. The way the transmitted bits were packed into larger codes varied by manufacturer. The most common solution allocates the bits from the least significant bit towards the most significant bit (leaving the three most significant bits of a byte unused).",
        "In ITA2, characters are expressed using five bits. ITA2 uses two code sub-sets, the \"letter shift\" (LTRS), and the \"figure shift\" (FIGS). The FIGS character (11011) signals that the following characters are to be interpreted as being in the FIGS set, until this is reset by the LTRS (11111) character. In use, the LTRS or FIGS shift key is pressed and released, transmitting the corresponding shift character to the other machine. The desired letters or figures characters are then typed. Unlike a typewriter or modern computer keyboard, the shift key isn't kept depressed whilst the corresponding characters are typed. \"ENQuiry\" will trigger the other machine's answerback. It means \"Who are you?\"",
        "CR is carriage return, LF is line feed, BEL is the bell character which rang a small bell (often used to alert operators to an incoming message), SP is space, and NUL is the null character (blank tape).",
        "Note: the binary conversions of the codepoints are often shown in reverse order, depending on (presumably) from which side one views the paper tape. Note further that the \"control\" characters were chosen so that they were either symmetric or in useful pairs so that inserting a tape \"upside down\" did not result in problems for the equipment and the resulting printout could be deciphered.  Thus FIGS (11011), LTRS (11111) and space (00100) are invariant, while CR (00010) and LF (01000), generally used as a pair, are treated the same regardless of order by page printers. LTRS could also be used to overpunch characters to be deleted on a paper tape (much like DEL in 7-bit ASCII).",
        "The sequence RYRYRY... is often used in test messages, and at the start of every transmission. Since R is 01010 and Y is 10101, the sequence exercises much of a teleprinter's mechanical components at maximum stress.  Also, at one time, fine-tuning of the receiver was done using two coloured lights (one for each tone).  'RYRYRY...' produced 0101010101..., which made the lights glow with equal brightness when the tuning was correct. This tuning sequence is only useful when ITA2 is used with two-tone FSK modulation, such as is commonly seen in radioteletype (RTTY) usage.\n\nUS implementations of Baudot code may differ in the addition of a few characters, such as #, & on the FIGS layer.",
        "US implementations of Baudot code may differ in the addition of a few characters, such as #, & on the FIGS layer.\n\nThe Russian version of Baudot code (MTK-2) used three shift modes; the Cyrillic letter mode was activated by the character (00000). Because of the larger number of characters in the Cyrillic alphabet, the characters !, &, £ were omitted and replaced by Cyrillics, and BEL has the same code as Cyrillic letter Ю. The Cyrillic letters Ъ and Ё are omitted, and Ч is merged with the numeral 4.\n\nSee also\n Bacon's cipher – A 5-bit binary encoding of the English alphabet devised by Francis Bacon in 1605.\n List of information system character sets\n CCIR 476\n\nExplanatory notes\n\nReferences\n\nFurther reading\n \n \n MTK-2 code table\n Baudot, Murray, ITA2, ITA5, etc.\n\nExternal links",
        "Explanatory notes\n\nReferences\n\nFurther reading\n \n \n MTK-2 code table\n Baudot, Murray, ITA2, ITA5, etc.\n\nExternal links \n \n\nAmateur radio\nCharacter encoding\nCharacter sets\nTelegraphy"
    ],
    [
        "Bayesian probability\nBayesian probability (  or  ) is an interpretation of the concept of probability, in which, instead of frequency or propensity of some phenomenon, probability is interpreted as reasonable expectation representing a state of knowledge or as quantification of a personal belief.\n\nThe Bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses; that is, with propositions whose truth or falsity is unknown. In the Bayesian view, a probability is assigned to a hypothesis, whereas under frequentist inference, a hypothesis is typically tested without being assigned a probability.",
        "Bayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the Bayesian probabilist specifies a prior probability. This, in turn, is then updated to a posterior probability in the light of new, relevant data (evidence). The Bayesian interpretation provides a standard set of procedures and formulae to perform this calculation.\n\nThe term Bayesian derives from the 18th-century mathematician and theologian Thomas Bayes, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference. Mathematician Pierre-Simon Laplace pioneered and popularized what is now called Bayesian probability.",
        "Bayesian methodology\nBayesian methods are characterized by concepts and procedures as follows:\n The use of random variables, or more generally unknown quantities, to model all sources of uncertainty in statistical models including uncertainty resulting from lack of information (see also aleatoric and epistemic uncertainty).\n The need to determine the prior probability distribution taking into account the available (prior) information.\n The sequential use of Bayes' theorem: as more data become available, calculate the posterior distribution using Bayes' theorem; subsequently, the posterior distribution becomes the next prior.",
        "The sequential use of Bayes' theorem: as more data become available, calculate the posterior distribution using Bayes' theorem; subsequently, the posterior distribution becomes the next prior.\n While for the frequentist, a hypothesis is a proposition (which must be either true or false) so that the frequentist probability of a hypothesis is either 0 or 1, in Bayesian statistics, the probability that can be assigned to a hypothesis can also be in a range from 0 to 1 if the truth value is uncertain.",
        "Objective and subjective Bayesian probabilities",
        "Broadly speaking, there are two interpretations of Bayesian probability. For objectivists, who interpret probability as an extension of logic, probability quantifies the reasonable expectation that everyone (even a \"robot\") who shares the same knowledge should share in accordance with the rules of Bayesian statistics, which can be justified by Cox's theorem. For subjectivists, probability corresponds to a personal belief. Rationality and coherence allow for substantial variation within the constraints they pose; the constraints are justified by the Dutch book argument or by decision theory and de Finetti's theorem. The objective and subjective variants of Bayesian probability differ mainly in their interpretation and construction of the prior probability.",
        "History",
        "The term Bayesian derives from Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem in a paper titled \"An Essay towards solving a Problem in the Doctrine of Chances\". In that special case, the prior and posterior distributions were beta distributions and the data came from Bernoulli trials. It was Pierre-Simon Laplace (1749–1827) who introduced a general version of the theorem and used it to approach problems in celestial mechanics, medical statistics, reliability, and jurisprudence. Early Bayesian inference, which used uniform priors following Laplace's principle of insufficient reason, was called \"inverse probability\" (because it infers backwards from observations to parameters, or from effects to causes). After the 1920s, \"inverse probability\" was",
        "principle of insufficient reason, was called \"inverse probability\" (because it infers backwards from observations to parameters, or from effects to causes). After the 1920s, \"inverse probability\" was largely supplanted by a collection of methods that came to be called frequentist statistics.",
        "In the 20th century, the ideas of Laplace developed in two directions, giving rise to objective and subjective currents in Bayesian practice.\nHarold Jeffreys' Theory of Probability (first published in 1939) played an important role in the revival of the Bayesian view of probability, followed by works by Abraham Wald (1950) and Leonard J. Savage (1954). The adjective Bayesian itself dates to the 1950s;  the derived Bayesianism, neo-Bayesianism is of 1960s coinage. In the objectivist stream, the statistical analysis depends on only the model assumed and the data analysed. No subjective decisions need to be involved. In contrast, \"subjectivist\" statisticians deny the possibility of fully objective analysis for the general case.",
        "In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of Markov chain Monte Carlo methods and the consequent removal of many of the computational problems, and to an increasing interest in nonstandard, complex applications. While frequentist statistics remains strong (as demonstrated by the fact that much of undergraduate teaching is based on it ), Bayesian methods are widely accepted and used, e.g., in the field of machine learning.\n\nJustification of Bayesian probabilities\nThe use of Bayesian probabilities as the basis of Bayesian inference has been supported by several arguments, such as Cox axioms, the Dutch book argument, arguments based on decision theory and de Finetti's theorem.",
        "Axiomatic approach\nRichard T. Cox showed that Bayesian updating follows from several axioms, including two functional equations and a hypothesis of differentiability. The assumption of differentiability or even continuity is controversial; Halpern found a counterexample based on his observation that the Boolean algebra of statements may be finite. Other axiomatizations have been suggested by various authors with the purpose of making the theory more rigorous.\n\nDutch book approach",
        "Dutch book approach\n\nBruno de Finetti proposed the Dutch book argument based on betting. A clever bookmaker makes a Dutch book by setting the odds and bets to ensure that the bookmaker profits—at the expense of the gamblers—regardless of the outcome of the event (a horse race, for example) on which the gamblers bet. It is associated with probabilities implied by the odds not being coherent.",
        "However, Ian Hacking noted that traditional Dutch book arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. For example, Hacking writes \"And neither the Dutch book argument, nor any other in the personalist arsenal of proofs of the probability axioms, entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour.\"",
        "In fact, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on \"probability kinematics\" following the publication of Richard C. Jeffrey's rule, which is itself regarded as Bayesian). The additional hypotheses sufficient to (uniquely) specify Bayesian updating are substantial and not universally seen as satisfactory.\n\nDecision theory approach\nA decision-theoretic justification of the use of Bayesian inference (and hence of Bayesian probabilities) was given by Abraham Wald, who proved that every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures. Conversely, every Bayesian procedure is admissible.",
        "Personal probabilities and objective methods for constructing priors",
        "Following the work on expected utility theory of Ramsey and von Neumann,  decision-theorists have accounted for rational behavior using a probability distribution for the agent. Johann Pfanzagl completed the Theory of Games and Economic Behavior by providing an axiomatization of subjective probability and utility, a task left uncompleted by von Neumann and Oskar Morgenstern: their original theory supposed that all the agents had the same probability distribution, as a convenience. Pfanzagl's axiomatization was endorsed by Oskar Morgenstern: \"Von Neumann and I have anticipated ... [the question whether probabilities] might, perhaps more typically, be subjective and have stated specifically that in the latter case axioms could be found from which could derive the desired numerical utility",
        "whether probabilities] might, perhaps more typically, be subjective and have stated specifically that in the latter case axioms could be found from which could derive the desired numerical utility together with a number for the probabilities (cf. p. 19 of The Theory of Games and Economic Behavior). We did not carry this out; it was demonstrated by Pfanzagl ... with all the necessary rigor\".",
        "Ramsey and Savage noted that the individual agent's probability distribution could be objectively studied in experiments. Procedures for testing hypotheses about probabilities (using finite samples) are due to Ramsey (1931) and de Finetti (1931, 1937, 1964, 1970). Both Bruno de Finetti and Frank P. Ramsey acknowledge their debts to pragmatic philosophy, particularly (for Ramsey) to Charles S. Peirce.\n\nThe \"Ramsey test\" for evaluating probability distributions is implementable in theory, and has kept experimental psychologists occupied for a half century.\nThis work demonstrates that Bayesian-probability propositions can be falsified, and so meet an empirical criterion of Charles S. Peirce, whose work inspired Ramsey. (This falsifiability-criterion was popularized by Karl Popper.)",
        "Modern work on the experimental evaluation of personal probabilities uses the randomization, blinding, and Boolean-decision procedures of the Peirce-Jastrow experiment. Since individuals act according to different probability judgments, these agents' probabilities are \"personal\" (but amenable to objective study).\n\nPersonal probabilities are problematic for science and for some applications where decision-makers lack the knowledge or time to specify an informed probability-distribution (on which they are prepared to act). To meet the needs of science and of human limitations, Bayesian statisticians have developed \"objective\" methods for specifying prior probabilities.",
        "Indeed, some Bayesians have argued the prior state of knowledge defines the (unique) prior probability-distribution for \"regular\" statistical problems; cf. well-posed problems. Finding the right method for constructing such \"objective\" priors (for appropriate classes of regular problems) has been the quest of statistical theorists from Laplace to John Maynard Keynes, Harold Jeffreys, and Edwin Thompson Jaynes. These theorists and their successors have suggested several methods for constructing \"objective\" priors (Unfortunately, it is not clear how to assess the relative \"objectivity\" of the priors proposed under these methods):\n Maximum entropy\n Transformation group analysis\n Reference analysis",
        "Each of these methods contributes useful priors for \"regular\" one-parameter problems, and each prior can handle some challenging statistical models (with \"irregularity\" or several parameters). Each of these methods has been useful in Bayesian practice. Indeed, methods for constructing \"objective\" (alternatively, \"default\" or \"ignorance\") priors have been developed by avowed subjective (or \"personal\") Bayesians like James Berger (Duke University) and José-Miguel Bernardo (Universitat de València), simply because such priors are needed for Bayesian practice, particularly in science. The quest for \"the universal method for constructing priors\" continues to attract statistical theorists.",
        "Thus, the Bayesian statistician needs either to use informed priors (using relevant expertise or previous data) or to choose among the competing methods for constructing \"objective\" priors.\n\nSee also\n\n An Essay towards solving a Problem in the Doctrine of Chances\n Bayesian epistemology\n Bertrand paradox—a paradox in classical probability\n Credal network\n Credence (statistics)\n De Finetti's game—a procedure for evaluating someone's subjective probability \n Evidence under Bayes' theorem\n Monty Hall problem\n QBism—an interpretation of quantum mechanics based on subjective Bayesian probability\n Reference class problem\n\nReferences\n\nBibliography",
        "References\n\nBibliography\n\n \n \n\n \n \n \n  (translation of de Finetti, 1931)\n  (translation of de Finetti, 1937, above)\n , , two volumes.\n Goertz, Gary and James Mahoney. 2012. A Tale of Two Cultures: Qualitative and Quantitative Research in the Social Sciences. Princeton University Press.\n.\n \n(Partly reprinted in )\n \n \n \n \n \n  (\n \n\n \n \n \n  \n \n \n  \n \n\nProbability\nJustification (epistemology)\nProbability interpretations\nPhilosophy of mathematics\nPhilosophy of science"
    ],
    [
        "BBS\nBBS may refer to:\n\nAmmunition\n BBs, BB gun metal bullets\n BBs, airsoft gun plastic pellets\n\nComputing and gaming\n Bulletin board system, a computer server users dial into via dial-up or telnet; precursor to the Internet\n BIOS Boot Specification, a firmware specification for the boot process\n Blum Blum Shub, a pseudorandom number generator\n Kingdom Hearts Birth by Sleep, a Disney-based video game for the PlayStation Portable\n\nOrganisations\n\nUnited Kingdom\n Birmingham Business School (University of Birmingham), a faculty\n British Blind Sport, a parasports charity\n British Boy Scouts, a national youth association\n British Bryological Society, a botanists' learned society",
        "United States\n BBS Productions, a film company of early 1970s New Hollywood\n Badger Boys State, a youth camp held in Wisconsin\n\nElsewhere\n BBS Kraftfahrzeugtechnik, a German wheel manufacturer\n Bahrain Bayan School\n Bangladesh Bureau of Statistics\n Baton Broadcast System, Canada\n Bhutan Broadcasting Service, Bhutan\n Bodu Bala Sena, Sri Lanka\n Bologna Business School, Italy\n Budapest Business School, Hungary\n\nScience\n Bardet–Biedl syndrome, a genetic disorder\n Behavioral and Brain Sciences, a peer-reviewed journal\n Behavior-based safety, the risk reduction subfield of behavioural engineering\n Berg Balance Scale, a medical function test\n Bogart–Bacall syndrome, a vocal misuse disorder\n Borate buffered saline, in biochemistry\n Breeding bird survey, to monitor avian populations",
        "Titles\n Bachelor of Business Studies, an academic degree\n Bronze Bauhinia Star, in the Hong Kong honors system\n\nTrain stations\n Bhubaneswar railway station, Odisha, India (by Indian Railways code)\n Bras Basah MRT station, Singapore (by MRT abbreviation)"
    ],
    [
        "BeBox\nThe BeBox is a dual CPU personal computer, briefly sold by Be Inc. to run the company's own operating system, BeOS. It has PowerPC CPUs, its I/O board has a custom \"GeekPort\", and the front bezel has \"Blinkenlights\".\n\nThe BeBox made its debut in October 1995 in a dual PowerPC 603 at 66 MHz configuration. The processors were upgraded to 133 MHz in August 1996 (BeBox Dual603e-133). Production was halted in January 1997, following the port of BeOS to the Macintosh, in order for the company to concentrate on software. Be sold around 1000 66 MHz BeBoxes and 800 133 MHz BeBoxes.",
        "BeBox creator Jean-Louis Gassée did not see the BeBox as a general consumer device, warning that \"Before we let you use the BeBox, we believe you must have some aptitude toward programming the standard language is C++.\"\n\nCPU configuration \nInitial prototypes are equipped with two AT&T Hobbit processors and three AT&T 9308S DSPs.\n\nProduction models use two 66 MHz PowerPC 603 processors or two 133 MHz PowerPC 603e processors to power the BeBox. Prototypes having dual 200 MHz CPUs or four CPUs exist, but were never publicly available.\n\nMain board \nThe main board is in a standard AT format commonly found on PC. It used standard PC components to make it as inexpensive as possible.",
        "Main board \nThe main board is in a standard AT format commonly found on PC. It used standard PC components to make it as inexpensive as possible.\n\n Two PowerPC 603/66 MHz or 603e/133 MHz processors\n Eight 72-pin SIMM sockets\n 128 KB Flash ROM\n Three PCI slots\n Five ISA slots\n Internal SCSI connector\n Internal IDE connector\n Internal floppy connector\n External SCSI-2 connector\n Parallel port\n Keyboard port, AT-style\n Three GeekPort fuses\n I/O Board connector\n Front panel connector\n Power connector\n\nI/O board \n\nThe I/O board offers four serial ports (9-pin D-sub), a PS/2 mouse port, and two joystick ports (15-pin D-sub).",
        "I/O board \n\nThe I/O board offers four serial ports (9-pin D-sub), a PS/2 mouse port, and two joystick ports (15-pin D-sub). \n\nThere are four DIN MIDI ports (two in, two out), two stereo pairs of RCA connectors audio line-level input and output, and a pair of 3.5 mm stereo phono jacks for microphone input and headphone output. There are also internal audio connectors: 5-pin strip for the audio CD line-level playback, and two 4-pin strips for microphone input and headphone output. The audio is produced with a 16-bit DAC stereo sound system capable of 48 kHz and 44.1 kHz.\n\nFor the more unusual uses, there are three 4-pin mini-DIN infrared (IR) I/O ports.",
        "For the more unusual uses, there are three 4-pin mini-DIN infrared (IR) I/O ports.\n\nGeekPort\nAn experimental-electronic-development oriented port, backed by three fuses on the mainboard, the 37-pin D-sub \"GeekPort\" provides digital and analog I/O and DC power on the ISA bus:\n Two independent, bidirectional 8-bit ports\n Four A/D pins routing to a 12-bit A/D converter\n Four D/A pins connected to an independent 8-bit D/A converter\n Two signal ground reference pins\n Eleven power and ground pins: Two at +5 V, one at +12 V, one at -12 V, seven ground pins\n\n\"Blinkenlights\" \nTwo yellow/green vertical LED arrays, dubbed the \"blinkenlights\", are built into the front bezel to illustrate the CPU load. The bottommost LED on the right side indicates hard disk activity.",
        "See also \n Multi Emulator Super System (MESS) able to emulate both BeBox 66  and 133\n\nReferences\n\nExternal links \n   An interview with Be Inc. CEO Jean-Louis Gassée and VP of Engineering Erich Ringewald.\n \n \n BeBox Photo Gallery (Joseph Palmer: Be HW Engineer)\n  (BeBox)\n\nBe Inc.\nComputer-related introductions in 1995\nComputer workstations\nPersonal computers\nPowerPC computers"
    ],
    [
        "BeOS\nBeOS is an operating system for personal computers first developed by Be Inc. in 1990. It was first written to run on BeBox hardware.\n \nBeOS was positioned as a multimedia platform that could be used by a substantial population of desktop users and a competitor to Classic Mac OS and Microsoft Windows. It was ultimately unable to achieve a significant market share, and did not prove commercially viable for Be Inc. The company was acquired by Palm, Inc. Today BeOS is mainly used, and derivatives developed, by a small population of enthusiasts.\n\nThe open-source operating system Haiku is a continuation of BeOS concepts and most of the application level compatibility. The latest version, Beta 4 released December 2022, still retains BeOS 5 compatibility in its x86 32-bit images.",
        "History\nInitially designed to run on AT&T Hobbit-based hardware, BeOS was later modified to run on PowerPC-based processors: first Be's own systems, later Apple Computer's PowerPC Reference Platform and Common Hardware Reference Platform, with the hope that Apple would purchase or license BeOS as a replacement for its aging Classic Mac OS. \n\nToward the end of 1996, Apple was still looking for a replacement to Copland in their operating system strategy. Amidst rumours of Apple's interest in purchasing BeOS, Be wanted to increase their user base, to try to convince software developers to write software for the operating system. Be courted Macintosh clone vendors to ship BeOS with their hardware.",
        "Apple CEO Gil Amelio started negotiations to buy Be Inc., but negotiations stalled when Be CEO Jean-Louis Gassée wanted $300 million; Apple was unwilling to offer any more than $125 million. Apple's board of directors decided NeXTSTEP was a better choice and purchased NeXT in 1996 for $429 million, bringing back Apple co-founder Steve Jobs.\n\nIn 1997, Power Computing began bundling BeOS (on a CD for optional installation) with its line of PowerPC-based Macintosh clones. These systems could dual boot either the Classic Mac OS or BeOS, with a start-up screen offering the choice. Motorola also announced in February 1997 that it would bundle BeOS with their Macintosh clones, the Motorola StarMax, along with MacOS.",
        "Due to Apple's moves and the mounting debt of Be Inc., BeOS was soon ported to the Intel x86 platform with its R3 release in March 1998. Through the late 1990s, BeOS managed to create a niche of followers, but the company failed to remain viable.  Be Inc. also released a stripped-down, but free, copy of BeOS R5 known as BeOS Personal Edition (BeOS PE). BeOS PE could be started from within Microsoft Windows or Linux, and was intended to nurture consumer interest in its product and give developers something to tinker with. Be Inc. also released a stripped-down version of BeOS for Internet appliances (BeIA), which soon became the company's business focus in place of BeOS.",
        "In 2001, Be's copyrights were sold to Palm, Inc. for some $11 million. BeOS R5 is considered the last official version, but BeOS R5.1 \"Dano\", which was under development before Be's sale to Palm and included the BeOS Networking Environment (BONE) networking stack, was leaked to the public shortly after the company's demise.\n\nIn 2002, Be Inc. sued Microsoft claiming that Hitachi had been dissuaded from selling PCs loaded with BeOS, and that Compaq had been pressured not to market an Internet appliance in partnership with Be. Be also claimed that Microsoft acted to artificially depress Be Inc.'s initial public offering (IPO). The case was eventually settled out of court for $23.25 million with no admission of liability on Microsoft's part.",
        "After the split from Palm, PalmSource used parts of BeOS's multimedia framework for its failed Palm OS Cobalt product. With the takeover of PalmSource, the BeOS rights now belong to Access Co.\n\nVersion history\n\nFeatures\n\nBeOS was built for digital media work and was written to take advantage of modern hardware facilities such as symmetric multiprocessing by utilizing modular I/O bandwidth, pervasive multithreading, preemptive multitasking and a 64-bit journaling file system known as BFS. The BeOS GUI was developed on the principles of clarity and a clean, uncluttered design.",
        "The API was written in C++ for ease of programming. The GUI was largely multithreaded: each window ran in its own thread, relying heavily on sending messages to communicate between threads; and these concepts are reflected into the API.\n\nIt has partial POSIX compatibility and access to a command-line interface through Bash, although internally it is not a Unix-derived operating system. Many Unix applications were ported to the BeOS command-line interface.\n\nBeOS uses Unicode as the default encoding in the GUI, though support for input methods such as bidirectional text input was never realized.\n\nLegacy",
        "BeOS uses Unicode as the default encoding in the GUI, though support for input methods such as bidirectional text input was never realized.\n\nLegacy\n\nProducts\nBeOS (and now Zeta) have been used in media appliances, such as the Edirol DV-7 video editors from Roland Corporation, which run on top of a modified BeOS and the Tunetracker Radio Automation software that used to run it on BeOS and Zeta, and it was also sold as a \"Station-in-a-Box\" with the Zeta operating system included. In 2015, Tunetracker released a Haiku distribution bundled with its broadcasting software.\n\nThe Tascam SX-1 digital audio recorder runs a heavily modified version of BeOS that will only launch the recording interface software.",
        "The Tascam SX-1 digital audio recorder runs a heavily modified version of BeOS that will only launch the recording interface software.\n\nThe RADAR 24, RADAR V and RADAR 6, hard disk-based, 24-track professional audio recorders from iZ Technology Corporation were based on BeOS 5.\n\nMagicbox, a manufacturer of signage and broadcast display machines, uses BeOS to power their Aavelin product line.\n\nFinal Scratch, a 12-inch vinyl timecode record-driven DJ software and hardware system, was first developed on BeOS. The \"ProFS\" version was sold to a few dozen DJs prior to the 1.0 release, which ran on a Linux virtual partition.\n\nContinuation",
        "Continuation\n\nAfter the closing of Be Inc., a few projects formed to recreate BeOS or its key elements with the eventual goal of then continuing where Be Inc. left off. This was facilitated by Be Inc. having released some components of BeOS under a free license.\n\nHaiku is a complete reimplementation of BeOS not based on Linux. Unlike Cosmoe and BlueEyedOS, it is directly compatible with BeOS applications.  It is open source software. As of 2022, it was the only BeOS clone still under development, with the fourth beta (December 2022) still keeping BeOS 5 compatibility in its x86 32-bit images, with an increased number of modern drivers and GTK apps ported.",
        "Zeta is a commercially available operating system based on the BeOS R5.1 codebase. Originally developed by yellowTAB, the operating system was then distributed by magnussoft. During development by yellowTAB, the company received criticism from the BeOS community for refusing to discuss its legal position with regard to the BeOS codebase (perhaps for contractual reasons). Access Co. (which bought PalmSource, until then the holder of the intellectual property associated with BeOS) has since declared that yellowTAB had no right to distribute a modified version of BeOS, and magnussoft has ceased distribution of the operating system.\n\nSee also\n Access Co.\n BeIA\n Comparison of operating systems\n Gobe Productive\n Hitachi Flora Prius\n\nReferences\n\nFurther reading",
        "See also\n Access Co.\n BeIA\n Comparison of operating systems\n Gobe Productive\n Hitachi Flora Prius\n\nReferences\n\nFurther reading\n\nExternal links \n The Dawn of Haiku, by Ryan Leavengood, IEEE Spectrum May 2012, p 40–43,51-54.\n Mirror of the old www.be.com site Other Mirror of the old www.be.com site\n BeOS Celebrating Ten Years\n BeGroovy  A blog dedicated to all things BeOS \n BeOS: The Mac OS X might-have-been, reghardware.co.uk\n Programming the Be Operating System: An O'Reilly Open Book (out of print, but can be downloaded)\n \n  (BeOS)\n\n \nDiscontinued operating systems\nObject-oriented operating systems\nPowerPC operating systems\nX86 operating systems"
    ],
    [
        "Berkeley DB\nBerkeley DB (BDB) is an unmaintained embedded database software library for key/value data, historically significant in open source software. Berkeley DB is written in C with API bindings for many other programming languages. BDB stores arbitrary key/data pairs as byte arrays, and supports multiple data items for a single key. Berkeley DB is not a relational database, although it has database features including database transactions, multiversion concurrency control and write-ahead logging. BDB runs on a wide variety of operating systems including most Unix-like and Windows systems, and real-time operating systems.",
        "BDB was commercially supported and developed by Sleepycat Software from 1996 to 2006.  Sleepycat Software was acquired by Oracle Corporation in February 2006, who continued to develop and sell the C Berkeley DB library. In 2013 Oracle re-licensed BDB under the AGPL license. and released new versions until May 2020. Bloomberg LP continues to develop a fork of the 2013 version of BDB within their Comdb2 database, under the original Sleepycat permissive license.",
        "Origin",
        "Berkeley DB originated at the University of California, Berkeley as part of BSD, Berkeley's version of the Unix operating system. After 4.3BSD (1986), the BSD developers attempted to remove or replace all code originating in the original AT&T Unix from which BSD was derived. In doing so, they needed to rewrite the Unix database package. Seltzer and Yigit created a new database, unencumbered by any AT&T patents: an on-disk hash table that outperformed the existing dbm libraries. Berkeley DB itself was first released in 1991 and later included with 4.4BSD. In 1996 Netscape requested that the authors of Berkeley DB improve and extend the library, then at version 1.86, to suit Netscape's requirements for an LDAP server and for use in the Netscape browser. That request led to the creation of",
        "of Berkeley DB improve and extend the library, then at version 1.86, to suit Netscape's requirements for an LDAP server and for use in the Netscape browser. That request led to the creation of Sleepycat Software. This company was acquired by Oracle Corporation in February 2006.",
        "Berkeley DB 1.x releases focused on managing key/value data storage and are referred to as \"Data Store\" (DS).  The 2.x releases added a locking system enabling concurrent access to data.  This is what is known as \"Concurrent Data Store\" (CDS).  The 3.x releases added a logging system for transactions and recovery, called \"Transactional Data Store\" (TDS).  The 4.x releases added the ability to replicate log records and create a distributed highly available single-master multi-replica database.  This is called the \"High Availability\" (HA) feature set.  Berkeley DB's evolution has sometimes led to minor API changes or log format changes, but very rarely have database formats changed.  Berkeley DB HA supports online upgrades from one version to the next by maintaining the ability to read and",
        "to minor API changes or log format changes, but very rarely have database formats changed.  Berkeley DB HA supports online upgrades from one version to the next by maintaining the ability to read and apply the prior release's log records.",
        "Starting with the 6.0.21 (Oracle 12c) release, all Berkeley DB products are licensed under the GNU AGPL. Previously, Berkeley DB was redistributed under the 4-clause BSD license (before version 2.0), and the Sleepycat Public License, which is an OSI-approved open-source license as well as an FSF-approved free software license. The product ships with complete source code, build script, test suite, and documentation. The comprehensive feature along with the licensing terms have led to its use in a multitude of free and open-source software. Those who do not wish to abide by the terms of the GNU AGPL, or use an older version with the Sleepycat Public License, have the option of purchasing another proprietary license for redistribution from Oracle Corporation. This technique is called dual",
        "AGPL, or use an older version with the Sleepycat Public License, have the option of purchasing another proprietary license for redistribution from Oracle Corporation. This technique is called dual licensing.",
        "Berkeley DB includes compatibility interfaces for some historic Unix database libraries: dbm, ndbm and hsearch (a System V and POSIX library for creating in-memory hash tables).\n\nArchitecture\nBerkeley DB has an architecture notably simpler than relational database management systems. Like SQLite and LMDB, it is not based on a server/client model, and does not provide support for network access programs access the database using in-process API calls. Oracle added support for SQL in 11g R2 release based on the popular SQLite API by including a version of SQLite in Berkeley DB (it uses Berkeley DB for storage).",
        "A program accessing the database is free to decide how the data is to be stored in a record. Berkeley DB puts no constraints on the record's data. The record and its key can both be up to four gigabytes long.\n\nBerkeley DB supports database features such as ACID transactions, fine-grained locking, hot backups and replication.\n\nOracle Corporation use of  name \"Berkeley DB\"\nThe name \"Berkeley DB\" is used by Oracle Corporation for three different products, only one of which is BDB:\n\n Berkeley DB, the C database library that is the subject of this article\n Berkeley DB Java Edition, a pure Java library whose design is modelled after the C library but is otherwise unrelated\n Berkeley DB XML, a C++ program that supports XQuery, and which includes a legacy version of the C database library",
        "Open source programs still using Berkeley DB\nBDB was once very widespread, but usage dropped steeply from 2013 (see licensing section). Notable software that still uses Berkeley DB for data storage include:\n Bogofilter – A free/open source spam filter that saves its wordlists using Berkeley DB by default\n Sendmail – A free/open source MTA first released in 1983 for Linux/Unix systems and no longer widely used\n Spamassassin – A free/open source anti-spam application\n The Elixir Cross Referencer – A web UI source code cross-referencer for C/C++ written in Python",
        "Open source operating systems, and languages such as Perl and Python still support old BerkelyDB interfaces. The FreeBSD and OpenBSD operating systems ship Berkeley DB 1.8x to support the dbopen() operating system call used by password programs such as pwb_mkdb. Linux operating systems including those based on Debian and Fedora ship Berkeley DB 5.3 libraries.\n\nLicensing\n\nBerkeley DB V2.0 and higher is available under a dual license:\n\n Oracle commercial license \n The GNU AGPL v3.",
        "Licensing\n\nBerkeley DB V2.0 and higher is available under a dual license:\n\n Oracle commercial license \n The GNU AGPL v3.\n\nSwitching the open source license in 2013 from the Sleepycat license to the AGPL had a major effect on open source software. Since BDB is a library, any application linking to it must be under an AGPL-compatible license. Many open source applications and all closed source applications would need to be relicensed to become AGPL-compatible, which was not acceptable to many developers and open source operating systems. By 2013 there were many alternatives to BDB, and Debian Linux was typical in their decision to completely phase out Berkeley DB, with a preference for the Lightning Memory-Mapped Database (LMDB).\n\nReferences\n\nExternal links",
        "References\n\nExternal links\n\n Oracle Berkeley DB\n Oracle Berkeley DB Downloads\n Oracle Berkeley DB Documentation\n Oracle Berkeley DB Licensing Information\n Licensing pitfalls for Oracle Technology Products\n Oracle Licensing Knowledge Net\n The Berkeley DB Book by Himanshu Yadava\n\nDatabase engines\nDatabase-related software for Linux\nEmbedded databases\nFree database management systems\nFree software programmed in C\nKey-value databases\nNoSQL\nOracle software\nStructured storage\nSoftware using the GNU AGPL license"
    ],
    [
        "Bidirectional text\nA bidirectional text contains two text directionalities, right-to-left (RTL) and left-to-right (LTR). It generally involves text containing different types of alphabets, but may also refer to boustrophedon, which is changing text direction in each row.\n\nMany computer programs fail to display bidirectional text correctly. For example, this page is mostly LTR English script, and here is the RTL Hebrew name Sarah: , spelled sin () on the right, resh () in the middle, and heh () on the left.",
        "Some so-called right-to-left scripts such as the Persian script and Arabic are mostly, but not exclusively, right-to-left—mathematical expressions, numeric dates and numbers bearing units are embedded from left to right. That also happens if text from a left-to-right language such as English is embedded in them; or vice versa, if Arabic is embedded in a left-to-right script such as English.\n\nBidirectional script support \n\nBidirectional script support is the capability of a computer system to correctly display bidirectional text. The term is often shortened to \"BiDi\" or \"bidi\".",
        "Early computer installations were designed only to support a single writing system, typically for left-to-right scripts based on the Latin alphabet only. Adding new character sets and character encodings enabled a number of other left-to-right scripts to be supported, but did not easily support right-to-left scripts such as Arabic or Hebrew, and mixing the two was not practical. Right-to-left scripts were introduced through encodings like ISO/IEC 8859-6 and ISO/IEC 8859-8, storing the letters (usually) in writing and reading order. It is possible to simply flip the left-to-right display order to a right-to-left display order, but doing this sacrifices the ability to correctly display left-to-right scripts. With bidirectional script support, it is possible to mix characters from different",
        "to a right-to-left display order, but doing this sacrifices the ability to correctly display left-to-right scripts. With bidirectional script support, it is possible to mix characters from different scripts on the same page, regardless of writing direction.",
        "In particular, the Unicode standard provides foundations for complete BiDi support, with detailed rules as to how mixtures of left-to-right and right-to-left scripts are to be encoded and displayed.\n\nUnicode bidi support",
        "Unicode bidi support \n\nThe Unicode standard calls for characters to be ordered 'logically', i.e. in the sequence they are intended to be interpreted, as opposed to 'visually', the sequence they appear. This distinction is relevant for bidi support because at any bidi transition, the visual presentation ceases to be the 'logical' one. Thus, in order to offer bidi support, Unicode prescribes an algorithm for how to convert the logical sequence of characters into the correct visual presentation. For this purpose, the Unicode encoding standard divides all its characters into one of four types: 'strong', 'weak', 'neutral', and 'explicit formatting'.",
        "Strong characters \nStrong characters are those with a definite direction. Examples of this type of character include most alphabetic characters, syllabic characters, Han ideographs, non-European or non-Arabic digits, and punctuation characters that are specific to only those scripts.\n\nWeak characters \nWeak characters are those with vague direction. Examples of this type of character include European digits, Eastern Arabic-Indic digits, arithmetic symbols, and currency symbols.\n\nNeutral characters \nNeutral characters have direction indeterminable without context. Examples include paragraph separators, tabs, and most other whitespace characters. Punctuation symbols that are common to many scripts, such as the colon, comma, full-stop, and the no-break-space also fall within this category.",
        "Explicit formatting \nExplicit formatting characters, also referred to as \"directional formatting characters\", are special Unicode sequences that direct the algorithm to modify its default behavior. These characters are subdivided into \"marks\", \"embeddings\", \"isolates\", and \"overrides\". Their effects continue until the occurrence of either a paragraph separator, or a \"pop\" character.\n\nMarks",
        "Marks \n\nIf a \"weak\" character is followed by another \"weak\" character, the algorithm will look at the first neighbouring \"strong\" character. Sometimes this leads to unintentional display errors. These errors are corrected or prevented with \"pseudo-strong\" characters. Such Unicode control characters are called marks. The mark ( or ) is to be inserted into a location to make an enclosed weak character inherit its writing direction.",
        "For example, to correctly display the  for an English name brand (LTR) in an Arabic (RTL) passage, an LRM mark is inserted after the trademark symbol if the symbol is not followed by LTR text (e.g. \"\"). If the LRM mark is not added, the weak character ™ will be neighbored by a strong LTR character and a strong RTL character. Hence, in an RTL context, it will be considered to be RTL, and displayed in an incorrect order (e.g. \"\").",
        "Embeddings \nThe \"embedding\" directional formatting characters are the classical Unicode method of explicit formatting, and as of Unicode 6.3, are being discouraged in favor of \"isolates\". An \"embedding\" signals that a piece of text is to be treated as directionally distinct. The text within the scope of the embedding formatting characters is not independent of the surrounding text. Also, characters within an embedding can affect the ordering of characters outside. Unicode 6.3 recognized that directional embeddings usually have too strong an effect on their surroundings and are thus unnecessarily difficult to use.",
        "Isolates \nThe \"isolate\" directional formatting characters signal that a piece of text is to be treated as directionally isolated from its surroundings. As of Unicode 6.3, these are the formatting characters that are being encouraged in new documents – once target platforms are known to support them. These formatting characters were introduced after it became apparent that directional embeddings usually have too strong an effect on their surroundings and are thus unnecessarily difficult to use. Unlike the legacy 'embedding' directional formatting characters, 'isolate' characters have no effect on the ordering of the text outside their scope. Isolates can be nested, and may be placed within embeddings and overrides.",
        "Overrides \nThe \"override\" directional formatting characters allow for special cases, such as for part numbers (e.g. to force a part number made of mixed English, digits and Hebrew letters to be written from right to left), and are recommended to be avoided wherever possible. As is true of the other directional formatting characters, \"overrides\" can be nested one inside another, and in embeddings and isolates.\n\nPops \nThe \"pop\" directional formatting characters terminate the scope of the most recent \"embedding\", \"override\", or \"isolate\".",
        "Pops \nThe \"pop\" directional formatting characters terminate the scope of the most recent \"embedding\", \"override\", or \"isolate\".\n\nRuns \nIn the algorithm, each sequence of concatenated strong characters is called a \"run\". A \"weak\" character that is located between two \"strong\" characters with the same orientation will inherit their orientation. A \"weak\" character that is located between two \"strong\" characters with a different writing direction will inherit the main context's writing direction (in an LTR document the character will become LTR, in an RTL document, it will become RTL).\n\nTable of possible BiDi character types\n\nSecurity \nUnicode bidirectional characters are used in the Trojan Source vulnerability.",
        "Table of possible BiDi character types\n\nSecurity \nUnicode bidirectional characters are used in the Trojan Source vulnerability.\n\nVisual Studio Code highlights BiDi control characters since version 1.62 released in October 2021.\n\nVisual Studio highlights BiDi control characters since version 17.0.3 released on December 14, 2021.\n\nScripts using bidirectional text\n\nEgyptian hieroglyphs \nEgyptian hieroglyphs were written bidirectionally, where the signs that had a distinct \"head\" or \"tail\" faced the beginning of the line.",
        "Chinese characters and other CJK scripts \nChinese characters can be written in either direction as well as vertically (top to bottom then right to left), especially in signs (such as plaques), but the orientation of the individual characters does not change. This can often be seen on tour buses in China, where the company name customarily runs from the front of the vehicle to its rear — that is, from right to left on the right side of the bus, and from left to right on the left side of the bus. English texts on the right side of the vehicle are also quite commonly written in reverse order. (See pictures of tour bus and post vehicle below.)",
        "Likewise, other CJK scripts made up of the same square characters, such as the Japanese writing system and Korean writing system, can also be written in any direction, although horizontally left-to-right, top-to-bottom and vertically top-to-bottom right-to-left are the two most common forms.\n\nBoustrophedon \nBoustrophedon is a writing style found in ancient Greek inscriptions, in Old Sabaic (an Old South Arabian language) and in Hungarian runes. This method of writing alternates direction, and usually reverses the individual characters, on each successive line.",
        "Moon type\nMoon type is an embossed adaptation of the Latin alphabet invented as a tactile alphabet for the blind.\nInitially the text changed direction (but not character orientation) at the end of the lines.\nSpecial embossed lines connected the end of a line and the beginning of the next.\nAround 1990, it changed to a left-to-right orientation.\n\nSee also \n Internationalization and localization\n Horizontal and vertical writing in East Asian scripts\n \n Combining Cyrillic Millions\n Right-to-left mark\n Transformation of text\n Boustrophedon\n\nReferences",
        "References\n\nExternal links \n Unicode Standards Annex #9 The Bidirectional Algorithm\n W3C guidelines on authoring techniques for bi-directional text - includes examples and good explanations\n ICU International Components for Unicode contains an implementation of the bi-directional algorithm — along with other internationalization services\n\nCharacter encoding\nUnicode algorithms\nInternationalization and localization\nWriting direction"
    ],
    [
        "Bilinear transform\nThe bilinear transform (also known as Tustin's method, after Arnold Tustin) is used in digital signal processing and discrete-time control theory to transform continuous-time system representations to discrete-time and vice versa.",
        "The bilinear transform is a special case of a conformal mapping (namely, a Möbius transformation), often used to convert a transfer function  of a linear, time-invariant (LTI) filter in the continuous-time domain (often called an analog filter) to a transfer function  of a linear, shift-invariant filter in the discrete-time domain (often called a digital filter although there are analog filters constructed with switched capacitors that are discrete-time filters). It maps positions on the  axis, , in the s-plane to the unit circle, , in the z-plane.  Other bilinear transforms can be used to warp the frequency response of any discrete-time linear system (for example to approximate the non-linear frequency resolution of the human auditory system) and are implementable in the discrete domain",
        "the frequency response of any discrete-time linear system (for example to approximate the non-linear frequency resolution of the human auditory system) and are implementable in the discrete domain by replacing a system's unit delays  with first order all-pass filters.",
        "The transform preserves stability and maps every point of the frequency response of the continuous-time filter,  to a corresponding point in the frequency response of the discrete-time filter,  although to a somewhat different frequency, as shown in the Frequency warping section below.  This means that for every feature that one sees in the frequency response of the analog filter, there is a corresponding feature, with identical gain and phase shift, in the frequency response of the digital filter but, perhaps, at a somewhat different frequency.  This is barely noticeable at low frequencies but is quite evident at frequencies close to the Nyquist frequency.",
        "Discrete-time approximation \nThe bilinear transform is a first-order Padé approximant of the natural logarithm function that is an exact mapping of the z-plane to the s-plane.  When the Laplace transform is performed on a discrete-time signal (with each element of the discrete-time sequence attached to a correspondingly delayed unit impulse), the result is precisely the Z transform of the discrete-time sequence with the substitution of\n\nwhere  is the numerical integration step size of the trapezoidal rule used in the bilinear transform derivation; or, in other words, the sampling period. The above bilinear approximation can be solved for  or a similar approximation for  can be performed.\n\nThe inverse of this mapping (and its first-order bilinear approximation) is",
        "The inverse of this mapping (and its first-order bilinear approximation) is\n\nThe bilinear transform essentially uses this first order approximation and substitutes into the continuous-time transfer function, \n\nThat is\n\nStability and minimum-phase property preserved \nA continuous-time causal filter is stable if the poles of its transfer function fall in the left half of the complex s-plane. A discrete-time causal filter is stable if the poles of its transfer function fall inside the unit circle in the complex z-plane. The bilinear transform maps the left half of the complex s-plane to the interior of the unit circle in the z-plane. Thus, filters designed in the continuous-time domain that are stable are converted to filters in the discrete-time domain that preserve that stability.",
        "Likewise, a continuous-time filter is minimum-phase if the zeros of its transfer function fall in the left half of the complex s-plane. A discrete-time filter is minimum-phase if the zeros of its transfer function fall inside the unit circle in the complex z-plane. Then the same mapping property assures that continuous-time filters that are minimum-phase are converted to discrete-time filters that preserve that property of being minimum-phase.\n\nTransformation of a General LTI System \nA general LTI system has the transfer function\n\nThe order of the transfer function  is the greater of  and  (in practice this is most likely  as the transfer function must be proper for the system to be stable). Applying the bilinear transform",
        "The order of the transfer function  is the greater of  and  (in practice this is most likely  as the transfer function must be proper for the system to be stable). Applying the bilinear transform\n\nwhere  is defined as either  or otherwise if using frequency warping, gives\n\nMultiplying the numerator and denominator by the largest power of  present, , gives\n\nIt can be seen here that after the transformation, the degree of the numerator and denominator are both .\n\nConsider then the pole-zero form of the continuous-time transfer function\n\nThe roots of the numerator and denominator polynomials,  and , are the zeros and poles of the system. The bilinear transform is a one-to-one mapping, hence these can be transformed to the z-domain using",
        "yielding some of the discretized transfer function's zeros and poles  and \n\nAs described above, the degree of the numerator and denominator are now both , in other words there is now an equal number of zeros and poles. The multiplication by  means the additional zeros or poles are\n\nGiven the full set of zeros and poles, the z-domain transfer function is then\n\nExample \nAs an example take a simple low-pass RC filter.  This continuous-time filter has a transfer function\n\nIf we wish to implement this filter as a digital filter, we can apply the bilinear transform by substituting for  the formula above; after some reworking, we get the following filter representation:\n\n{|\n|-\n|\n|\n|-\n|\n|\n|-\n|\n|\n|-\n|\n|\n|}",
        "{|\n|-\n|\n|\n|-\n|\n|\n|-\n|\n|\n|-\n|\n|\n|}\n\nThe coefficients of the denominator are the 'feed-backward' coefficients and the coefficients of the numerator are the 'feed-forward' coefficients used to implement a real-time digital filter.\n\nTransformation for a general first-order continuous-time filter \nIt is possible to relate the coefficients of a continuous-time, analog filter with those of a similar discrete-time digital filter created through the bilinear transform process. Transforming a general, first-order continuous-time filter with the given transfer function\n\nusing the bilinear transform (without prewarping any frequency specification) requires the substitution of\n\nwhere\n\n.",
        "using the bilinear transform (without prewarping any frequency specification) requires the substitution of\n\nwhere\n\n.\n\nHowever, if the frequency warping compensation as described below is used in the bilinear transform, so that both analog and digital filter gain and phase agree at frequency , then\n\n.\n\nThis results in a discrete-time digital filter with coefficients expressed in terms of the coefficients of the original continuous time filter:\n\nNormally the constant term in the denominator must be normalized to 1 before deriving the corresponding difference equation.  This results in\n\nThe difference equation (using the Direct form I) is\n\nGeneral second-order biquad transformation \nA similar process can be used for a general second-order filter with the given transfer function",
        "The difference equation (using the Direct form I) is\n\nGeneral second-order biquad transformation \nA similar process can be used for a general second-order filter with the given transfer function\n\nThis results in a discrete-time digital biquad filter with coefficients expressed in terms of the coefficients of the original continuous time filter:\n\nAgain, the constant term in the denominator is generally normalized to 1 before deriving the corresponding difference equation.  This results in\n\nThe difference equation (using the Direct form I) is",
        "The difference equation (using the Direct form I) is\n\nFrequency warping \nTo determine the frequency response of a continuous-time filter, the transfer function  is evaluated at  which is on the  axis.  Likewise, to determine the frequency response of a discrete-time filter, the transfer function  is evaluated at  which is on the unit circle, .  The bilinear transform maps the  axis of the s-plane (of which is the domain of ) to the unit circle of the z-plane,  (which is the domain of ), but it is not the same mapping  which also maps the  axis to the unit circle.  When the actual frequency of  is input to the discrete-time filter designed by use of the bilinear transform, then it is desired to know at what frequency, , for the continuous-time filter that this  is mapped to.",
        "{|\n|-\n|\n|\n|-\n|\n|\n|-\n|\n|\n|-\n|\n|\n|-\n|\n|\n|-\n|\n|\n|}\n\nThis shows that every point on the unit circle in the discrete-time filter z-plane,  is mapped to a point on the  axis on the continuous-time filter s-plane, . That is, the discrete-time to continuous-time frequency mapping of the bilinear transform is\n\nand the inverse mapping is",
        "and the inverse mapping is\n\nThe discrete-time filter behaves at frequency  the same way that the continuous-time filter behaves at frequency .  Specifically, the gain and phase shift that the discrete-time filter has at frequency  is the same gain and phase shift that the continuous-time filter has at frequency .  This means that every feature, every \"bump\" that is visible in the frequency response of the continuous-time filter is also visible in the discrete-time filter, but at a different frequency.  For low frequencies (that is, when  or ), then the features are mapped to a slightly different frequency; .\n\nOne can see that the entire continuous frequency range\n\n \n\nis mapped onto the fundamental frequency interval",
        "One can see that the entire continuous frequency range\n\n \n\nis mapped onto the fundamental frequency interval\n\n \n\nThe continuous-time filter frequency  corresponds to the discrete-time filter frequency  and the continuous-time filter frequency  correspond to the discrete-time filter frequency \n\nOne can also see that there is a nonlinear relationship between  and   This effect of the bilinear transform is called frequency warping. The continuous-time filter can be designed to compensate for this frequency warping by setting  for every frequency specification that the designer has control over (such as corner frequency or center frequency).  This is called pre-warping the filter design.",
        "It is possible, however, to compensate for the frequency warping by pre-warping a frequency specification  (usually a resonant frequency or the frequency of the most significant feature of the frequency response) of the continuous-time system.  These pre-warped specifications may then be used in the bilinear transform to obtain the desired discrete-time system. When designing a digital filter as an approximation of a continuous time filter, the frequency response (both amplitude and phase) of the digital filter can be made to match the frequency response of the continuous filter at a specified frequency , as well as matching at DC, if the following transform is substituted into the continuous filter transfer function. This is a modified version of Tustin's transform shown above.",
        "However, note that this transform becomes the original transform\n\nas .\n\nThe main advantage of the warping phenomenon is the absence of aliasing distortion of the frequency response characteristic, such as observed with Impulse invariance.\n\nSee also\n Impulse invariance\n Matched Z-transform method\n\nReferences\n\nExternal links\n MIT OpenCourseWare Signal Processing: Continuous to Discrete Filter Design\n Lecture Notes on Discrete Equivalents\n The Art of VA Filter Design\n\nDigital signal processing\nTransforms\nControl theory"
    ],
    [
        "Bistability\nIn a dynamical system, bistability means the system has two stable equilibrium states. A bistable structure can be resting in either of two states.   An example of a mechanical device which is bistable is a light switch. The switch lever is designed to rest in the \"on\" or \"off\" position, but not between the two.  Bistable behavior can occur in mechanical linkages, electronic circuits, nonlinear optical systems, chemical reactions, and physiological and biological systems.",
        "In a conservative force field, bistability stems from the fact that the potential energy has two local minima, which are the stable equilibrium points. These rest states need not have equal potential energy.  By mathematical arguments, a local maximum, an unstable equilibrium point, must lie between the two minima.  At rest, a particle will be in one of the minimum equilibrium positions, because that corresponds to the state of lowest energy. The maximum can be visualized as a barrier between them.",
        "A system can transition from one state of minimal energy to the other if it is given enough activation energy to penetrate the barrier (compare activation energy and Arrhenius equation for the chemical case). After the barrier has been reached, assuming the system has damping, it will relax into the other minimum state in a time called the relaxation time.",
        "Bistability is widely used in digital electronics devices to store binary data. It is the essential characteristic of the flip-flop, a circuit which is a fundamental building block of computers and some types of semiconductor memory.  A bistable device can store one bit of binary data, with one state representing a \"0\" and the other state a \"1\".  It is also used in relaxation oscillators, multivibrators, and the Schmitt trigger.\nOptical bistability is an attribute of certain optical devices where two resonant transmissions states are possible and stable, dependent on the input.\nBistability can also arise in biochemical systems, where it creates digital, switch-like outputs from the constituent chemical concentrations and activities. It is often associated with hysteresis in such systems.",
        "Mathematical modelling\n\nIn the mathematical language of dynamic systems analysis, one of the simplest bistable systems is\n\nThis system describes a ball rolling down a curve with shape , and has three equilibrium points: , , and . The middle point  is marginally stable ( is stable but  will not converge to ), while the other two points are stable. The direction of change of  over time depends on the initial condition .  If the initial condition is positive (), then the solution  approaches 1 over time, but if the initial condition is negative (), then  approaches −1 over time. Thus, the dynamics are \"bistable\". The final state of the system can be either  or , depending on the initial conditions.\n\nThe appearance of a bistable region can be understood for the model system",
        "The appearance of a bistable region can be understood for the model system\n\nwhich undergoes a supercritical pitchfork bifurcation with bifurcation parameter .\n\nIn biological and chemical systems\n\nBistability is key for understanding basic phenomena of cellular functioning, such as decision-making processes in cell cycle progression, cellular differentiation, and apoptosis. It is also involved in loss of cellular homeostasis associated with early events in cancer onset and in prion diseases as well as in the origin of new species (speciation).",
        "Bistability can be generated by a positive feedback loop with an ultrasensitive regulatory step.  Positive feedback loops, such as the simple X activates Y and Y activates X motif, essentially link output signals to their input signals and have been noted to be an important regulatory motif in cellular signal transduction because positive feedback loops can create switches with an all-or-nothing decision. Studies have shown that numerous biological systems, such as Xenopus oocyte maturation, mammalian calcium signal transduction, and polarity in budding yeast, incorporate multiple positive feedback loops with different time scales (slow and fast). Having multiple linked positive feedback loops with different time scales (\"dual-time switches\") allows for (a) increased regulation: two",
        "feedback loops with different time scales (slow and fast). Having multiple linked positive feedback loops with different time scales (\"dual-time switches\") allows for (a) increased regulation: two switches that have independent changeable activation and deactivation times; and (b) noise filtering.",
        "Bistability can also arise in a biochemical system only for a particular range of parameter values, where the parameter can often be interpreted as the strength of the feedback. In several typical examples, the system has only one stable fixed point at low values of the parameter. A saddle-node bifurcation gives rise to a pair of new fixed points emerging, one stable and the other unstable, at a critical value of the parameter. The unstable solution can then form another saddle-node bifurcation with the initial stable solution at a higher value of the parameter, leaving only the higher fixed solution. Thus, at values of the parameter between the two critical values, the system has two stable solutions. An example of a dynamical system that demonstrates similar features is",
        "where  is the output, and  is the parameter, acting as the input.\n\nBistability can be modified to be more robust and to tolerate significant changes in concentrations of reactants, while still maintaining its \"switch-like\" character.  Feedback on both the activator of a system and inhibitor make the system able to tolerate a wide range of concentrations.  An example of this in cell biology is that activated CDK1 (Cyclin Dependent Kinase 1) activates its activator Cdc25 while at the same time inactivating its inactivator, Wee1, thus allowing for progression of a cell into mitosis.  Without this double feedback, the system would still be bistable, but would not be able to tolerate such a wide range of concentrations.",
        "Bistability has also been described in the embryonic development of Drosophila melanogaster (the fruit fly). Examples are anterior-posterior and dorso-ventral axis formation and eye development.",
        "A prime example of bistability in biological systems is that of Sonic hedgehog (Shh), a secreted signaling molecule, which plays a critical role in development. Shh functions in diverse processes in development, including patterning limb bud tissue differentiation. The Shh signaling network behaves as a bistable switch, allowing the cell to abruptly switch states at precise Shh concentrations. gli1 and gli2 transcription is activated by Shh, and their gene products act as transcriptional activators for their own expression and for targets downstream of Shh signaling. Simultaneously, the Shh signaling network is controlled by a negative feedback loop wherein the Gli transcription factors activate the enhanced transcription of a repressor (Ptc). This signaling network illustrates the",
        "Shh signaling network is controlled by a negative feedback loop wherein the Gli transcription factors activate the enhanced transcription of a repressor (Ptc). This signaling network illustrates the simultaneous positive and negative feedback loops whose exquisite sensitivity helps create a bistable switch.",
        "Bistability can only arise in biological and chemical systems if three necessary conditions are fulfilled: positive feedback, a mechanism to filter out small stimuli and a mechanism to prevent increase without bound.\n\nBistable chemical systems have been studied extensively to analyze relaxation kinetics, non-equilibrium thermodynamics, stochastic resonance, as well as climate change. In bistable spatially extended systems the onset of local correlations and propagation of traveling waves have been analyzed.",
        "Bistability is often accompanied by hysteresis. On a population level, if many realisations of a bistable system are considered (e.g. many bistable cells (speciation)), one typically observes bimodal distributions. In an ensemble average over the population, the result may simply look like a smooth transition, thus showing the value of single-cell resolution.\n\nA specific type of instability is known as modehopping, which is bi-stability in the frequency space. Here trajectories can shoot between two stable limit cycles, and thus show similar characteristics as normal bi-stability when measured inside a Poincare section.\n\nIn mechanical systems",
        "In mechanical systems \n\nBistability as applied in the design of mechanical systems is more commonly said to be \"over centre\"—that is, work is done on the system to move it just past the peak, at which point the mechanism goes \"over centre\" to its secondary stable position. The result is a toggle-type action- work applied to the system below a threshold sufficient to send it 'over center' results in no change to the mechanism's state.\n\nSprings are a common method of achieving an \"over centre\" action. A spring attached to a simple two position ratchet-type mechanism can create a button or plunger that is clicked or toggled between two mechanical states. Many ballpoint and rollerball retractable pens employ this type of bistable mechanism.",
        "An even more common example of an over-center device is an ordinary electric wall switch.  These switches are often designed to snap firmly into the \"on\" or \"off\" position once the toggle handle has been moved a certain distance past the center-point.\n\nA ratchet-and-pawl is an elaboration—a multi-stable \"over center\" system used to create irreversible motion.  The pawl goes over center as it is turned in the forward direction. In this case, \"over center\" refers to the ratchet being stable and \"locked\" in a given position until clicked forward again; it has nothing to do with the ratchet being unable to turn in the reverse direction.\n\nGallery",
        "Gallery\n\nSee also\n Multistability – the generalized case of more than two stable points\n In psychology\n ferroelectric, ferromagnetic, hysteresis, bistable perception\n Schmitt trigger\n strong Allee effect\n Interferometric modulator display, a bistable reflective display technology found in mirasol displays by Qualcomm\n\nReferences\n\nExternal links\n BiStable Reed Sensor\n\nDigital electronics\n2 (number)\n\nes:Biestable"
    ],
    [
        "Blitz BASIC\nBlitz BASIC is the programming language dialect of the first Blitz compilers, devised by New Zealand-based developer Mark Sibly. Being derived from BASIC, Blitz syntax was designed to be easy to pick up for beginners first learning to program. The languages are game-programming oriented but are often found general purpose enough to be used for most types of application. The Blitz language evolved as new products were released, with recent incarnations offering support for more advanced programming techniques such as object-orientation and multithreading. This led to the languages losing their BASIC moniker in later years.\n\nHistory",
        "History\n\nThe first iteration of the Blitz language was created for the Amiga platform and published by the Australian firm Memory and Storage Technology. Returning to New Zealand, Blitz BASIC 2 was published several years later (around 1993 according this press release ) by Acid Software (a local Amiga game publisher). Since then, Blitz compilers have been released on several platforms. Following the demise of the Amiga as a commercially viable platform, the Blitz BASIC 2 source code was released to the Amiga community. Development continues to this day under the name AmiBlitz.",
        "BlitzBasic\nIdigicon published BlitzBasic for Microsoft Windows in October 2000. The language included a built-in API for performing basic 2D graphics and audio operations. Following the release of Blitz3D, BlitzBasic is often synonymously referred to as Blitz2D.\n\nRecognition of BlitzBasic increased when a limited range of \"free\" versions were distributed in popular UK computer magazines such as PC Format. This resulted in a legal dispute between the developer and publisher which was eventually resolved amicably.",
        "BlitzPlus\nIn February 2003, Blitz Research Ltd. released BlitzPlus also for Microsoft Windows. It lacked the 3D engine of Blitz3D, but did bring new features to the 2D side of the language by implementing limited Microsoft Windows control support for creating native GUIs. Backwards compatibility of the 2D engine was also extended, allowing compiled BlitzPlus games and applications to run on systems that might only have DirectX 1.\n\nBlitzMax",
        "BlitzMax\n\nThe first BlitzMax compiler was released in December 2004 for Mac OS X. This made it the first Blitz dialect that could be compiled on *nix platforms. Compilers for Microsoft Windows and Linux were subsequently released in May 2005. BlitzMax brought the largest change of language structure to the modern range of Blitz products by extending the type system to include object-oriented concepts and modifying the graphics API to better suit OpenGL. BlitzMax was also the first of the Blitz languages to represent strings internally using UCS-2, allowing native-support for string literals composed of non-ASCII characters.",
        "BlitzMax's platform-agnostic command-set allows developers to compile and run source code on multiple platforms. However the official compiler and build chain will only generate binaries for the platform that it is executing on. Unofficially, users have been able to get Linux and Mac OS X to cross-compile to the Windows platform.",
        "BlitzMax is also the first modular version of the Blitz languages, improving the extensibility of the command-set. In addition, all of the standard modules shipped with the compiler are open-source and so can be tweaked and recompiled by the programmer if necessary. The official BlitzMax cross-platform GUI module (known as MaxGUI) allows developers to write GUI interfaces for their applications on Linux (FLTK), Mac (Cocoa) and Windows. Various user-contributed modules extend the use of the language by wrapping such libraries as wxWidgets, Cairo, and Fontconfig as well as a selection of database modules. There are also a selection of third-party 3D modules available namely MiniB3D - an open-source OpenGL engine which can be compiled and used on all three of BlitzMax's supported platforms.",
        "In October 2007, BlitzMax 1.26 was released which included the addition of a reflection module. BlitzMax 1.32 shipped new threading and Lua scripting modules and most of the standard library functions have been updated so that they are unicode friendly.\n\nBlitz3D SDK\nBlitz3D SDK is a 3D graphics engine based on the engine in Blitz3D. It was marketed for use with C++, C#, BlitzMax, and PureBasic, however it could also be used with other languages that follow compatible calling conventions.\n\nMax3D module\nIn 2008, the source code to Max3D - a C++-based cross-platform 3D engine - was released under a BSD license. This engine focused on OpenGL but had an abstract backend for other graphics drivers (such as DirectX) and made use of several open-source libraries, namely Assimp, Boost, and ODE.",
        "Despite the excitement in the Blitz community of Max3D being the eagerly awaited successor to Blitz3D, interest and support died off soon after the source code was released and eventually development came to a halt. There is no indication that Blitz Research will pick up the project again.\n\nOpen-source release\nBlitzPlus was released as open-source on 28 April 2014 under the zlib license on GitHub. Blitz3D followed soon after and was released as Open Source on 3 August 2014. BlitzMax was later released as Open Source on 21 September 2015.",
        "Examples\nHello World program that prints to the screen, waits until a key is pressed, and then terminates: Print \"Hello World\"   ; Prints to the screen.\nWaitKey()             ; Pauses execution until a key is pressed.\nEnd                   ; Ends Program.Program that demonstrates the declaration of variables using the three main data types (strings, integers and floats) and printing them onto the screen:name$        = \"John\"   ; Create a string variable ($) \nage          = 36       ; Create an integer variable (No Suffix)\ntemperature# = 27.3     ; Create a float variable (#)\n\nprint \"My name is \" + name$ + \" and I am \" + age + \" years old.\"\nprint \"Today, the temperature is \" + temperature# + \" degrees.\"",
        "print \"My name is \" + name$ + \" and I am \" + age + \" years old.\"\nprint \"Today, the temperature is \" + temperature# + \" degrees.\"\n\nWaitkey()               ; Pauses execution until a key is pressed.\nEnd                     ; Ends program.\n\nProgram that creates a windowed application that shows the current time in binary and decimal format. See below for the BlitzMax and BlitzBasic versions:\n\nSoftware written using BlitzBasic\n\nEschalon: Book I - BlitzMax\nEschalon: Book II - BlitzMax\nFairway Solitaire - BlitzMax\nGridWars - BlitzMax\nTVTower (open source clone of MadTV) - BlitzMax\nPlatypus - Blitz2D (Mac port, BlitzMax)\nSCP – Containment Breach - Blitz3D\nWorms - originally titled Total Wormage and developed in Blitz Basic on the Amiga before its commercial release",
        "Legacy\nIn 2011, BRL released a new cross-platform programming language called Monkey and its first official module called Mojo. Monkey has a similar syntax to BlitzMax, but instead of compiling direct to assembly code, it translates Monkey source files directly into source code for a chosen language, framework or platform e.g. Windows, Mac OS X, iOS, Android, HTML5, and Adobe Flash.\n\nSince 2015 development of Monkey X has been halted in favour of Monkey 2, an updated version of the language by Mark Sibly.\n\nReferences",
        "Since 2015 development of Monkey X has been halted in favour of Monkey 2, an updated version of the language by Mark Sibly.\n\nReferences\n\nExternal links\nBlitz Research subsite on itch.io (BlitzPlus, Blitz 3D, Monkey X, Monkey 2)\nMonkey X subsite (open source)\nMonkey 2 subsite\nblitz-research (Mark Sibly) on GitHub (BlitzPlus, BlitzMax, Blitz3D, Monkey, BlitzMax, Blitz3D for MSVC-CE 2017)\nBlitz Research website (archived 3 June 2017)\nMonkey X website (archived 15 July 2017)\n\nAmiga development software\nArticles with example BASIC code\nBASIC compilers\nBASIC programming language family\nFormerly proprietary software\nFree game engines\nFree software\nObject-oriented programming languages\nSoftware using the zlib license\nVideo game development software\nVideo game IDE"
    ],
    [
        "Blizzard Entertainment\nBlizzard Entertainment, Inc. is an American video game developer and publisher based in Irvine, California. A subsidiary of Activision Blizzard, the company was founded in February 1991 as Silicon & Synapse, Inc. by three graduates of the University of California, Los Angeles: Michael Morhaime, Frank Pearce and Allen Adham. The company originally concentrated on the creation of game ports for other studios' games before beginning development of their own software in 1993, with games like Rock n' Roll Racing and The Lost Vikings. In 1993, the company became Chaos Studios, Inc., and then Blizzard Entertainment soon after being acquired by distributor Davidson & Associates early in the following year. Shortly after, Blizzard released Warcraft: Orcs & Humans.",
        "Since then, Blizzard Entertainment has created several Warcraft sequels, including highly influential massively multiplayer online role-playing game World of Warcraft in 2004, as well as three other multi-million selling video game franchises: Diablo, StarCraft and Overwatch. Their most recent projects include the online collectible card game Hearthstone; the multiplayer online battle arena Heroes of the Storm; the remaster of the original StarCraft and its expansion Brood War, StarCraft: Remastered; the replacement and sequel to the multiplayer first-person hero shooter Overwatch, Overwatch 2; the ninth expansion for World of Warcraft, Dragonflight; and the sequel to Diablo III, Diablo IV. The games operate through Blizzard's online gaming service Battle.net.",
        "On July 9, 2008, Activision merged with Vivendi Games, culminating in the inclusion of the Blizzard brand name in the title of the resulting holding company. On July 25, 2013, Activision Blizzard announced the purchase of 429 million shares from the majority owner Vivendi, which resulted in Activision Blizzard becoming a completely independent company. Since 2018, the company's reputation has suffered from a series of poorly received games, controversies involving players and staff, and allegations of sexual harassment and other misconduct against leading Blizzard employees. Activision Blizzard was acquired by Microsoft in 2023.",
        "Blizzard Entertainment hosts annual gaming conventions for fans to meet and to promote their games: the first BlizzCon was held in October 2005 at the Anaheim Convention Center in Anaheim, California, which is where all of their conventions have been held since. BlizzCon features game-related announcements, previews of upcoming Blizzard Entertainment games and content, Q&A sessions and panels, costume contests, and playable demos of various Blizzard games. Blizzard WorldWide Invitationals were events similar to BlizzCon held in South Korea and France between 2004 and 2008.\n\nHistory\n\nFounding (1991–1994)",
        "Blizzard Entertainment was founded by Michael Morhaime, Allen Adham, and Frank Pearce as Silicon & Synapse in February 1991, after all three had earned their bachelor's degrees from the University of California, Los Angeles, the year prior. The name \"Silicon & Synapse\" was a high concept from the three founders, with \"silicon\" representing the building block of a computer, while \"synapse\" the building block of the brain. The initial logo was created by Stu Rose. To fund the company, each of them contributed about $10,000, Morhaime borrowing the sum interest-free from his grandmother. During the first two years, the company focused on creating game ports for other studios. Ports include titles such as J.R.R. Tolkien's The Lord of the Rings, Vol. I and Battle Chess II: Chinese Chess. In",
        "the first two years, the company focused on creating game ports for other studios. Ports include titles such as J.R.R. Tolkien's The Lord of the Rings, Vol. I and Battle Chess II: Chinese Chess. In 1993, the company developed games such as Rock n' Roll Racing and The Lost Vikings (published by Interplay Productions).",
        "Around 1993, co-founder Adham told the other executives that he did not like the name \"Silicon & Synapse\" anymore, as people outside the company were confusing the meaning of silicon the chemical element used in microchips with silicone the materials used in breast implants. By the end of 1993, Adham changed the name to \"Chaos Studios\", reflecting on the haphazardness of their development processes.",
        "In early 1994, they were acquired by distributor Davidson & Associates for $6.75 million ($ million today). Shortly after this point, they were contacted by a Florida company, Chaos Technologies, who wanted the company to pay  () to keep the name. Not wanting to pay that sum, the executives decided to change the studio's name to \"Ogre Studios\" by April 1994. However, Davidson & Associates did not like this name, and forced the company to change it. According to Morhaime, Adham began running through a dictionary from the start, writing down any word that seemed interesting and passing it to the legal department to see if it had any complications. One of the first words they found to be interesting and cleared the legal check was \"blizzard\", leading them to change their name to \"Blizzard",
        "the legal department to see if it had any complications. One of the first words they found to be interesting and cleared the legal check was \"blizzard\", leading them to change their name to \"Blizzard Entertainment\" by May 1994.",
        "Shortly thereafter, Blizzard Entertainment shipped their breakthrough hit Warcraft: Orcs & Humans, a real-time strategy (RTS) game in a high-fantasy setting.",
        "Acquisition by Vivendi and World of Warcraft (1995–2007)",
        "Blizzard Entertainment has changed hands several times since then. Davidson was acquired along with Sierra On-Line by a company called CUC International in 1996. CUC then merged with a hotel, real-estate, and car-rental franchiser called HFS Corporation to form Cendant in 1997. In 1998 it became apparent that CUC had engaged in accounting fraud for years before the merger. Cendant's stock lost 80% of its value over the next six months in the ensuing widely discussed accounting scandal. The company sold its consumer software operations, Sierra On-line (which included Blizzard) to French publisher Havas in 1998, the same year Havas was purchased by Vivendi. Blizzard, at this point numbering about 200 employees, became part of the Vivendi Games group of Vivendi.",
        "In 1996, Blizzard Entertainment acquired Condor Games of San Mateo, California, which had been working on the action role-playing game (ARPG) Diablo for Blizzard at the time. Condor was renamed Blizzard North, with Blizzard's main headquarters in Irvine renamed to Blizzard South to distinguish the two. Diablo was released at the very start of 1997 alongside Battle.net, a matchmaking service for the game. Blizzard North developed the sequel Diablo II (2000), and its expansion pack Lord of Destruction (2001). Following these releases, a number of key staff from Blizzard North departed for other opportunities, such as Bill Roper. Blizzard's management mentioned made the decision August 2005 to consolidate Blizzard North into Blizzard South, relocating staff to the main Blizzard offices in",
        "opportunities, such as Bill Roper. Blizzard's management mentioned made the decision August 2005 to consolidate Blizzard North into Blizzard South, relocating staff to the main Blizzard offices in Irvine, and subsequently dropping the \"Blizzard South\" name.",
        "Following the success of Warcraft II: Tides of Darkness, Blizzard began development on a science-fiction themed RTS StarCraft and released the title in March 1998. The title was the top-selling PC game for the year, and led to further growth of the Battle.net service and the use of the game for esports. Around 2000, Blizzard engaged with Nihilistic Software to work on a version of StarCraft for home consoles for Blizzard. Nihilisitic was co-founded by Robert Huebner, who had worked on StarCraft and other games while a Blizzard employee before leaving to found the studio. The game, StarCraft: Ghost, was a stealth-oriented game compared to the RTS features of StarCraft, and was a major feature of the 2002 Tokyo Game Show. However, over the next few years, the game entered development hell",
        "Ghost, was a stealth-oriented game compared to the RTS features of StarCraft, and was a major feature of the 2002 Tokyo Game Show. However, over the next few years, the game entered development hell with conflicts between Nihilisitic and Blizzard on its direction. Blizzard ordered Nihilistic to stop work on StarCraft: Ghost in July 2004, and instead brought on Swingin' Ape Studios, a third-party studio that had just successfully released Metal Arms: Glitch in the System in 2003, to reboot the development of Ghost. Blizzard fully acquired Swingin' Ape Studios in May 2005 to continue on Ghost. However, while the game was scheduled to be released in 2005, it was targeted at the consoles of the sixth generation, such as the PlayStation 2 and original Xbox, while the industry was transitioning",
        "while the game was scheduled to be released in 2005, it was targeted at the consoles of the sixth generation, such as the PlayStation 2 and original Xbox, while the industry was transitioning to the seventh generation. Blizzard decided to cancel Ghost rather than extend its development period to work on the newer consoles.",
        "Blizzard started to work on a sequel to the Warcraft II in early 1998, which was announced as a \"role-playing strategy\" game. Warcraft III: Reign of Chaos, the third title set in the Warcraft fictional universe, was released in July 2002. Warcraft III has inspired many future games, having the influence on real-time strategy and multiplayer online battle arena genre. Many of the characters, locations and concepts introduced in Warcraft III and its expansion went on to play major roles in numerous future Blizzard's titles.\n\nIn 2002, Blizzard was able to reacquire rights for three of its earlier Silicon & Synapse titles, The Lost Vikings, Rock n' Roll Racing and Blackthorne, from Interplay Entertainment and re-release them for the Game Boy Advance handheld console.",
        "In 2004, Blizzard opened European offices in the Paris suburb of Vélizy, Yvelines, France.",
        "Blizzard Entertainment released World of Warcraft, a massively multiplayer online role-playing game (MMORPG) based on the Warcraft franchise, on November 23, 2004, in North America, and on February 11, 2005, in Europe. By December 2004, the game was the fastest-selling PC game in the United States, and by March 2005, had reached 1.5 million subscribers worldwide. Blizzard partnered with Chinese publisher The9 to publish and distribute World of Warcraft in China, as foreign companies could not directly publish into the country themselves. World of Warcraft launched in China in June 2005. By the end of 2007, World of Warcraft was considered a global phenomenon, having reached over 9 million subscribers and exceeded  in revenue since its release. In April 2008, World of Warcraft was",
        "By the end of 2007, World of Warcraft was considered a global phenomenon, having reached over 9 million subscribers and exceeded  in revenue since its release. In April 2008, World of Warcraft was estimated to hold 62 percent of the MMORPG subscription market. Blizzard's staff quadrupled from around 400 employees in 2004 to 1600 by 2006 to provide more resources to the game and its various expansions, and Blizzard moved their headquarters to 16215 Alton Parkway in Irvine, California in 2007 to support the additional staff.",
        "With the success of World of Warcraft, Blizzard Entertainment organized the first BlizzCon fan convention in October 2005 held at the Anaheim Convention Center. The inaugural event drew about 6,000 people and became an annual event which Blizzard uses to announce new games, expansions, and content for its properties.",
        "Vivendi merger with Activision and continued growth (2008–2017)",
        "Up through 2006, Bobby Kotick, the CEO of Activision, had been working to rebound the company from near-bankruptcy, and had established a number of new studios. However, Activision lacked anything in the MMO market. Kotick saw that World of Warcraft was bringing in over  a year in subscription fees, and began approaching Vivendi's CEO Jean-Bernard Lévy about potential acquisition of their struggling Vivendi Games division, which included Blizzard Entertainment. Lévy was open to a merger, but would only allow it if he controlled the majority of the combined company, knowing the value of World of Warcraft to Kotick. Among those Kotick spoke to for advice included Blizzard's Morhaime, who told Kotick that they had begun establishing lucrative in-roads into the Chinese market. Kotick accepted",
        "Warcraft to Kotick. Among those Kotick spoke to for advice included Blizzard's Morhaime, who told Kotick that they had begun establishing lucrative in-roads into the Chinese market. Kotick accepted Lévy's deal, with the deal approved by shareholders in December 2007. By July 2008, the merger was complete, with Vivendi Games effectively dissolved except for Blizzard Entertainment, and the new company was named Activision Blizzard.",
        "Blizzard established a distribution agreement with the Chinese company NetEase in August 2008 to publish Blizzard's games in China. The deal focused on StarCraft II which was gaining popularity as an esport within southeast Asia, as well as for other Blizzard games with the exception of World of Warcraft, still being handled by The9. The two companies established the Shanghai EaseNet Network Technology for managing the games within China. Blizzard and The9 prepared to launch the World of Warcraft expansion Wrath of the Lich King, but the expansion came under scrutiny by China's content regulation board, the General Administration of Press and Publication, which rejected publication of it within China in March 2009, even with preliminary modifications made by The9 to clear it. Rumors of",
        "board, the General Administration of Press and Publication, which rejected publication of it within China in March 2009, even with preliminary modifications made by The9 to clear it. Rumors of Blizzard's dissatisfaction with The9 from this and other previous complications with World of Warcraft came to a head when, in April 2009, Blizzard announced it was terminating its contract with The9, and transferred operation of World of Warcraft in China to NetEase.",
        "They released an improved version of Battle.net (Battle.net 2.0) in March 2009 which included improved matchmaking, storefront features, and better support for all of Blizzard's existing titles particularly World of Warcraft.\n\nHaving peaked at 12 million monthly subscriptions in 2010, World of Warcraft subscriptions sunk to 6.8 million in 2014, the lowest number since the end of 2006, prior to The Burning Crusade expansion. However, World of Warcraft is still the world's most-subscribed MMORPG, and holds the Guinness World Record for the most popular MMORPG by subscribers. In 2008, Blizzard was honored at the 59th Annual Technology & Engineering Emmy Awards for the creation of World of Warcraft. Mike Morhaime accepted the award.",
        "Following the merger, Blizzard found it was relying on its well-established properties, but at the same time, the industry was experiencing a shift towards indie games. Blizzard established a few small teams within the company to work on developing new concepts based on the indie development approach that it could potentially use. One of these teams quickly came onto the idea of a collectible card game based on the Warcraft narrative universe, which ultimately became Hearthstone, released as a free-to-play title in March 2014. Hearthstone reached over 25 million players by the end of 2014, and exceeded 100 million players by 2018.",
        "Another small internal team began work around 2008 on a new intellectual property known as Titan, a more contemporary or near-future MMORPG that would have co-existed alongside World of Warcraft. The project gained more visibility in 2010 as a result of some information leaks. Blizzard continued to speak on Titans development over the next few years, with over 100 people within Blizzard working on the project. However, Titans development was troubled, and, internally, in May 2013, Blizzard cancelled the project (publicly reporting this in 2014), and reassigned most of the staff but left about 40 people, led by Jeff Kaplan, to either come up with a fresh idea within a few weeks or have their team reassigned to Blizzard's other departments. The small team came upon the idea of a team-based",
        "40 people, led by Jeff Kaplan, to either come up with a fresh idea within a few weeks or have their team reassigned to Blizzard's other departments. The small team came upon the idea of a team-based multiplayer shooter game, reusing many of the assets from Titan but set in a new near-future narrative. The new project was greenlit by Blizzard and became known as Overwatch, which was released in May 2016. Overwatch became the fourth main intellectual property of Blizzard, following Warcraft, Starcraft, and Diablo.",
        "In addition to Hearthstone and Overwatch, Blizzard Entertainment continued to produce sequels and expansions to its established properties during this period, including StarCraft II: Wings of Liberty (2010) and Diablo III (2012). Their major crossover title, Heroes of the Storm, was released as a MOBA game in 2015. The game featured various characters from Blizzard's franchises as playable heroes, as well as different battlegrounds based on Warcraft, Diablo, StarCraft, and Overwatch universes. In the late 2010s, Blizzard released StarCraft: Remastered (2017) and Warcraft III: Reforged (2020), remastered versions of the original StarCraft and Warcraft III, respectively.",
        "The May 2016 release of Overwatch was highly successful, and was the highest-selling game on PC for 2016. Several traditional esport events had been established within the year of Overwatch release, such as the Overwatch World Cup, but Blizzard continued to expand this and announced the first esports professional league, the Overwatch League at the 2016 BlizzCon event. The company purchased a studio at The Burbank Studios in Burbank, California, that it converted into a dedicated esports venue, the Blizzard Arena, to be used for the Overwatch League and other events. The inaugural season of the Overwatch League launched on January 10, 2018, with 12 global teams playing. By the second season in 2019 it had expanded the League to 20 teams, and with its third season in 2020, it will have",
        "the Overwatch League launched on January 10, 2018, with 12 global teams playing. By the second season in 2019 it had expanded the League to 20 teams, and with its third season in 2020, it will have these teams traveling across the globe in a transitional home/away-style format.",
        "In 2012, Blizzard Entertainment had 4,700 employees, with offices across 11 cities including Austin, Texas, and countries around the globe. , the company's headquarters in Irvine, California had 2,622 employees.\n\nChange of leadership (2018–2022) \nOn October 3, 2018, Mike Morhaime announced his plans to step down as the company president and CEO while remaining an advisor to the company; he formally left on April 7, 2019. Morhaime was replaced by J. Allen Brack, the executive producer on World of Warcraft.",
        "Frank Pearce announced he would be stepping down as Blizzard's Chief Development Officer on July 19, 2019, though will remain in an advisory role similar to Morhaime. Michael Chu, lead writer on many of Blizzard's franchises including Diablo, Warcraft, and Overwatch, announced he was leaving the company after 20 years in March 2020.",
        "On January 22, 2021, Activision transferred Vicarious Visions over to Blizzard Entertainment, stating that the Vicarious Visions team had better opportunity for long-term support for Blizzard. Vicarious had been working with Blizzard for about two years prior to this announcement on the planned remaster of Diablo II, Diablo II: Resurrected, and according to Brack, it made sense to incorporate Vicarious into Blizzard for ongoing support of the game and for other Diablo games including Diablo IV. Vicarious was completely merged into Blizzard by April 12, 2022.",
        "In celebration of the company's 30th anniversary, Blizzard Entertainment released a compilation called Blizzard Arcade Collection in February 2021, for various video game platforms. The collection includes their three classic video games: The Lost Vikings, Rock n' Roll Racing, and Blackthorne, each of which containing additional upgrades and numerous modern features.",
        "Activision Blizzard was the subject of a lawsuit from the California Department of Fair Employment and Housing in July 2021, asserting that for several years the management within Blizzard as well as Activision promoted a \"frat boy\" atmosphere that allowed and encouraged sexual misconduct towards female employees and discrimination in hiring practices. The lawsuit drew a large response from employees and groups outside of Activision Blizzard. In the wake of these events, Brack, one of the few individuals directly named in the suit, announced he was leaving Blizzard to \"pursue new opportunities\", and will be replaced by co-leads Jen Oneal, the lead of Vicarious Visions and the first woman in a leadership role for the company, and Mike Ybarra, a Blizzard executive vice president. Oneal",
        "and will be replaced by co-leads Jen Oneal, the lead of Vicarious Visions and the first woman in a leadership role for the company, and Mike Ybarra, a Blizzard executive vice president. Oneal announced in November 2021 that she would be leaving the company by the end of 2021, leaving Ybarra as the sole leader of Blizzard.",
        "As a result of the California lawsuit and of delays and release issues with their more recent games, Activision Blizzard's stock faced severe pressure. Subsequently, Microsoft seized the opportunity to become one of the largest video game companies in the world and announced its intent to acquire Activision Blizzard and its subsidiaries, including Blizzard, for  in January 2022. This exchange marks the largest acquisition in tech history, surpassing the $67 billion Dell-EMC merger from 2016. The deal closed on October 13, 2023, and Activision Blizzard  moved into the Microsoft Gaming division.",
        "Blizzard acquired Proletariat, the developers of Spellbreak, in June 2022 as to help support World of Warcraft. The 100-employee studio remained in Boston but will shutter Spellbreak as they move onto Warcraft.",
        "Challenges with NetEase (2023–present)",
        "Ahead of their license renewal in January 2023, Blizzard (via Activision Blizzard) and NetEase stated in November 2022 that they had been unable to come to an agreement on the renewal terms for their license, and thus most Blizzard games will cease operations in January 2023 until the situation can be resolved. According to a report by The New York Times, several factors influenced Activision Blizzard's decision to terminate the agreement, which included stronger demands made by the Chinese government to know of Activision Blizzard's internal business matters, NetEase's desire to license the games directly rather than run the license through a joint venture, and Activision Blizzard's concerns that NetEase was trying to start their own ventures, including the  payment towards Bungie in",
        "games directly rather than run the license through a joint venture, and Activision Blizzard's concerns that NetEase was trying to start their own ventures, including the  payment towards Bungie in 2018. NetEase was further concerned about the impact of the pending acquisition of Activision Blizzard by Microsoft. Activision Blizzard stated they were looking to other Chinese firms as replacements for NetEase as to restore their games in China.",
        "Games \n\nBlizzard Entertainment has developed 19 games since the inception of the company in 1991.",
        "Main franchises",
        "The majority of the games Blizzard published are in the Warcraft, Diablo, and StarCraft series. Since the release of Warcraft: Orcs & Humans (1994), Diablo (1997), and StarCraft (1998), the focus has been almost exclusively on those three franchises. Overwatch (2016) became an exception years later, bringing the number of main franchises to four. Each franchise is supported by other media based around its intellectual property such as novels, collectible card games, comics and video shorts. Blizzard announced in 2006 that they would be producing a Warcraft live-action movie. The movie was directed by Duncan Jones, financed and produced by Legendary Pictures, Atlas Entertainment, and others, and distributed by Universal Pictures. It was released in June 2016. On October 4, 2022 Overwatch",
        "by Duncan Jones, financed and produced by Legendary Pictures, Atlas Entertainment, and others, and distributed by Universal Pictures. It was released in June 2016. On October 4, 2022 Overwatch servers were officially shut off, Later Overwatch 2 (2022) servers went up.",
        "Spin-offs \nBlizzard has released two spin-offs to the main franchises: Hearthstone (2014), which is set in the existing Warcraft lore, and Heroes of the Storm (2015), which features playable characters from all four of Blizzard's franchises.\n\nRemasters \nIn 2015, Blizzard Entertainment formed \"Classic Games division\", a team focused on updating and remastering some of their older titles, with an initially announced focus on StarCraft: Remastered (2017), Warcraft III: Reforged (2020), and Diablo II: Resurrected (2021).",
        "Re-released games",
        "In February 2021, Blizzard Entertainment released a compilation called Blizzard Arcade Collection for Microsoft Windows, Xbox One, PlayStation 4, and Nintendo Switch. The collection includes five Blizzard's classic video games: The Lost Vikings, Rock n' Roll Racing, Blackthorne, The Lost Vikings 2 and RPM Racing, with the last two games added in April 2021. Some of the modern features include 16:9 resolution, 4-player split-screen, rewinding and saving of game progress, watching replays, and adding graphic filters to change the look of player's game. Additionally, it contains upgrades for each game such as enhanced local multiplayer for The Lost Vikings, new songs and artist performances for Rock n' Roll Racing, as well as a new level map for Blackthorne. A digital museum, which is",
        "each game such as enhanced local multiplayer for The Lost Vikings, new songs and artist performances for Rock n' Roll Racing, as well as a new level map for Blackthorne. A digital museum, which is included in the collection, features game art, unused content, and interviews.",
        "Unreleased and future games \nNotable unreleased titles include Warcraft Adventures: Lord of the Clans, which was canceled on May 22, 1998, Shattered Nations, and StarCraft: Ghost, which was \"Postponed indefinitely\" on March 24, 2006, after being in development hell for much of its lifespan. After seven years of development, Blizzard revealed the cancellation of an unannounced MMO codenamed Titan on September 23, 2014. The company also has a history of declining to set release dates, choosing to instead take as much time as needed, generally saying a given product is \"done when it's done.\"",
        "Pax Imperia II was originally announced as a title to be published by Blizzard. Blizzard eventually dropped Pax Imperia II, though, when it decided it might be in conflict with their other space strategy project, which became known as StarCraft. THQ eventually contracted with Heliotrope and released the game in 1997 as Pax Imperia: Eminent Domain.\n\nThe company announced in January 2022 that it was near release of another new intellectual property, a survival game that had been at work at the studio for nearly five years.\n\nPorts \nThe company, known at the time as the Silicon & Synapse, initially concentrated on porting other studio's games to computer platforms, developing 8 ports between 1992 and 1993.",
        "Company structure \nAs with most studios with multiple franchises, Blizzard Entertainment has organized different departments to oversee these franchises. Formally, since around the time of World of Warcraft in 2004, these have been denoted through simply numerical designations. The original three teams were:",
        "Team 1 manages the StarCraft property. The team also oversaw the development of the StarCraft spin-off Heroes of the Storm. Team 1 also included the Classics Team to work on remastering Blizzard's earlier properties for modern computers, which have included StarCraft: Remastered and Warcraft III: Reforged. The Classic Games team was disbanded around August 2020, about eight months after Warcraft III: Reforged was released; according to Jason Schreier of Bloomberg News, this was due to Activision Blizzard driving Blizzard away from remastering its old properties, which figured into the launch issues with Warcraft III: Reforged.\n Team 2 continues to manage and create content for World of Warcraft.\n Team 3 oversees the Diablo franchise.",
        "Since 2004, two new teams were created:\n Team 4 was created around 2007 to work on Blizzard's first new IP since World of Warcraft, that being Titan. Titan had development difficulties near 2013, and most of Team 4 was reallocated to the other teams, but the remaining members, led by Jeff Kaplan, revised Titans concept into Overwatch, which remains in Team 4's hands since its release in 2016.\n Team 5 was created in 2008 to explore smaller games that could fit into Blizzard's portfolio. This resulted in the creation of Hearthstone, a collectible card game based on the Warcraft property, which became Team 5's priority.\n\nTechnology\n\nBattle.net 2.0",
        "Technology\n\nBattle.net 2.0 \n\nBlizzard Entertainment released its revamped Battle.net service in 2009. The platform provides online gaming, digital distribution, digital rights management, and social networking service. Battle.net allows people who have purchased Blizzard products to download digital copies of games they have purchased, without needing any physical media.\n\nOn November 11, 2009, Blizzard required all World of Warcraft accounts to switch over to Battle.net accounts. This transition means that all current Blizzard titles can be accessed, downloaded, and played with a singular Battle.net login.",
        "Battle.net 2.0 is the platform for matchmaking service for Blizzard games, which offers players a host of additional features. Players are able to track their friend's achievements, view match history, avatars, etc. Players are able to unlock a wide range of achievements for Blizzard games.\n\nThe service provides the user with community features such as friends lists and groups, and allows players to chat simultaneously with players from other Blizzard games using VoIP and instant messaging. For example, players no longer need to create multiple user names or accounts for most Blizzard products. To enable cross-game communication, players need to become either Battletag or Real ID friends.",
        "Warden client \nBlizzard Entertainment has made use of a special form of software known as the 'Warden Client'. The Warden client is known to be used with Blizzard's online games such as Diablo and World of Warcraft, and the Terms of Service contain a clause consenting to the Warden software's RAM scans while a Blizzard game is running.",
        "The Warden client scans a small portion of the code segment of running processes in order to determine whether any third-party programs are running. The goal of this is to detect and address players who may be attempting to run unsigned code or third party programs in the game. This determination of third party programs is made by hashing the scanned strings and comparing the hashed value to a list of hashes assumed to correspond to banned third party programs. The Warden's reliability in correctly discerning legitimate versus illegitimate actions was called into question when a large-scale incident happened. This incident banned many Linux users after an update to Warden caused it to incorrectly detect Cedega as a cheat program. Blizzard issued a statement claiming they had correctly",
        "incident happened. This incident banned many Linux users after an update to Warden caused it to incorrectly detect Cedega as a cheat program. Blizzard issued a statement claiming they had correctly identified and restored all accounts and credited them with 20 days' play. Warden scans all processes running on a computer, not just the game, and could possibly run across what would be considered private information and other personally identifiable information. It is because of these peripheral scans that Warden has been accused of being spyware and has run afoul of controversy among privacy advocates.",
        "Controversies and legal disputes",
        "Blizzard Entertainment, Inc. v. Valve Corporation",
        "Shortly after Valve filed its trademark for \"Dota\" to secure the franchising rights for Dota 2, DotA-Allstars, LLC, run by former contributors to the game's predecessor, Defense of the Ancients, filed an opposing trademark in August 2010. DotA All-Stars, LLC was sold to Blizzard Entertainment in 2011. After the opposition was over-ruled in Valve's favor, Blizzard filed an opposition against Valve in November 2011, citing their license agreement with developers, as well as their ownership of DotA-Allstars, LLC. Blizzard conceded their case in May 2012, however, giving Valve undisputed commercial rights to Dota name, while Blizzard would rename their StarCraft II: Heart of the Swarm mod \"Blizzard All-Stars\", which would eventually become the stand-alone game, Heroes of the Storm.",
        "California Department of Fair Employment and Housing v. Activision Blizzard",
        "Following a two-year investigation, the California Department of Fair Employment and Housing (DFEH) filed a lawsuit against Activision Blizzard in July 2021 for gender-based discrimination and sexual harassment, principally within the Blizzard Entertainment workplace. The DFEH alleges that female employees were subjected to constant sexual harassment, unequal pay, retaliation, as well as discrimination based on pregnancy. The suit also described a \"pervasive frat boy workplace culture\" at Blizzard that included objectification of women's bodies and jokes about rape. Activision Blizzard's statement described the suit as meritless, contending that action had been taken in any instances of misconduct. The company also objected to the DFEH not approaching them prior to filing. The lawsuit",
        "described the suit as meritless, contending that action had been taken in any instances of misconduct. The company also objected to the DFEH not approaching them prior to filing. The lawsuit prompted an employee walkout, as well as leading J Allen Brack, and head of human resources, Jesse Meschuk, to step down. Because of these allegations, Blizzard changed names that referenced employees in multiple of its franchises, including Overwatch and World of Warcraft.",
        "Founder Electronics infringement lawsuit \nOn August 14, 2007, Beijing University Founder Electronics Co., Ltd. sued Blizzard Entertainment Limited for copyright infringement claiming 100 million yuan in damages. The lawsuit alleged the Chinese edition of World of Warcraft reproduced a number of Chinese typefaces made by Founder Electronics without permission.\n\nFreeCraft \n\nOn June 20, 2003, Blizzard issued a cease and desist letter to the developers of an open-source clone of the Warcraft engine called FreeCraft, claiming trademark infringement. This hobby project had the same gameplay and characters as Warcraft II, but came with different graphics and music.",
        "As well as a similar name, FreeCraft enabled players to use Warcraft II graphics, provided they had the Warcraft II CD. The programmers of the clone shut down their site without challenge. Soon after that the developers regrouped to continue the work by the name of Stratagus.\n\nHearthstone ban and Hong Kong protests",
        "Hearthstone ban and Hong Kong protests \n\nDuring an October 2019 Hearthstone Grandmasters streaming event in Taiwan, one player Ng Wai Chung, going by his online alias \"Blitzchung\" used an interview period to show support for the protestors in the 2019–20 Hong Kong protests. Shortly afterwards, on October 7, 2019, Blitzchung was disqualified from the current tournament and forfeited his winnings to date, and banned for a one-year period. The two shoutcasters engaged in the interview were also penalized with similar bans. Blizzard justified the ban as from its Grandmasters tournament rules that prevents players from anything that \"brings [themselves] into public disrepute, offends a portion or group of the public, or otherwise damages [Blizzard's] image\".",
        "Blizzard's response led to several protests from current Hearthstone players, other video game players, and criticism from Blizzard's employees, fearing that Blizzard was giving into the censorship of the Chinese government. Protests were held, including through the 2019 BlizzCon in early November, to urge Blizzard to reverse their bans. The situation also drew the attention of several U.S. lawmakers, fearing that Blizzard, as a U.S. company, was letting China dictate how it handled speech and also urged the bans to be reversed.",
        "Blizzard CEO J. Allen Brack wrote an open letter on October 11, 2019, apologizing for the way Blizzard handled the situation, and reduced the bans for both Blitzchung and the casters to six months. Brack reiterated that while they support free speech and their decision was in no way tied to the Chinese government, they want players and casters to avoid speaking beyond the tournament and the games in such interviews.",
        "King's \"Diversity Tool\" controversy",
        "On May 12, 2022, Blizzard Entertainment released a blog post about the Diversity Space Tool, developed by a team at King – a mobile business unit at Activision Blizzard – alongside the MIT Game Lab. Jacqueline Chomatas, King's globalization project manager, described the tool as a \"measurement device\" to analyze how diverse the characters are \"when compared to the 'norm'\". The post showed example images of the tool being used on Overwatch's cast, with graphs showing breakdowns of the character attributes, and stated that \"The Overwatch 2 team at Blizzard has also had a chance to experiment with the tool, with equally enthusiastic first impressions.\" Blizzard shared the intent to release the tool during the summer and fall of 2022, with the goal of \"making the tool available to the",
        "with the tool, with equally enthusiastic first impressions.\" Blizzard shared the intent to release the tool during the summer and fall of 2022, with the goal of \"making the tool available to the industry as a whole\".",
        "The tool received heavy backlash online. Many people asked why Blizzard would create the tool instead of hiring diverse teams, and raised questions regarding the tool's rating scale. The blog post originally suggested that the tool was used in an active development, mainly for Overwatch, which led some Blizzard employees working on the game to publicly deny the tool was used in Overwatch development and to criticize the tool further. On May 13, 2022, the blog post was edited to remove the example images of the tool and any mention of Overwatch. Later, the post was deleted altogether.\n\nMDY Industries, LLC v. Blizzard Entertainment, Inc.",
        "MDY Industries, LLC v. Blizzard Entertainment, Inc. \n\nOn July 14, 2008, the United States District Court for the District of Arizona ruled on the case MDY Industries, LLC v. Blizzard Entertainment, Inc.. The Court found that MDY was liable for copyright infringement since users of its Glider bot program were breaking the End User License Agreement and Terms of Use for World of Warcraft. MDY Industries appealed the judgment of the district court, and a judgment was delivered by the Ninth Circuit Court of Appeals on December 14, 2010, in which the summary judgment against MDY for contributory copyright infringement was reversed. Nevertheless, they ruled that the bot violated the DMCA and the case was sent back to the district court for review in light of this decision.",
        "MDY v. Blizzards decision did affirm a prior Ninth Circuit ruling in Vernor v. Autodesk, Inc. that software licenses, such as the one used by Blizzard for WoW, were enforceable and enshrined the principle that video games could be sold as licenses to players rather than purchased. This ruling, though limited to the states of the Ninth Circuit, has been used by the industry to continue to sell games as licenses to users.",
        "Privacy controversy and Real ID \nOn July 6, 2010, Blizzard Entertainment announced that they were changing the way their forums worked to require that users identify themselves with their real name. The reaction from the community was overwhelmingly negative with multiple game magazines calling the change \"foolhardy\" and an \"epic fail\". It resulted in a significant user response on the Blizzard forums, including one thread on the issue reaching over 11,000 replies. This included personal details of a Blizzard employee who gave his real name \"to show it wasn't a big deal\". Shortly after revealing his real name, forum users posted personal information including his phone number, picture, age, and home address.",
        "Some technology media outlets suggested that displaying real names through Real ID is a good idea and would benefit both Battle.net and the Blizzard community. But others were worried that Blizzard was opening their fans up to real-life dangers such as stalking, harassment, and employment issues, since a simple Internet search by someone's employer can reveal their online activities.",
        "Blizzard initially responded to some of the concerns by saying that the changes would not be retroactive to previous posts, that parents could set up the system so that minors cannot post, and that posting to the forums is optional. However, due to the significant negative response, Blizzard President Michael Morhaime issued a statement rescinding the plan to use real names on Blizzard's forums for the time being. The idea behind this plan was to allow players who had a relationship outside of the games to find each other more easily across all the Blizzard game titles.",
        "StarCraft privacy and other lawsuits \nIn 1998, Donald P. Driscoll, an Albany, California, attorney filed a suit on behalf of Intervention, Inc., a California consumer group, against Blizzard Entertainment for \"unlawful business practices\" for the action of collecting data from a user's computer without their permission. \n\nOn May 19, 2014, Blizzard Entertainment filed a lawsuit in federal court in California, alleging that the unidentified programmers were involved in creation of software that hacks Starcraft II. Most of the alleged charges are related to copyright infringement.",
        "Back in May 2010, MBCPlus Media, which operates the network MBCGame (Korean television stations that are broadcasting tournaments built around StarCraft), was revealed to be sued by Blizzard for broadcasting StarCraft tournaments without the company's consent, insisting that StarCraft is not a public domain offering, as Blizzard has invested significant money and resources to create the StarCraft game.",
        "World of Warcraft private server complications \nOn December 5, 2008, Blizzard Entertainment issued a cease and desist letter to many administrators of high population World of Warcraft private servers (essentially slightly altered hosting servers of the actual World of Warcraft game, that players do not have to pay for). Blizzard used the Digital Millennium Copyright Act to influence many private servers to fully shut down and cease to exist.",
        "Related companies \nOver the years, some former Blizzard Entertainment employees have moved on and established gaming companies of their own. Several of these occurred following the merger between Activision Holdings and Blizzard's parent company at the time, Vivendi Games in 2008, and more recently as Activision Blizzard has directed Blizzard away from properties like Warcraft and Starcraft that are not seen as financial boons to the larger company. These employees left to form their smaller studios to give themselves the creative freedom that they were lacking at Blizzard. Collectively these studios are known as \"Blizzard 2.0\".\n ArenaNet, creators of the Guild Wars franchise.\n Bonfire Studios, founded by Rob Pardo.",
        "ArenaNet, creators of the Guild Wars franchise.\n Bonfire Studios, founded by Rob Pardo.\n Carbine Studios, now defunct as of September 2018, after releasing a massively multiplayer title WildStar.\n Castaway Entertainment, now defunct, after working on a game similar to the Diablo series, Djinn.\n Dreamhaven, founded by Michael Morhaime.\n Flagship Studios, now defunct, creators of Hellgate: London, also worked on Mythos.\n Frost Giant Studios, founded by Tim Morten and Tim Campbell, currently developing real-time strategy game Stormgate.\n Hyboreal Games, founded by Michio Okamura.\n Ready at Dawn Studios, creators of The Order: 1886, Daxter, God of War: Chains of Olympus and an Ōkami port for the Wii.\n Red 5 Studios, now defunct, creators of Firefall, a free to play game MMOG.",
        "Ready at Dawn Studios, creators of The Order: 1886, Daxter, God of War: Chains of Olympus and an Ōkami port for the Wii.\n Red 5 Studios, now defunct, creators of Firefall, a free to play game MMOG.\n Runic Games, now defunct, founded by Travis Baldree, Erich Schaefer, and Max Schaefer; creators of Torchlight.\n Second Dinner, founded by Ben Brode, creators of Marvel Snap.\n Uncapped Games, founded by David Kim and Jason Hughes.",
        "References\n\nFurther reading\n\nExternal links \n\n \n\n \nCompanies based in Irvine, California\nFormer Vivendi subsidiaries\nVideo game companies established in 1991\nVideo game companies of the United States\nVideo game development companies\nVideo game publishers\nSpike Video Game Award winners\nVideo game controversies"
    ],
    [
        "Block cipher\nIn cryptography, a block cipher is a deterministic algorithm that operates on fixed-length groups of bits, called blocks. Block ciphers are the elementary building blocks of many cryptographic protocols. They are ubiquitous in the storage and exchange of data, where such data is secured and authenticated via encryption.",
        "A block cipher uses blocks as an unvarying transformation. Even a secure block cipher is suitable for the encryption of only a single block of data at a time, using a fixed key. A multitude of modes of operation have been designed to allow their repeated use in a secure way to achieve the security goals of confidentiality and authenticity. However, block ciphers may also feature as building blocks in other cryptographic protocols, such as universal hash functions and pseudorandom number generators.",
        "Definition\nA block cipher consists of two paired algorithms, one for encryption, , and the other for decryption, . Both algorithms accept two inputs: an input block of size  bits and a key of size  bits; and both yield an -bit output block. The decryption algorithm  is defined to be the inverse function of encryption, i.e., . More formally, a block cipher is specified by an encryption function\n\nwhich takes as input a key , of bit length  (called the key size), and a bit string , of length  (called the block size), and returns a string  of  bits.  is called the plaintext, and  is termed the ciphertext. For each , the function () is required to be an invertible mapping on . The inverse for  is defined as a function\n\ntaking a key  and a ciphertext  to return a plaintext value , such that",
        "taking a key  and a ciphertext  to return a plaintext value , such that\n\nFor example, a block cipher encryption algorithm might take a 128-bit block of plaintext as input, and output a corresponding 128-bit block of ciphertext. The exact transformation is controlled using a second input – the secret key. Decryption is similar: the decryption algorithm takes, in this example, a 128-bit block of ciphertext together with the secret key, and yields the original 128-bit block of plain text.\n\nFor each key K, EK is a permutation (a bijective mapping) over the set of input blocks. Each key selects one permutation from the set of  possible permutations.",
        "History\nThe modern design of block ciphers is based on the concept of an iterated product cipher. In his seminal 1949 publication, Communication Theory of Secrecy Systems, Claude Shannon analyzed product ciphers and suggested them as a means of effectively improving security by combining simple operations such as substitutions and permutations. Iterated product ciphers carry out encryption in multiple rounds, each of which uses a different subkey derived from the original key. One widespread implementation of such ciphers named a Feistel network after Horst Feistel is notably implemented in the DES cipher. Many other realizations of block ciphers, such as the AES, are classified as substitution–permutation networks.",
        "The root of all cryptographic block formats used within the Payment Card Industry Data Security Standard (PCI DSS) and American National Standards Institute (ANSI) standards lies with the Atalla Key Block (AKB), which was a key innovation of the Atalla Box, the first hardware security module (HSM). It was developed in 1972 by Mohamed M. Atalla, founder of Atalla Corporation (now Utimaco Atalla), and released in 1973. The AKB was a key block, which is required to securely interchange symmetric keys or PINs with other actors in the banking industry. This secure interchange is performed using the AKB format. The Atalla Box protected over 90% of all ATM networks in operation as of 1998, and Atalla products still secure the majority of the world's ATM transactions as of 2014.",
        "The publication of the DES cipher by the United States National Bureau of Standards (subsequently the U.S. National Institute of Standards and Technology, NIST) in 1977 was fundamental in the public understanding of modern block cipher design. It also influenced the academic development of cryptanalytic attacks. Both differential and linear cryptanalysis arose out of studies on DES design. , there is a palette of attack techniques against which a block cipher must be secure, in addition to being robust against brute-force attacks.\n\nDesign",
        "Design\n\nIterated block ciphers\nMost block cipher algorithms are classified as iterated block ciphers which means that they transform fixed-size blocks of plaintext into identically sized blocks of ciphertext, via the repeated application of an invertible transformation known as the round function, with each iteration referred to as a round.\n\nUsually, the round function R takes different round keys Ki as a second input, which is derived from the original key:\n\nwhere  is the plaintext and  the ciphertext, with r being the number of rounds.\n\nFrequently, key whitening is used in addition to this. At the beginning and the end, the data is modified with key material (often with XOR, but simple arithmetic operations like adding and subtracting are also used):",
        "Given one of the standard iterated block cipher design schemes, it is fairly easy to construct a block cipher that is cryptographically secure, simply by using a large number of rounds. However, this will make the cipher inefficient. Thus, efficiency is the most important additional design criterion for professional ciphers. Further, a good block cipher is designed to avoid side-channel attacks, such as branch prediction and input-dependent memory accesses that might leak secret data via the cache state or the execution time. In addition, the cipher should be concise, for small hardware and software implementations. Finally, the cipher should be easily crypt analyzable, such that it can be shown how many rounds the cipher needs to be reduced to so that the existing cryptographic attacks",
        "software implementations. Finally, the cipher should be easily crypt analyzable, such that it can be shown how many rounds the cipher needs to be reduced to so that the existing cryptographic attacks would work – and, conversely, that it can be shown that the number of actual rounds is large enough to protect against them.",
        "Substitution–permutation networks\n\nOne important type of iterated block cipher known as a substitution–permutation network (SPN) takes a block of the plaintext and the key as inputs and applies several alternating rounds consisting of a substitution stage followed by a permutation stage—to produce each block of ciphertext output. The non-linear substitution stage mixes the key bits with those of the plaintext, creating Shannon's confusion. The linear permutation stage then dissipates redundancies, creating diffusion.",
        "A substitution box (S-box) substitutes a small block of input bits with another block of output bits. This substitution must be one-to-one, to ensure invertibility (hence decryption). A secure S-box will have the property that changing one input bit will change about half of the output bits on average, exhibiting what is known as the avalanche effect—i.e. it has the property that each output bit will depend on every input bit.\n\nA permutation box (P-box) is a permutation of all the bits: it takes the outputs of all the S-boxes of one round, permutes the bits, and feeds them into the S-boxes of the next round. A good P-box has the property that the output bits of any S-box are distributed to as many S-box inputs as possible.",
        "At each round, the round key (obtained from the key with some simple operations, for instance, using S-boxes and P-boxes) is combined using some group operation, typically XOR.\n\nDecryption is done by simply reversing the process (using the inverses of the S-boxes and P-boxes and applying the round keys in reversed order).\n\nFeistel ciphers\n\nIn a Feistel cipher, the block of plain text to be encrypted is split into two equal-sized halves. The round function is applied to one half, using a subkey, and then the output is XORed with the other half. The two halves are then swapped.\n\nLet  be the round function and let\n be the sub-keys for the rounds  respectively.\n\nThen the basic operation is as follows:\n\nSplit the plaintext block into two equal pieces, (, )\n\nFor each round , compute\n\n.",
        "Then the basic operation is as follows:\n\nSplit the plaintext block into two equal pieces, (, )\n\nFor each round , compute\n\n.\n\nThen the ciphertext is .\n\nThe decryption of a ciphertext  is accomplished by computing for \n\n.\n\nThen  is the plaintext again.\n\nOne advantage of the Feistel model compared to a substitution–permutation network is that the round function  does not have to be invertible.\n\nLai–Massey ciphers\n\nThe Lai–Massey scheme offers security properties similar to those of the Feistel structure. It also shares the advantage that the round function  does not have to be invertible. Another similarity is that it also splits the input block into two equal pieces. However, the round function is applied to the difference between the two, and the result is then added to both half blocks.",
        "Let  be the round function and  a half-round function and let  be the sub-keys for the rounds  respectively.\n\nThen the basic operation is as follows:\n\nSplit the plaintext block into two equal pieces, (, )\n\nFor each round , compute\n\nwhere  and \n\nThen the ciphertext is .\n\nThe decryption of a ciphertext  is accomplished by computing for \n\nwhere  and \n\nThen  is the plaintext again.\n\nOperations\n\nARX (add–rotate–XOR)\nMany modern block ciphers and hashes are ARX algorithms—their round function involves only three operations: (A) modular addition, (R) rotation with fixed rotation amounts, and (X) XOR. Examples include ChaCha20, Speck, XXTEA, and BLAKE. Many authors draw an ARX network, a kind of data flow diagram, to illustrate such a round function.",
        "These ARX operations are popular because they are relatively fast and cheap in hardware and software, their implementation can be made extremely simple, and also because they run in constant time, and therefore are immune to timing attacks. The rotational cryptanalysis technique attempts to attack such round functions.\n\nOther operations\nOther operations often used in block ciphers include data-dependent rotations as in RC5 and RC6, a substitution box implemented as a lookup table as in Data Encryption Standard and Advanced Encryption Standard, a permutation box, and multiplication as in IDEA.\n\nModes of operation",
        "Modes of operation\n\nA block cipher by itself allows encryption only of a single data block of the cipher's block length. For a variable-length message, the data must first be partitioned into separate cipher blocks. In the simplest case, known as electronic codebook (ECB) mode, a message is first split into separate blocks of the cipher's block size (possibly extending the last block with padding bits), and then each block is encrypted and decrypted independently. However, such a naive method is generally insecure because equal plaintext blocks will always generate equal ciphertext blocks (for the same key), so patterns in the plaintext message become evident in the ciphertext output.",
        "To overcome this limitation, several so-called block cipher modes of operation have been designed and specified in national recommendations such as NIST 800-38A and BSI TR-02102 and international standards such as ISO/IEC 10116. The general concept is to use randomization of the plaintext data based on an additional input value, frequently called an initialization vector, to create what is termed probabilistic encryption. In the popular cipher block chaining (CBC) mode, for encryption to be secure the initialization vector passed along with the plaintext message must be a random or pseudo-random value, which is added in an exclusive-or manner to the first plaintext block before it is encrypted. The resultant ciphertext block is then used as the new initialization vector for the next",
        "value, which is added in an exclusive-or manner to the first plaintext block before it is encrypted. The resultant ciphertext block is then used as the new initialization vector for the next plaintext block. In the cipher feedback (CFB) mode, which emulates a self-synchronizing stream cipher, the initialization vector is first encrypted and then added to the plaintext block. The output feedback (OFB) mode repeatedly encrypts the initialization vector to create a key stream for the emulation of a synchronous stream cipher. The newer counter (CTR) mode similarly creates a key stream, but has the advantage of only needing unique and not (pseudo-)random values as initialization vectors; the needed randomness is derived internally by using the initialization vector as a block counter and",
        "the advantage of only needing unique and not (pseudo-)random values as initialization vectors; the needed randomness is derived internally by using the initialization vector as a block counter and encrypting this counter for each block.",
        "From a security-theoretic point of view, modes of operation must provide what is known as semantic security. Informally, it means that given some ciphertext under an unknown key one cannot practically derive any information from the ciphertext (other than the length of the message) over what one would have known without seeing the ciphertext. It has been shown that all of the modes discussed above, with the exception of the ECB mode, provide this property under so-called chosen plaintext attacks.\n\nPadding",
        "Padding\n\nSome modes such as the CBC mode only operate on complete plaintext blocks. Simply extending the last block of a message with zero bits is insufficient since it does not allow a receiver to easily distinguish messages that differ only in the number of padding bits. More importantly, such a simple solution gives rise to very efficient padding oracle attacks. A suitable padding scheme is therefore needed to extend the last plaintext block to the cipher's block size. While many popular schemes described in standards and in the literature have been shown to be vulnerable to padding oracle attacks, a solution that adds a one-bit and then extends the last block with zero-bits, standardized as \"padding method 2\" in ISO/IEC 9797-1, has been proven secure against these attacks.",
        "Cryptanalysis\n\nBrute-force attacks\n This property results in the cipher's security degrading quadratically, and needs to be taken into account when selecting a block size. There is a trade-off though as large block sizes can result in the algorithm becoming inefficient to operate. Earlier block ciphers such as the DES have typically selected a 64-bit block size, while newer designs such as the AES support block sizes of 128 bits or more, with some ciphers supporting a range of different block sizes.\n\nDifferential cryptanalysis\n\nLinear cryptanalysis\n\nA linear cryptanalysis is a form of cryptanalysis based on finding affine approximations to the action of a cipher. Linear cryptanalysis is one of the two most widely used attacks on block ciphers; the other being differential cryptanalysis.",
        "The discovery is attributed to Mitsuru Matsui, who first applied the technique to the FEAL cipher (Matsui and Yamagishi, 1992).\n\nIntegral cryptanalysis",
        "Integral cryptanalysis is a cryptanalytic attack that is particularly applicable to block ciphers based on substitution–permutation networks. Unlike differential cryptanalysis, which uses pairs of chosen plaintexts with a fixed XOR difference, integral cryptanalysis uses sets or even multisets of chosen plaintexts of which part is held constant and another part varies through all possibilities. For example, an attack might use 256 chosen plaintexts that have all but 8 of their bits the same, but all differ in those 8 bits. Such a set necessarily has an XOR sum of 0, and the XOR sums of the corresponding sets of ciphertexts provide information about the cipher's operation. This contrast between the differences between pairs of texts and the sums of larger sets of texts inspired the name",
        "corresponding sets of ciphertexts provide information about the cipher's operation. This contrast between the differences between pairs of texts and the sums of larger sets of texts inspired the name \"integral cryptanalysis\", borrowing the terminology of calculus.",
        "Other techniques\n\nIn addition to linear and differential cryptanalysis, there is a growing catalog of attacks: truncated differential cryptanalysis, partial differential cryptanalysis, integral cryptanalysis, which encompasses square and integral attacks, slide attacks, boomerang attacks, the XSL attack, impossible differential cryptanalysis, and algebraic attacks. For a new block cipher design to have any credibility, it must demonstrate evidence of security against known attacks.",
        "Provable security",
        "When a block cipher is used in a given mode of operation, the resulting algorithm should ideally be about as secure as the block cipher itself. ECB (discussed above) emphatically lacks this property: regardless of how secure the underlying block cipher is, ECB mode can easily be attacked. On the other hand, CBC mode can be proven to be secure under the assumption that the underlying block cipher is likewise secure. Note, however, that making statements like this requires formal mathematical definitions for what it means for an encryption algorithm or a block cipher to \"be secure\". This section describes two common notions for what properties a block cipher should have. Each corresponds to a mathematical model that can be used to prove properties of higher-level algorithms, such as CBC.",
        "This general approach to cryptography – proving higher-level algorithms (such as CBC) are secure under explicitly stated assumptions regarding their components (such as a block cipher) – is known as provable security.\n\nStandard model\n\nInformally, a block cipher is secure in the standard model if an attacker cannot tell the difference between the block cipher (equipped with a random key) and a random permutation.",
        "Informally, a block cipher is secure in the standard model if an attacker cannot tell the difference between the block cipher (equipped with a random key) and a random permutation.\n\nTo be a bit more precise, let E be an n-bit block cipher. We imagine the following game:\n The person running the game flips a coin.\n If the coin lands on heads, he chooses a random key K and defines the function f = EK.\n If the coin lands on tails, he chooses a random permutation  on the set of n-bit strings and defines the function f = .\n The attacker chooses an n-bit string X, and the person running the game tells him the value of f(X).\n Step 2 is repeated a total of q times. (Each of these q interactions is a query.)\n The attacker guesses how the coin landed. He wins if his guess is correct.",
        "The attacker, which we can model as an algorithm, is called an adversary. The function f (which the adversary was able to query) is called an oracle.",
        "Note that an adversary can trivially ensure a 50% chance of winning simply by guessing at random (or even by, for example, always guessing \"heads\"). Therefore, let PE(A) denote the probability that adversary A wins this game against E, and define the advantage of A as 2(PE(A) − 1/2). It follows that if A guesses randomly, its advantage will be 0; on the other hand, if A always wins, then its advantage is 1. The block cipher E is a pseudo-random permutation (PRP) if no adversary has an advantage significantly greater than 0, given specified restrictions on q and the adversary's running time. If in Step 2 above adversaries have the option of learning f−1(X) instead of f(X) (but still have only small advantages) then E is a strong PRP (SPRP). An adversary is non-adaptive if it chooses all q",
        "in Step 2 above adversaries have the option of learning f−1(X) instead of f(X) (but still have only small advantages) then E is a strong PRP (SPRP). An adversary is non-adaptive if it chooses all q values for X before the game begins (that is, it does not use any information gleaned from previous queries to choose each X as it goes).",
        "These definitions have proven useful for analyzing various modes of operation. For example, one can define a similar game for measuring the security of a block cipher-based encryption algorithm, and then try to show (through a reduction argument) that the probability of an adversary winning this new game is not much more than PE(A) for some A. (The reduction typically provides limits on q and the running time of A.) Equivalently, if PE(A) is small for all relevant A, then no attacker has a significant probability of winning the new game. This formalizes the idea that the higher-level algorithm inherits the block cipher's security.\n\nIdeal cipher model",
        "Practical evaluation\nBlock ciphers may be evaluated according to multiple criteria in practice. Common factors include:\n Key parameters, such as its key size and block size, both of which provide an upper bound on the security of the cipher.\n The estimated security level, which is based on the confidence gained in the block cipher design after it has largely withstood major efforts in cryptanalysis over time, the design's mathematical soundness, and the existence of practical or certificational attacks.\n The cipher's complexity and its suitability for implementation in hardware or software. Hardware implementations may measure the complexity in terms of gate count or energy consumption, which are important parameters for resource-constrained devices.",
        "The cipher's performance in terms of processing throughput on various platforms, including its memory requirements.\n The cost of the cipher refers to licensing requirements that may apply due to intellectual property rights.\n The flexibility of the cipher includes its ability to support multiple key sizes and block lengths.",
        "Notable block ciphers\n\nLucifer / DES\n\nLucifer is generally considered to be the first civilian block cipher, developed at IBM in the 1970s based on work done by Horst Feistel. A revised version of the algorithm was adopted as a U.S. government Federal Information Processing Standard: FIPS PUB 46 Data Encryption Standard (DES). It was chosen by the U.S. National Bureau of Standards (NBS) after a public invitation for submissions and some internal changes by NBS (and, potentially, the NSA). DES was publicly released in 1976 and has been widely used.",
        "DES was designed to, among other things, resist a certain cryptanalytic attack known to the NSA and rediscovered by IBM, though unknown publicly until rediscovered again and published by Eli Biham and Adi Shamir in the late 1980s. The technique is called differential cryptanalysis and remains one of the few general attacks against block ciphers; linear cryptanalysis is another but may have been unknown even to the NSA, prior to its publication by Mitsuru Matsui. DES prompted a large amount of other work and publications in cryptography and cryptanalysis in the open community and it inspired many new cipher designs.",
        "DES has a block size of 64 bits and a key size of 56 bits. 64-bit blocks became common in block cipher designs after DES. Key length depended on several factors, including government regulation. Many observers in the 1970s commented that the 56-bit key length used for DES was too short. As time went on, its inadequacy became apparent, especially after a special-purpose machine designed to break DES was demonstrated in 1998 by the Electronic Frontier Foundation. An extension to DES, Triple DES, triple-encrypts each block with either two independent keys (112-bit key and 80-bit security) or three independent keys (168-bit key and 112-bit security). It was widely adopted as a replacement. As of 2011, the three-key version is still considered secure, though the National Institute of Standards",
        "independent keys (168-bit key and 112-bit security). It was widely adopted as a replacement. As of 2011, the three-key version is still considered secure, though the National Institute of Standards and Technology (NIST) standards no longer permit the use of the two-key version in new applications, due to its 80-bit security level.",
        "IDEA\nThe International Data Encryption Algorithm (IDEA) is a block cipher designed by James Massey of ETH Zurich and Xuejia Lai; it was first described in 1991, as an intended replacement for DES.\n\nIDEA operates on 64-bit blocks using a 128-bit key and consists of a series of eight identical transformations (a round) and an output transformation (the half-round). The processes for encryption and decryption are similar. IDEA derives much of its security by interleaving operations from different groups – modular addition and multiplication, and bitwise exclusive or (XOR) – which are algebraically \"incompatible\" in some sense.",
        "The designers analysed IDEA to measure its strength against differential cryptanalysis and concluded that it is immune under certain assumptions. No successful linear or algebraic weaknesses have been reported. , the best attack which applies to all keys can break a full 8.5-round IDEA using a narrow-bicliques attack about four times faster than brute force.\n\nRC5\n\nRC5 is a block cipher designed by Ronald Rivest in 1994 which, unlike many other ciphers, has a variable block size (32, 64, or 128 bits), key size (0 to 2040 bits), and a number of rounds (0 to 255). The original suggested choice of parameters was a block size of 64 bits, a 128-bit key, and 12 rounds.",
        "A key feature of RC5 is the use of data-dependent rotations; one of the goals of RC5 was to prompt the study and evaluation of such operations as a cryptographic primitive. RC5 also consists of a number of modular additions and XORs. The general structure of the algorithm is a Feistel-like a network. The encryption and decryption routines can be specified in a few lines of code. The key schedule, however, is more complex, expanding the key using an essentially one-way function with the binary expansions of both e and the golden ratio as sources of \"nothing up my sleeve numbers\". The tantalizing simplicity of the algorithm together with the novelty of the data-dependent rotations has made RC5 an attractive object of study for cryptanalysts.",
        "12-round RC5 (with 64-bit blocks) is susceptible to a differential attack using 244 chosen plaintexts. 18–20 rounds are suggested as sufficient protection.\n\nRijndael / AES\n\nThe Rijndael cipher developed by Belgian cryptographers, Joan Daemen and Vincent Rijmen was one of the competing designs to replace DES. It won the 5-year public competition to become the AES, (Advanced Encryption Standard).",
        "Adopted by NIST in 2001, AES has a fixed block size of 128 bits and a key size of 128, 192, or 256 bits, whereas Rijndael can be specified with block and key sizes in any multiple of 32 bits, with a minimum of 128 bits. The block size has a maximum of 256 bits, but the key size has no theoretical maximum. AES operates on a 4×4 column-major order matrix of bytes, termed the state (versions of Rijndael with a larger block size have additional columns in the state).\n\nBlowfish",
        "Blowfish\n\nBlowfish is a block cipher, designed in 1993 by Bruce Schneier and included in a large number of cipher suites and encryption products. Blowfish has a 64-bit block size and a variable key length from 1 bit up to 448 bits. It is a 16-round Feistel cipher and uses large key-dependent S-boxes. Notable features of the design include the key-dependent S-boxes and a highly complex key schedule.",
        "It was designed as a general-purpose algorithm, intended as an alternative to the aging DES and free of the problems and constraints associated with other algorithms. At the time Blowfish was released, many other designs were proprietary, encumbered by patents, or were commercial/government secrets. Schneier has stated that \"Blowfish is unpatented, and will remain so in all countries. The algorithm is hereby placed in the public domain, and can be freely used by anyone.\" The same applies to Twofish, a successor algorithm from Schneier.\n\nGeneralizations\n\nTweakable block ciphers",
        "Generalizations\n\nTweakable block ciphers\n\nM. Liskov, R. Rivest, and D. Wagner have described a generalized version of block ciphers called \"tweakable\" block ciphers. A tweakable block cipher accepts a second input called the tweak along with its usual plaintext or ciphertext input. The tweak, along with the key, selects the permutation computed by the cipher. If changing tweaks is sufficiently lightweight (compared with a usually fairly expensive key setup operation), then some interesting new operation modes become possible. The disk encryption theory article describes some of these modes.\n\nFormat-preserving encryption",
        "Block ciphers traditionally work over a binary alphabet. That is, both the input and the output are binary strings, consisting of n zeroes and ones. In some situations, however, one may wish to have a block cipher that works over some other alphabet; for example, encrypting 16-digit credit card numbers in such a way that the ciphertext is also a 16-digit number might facilitate adding an encryption layer to legacy software. This is an example of format-preserving encryption. More generally, format-preserving encryption requires a keyed permutation on some finite language. This makes format-preserving encryption schemes a natural generalization of (tweakable) block ciphers. In contrast, traditional encryption schemes, such as CBC, are not permutations because the same plaintext can encrypt",
        "encryption schemes a natural generalization of (tweakable) block ciphers. In contrast, traditional encryption schemes, such as CBC, are not permutations because the same plaintext can encrypt multiple different ciphertexts, even when using a fixed key.",
        "Relation to other cryptographic primitives\nBlock ciphers can be used to build other cryptographic primitives, such as those below. For these other primitives to be cryptographically secure, care has to be taken to build them the right way.",
        "Stream ciphers can be built using block ciphers. OFB mode and CTR mode are block modes that turn a block cipher into a stream cipher.\n Cryptographic hash functions can be built using block ciphers. See the one-way compression function for descriptions of several such methods. The methods resemble the block cipher modes of operation usually used for encryption.\n Cryptographically secure pseudorandom number generators (CSPRNGs) can be built using block ciphers.\n Secure pseudorandom permutations of arbitrarily sized finite sets can be constructed with block ciphers; see Format-Preserving Encryption.",
        "Secure pseudorandom permutations of arbitrarily sized finite sets can be constructed with block ciphers; see Format-Preserving Encryption.\n A publicly known unpredictable permutation combined with key whitening is enough to construct a block cipher -- such as the single-key Even–Mansour cipher, perhaps the simplest possible provably secure block cipher.\n Message authentication codes (MACs) are often built from block ciphers. CBC-MAC, OMAC, and PMAC are such MACs.\n Authenticated encryption is also built from block ciphers. It means to both encrypt and MAC at the same time. That is to both provide confidentiality and authentication. CCM, EAX, GCM, and OCB are such authenticated encryption modes.",
        "Just as block ciphers can be used to build hash functions, like SHA-1 and SHA-2 are based on block ciphers which are also used independently as SHACAL, hash functions can be used to build block ciphers. Examples of such block ciphers are BEAR and LION.\n\nSee also\n Cipher security summary\n Topics in cryptography\n XOR cipher\n\nReferences\n\nFurther reading\n\nExternal links\n A list of many symmetric algorithms, the majority of which are block ciphers.\n The block cipher lounge\n What is a block cipher? from RSA FAQ\n Block Cipher based on Gold Sequences and Chaotic Logistic Tent System\n\nCryptographic primitives\nArab inventions\nEgyptian inventions"
    ],
    [
        "Bob Young (businessman)\nRobert Young (born 1953/1954) is a serial entrepreneur who is best known for founding Red Hat Inc., the open source software company. He owns the franchises for Forge FC of the Canadian Premier League as well as the Hamilton Tiger-Cats of the Canadian Football League for which he serves as self-styled Caretaker of the team.\n\nEarly life\nHe was born in Hamilton, Ontario, Canada. He attended Trinity College School in Port Hope, Ontario. He received a Bachelor of Arts from Victoria College at the University of Toronto.\n\nCareer",
        "Career\n\nPrior to Red Hat, Young built a couple of computer rental and leasing businesses, including founding Vernon Computer Rentals in 1984. Descendants of Vernon are still operating under that name. After leaving Vernon, Young founded the ACC Corp Inc. in 1993.\n\nMarc Ewing and Young co-founded open-source software company Red Hat. Red Hat was a member of the S&P 500 Index before being purchased by IBM on July 9, 2019. Marc Ewing and Young's partnership started in 1994 when ACC acquired the Red Hat trademarks from Ewing. In early 1995, ACC changed its name to Red Hat Software, which has subsequently been shortened to simply Red Hat, Inc. Young served as Red Hat's CEO until 1999.",
        "In 2002, Young founded Lulu.com, a print-on-demand, self-publishing company, and served as CEO. In 2006, Young established the Lulu Blooker Prize, a book prize for books that began as blogs. He launched the prize partly as a means to promote Lulu.\n\nYoung served as CEO of PrecisionHawk, a commercial drone technology company, from 2015 to 2017. Prior to being named PrecisionHawk's CEO in 2015, he was an early investor in the company. He continues to serve on its board as chairman.\n\nYoung also co-founded Linux Journal in 1994, and in 2003, he purchased the Hamilton Tiger-Cats of the Canadian Football League. In 2022, he sold minority stakes in the Tiger-Cats to Jim Lawson, team President Scott Mitchell, and American steel manufacturer Stelco.",
        "Young focuses his philanthropic efforts on access to information and advancement of knowledge.  In 1999, he co-founded The Center for the Public Domain. Young has supported the Creative Commons, Public Knowledge.org, the Dictionary of Old English, Loran Scholarship Foundation, ibiblio.org,  and the NCSU eGames, among others.\n\nReferences\n\nYear of birth missing (living people)\nLiving people\nOpen source people\nRed Hat people\nUniversity of Toronto alumni\nBusinesspeople from Hamilton, Ontario\nHamilton Tiger-Cats owners\nForge FC non-playing staff\nCanadian soccer chairmen and investors"
    ],
    [
        "Booch method\nThe Booch method is a method for object-oriented software development.  It is composed of an object modeling language, an iterative object-oriented development process, and a set of recommended practices.\n\nThe method was authored by Grady Booch when he was working for Rational Software (acquired by IBM), published in 1992 and revised in 1994.  It was widely used in software engineering for object-oriented analysis and design and benefited from ample documentation and support tools.",
        "The notation aspect of the Booch methodology was superseded by the Unified Modeling Language (UML), which features graphical elements from the Booch method along with elements from the object-modeling technique (OMT) and object-oriented software engineering (OOSE).  Methodological aspects of the Booch method have been incorporated into several methodologies and processes, the primary such methodology being the Rational Unified Process (RUP).\n\nContent of the method\n\nThe Booch notation is characterized by cloud shapes to represent classes and distinguishes the following diagrams:\n\n \nThe process is organized around a macro and a micro process.",
        "The Booch notation is characterized by cloud shapes to represent classes and distinguishes the following diagrams:\n\n \nThe process is organized around a macro and a micro process.\n\nThe macro process identifies the following activities cycle: \n Conceptualization : establish core requirements\n Analysis : develop a model of the desired behavior\n Design : create an architecture \n Evolution: for the implementation\n Maintenance : for evolution after the delivery\n\nThe micro process is applied to new classes, structures or behaviors that emerge during the macro process.  It is made of the following cycle: \n Identification of classes and objects\n Identification of their semantics \n Identification of their relationships\n Specification of their interfaces and implementation\n\nReferences",
        "References\n\nExternal links\nClass diagrams, Object diagrams, State Event diagrams and Module diagrams.\n The Booch Method of Object-Oriented Analysis & Design\n\nSoftware design\nObject-oriented programming\nProgramming principles\n\nde:Grady Booch#Booch-Notation"
    ],
    [
        "Boolean satisfiability problem",
        "In logic and computer science, the Boolean satisfiability problem (sometimes called propositional satisfiability problem and abbreviated SATISFIABILITY, SAT or B-SAT) is the problem of determining if there exists an interpretation that satisfies a given Boolean formula. In other words, it asks whether the variables of a given Boolean formula can be consistently replaced by the values TRUE or FALSE in such a way that the formula evaluates to TRUE. If this is the case, the formula is called satisfiable. On the other hand, if no such assignment exists, the function expressed by the formula is FALSE for all possible variable assignments and the formula is unsatisfiable. For example, the formula \"a AND NOT b\" is satisfiable because one can find the values a = TRUE and b = FALSE, which make (a",
        "for all possible variable assignments and the formula is unsatisfiable. For example, the formula \"a AND NOT b\" is satisfiable because one can find the values a = TRUE and b = FALSE, which make (a AND NOT b) = TRUE. In contrast, \"a AND NOT a\" is unsatisfiable.",
        "SAT is the first problem that was proven to be NP-complete; see Cook–Levin theorem. This means that all problems in the complexity class NP, which includes a wide range of natural decision and optimization problems, are at most as difficult to solve as SAT. There is no known algorithm that efficiently solves each SAT problem, and it is generally believed that no such algorithm exists; yet this belief has not been proved mathematically, and resolving the question of whether SAT has a polynomial-time algorithm is equivalent to the P versus NP problem, which is a famous open problem in the theory of computing.",
        "Nevertheless, as of 2007, heuristic SAT-algorithms are able to solve problem instances involving tens of thousands of variables and formulas consisting of millions of symbols, which is sufficient for many practical SAT problems from, e.g., artificial intelligence, circuit design, and automatic theorem proving.",
        "Definitions\nA propositional logic formula, also called Boolean expression, is built from variables, operators AND (conjunction, also denoted by ∧), OR (disjunction, ∨), NOT (negation, ¬), and parentheses.\nA formula is said to be satisfiable if it can be made TRUE by assigning appropriate logical values (i.e. TRUE, FALSE) to its variables.\nThe Boolean satisfiability problem (SAT) is, given a formula, to check whether it is satisfiable.\nThis decision problem is of central importance in many areas of computer science, including theoretical computer science, complexity theory, algorithmics, cryptography and artificial intelligence.\n\nConjunctive normal form",
        "A literal is either a variable (in which case it is called a positive literal) or the negation of a variable (called a negative literal).\nA clause is a disjunction of literals (or a single literal). A clause is called a Horn clause if it contains at most one positive literal.\nA formula is in conjunctive normal form (CNF) if it is a conjunction of clauses (or a single clause).",
        "A formula is in conjunctive normal form (CNF) if it is a conjunction of clauses (or a single clause).\nFor example,  is a positive literal,  is a negative literal,  is a clause. The formula  is  in conjunctive normal form; its first and third clauses are Horn clauses, but its second clause is not. The formula is satisfiable, by choosing x1 = FALSE, x2 = FALSE, and x3 arbitrarily, since (FALSE ∨ ¬FALSE) ∧ (¬FALSE ∨ FALSE ∨ x3) ∧ ¬FALSE evaluates to (FALSE ∨ TRUE) ∧ (TRUE ∨ FALSE ∨ x3) ∧ TRUE, and in turn to TRUE ∧ TRUE ∧ TRUE (i.e. to TRUE). In contrast, the CNF formula a ∧ ¬a, consisting of two clauses of one literal, is unsatisfiable, since for a=TRUE or a=FALSE it evaluates to TRUE ∧ ¬TRUE (i.e., FALSE) or FALSE ∧ ¬FALSE (i.e., again FALSE), respectively.",
        "For some versions of the SAT problem, it is useful to define the notion of a generalized conjunctive normal form formula, viz. as a conjunction of arbitrarily many generalized clauses, the latter being of the form  for some Boolean function R and (ordinary) literals . Different sets of allowed boolean functions lead to different problem versions. As an example, R(¬x,a,b) is a generalized clause, and R(¬x,a,b) ∧ R(b,y,c) ∧ R(c,d,¬z) is a generalized conjunctive normal form. This formula is used below, with R being the ternary operator that is TRUE just when exactly one of its arguments is.",
        "Using the laws of Boolean algebra, every propositional logic formula can be transformed into an equivalent conjunctive normal form, which may, however, be exponentially longer. For example, transforming the formula\n(x1∧y1) ∨ (x2∧y2) ∨ ... ∨ (xn∧yn)\ninto conjunctive normal form yields\n\n;\nwhile the former is a disjunction of n conjunctions of 2 variables, the latter consists of 2n clauses of n variables.\n\nHowever, with use of the Tseytin transformation, we may find an equisatisfiable conjunctive normal form formula with length linear in the size of the original propositional logic formula.\n\nComplexity",
        "Complexity\n\nSAT was the first known NP-complete problem, as proved by Stephen Cook at the University of Toronto in 1971 and independently by Leonid Levin at the Russian Academy of Sciences in 1973. Until that time, the concept of an NP-complete problem did not even exist.\nThe proof shows how every decision problem in the complexity class NP can be reduced to the SAT problem for CNF formulas, sometimes called CNFSAT.\nA useful property of Cook's reduction is that it preserves the number of accepting answers. For example, deciding whether a given graph has a 3-coloring is another problem in NP; if a graph has 17 valid 3-colorings, the SAT formula produced by the Cook–Levin reduction will have 17 satisfying assignments.",
        "NP-completeness only refers to the run-time of the worst case instances.  Many of the instances that occur in practical applications can be solved much more quickly.  See Algorithms for solving SAT below.\n\n3-satisfiability\n\nLike the satisfiability problem for arbitrary formulas, determining the satisfiability of a formula in conjunctive normal form where each clause is limited to at most three literals is NP-complete also; this problem is called 3-SAT, 3CNFSAT, or 3-satisfiability.\nTo reduce the unrestricted SAT problem to 3-SAT, transform each clause  to a conjunction of  clauses",
        "where  are fresh variables not occurring elsewhere.\nAlthough the two formulas are not logically equivalent, they are equisatisfiable. The formula resulting from transforming all clauses is at most 3 times as long as its original, i.e. the length growth is polynomial.",
        "3-SAT is one of Karp's 21 NP-complete problems, and it is used as a starting point for proving that other problems are also NP-hard. This is done by polynomial-time reduction from 3-SAT to the other problem. An example of a problem where this method has been used is the clique problem: given a CNF formula consisting of c clauses, the corresponding graph consists of a vertex for each literal, and an edge between each two non-contradicting literals from different clauses, cf. picture. The graph has a c-clique if and only if the formula is satisfiable.\n\nThere is a simple randomized algorithm due to Schöning (1999) that runs in time (4/3)n where n is the number of variables in the 3-SAT proposition, and succeeds with high probability to correctly decide 3-SAT.",
        "The exponential time hypothesis asserts that no algorithm can solve 3-SAT (or indeed k-SAT for any ) in  time (i.e., fundamentally faster than exponential in n).\n\nSelman, Mitchell, and Levesque (1996) give empirical data on the difficulty of randomly generated 3-SAT formulas, depending on their size parameters.\nDifficulty is measured in number recursive calls made by a DPLL algorithm. They identified a phase transition region from almost certainly satisfiable to almost certainly unsatisfiable formulas at the clauses-to-variables ratio at about 4.26.",
        "3-satisfiability can be generalized to k-satisfiability (k-SAT, also k-CNF-SAT), when formulas in CNF are considered with each clause containing up to k literals.\nHowever, since for any k ≥ 3, this problem can neither be easier than 3-SAT nor harder than SAT, and the latter two are NP-complete, so must be k-SAT.",
        "Some authors restrict k-SAT to CNF formulas with exactly k literals. This doesn't lead to a different complexity class either, as each clause  with j < k literals can be padded with fixed dummy variables to\n.\nAfter padding all clauses, 2k-1 extra clauses have to be appended to ensure that only  can lead to a satisfying assignment. Since k doesn't depend on the formula length, the extra clauses lead to a constant increase in length. For the same reason, it does not matter whether duplicate literals are allowed in clauses, as in .\n\nSpecial cases of SAT",
        "Special cases of SAT\n\nConjunctive normal form\nConjunctive normal form (in particular with 3 literals per clause) is often considered the canonical representation for SAT formulas. As shown above, the general SAT problem reduces to 3-SAT, the problem of determining satisfiability for formulas in this form.",
        "Disjunctive normal form",
        "SAT is trivial if the formulas are restricted to those in disjunctive normal form, that is, they are a disjunction of conjunctions of literals. Such a formula is indeed satisfiable if and only if at least one of its conjunctions is satisfiable, and a conjunction is satisfiable if and only if it does not contain both x and NOT x for some variable x. This can be checked in linear time. Furthermore, if they are restricted to being in full disjunctive normal form, in which every variable appears exactly once in every conjunction, they can be checked in constant time (each conjunction represents one satisfying assignment).  But it can take exponential time and space to convert a general SAT problem to disjunctive normal form; for an example exchange \"∧\" and \"∨\" in the above exponential blow-up",
        "satisfying assignment).  But it can take exponential time and space to convert a general SAT problem to disjunctive normal form; for an example exchange \"∧\" and \"∨\" in the above exponential blow-up example for conjunctive normal forms.",
        "Exactly-1 3-satisfiability",
        "A variant of the 3-satisfiability problem is the one-in-three 3-SAT (also known variously as 1-in-3-SAT and exactly-1 3-SAT).\nGiven a conjunctive normal form with three literals per clause, the problem is to determine whether there exists a truth assignment to the variables so that each clause has exactly one TRUE literal (and thus exactly two FALSE literals). In contrast, ordinary 3-SAT requires that every clause has at least one TRUE literal.\nFormally, a one-in-three 3-SAT problem is given as a generalized conjunctive normal form with all generalized clauses using a ternary operator R that is TRUE just if exactly one of its arguments is. When all literals of a one-in-three 3-SAT formula are positive, the satisfiability problem is called one-in-three positive 3-SAT.",
        "One-in-three 3-SAT, together with its positive case, is listed as NP-complete problem \"LO4\" in the standard reference, Computers and Intractability: A Guide to the Theory of NP-Completeness\nby Michael R. Garey and David S. Johnson.  One-in-three 3-SAT was proved to be NP-complete by Thomas Jerome Schaefer as a special case of Schaefer's dichotomy theorem, which asserts that any problem generalizing Boolean satisfiability in a certain way is either in the class P or is NP-complete.\n\nSchaefer gives a construction allowing an easy polynomial-time reduction from 3-SAT to one-in-three 3-SAT.  Let \"(x or y or z)\" be a clause in a 3CNF formula.  Add six fresh boolean variables a, b, c, d, e, and f, to be used to simulate this clause and no other.",
        "Then the formula R(x,a,d) ∧ R(y,b,d) ∧ R(a,b,e) ∧ R(c,d,f) ∧ R(z,c,FALSE) is satisfiable by some setting of the fresh variables if and only if at least one of x, y, or z is TRUE, see picture (left).  Thus any 3-SAT instance with m clauses and n variables may be converted into an equisatisfiable one-in-three 3-SAT instance with 5m clauses and n+6m variables.\nAnother reduction involves only four fresh variables and three clauses: R(¬x,a,b) ∧ R(b,y,c) ∧ R(c,d,¬z), see picture (right).\n\nNot-all-equal 3-satisfiability",
        "Not-all-equal 3-satisfiability\n\nAnother variant is the not-all-equal 3-satisfiability problem (also called NAE3SAT).\nGiven a conjunctive normal form with three literals per clause, the problem is to determine if an assignment to the variables exists such that in no clause all three literals have the same truth value. This problem is NP-complete, too, even if no negation symbols are admitted, by Schaefer's dichotomy theorem.\n\nLinear SAT \nA 3-SAT formula is Linear SAT (LSAT) if each clause (viewed as a set of literals) intersects at most one other clause, and, moreover, if two clauses intersect, then they have exactly one literal in common. An LSAT formula can be depicted as a set of disjoint semi-closed intervals on a line. Deciding whether an LSAT formula is satisfiable is NP-complete.",
        "2-satisfiability\n\nSAT is easier if the number of literals in a clause is limited to at most 2, in which case the problem is called 2-SAT. This problem can be solved in polynomial time, and in fact is complete for the complexity class NL. If additionally all OR operations in literals are changed to XOR operations, the result is called exclusive-or 2-satisfiability, which is a problem complete for the complexity class SL = L.\n\nHorn-satisfiability",
        "Horn-satisfiability\n\nThe problem of deciding the satisfiability of a given conjunction of Horn clauses is called Horn-satisfiability, or HORN-SAT.\nIt can be solved in polynomial time by a single step of the Unit propagation algorithm, which produces the single minimal model of the set of Horn clauses (w.r.t. the set of literals assigned to TRUE).\nHorn-satisfiability is P-complete. It can be seen as P's version of the Boolean satisfiability problem.\nAlso, deciding the truth of quantified Horn formulas can be done in polynomial time.",
        "Horn clauses are of interest because they are able to express implication of one variable from a set of other variables. Indeed, one such clause ¬x1 ∨ ... ∨ ¬xn ∨ y can be rewritten as x1 ∧ ... ∧ xn → y, that is, if x1,...,xn are all TRUE, then y needs to be TRUE as well.",
        "A generalization of the class of Horn formulae is that of renameable-Horn formulae, which is the set of formulae that can be placed in Horn form by replacing some variables with their respective negation.\nFor example, (x1 ∨ ¬x2) ∧ (¬x1 ∨ x2 ∨ x3) ∧ ¬x1 is not a Horn formula, but can be renamed to the Horn formula (x1 ∨ ¬x2) ∧ (¬x1 ∨ x2 ∨ ¬y3) ∧ ¬x1 by introducing y3 as negation of x3.\nIn contrast, no renaming of (x1 ∨ ¬x2 ∨ ¬x3) ∧ (¬x1 ∨ x2 ∨ x3) ∧ ¬x1 leads to a Horn formula.\nChecking the existence of such a replacement can be done in linear time; therefore, the satisfiability of such formulae is in P as it can be solved by first performing this replacement and then checking the satisfiability of the resulting Horn formula.\n\nXOR-satisfiability",
        "Another special case is the class of problems where each clause contains XOR (i.e. exclusive or) rather than (plain) OR operators.",
        "This is in P, since an XOR-SAT formula can also be viewed as a system of linear equations mod 2, and can be solved in cubic time by Gaussian elimination; see the box for an example. This recast is based on the kinship between Boolean algebras and Boolean rings, and the fact that arithmetic modulo two forms a finite field. Since a XOR b XOR c evaluates to TRUE if and only if exactly 1 or 3 members of {a,b,c} are TRUE, each solution of the 1-in-3-SAT problem for a given CNF formula is also a solution of the XOR-3-SAT problem, and in turn each solution of XOR-3-SAT is a solution of 3-SAT, cf. picture. As a consequence, for each CNF formula, it is possible to solve the XOR-3-SAT problem defined by the formula, and based on the result infer either that the 3-SAT problem is solvable or that the",
        "As a consequence, for each CNF formula, it is possible to solve the XOR-3-SAT problem defined by the formula, and based on the result infer either that the 3-SAT problem is solvable or that the 1-in-3-SAT problem is unsolvable.",
        "Provided that the complexity classes P and NP are not equal, neither 2-, nor Horn-, nor XOR-satisfiability is NP-complete, unlike SAT.\n\nSchaefer's dichotomy theorem\n\nThe restrictions above (CNF, 2CNF, 3CNF, Horn, XOR-SAT) bound the considered formulae to be conjunctions of subformulae; each restriction states a specific form for all subformulae: for example, only binary clauses can be subformulae in 2CNF.\n\nSchaefer's dichotomy theorem states that, for any restriction to Boolean functions that can be used to form these subformulae, the corresponding satisfiability problem is in P or NP-complete.  The membership in P of the satisfiability of 2CNF, Horn, and XOR-SAT formulae are special cases of this theorem.\n\nThe following table summarizes some common variants of SAT.",
        "The following table summarizes some common variants of SAT.\n\nExtensions of SAT\nAn extension that has gained significant popularity since 2003 is satisfiability modulo theories (SMT) that can enrich CNF formulas with linear constraints, arrays, all-different constraints, uninterpreted functions, etc. Such extensions typically remain NP-complete, but very efficient solvers are now available that can handle many such kinds of constraints.",
        "The satisfiability problem becomes more difficult if both \"for all\" (∀) and \"there exists\" (∃) quantifiers are allowed to bind the Boolean variables.\nAn example of such an expression would be ; it is valid, since for all values of x and y, an appropriate value of z can be found, viz. z=TRUE if both x and y are FALSE, and z=FALSE else.\nSAT itself (tacitly) uses only ∃ quantifiers.\nIf only ∀ quantifiers are allowed instead, the so-called tautology problem is obtained, which is co-NP-complete.",
        "SAT itself (tacitly) uses only ∃ quantifiers.\nIf only ∀ quantifiers are allowed instead, the so-called tautology problem is obtained, which is co-NP-complete.\nIf both quantifiers are allowed, the problem is called the quantified Boolean formula problem (QBF), which can be shown to be PSPACE-complete. It is widely believed that PSPACE-complete problems are strictly harder than any problem in NP, although this has not yet been proved. Using highly parallel P systems, QBF-SAT problems can be solved in linear time.",
        "Ordinary SAT asks if there is at least one variable assignment that makes the formula true. A variety of variants deal with the number of such assignments:\n MAJ-SAT asks if the majority of all assignments make the formula TRUE. It is known to be complete for PP, a probabilistic class.\n #SAT, the problem of counting how many variable assignments satisfy a formula, is a counting problem, not a decision problem, and is #P-complete.\n UNIQUE SAT is the problem of determining whether a formula has exactly one assignment. It is complete for US, the complexity class describing problems solvable by a non-deterministic polynomial time Turing machine that accepts when there is exactly one nondeterministic accepting path and rejects otherwise.",
        "UNAMBIGUOUS-SAT is the name given to the satisfiability problem when the input is restricted to formulas having at most one satisfying assignment. The problem is also called USAT. A solving algorithm for UNAMBIGUOUS-SAT is allowed to exhibit any behavior, including endless looping, on a formula having several satisfying assignments. Although this problem seems easier, Valiant and Vazirani have shown that if there is a practical (i.e. randomized polynomial-time) algorithm to solve it, then all problems in NP can be solved just as easily.",
        "MAX-SAT, the maximum satisfiability problem, is an FNP generalization of SAT. It asks for the maximum number of clauses which can be satisfied by any assignment. It has efficient approximation algorithms, but is NP-hard to solve exactly. Worse still, it is APX-complete, meaning there is no polynomial-time approximation scheme (PTAS) for this problem unless P=NP.\nWMSAT is the problem of finding an assignment of minimum weight that satisfy a monotone Boolean formula (i.e. a formula without any negation). Weights of propositional variables are given in the input of the problem. The weight of an assignment is the sum of weights of true variables. That problem is NP-complete (see Th. 1 of ).",
        "Other generalizations include satisfiability for first- and second-order logic, constraint satisfaction problems, 0-1 integer programming.",
        "Finding a satisfying assignment\nWhile SAT is a decision problem, the search problem of finding a satisfying assignment reduces to SAT. That is, each algorithm which correctly answers if an instance of SAT is solvable can be used to find a satisfying assignment. First, the question is asked on the given formula Φ. If the answer is \"no\", the formula is unsatisfiable. Otherwise, the question is asked on the partly instantiated formula Φ{x1=TRUE}, i.e. Φ with the first variable x1 replaced by TRUE, and simplified accordingly. If the answer is \"yes\", then x1=TRUE, otherwise x1=FALSE. Values of other variables can be found subsequently in the same way. In total, n+1 runs of the algorithm are required, where n is the number of distinct variables in Φ.",
        "This property is used in several theorems in complexity theory:\n\n NP ⊆ P/poly ⇒ PH = Σ2   (Karp–Lipton theorem)\n NP ⊆ BPP ⇒ NP = RP\n P = NP ⇒ FP = FNP\n\nAlgorithms for solving SAT",
        "Algorithms for solving SAT\n\nSince the SAT problem is NP-complete, only algorithms with exponential worst-case complexity are known for it. In spite of this, efficient and scalable algorithms for SAT were developed during the 2000s and have contributed to dramatic advances in our ability to automatically solve problem instances involving tens of thousands of variables and millions of constraints (i.e. clauses). Examples of such problems in electronic design automation (EDA) include formal equivalence checking, model checking, formal verification of pipelined microprocessors, automatic test pattern generation, routing of FPGAs, planning, and scheduling problems, and so on. A SAT-solving engine is also considered to be an essential component in the electronic design automation toolbox.",
        "Major techniques used by modern SAT solvers include the Davis–Putnam–Logemann–Loveland algorithm (or DPLL), conflict-driven clause learning (CDCL), and stochastic local search algorithms such as WalkSAT. Almost all SAT solvers include time-outs, so they will terminate in reasonable time even if they cannot find a solution.\nDifferent SAT solvers will find different instances easy or hard, and some excel at proving unsatisfiability, and others at finding solutions. Recent attempts have been made to learn an instance's satisfiability using deep learning techniques.",
        "SAT solvers are developed and compared in SAT-solving contests. Modern SAT solvers are also having significant impact on the fields of software verification, constraint solving in artificial intelligence, and operations research, among others.\n\nSee also\nUnsatisfiable core\nSatisfiability modulo theories\nCounting SAT\nPlanar SAT\nKarloff–Zwick algorithm\nCircuit satisfiability\n\nNotes\n\nExternal links \n\n SAT Game: try solving a Boolean satisfiability problem yourself\n The international SAT competition website\n International Conference on Theory and Applications of Satisfiability Testing\n Journal on Satisfiability, Boolean Modeling and Computation\n SAT Live, an aggregate website for research on the satisfiability problem\n Yearly evaluation of MaxSAT solvers\n\nReferences",
        "References\n\nSources\n This article includes material from https://web.archive.org/web/20070708233347/http://www.sigda.org/newsletter/2006/eNews_061201.html by Prof. Karem A. Sakallah.\n\nFurther reading\n(by date of publication)\n\n \n \n \n \n \n \n \n \n \n\nBoolean algebra\nElectronic design automation\nFormal methods\nLogic in computer science\nNP-complete problems\nSatisfiability problems"
    ],
    [
        "Braille embosser\nA braille embosser is an impact printer that renders text as tactile braille cells. Using braille translation software, a document or digital text can be embossed with relative ease. This makes braille production efficient and cost-effective. Braille translation software may be free and open-sourced or paid.\n\nBlind users tend to call other printers \"ink printers,\" to distinguish them from their braille counterparts. This is often the case regardless of the type of printer being discussed (e.g., thermal printers being called \"ink printers\" even though they use no ink).\n\nAs with ink printers and presses, embossers range from those intended for consumers to those used by large publishers. The price of embossers increase with the volume of braille it produces .",
        "As with ink printers and presses, embossers range from those intended for consumers to those used by large publishers. The price of embossers increase with the volume of braille it produces .\n\nTypes \nThe fastest industrial braille embosser is probably the $92,000 Belgian-made NV Interpoint 55, first produced in 1991, which uses a separate air compressor to drive the embossing head and can output up to 800 braille characters per second. Adoption was slow at first; in 2000 the National Federation of the Blind said there were only three of these in the US, one owned by the NFB itself and the other two by the Watchtower Bible and Tract Society. As of 2008, there are more than 60 in use across the world.",
        "Smaller desktop braille embossers are more common and can be found in libraries, universities, and specialist education centers, as well as being privately owned by blind individuals. It may be necessary to use an acoustic cabinet or hood to dampen the noise level.\n\nBraille embossers usually need special braille paper which is thicker and more expensive than normal paper. Some high-end embossers are capable of printing on normal paper. Embossers can be either one-sided or two-sided. Two-sided embossing requires lining up the dots so they do not overlap (called \"interpoint\" because the points on the other side are placed in between the points on the first side). Two-sided embossing uses less paper and reduces the size of the output.",
        "Once one copy of a document has been produced, printing further copies is often quicker by means of a device called a thermoform, which produces copies on soft plastic. However, the resulting braille is not as easily readable as braille that has been freshly embossed, in much the same way that a poor-quality photocopy is not as readable as the original. Hence large publishers do not generally use thermoforms.\n\nSome embossers can produce \"dotty Moon\", i.e., Moon type shapes formed by dots.\n\nSee also \n Braigo\n Mountbatten Brailler\n Book\n E-book\n Braille e-book\n Braille translator\n\nReferences \n\nEmbosser"
    ],
    [
        "Break key\nThe Break key (or the symbol ⎉) of a computer keyboard refers to breaking a telegraph circuit and originated with 19th century practice. In modern usage, the key has no well-defined purpose, but while this is the case, it can be used by software for miscellaneous tasks, such as to switch between multiple login sessions, to terminate a program, or to interrupt a modem connection.\n\nBecause the break function is usually combined with the pause function on one key since the introduction of the IBM Model M 101-key keyboard in 1985, the Break key is also called the Pause key. It can be used to pause some computer games.",
        "History\nA standard telegraph circuit connects all the keys, sounders and batteries in a single series loop. Thus the sounders actuate only when both keys are down (closed, also known as \"marking\" — after the ink marks made on paper tape by early printing telegraphs). So the receiving operator has to hold their key down or close a built-in shorting switch in order to let the other operator send. As a consequence, the receiving operator could interrupt the sending operator by opening their key, breaking the circuit and forcing it into a \"spacing\" condition. Both sounders stop responding to the sender's keying, alerting the sender. (A physical break in the telegraph line would have the same effect.)",
        "The teleprinter operated in a very similar fashion except that the sending station kept the loop closed (logic 1, or \"marking\") even during short pauses between characters. Holding down a special \"break\" key opened the loop, forcing it into a continuous logic 0, or \"spacing\", condition. When this occurred, the teleprinter mechanisms continually actuated without printing anything, as the all-0s character is the non-printing NUL in both Baudot and ASCII. The resulting noise got the sending operator's attention.",
        "This practice carried over to teleprinter use on time-sharing computers. A continuous spacing (logical 0) condition violates the rule that every valid character has to end with one or more logic 1 (marking) \"stop\" bits. The computer (specifically the UART) recognized this as a special \"break\" condition and generated an interrupt that typically stopped a running program or forced the operating system to prompt for a login. Although asynchronous serial telegraphy is now rare, the  key once used with terminal emulators can still be used by software for similar purposes.",
        "Sinclair\nOn the ZX80 and ZX81 computers, the Break is accessed by pressing . On the ZX Spectrum it is accessed by . The Spectrum+ and later computers have a dedicated  key. It does not trigger an interrupt but will halt any running BASIC program, or terminate the loading or saving of data to cassette tape. An interrupted BASIC program can usually be resumed with the CONTINUE command. The Sinclair QL computer, without a  key, maps the function to .",
        "BBC Micro\nOn a BBC Micro computer, the  key generates a hardware reset which would normally cause a warm restart of the computer. A cold restart is triggered by pressing . If a filing system is installed,  will cause the computer to search for and load or run a file called !Boot on the filing system's default device (e.g. floppy disk 0, network user BOOT). The latter two behaviours were inherited by the successor to Acorn MOS, RISC OS. These behaviours could be changed or exchanged in software, and were often used in rudimentary anti-piracy techniques.\n\nBecause of the BBC Micro's near universal usage in British schools, later versions of the machine incorporated a physical lock on the Break key to stop children from intentionally resetting the computer.",
        "Modern keyboards",
        "On many modern PCs,  interrupts screen output by BIOS until another key is pressed. This is effective during boot in text mode and in a DOS box in Windows safe mode with 50 lines. On early keyboards without a  key (before the introduction of 101/102-key keyboards) the Pause function was assigned to , and the Break function to ; these key-combinations still work with most programs, even on modern PCs with modern keyboards. Pressing the dedicated  key on 101/102-key keyboards sends the same scancodes as pressing , then , then releasing them in the reverse order would do; additionally, an E1hex prefix is sent, which enables 101/102-key-aware software to discern the two situations, while older software usually just ignores the prefix. The  key is different from all other keys in that it sends",
        "is sent, which enables 101/102-key-aware software to discern the two situations, while older software usually just ignores the prefix. The  key is different from all other keys in that it sends no scancodes at all on release in PS/2 modes 1 or 2, so it is impossible to determine whether this key is being held down with older devices. In PS/2 mode 3 or USB HID mode, there is a release scancode, so it is possible to determine whether this key is being held down on modern computers.",
        "On modern keyboards, the  key is usually labeled Pause with Break below, sometimes separated by a line: , or Pause on the top of the keycap and Break on the front, or only Pause without Break at all. In most Windows environments, the key combination  brings up the system properties.\n\nKeyboards without Break key\nCompact and notebook keyboards often do not have a dedicated  key.\n\nSubstitutes for :\n  or  or  on certain Lenovo laptops.\n  or  on certain Dell laptops.\n on some other Dell laptops.\n on Samsung.\n  on certain HP laptops.\n  on certain HP laptops.\n  on certain Logitech (LOGI) keyboards.\n\nSubstitutes for :\n  or  or  on certain Lenovo laptops.\n   on certain Dell laptops.\n  on certain HP laptops.\n  on certain HP laptops.",
        "Substitutes for :\n  or  or  on certain Lenovo laptops.\n   on certain Dell laptops.\n  on certain HP laptops.\n  on certain HP laptops.\n\nFor some Dell laptops, without a  key, press the  and select \"Interrupt\".",
        "Usage for breaking the program's execution",
        "While both  and  combination are commonly implemented as a way of breaking the execution of a console application, they are also used for similar effect in integrated development environments. Although these two are often considered interchangeable, compilers and execution environments usually assign different signals to these. Additionally, in some kernels (e.g. miscellaneous DOS variants)  is detected only at the time OS tries reading from a keyboard buffer and only if it's the only key sequence in the buffer, while  is often translated instantly (e.g. by INT 1Bh under DOS). Because of this,  is usually a more effective choice under these operating systems; sensitivity for these two combinations can be enhanced by the BREAK=ON CONFIG.SYS statement.",
        "See also\nSystem request\nScroll lock\nNum lock\n\nReferences\n\nExternal links\n\nComputer keys\nOut-of-band management"
    ],
    [
        "BT Group\nBT Group plc (trading as BT and formerly British Telecom) is a British multinational telecommunications holding company headquartered in London, England. It has operations in around 180 countries and is the largest provider of fixed-line, broadband and mobile services in the UK, and also provides subscription television and IT services.",
        "BT's origins date back to the founding in 1846 of the Electric Telegraph Company, the world's first public telegraph company, which developed a nationwide communications network. BT Group as it came to be started in 1912, when the General Post Office, a government department, took over the system of the National Telephone Company becoming the monopoly telecoms supplier in the United Kingdom. The Post Office Act of 1969 led to the GPO becoming a public corporation, Post Office Telecommunications. The British Telecom brand was introduced in 1980, and became independent of the Post Office in 1981, officially trading under the name. British Telecom was privatised in 1984, becoming British Telecommunications plc, with some 50 percent of its shares sold to investors. The Government sold its",
        "1981, officially trading under the name. British Telecom was privatised in 1984, becoming British Telecommunications plc, with some 50 percent of its shares sold to investors. The Government sold its remaining stake in further share sales in 1991 and 1993. BT holds a royal warrant and has a primary listing on the London Stock Exchange, and is a constituent of the FTSE 100 Index.",
        "BT controls a number of large subsidiaries. Its BT Enterprise division supplies telecoms services to corporate and government customers worldwide, and its BT Consumer division supplies telephony, broadband, and subscription television services in the United Kingdom to around 18 million customers.\n\nHistory\n\nElectrical telegraphy \n\nA number of privately owned electrical telegraph companies operated in the United Kingdom of Great Britain and Ireland from 1846 onwards. Among them were:\n Electric Telegraph Company, the world's first public telegraph company, which developed a nationwide communications network\n British and Irish Magnetic Telegraph Company\n British Telegraph Company\n London District Telegraph Company\n United Kingdom Telegraph Company",
        "General Post Office \nThe Telegraph Act 1868 passed the control of all these to the Postal Telegraphs Department of the newly formed General Post Office (GPO). The Telegraph Act 1869 granted the GPO a monopoly over communications.",
        "With the invention of the telephone by Alexander Graham Bell in 1876 the GPO began to provide telephone services from some of its telegraph exchanges. It was confirmed in 1880 that the 1869 Act included telephony even though the telephone had not been invented when the Act was first conceived. In 1882, the Postmaster-General, Henry Fawcett started to issue licences to operate a telephone service to private businesses and the telephone system grew under the GPO in some areas and private ownership in others. The GPO's main competitor, the National Telephone Company (NTC), emerged in this market by absorbing other private telephone companies. It controlled most of telephony in Britain before the 1880 ruling on the Telegraph Act 1869 mandated a nationalised service – which was instated in",
        "by absorbing other private telephone companies. It controlled most of telephony in Britain before the 1880 ruling on the Telegraph Act 1869 mandated a nationalised service – which was instated in 1911 prior to the absorption of the NTC into the GPO in 1912.",
        "The trunk network was unified under GPO control in 1896 and the local distribution network in 1912. A few municipally owned services remained outside of GPO control. These were Kingston upon Hull, Portsmouth and Guernsey. Hull still retains an independent operator, Kingston Communications, though it is no longer municipally controlled.\n\nPost Office engineers in the inter-war period had considerable expertise in both telecommunications and hearing assistive devices.\n\nPost Office Telecommunications",
        "Post Office Telecommunications \n\nIn 1969 the GPO, a government department, became the Post Office, a nationalised industry separate from government. Post Office Telecommunications was set up as a division of the Post Office, in October 1969. The Post Office Act 1969 was passed to provide for greater efficiency in post and telephone services; rather than run a range of services, each organisation would be able to focus on their respective service, with dedicated management. By law, the Post Office retained the exclusive right to operate the UK national telecom network, (although since 1914 had licensed Hull City Council to operate its own local telephone network, Kingston Communications).",
        "The 1970s was a period of great expansion for the Post Office. Most exchanges were modernised and expanded, and many services, such as STD and international dialling were extended. By the early 1970s, subscribers in most cities could dial direct to Western Europe, the US, and Canada; by the end of the decade, most of the world could be dialled direct. The System X digital switching platform was developed, and the first digital exchanges began to be installed. The Post Office also procured their own fleet of vans, based on the Commer FC model.\n\nPost Office Telecommunications researched and implemented data communications using packet switching in the 1970s, resulting in the EPSS, International Packet Switched Service, and Packet Switch Stream.\n\nBritish Telecom",
        "British Telecom \n\nIn 1979 the Conservatives decided that telecommunications should be fully separated from the Post Office. The British Telecom brand was introduced in 1980. On 1 October 1981, this became the official name of Post Office Telecommunications, which became a state-owned corporation independent of the Post Office under the provisions of the British Telecommunications Act 1981. In 1982 BT's monopoly on telecommunications was broken with the granting of a licence to Mercury Communications.",
        "Privatisation \nOn 19 July 1982, the Government announced its intention to sell shares in British Telecom to the public. On 1 April 1984, British Telecommunications was incorporated as a public limited company (plc) in anticipation of the passing of the Telecommunications Bill. This Bill received Royal Assent on 12 April, and the transfer to British Telecommunications plc from British Telecom as a statutory corporation of its business, its property, its rights and liabilities took place on 6 August 1984.",
        "Initially all shares in the new plc were owned by the Government. In November 1984, 50.2% of the new company was offered for sale to the public and employees. Shares were listed in London, New York, and Toronto and the first day of trading on was 3 December 1984. The Government sold half its remaining interest in December 1991 and the other half in July 1993. In July 1997, the new Labour Government relinquished its Special Share (\"Golden Share\"), retained at the time of the flotation, which had effectively given it the power to block a takeover of the company, and to appoint two non-executive directors to the Board.",
        "The company changed its trading name to \"BT\" on 2 April 1991. In 1996 Peter Bonfield was appointed CEO and chairman of the executive committee, promising a \"rollercoaster ride\".\n\nIn the 1990s, BT entered the Irish telecommunications market through a joint venture with the Electricity Supply Board, the Irish state owned power provider. This venture, entitled Ocean, found its main success through the launch of Ireland's first subscription-free dial-up ISP, oceanfree.net.  As a telecoms company it found much less success, mainly targeting corporate customers. BT acquired 100% of this venture in 1999.\n\nAttempted global alliances",
        "Attempted global alliances\n\nMCI \nIn June 1994  BT and MCI Communications launched Concert Communications Services which was a $1 billion joint venture between the two companies. Its aim was to build a network which would provide easy global connectivity to multinational corporations.\n\nThis alliance progressed further on 3 November 1996 when the two companies announced that they had agreed to a merger, creating a global telecommunications company called Concert plc. The proposal gained approval from the European Commission, the US Department of Justice, and the US Federal Communications Commission and looked set to proceed.",
        "However, in light of pressure from investors reacting to the slide in BT's share price on the London Stock Exchange, BT reduced its bid price for MCI, releasing MCI from its exclusivity clause and allowing it to speak to other interested parties. On 1 October 1997, Worldcom made a rival bid for MCI which was followed by a counter-bid from GTE.\n\nBT sold its stake in MCI to Worldcom in 1998 for £4,159 million. As part of the deal, BT also bought out from MCI its 24.9% interest in Concert Communications, thereby making Concert a wholly owned part of BT.\n\nThe reaction to the failure of the deal in the City of London was critical of then Chairman Iain Vallance and CEO Peter Bonfield, and the lack of confidence from the failed merger led to their removal.",
        "AT&T \nAs BT now owned Concert, and still wanted access to the North American market, it needed a new partner. An AT&T/BT option had been mooted in the past, but stopped on regulatory grounds due to their individual virtual monopolies in their home markets. By 1996, this had receded to the point where a deal was possible and a deal was consummated in 1998.\n\nAt its height, the Concert managed network was extensive. Although Concert continued signing customers, its rate of revenue growth slowed, so that in 1999 David Dorman was made CEO with a brief to revive it.",
        "In late 2000 the BT and AT&T boards fell-out – partly due to each partner's excess debt, and the resulting board room clear-outs – partly due to Concert's extensive annual losses. AT&T recognized that Concert was a threat to its ambitions if left intact, and so negotiated a deal where Concert was split in two in 2001: North America and Eastern Asia went to AT&T, the rest of the world and $400M to BT. BT's remaining Concert assets were merged into its BT Ignite, later BT Global Services group.",
        "BT Ireland \nIn 2000, BT acquired Esat Telecom Group plc, and all its subsidiary companies, and Ireland On Line. It also purchased Telenor's minority shareholding in Esat Digifone. The Esat Telecom Group was split in two with the landline and internet operations were combining with Ocean to become part of BT Ignite.  Esat Group was renamed Esat BT in July 2002, and eventually BT Ireland in April 2005. Esat Digifone became part of BT Wireless, before being spun off into a separate independent company mmo2 plc (now Telefónica Europe). EsatBT installed the first DSL lines in Ireland, to try and compete heavily with former state telecoms company Eircom and operate one exchange, in Limerick.",
        "2001 debt crisis \nBy 2001, BT had a debt of £30 billion, much of which was acquired during the bidding round for the 3rd generation mobile telephony (commonly known as 3G) licences. It had also failed in its series of proposed global mergers, and the funds flowing from its then virtual monopoly of the UK market place had been largely removed. It was also headed by two executives who had little support from the London Stock Exchange, particularly in light of a 60% drop in share price in sixteen months.\n\nPhilip Hampton joined as CFO, and in April 2001 Sir Iain Vallance was replaced as chairman by recognised turn around expert Sir Christopher Bland.",
        "Philip Hampton joined as CFO, and in April 2001 Sir Iain Vallance was replaced as chairman by recognised turn around expert Sir Christopher Bland.\n\nEurope's largest rights issue \nIn May 2001  BT carried out corporate Europe's largest ever rights issue, allowing it to raise £5.9 billion. A few days before, it sold stakes in Japan Telecom, in mobile operator J-Phone Communications, and in Airtel of India to Vodafone.\n\nSale of Yell Group and demerger of O2 \nIn June 2001 BT's directory business was sold as Yell Group to a combination of private equity firms Apax Partners and Hicks, Muse, Tate & Furst for £2.1 billion.",
        "A demerger followed in November 2001, when the former mobile telecommunications business of BT, BT Cellnet, was hived off as a separate business named \"mmO2\". This included BT owned or operated networks in other countries, including BT Cellnet (UK), Esat Digifone (Ireland), and Viag Interkom (Germany). All networks now owned or operated by mmO2 (except Manx Telecom) were renamed as O2. The de-merger was accomplished via a share-swap, all British Telecommunications plc shareholders received one mmO2 plc and one BT Group plc (of which British Telecommunications is now a wholly owned subsidiary) share for each share they owned. British Telecommunications plc was de-listed on 16 November, and the two new companies started trading on 19 November.",
        "Aftermath \nAt the end of the series of sales, in October 2001  Sir Peter Bonfield resigned, and was replaced by former Lucent CEO Ben Verwaayen.\n\nDuring Bonfield's tenure the share price went from £4 to £15, and back again to £5. Bonfield's salary to 31 March 2001 was a basic of £780,000 (increasing to £820,000) plus a £481,000 bonus and £50,000 of other benefits including pension. He also received a deferred bonus, payable in shares three years' later, of £481,000, and additional bonuses of £3.3 million.",
        "mmO2 plc was replaced by O2 plc in a further share-swap in 2005, and subsequently bought in an agreed takeover by Telefónica for £18 billion and delisted. In 2004, BT launched Consult 21, a consultation organisation that was to aid  BT 21CN in the eventual conversion to digital telephony.\n\nIn 2004, BT was awarded the contract to deliver and manage N3, a secure and fast broadband network for the NHS National Programme for IT (NPfIT) program, on behalf of the English National Health Service (NHS).",
        "In 2005 BT made a number of acquisitions. In February 2005, BT acquired Infonet (now re-branded BT Infonet), a large telecoms company based in El Segundo, California, giving BT access to new geographies. It also acquired the Italian company Albacom. Then in April 2005, it bought Radianz from Reuters (now rebranded as BT Radianz), which expanded BT's coverage and provided BT with more buying power in certain countries.\n\nIn August 2006, BT acquired online electrical retailer Dabs.com for £30.6 million. The BT Home Hub manufactured by Inventel was also launched in June 2006.",
        "In August 2006, BT acquired online electrical retailer Dabs.com for £30.6 million. The BT Home Hub manufactured by Inventel was also launched in June 2006.\n\nIn October 2006, BT confirmed that it would be investing 75% of its total capital spending, put at £10 billion over five years, in its new Internet Protocol (IP) based 21st century network (21CN). Annual savings of £1 billion per annum were expected when the transition to the new network was to have been completed in 2010, with over 50% of its customers to have been transferred by 2008. That month the first customers on to 21CN was successfully tested at Adastral Park in Suffolk.",
        "2007 to 2012 \nIn January 2007, BT acquired Sheffield-based ISP, PlusNet plc, adding 200,000 customers. BT stated that PlusNet will continue to operate separately out of its Sheffield head-office. On 1 February 2007, BT announced agreed terms to acquire International Network Services Inc. (INS), an international provider of IT consultancy and software.\n\nIn February 2007, Sir Michael Rake succeeded Sir Christopher Bland. In April that year, they acquired COMSAT International, followed in October by the acquisition of Lynx Technology.",
        "In February 2007, Sir Michael Rake succeeded Sir Christopher Bland. In April that year, they acquired COMSAT International, followed in October by the acquisition of Lynx Technology.\n\nBT acquired Wire One Communications in June 2008 and folded the company into \"BT Conferencing\", its existing conferencing unit, as a new video business unit\nIn July 2008, BT acquired the online business directory firm Ufindus for £20 million in order to expand its position in the local information market in GB. On 28 July 2008, BT acquired Ribbit, of Mountain View, California, \"Silicon Valley's First Phone Company\". Ribbit provides Adobe Flash/Flex APIs, allowing web developers to incorporate telephony features into their software as a service (SaaS) applications.",
        "In the early days of its fibre broadband rollout, BT said it would deliver fibre-to-the-premises (FTTP) to around 25% of the Country, with the rest catered for by the slower fibre-to-the-cabinet (FTTC), which uses copper wiring to deliver the final stretch of the connection. In 2014, with less than 0.7% of the company's fibre network being FTTP, BT dropped the 25% target, saying that it was \"far less relevant today\" because of improvements made to the headline speed of FTTC, which had doubled to 80Mbit/s since its fibre broadband rollout was first announced. To supplement FTTC, BT offered an 'FTTP on Demand' product. In January 2015, BT stopped taking orders for the on-demand product.",
        "On 1 April 2009, BT Engage IT was created from the merger of two previous BT acquisitions, Lynx Technology and Basilica. Apart from the name change not much else changed in operations for another 12 months. On 14 May 2009, BT said it was cutting up to 15,000 jobs in the coming year after it announced its results for the year to 31 March 2009. Then in July 2009, BT offered workers a long holiday for an up front sum of 25% of their annual wage or a one-off payment of £1000 if they agree to go part-time.",
        "On 6 April 2011, BT launched the first online not-for-profit fundraising service for UK charities called BT MyDonate as part of its investment to the community. The service will pass on 100% of all donations made through the site to the charity, and unlike other services which take a proportion as commission and charge charities for using their services, BT will only pass on credit/debit card charges for each donation. The service allows people to register to give money to charity or collect fundraising donations. BT developed MyDonate with the support of Cancer Research UK, Changing Faces, KidsOut, NSPCC and Women's Aid.\n\n2013 to present \n\nIn March 2013, BT was allocated 4G spectrum in the UK following an auction and assignment by Ofcom, after paying £201.5m.",
        "On 1 August 2013, BT launched its first television channels, BT Sport, to compete with rival broadcaster Sky Sports. Plans for the channels' launch came about when it was announced in June 2012 that BT had been awarded a package of broadcast rights for the Premier League from the 2013–14 to 2015–16 season, broadcasting 38 matches from each season. In February 2013, BT acquired ESPN Inc.'s UK and Ireland TV channels, continuing its expansion into sports broadcasting. ESPN America and ESPN Classic were both closed, while ESPN continued to be operated by BT. On 9 November 2013, BT announced it had acquired exclusive rights to the Champions League and Europa League for £897m, from the 2015 season, with some free games remaining including both finals.",
        "On 1 November 2014, BT created a new central business services (CBS) organisation to provide customer services and improve operational efficiency levels.\n\nOn 24 November 2014, shares in BT rose considerably on the announcement that the company was in talks to buy back O2, while at the same time confirmed it was also in talks to acquire EE. BT subsequently entered into exclusive talks to buy EE for £12.5 billion on 15 December 2014 and confirmed on 5 February 2015, subject to regulatory approval. The deal will combine BT's 10 million retail customers and EE's 24.5 million direct mobile subscribers. Deutsche Telekom will own 12% of BT, while Orange S.A. will own 4%.",
        "In March 2015, launched a 4G service as BT Mobile BT Group CEO Gavin Patterson announced that BT plans to migrate all of its customers onto the IP network by 2025, switching off the company's ISDN network.\n\nOn 15 January 2016, BT received final unconditional approval by the Competition and Markets Authority to acquire EE. The deal was officially completed on 29 January 2016 with Deutsche Telekom now owning 12% of BT, while Orange S.A. own 4%.",
        "On 1 February 2016, BT announced a new organisational structure that will take effect from April 2016 following the successful acquisition of EE. The EE brand, network and high street stores will be retained and will become a second consumer division, operating alongside BT Consumer. It will serve customers with mobile services, broadband and TV and will continue to deliver the Emergency Services Network contract which was awarded to EE in late 2015. There will be a new BT Business and Public Sector division that will have around £5bn of revenues and will serve small and large businesses as well as the public sector in the UK and Ireland. It will comprise the existing BT Business division along with EE's business division and those parts of BT Global Services that are UK focused. There",
        "well as the public sector in the UK and Ireland. It will comprise the existing BT Business division along with EE's business division and those parts of BT Global Services that are UK focused. There will also be another new division; BT Wholesale and Ventures that will comprise the existing BT Wholesale division along with EE's MVNO business as well as some specialist businesses such as Fleet, Payphones and Directories. Gerry McQuade, currently Chief Sales and Marketing Officer, Business at EE, will be its CEO.",
        "On 8 June 2017, BT appointed KPMG as its new auditor to replace PwC in the wake of the fraud scandal in Italy that triggered a major profit warning earlier that year. Also in of that year, KPMG fired six US employees over a scandal that calls into question efforts to ensure that public company accounts are being properly scrutinised.\n\nOn 8 July 2017, The Daily Telegraph reported that BT \"has called in consultants from McKinsey to conduct a review of its businesses in the hope of saving hundreds of millions of pounds per year. The work, dubbed 'Project Novator', is understood to include a potential merger of BT's struggling global services corporate networking and IT unit with its business and public sector division\".",
        "On 28 July 2017, BT announced organisational changes to \"simplify its operating model, strengthen accountabilities and accelerate its transformation\" and involves bringing together its BT Consumer and EE divisions into a new unified BT Consumer division that will operate across three brands – BT, EE and Plusnet. It will take effect from 1 April 2018.",
        "On 18 April 2018, BT announced further organisational changes following unification of its BT Consumer and EE divisions, and involves bringing together its BT Business and Public Sector and BT Wholesale and Ventures divisions into a new unified division known as BT Enterprise. It will also include BT's Ventures business which \"acts as an incubator for potential new growth areas of the company\" and will report as a single unit from 1 October 2018.\n\nIn June 2021, the French telecommunications company, Altice acquired a 12% stake in BT, increasing that stake to 18% in December 2021. Drahi's Altice increased its holding to 24.5% in May 2023. Drahi's purchase of 650 million shares cost about £960 million, at BT's share price of 147.85 pence at the close of trading on 22 May 2023.",
        "Altice's increasing ownership in BT Group posed questions around the national security international ownership of the United Kingdoms infrastructure asset presented. The UK government opened an investigation in May 2022 to look into possible security implications of Patrick Drahi and Alrice's ownership. In August 2022 the Government concluded its investigation and Patrick Drahi would not be forced to cut his stake in BT after the UK government ruled the investment did not pose any national security concerns.\n\nIn July 2023, BT announced the appointment of Allison Kirkby as its new Chief Executive, replacing Philip Jansen by January 2024.\n\nOperations",
        "In July 2023, BT announced the appointment of Allison Kirkby as its new Chief Executive, replacing Philip Jansen by January 2024.\n\nOperations \n\nBT Group is a holding company; the majority of its businesses and assets are held by its wholly owned subsidiary British Telecommunications plc. BT's businesses are operated under special government regulation by the British telecoms regulator Ofcom (formerly Oftel). BT has been found to have significant market power in some markets following market reviews by Ofcom. In these markets, BT is required to comply with additional obligations such as meeting reasonable requests to supply services and not to discriminate.",
        "BT runs the telephone exchanges, trunk network and local loop connections for the vast majority of British fixed-line telephones. Currently BT is responsible for approximately 28 million telephone lines in GB. Apart from KCOM Group, which serves Kingston upon Hull, BT is the only UK telecoms operator to have a Universal service Obligation, (USO) which means it must provide a fixed telephone line to any address in the UK. It is also obliged to provide public call boxes.",
        "As well as continuing to provide service in those traditional areas in which BT has an obligation to provide services or is closely regulated, BT has expanded into more profitable products and services where there is less regulation. These are principally, broadband internet service and bespoke solutions in telecommunications and information technology.\n\nBranding \n\nIn 2019, a simplified BT logo and brand was launched to replace the previous mutli-coloured globe logo. In April 2022, BT announced its intentions to focus on the EE brand for consumer products.\n\nCorporate affairs\n\nBuildings and facilities",
        "Corporate affairs\n\nBuildings and facilities \n\nAs BT operates in around 180 countries, it owns and leases a range of buildings and facilities in the UK and around the world. In 2001, it sold some of its UK property portfolio for £2.38 billion to Telereal Trillium in a 30-year leaseback. The deal included 6,700 properties and contributed towards alleviating its debt at the time, with the main advantage being flexibility as it allows BT to vacate property over time, so as to adapt to changing operational requirements.\n\nHeadquarters",
        "Headquarters \n\nUntil December 2021, BT Group's world headquarters and registered office was the BT Centre, a 10-storey office building at 81 Newgate Street in the City of London, opposite St Paul's tube station.  In November 2021 BT relocated to new headquarters at One Braham, a brand new 18-storey building completed earlier in 2021.\n\nBuildings and stations \nSome of its UK buildings and stations are:",
        "Buildings and stations \nSome of its UK buildings and stations are:\n\n Adastral Park – headquarters of BT Labs in Suffolk\n The Assembly – building in Bristol\n Baynard House – building in City of London\n BT Riverside Tower – headquarters of BT Northern Ireland in Belfast\n BT Tower – building in Swansea\n Goonhilly Satellite Earth Station – satellite earth station in Cornwall\n Guardian telephone exchange – telephone exchange in Manchester\n Madley Communications Centre – satellite earth station in Herefordshire\n National Network Management Centre – network operations centre in Shropshire\n Stadium House – building in Cardiff\n\nTelecommunications towers",
        "BT remains one of the largest owners of telecommunications towers in the UK and were a major node in its microwave network. Its BT Tower in London is notable for numerous reasons such as being the tallest building in the UK from its construction in the 1960s until the early 1980s, its revolving restaurant at the top known as 'Top of the Tower' in operation through the late 1960s and 1970s, and remains one of the UK's most important communications nerve centres, the heart of a vast broadcasting and communications network. It carries approximately 95% of the UK's TV content, including live broadcasts and 99% of all live football games as well as pioneering the first international HD, 3D and 4K television transmissions. It serves media production and distribution customers around the world",
        "and 99% of all live football games as well as pioneering the first international HD, 3D and 4K television transmissions. It serves media production and distribution customers around the world and as part of the Things Connected Network launched in London, it became the highest building in the world to host an Internet of things (IoT) base station in September 2016. Some of its towers are:",
        "Other \nSome of its other UK facilities are:\n\nDivisions \nBT Group is organised into the following divisions:",
        "Customer facing \n BT Consumer – provides retail telecoms services to consumers in the UK including:\n BT Broadband\n BT Mobile\n BT Superfast Fibre\n BT TV\n BT Wi-fi\n EE – mobile network operator, provides mobile and fixed communications services to consumers in the UK\n Plusnet – internet service provider, provides mobile and fixed communications services to consumers in the UK\n TNT Sports (joint venture, operated by Warner Bros. Discovery)\n BT Business – products and services to organisations in the small-to-medium-sized business, corporate and public sectors in the UK and globally, and wholesale services. Formed from the merger of BT Enterprise and BT Global Services.",
        "Openreach – fenced-off wholesale division, established in 2005 following a review by Ofcom and commenced operations in 2006, employing 25,000 engineers previously employed by BT. Its purpose is to ensure that other communications providers have the same operational conditions as BT, and is responsible for the provision and repair in the \"last mile\" of copper wire.",
        "Internal services \n Networks – Responsible for designing, building and running the networks and technology platforms that BT and its customers use.\n BT Research\n Digital – Responsible for leading BT's digital transformation, driving experience innovation and delivering the products and services customers use.\nBT's procurement arm, \"BT Sourced\", was established in February 2021 and is based in Dublin.\n\nCorporate governance \n\nBT's board of directors as of November 2021:\n\nBT's executive committee as of March 2018:",
        "Corporate governance \n\nBT's board of directors as of November 2021:\n\nBT's executive committee as of March 2018:\n\nPension fund \nBT has the second largest defined benefit pension plan of any UK public company. The trustees valued the scheme at £36.7 billion at the end of 2010; an actuarial valuation valued the deficit of the scheme at £9.043 billion as of 31 December 2008.\nFollowing a change in the regulations governing inflation index linking, the deficit was estimated at £5.2 billion in November 2010.\n\nSponsorships \nBT sponsored Scotland's domestic rugby union championship and cup competitions between 1999 and 2006.",
        "Sponsorships \nBT sponsored Scotland's domestic rugby union championship and cup competitions between 1999 and 2006.\n\nOn 31 July 2012, it was announced that BT agreed a three-year sponsorship deal with Ulster Rugby and sees BT become the Official Communications Partner. BT's logo will appear on the Ulster Rugby shirt sleeve for all friendlies, Heineken Cup and RaboDirect Pro12 matches as well as a significant brand presence at their home ground; Ravenhill Stadium.",
        "On 29 July 2013, it was announced that BT had partnered up with Scottish Rugby Union in a four-year sponsorship deal with its two professional clubs; Edinburgh Rugby and Glasgow Warriors that will commence from August 2013. The deal involves BT Sport becoming the new shirt sponsor for both clubs as well as being promoted with BT Group at their respective home grounds; Scotstoun Stadium and Murrayfield Stadium.\n\nOn 13 May 2014, BT joined Sky, TalkTalk and Virgin Media as founding partners of Internet Matters, a not-for-profit organisation that provides online safety advice for parents and their children.",
        "On 13 May 2014, BT joined Sky, TalkTalk and Virgin Media as founding partners of Internet Matters, a not-for-profit organisation that provides online safety advice for parents and their children.\n\nOn 28 May 2014, it was announced that BT agreed a £20 million four-year sponsorship deal with Scottish Rugby Union which includes BT securing the naming rights for Murrayfield Stadium which becomes BT Murrayfield Stadium, become sponsor of the Scotland sevens team, become principal and exclusive sponsor of Scotland's domestic league and cup competitions from next season, taking over the role from The Royal Bank of Scotland and become sponsor of Scottish Rugby's four new academies that aims to drive forward standards for young players who have aspirations to play professionally.",
        "On 14 April 2015, it was announced that as part of BT's current £20 million four-year sponsorship deal with Scottish Rugby Union that was announced in May 2014, BT has completed its sponsorship portfolio following an additional investment of £3.6 million for the 3 years remaining of its sponsorship deal, to become the new shirt sponsor for the Scotland national teams.",
        "On 27 January 2016, it was announced that BT, alongside YouTube will be the new joint headline sponsors in a three-year deal with Edinburgh International Television Festival. The two companies will \"share prominence across all branding of the 41st TV Festival, including the famous MacTaggart Lecture and will work closely with the festival organisers in their bid to reflect new trends in a rapidly transforming industry, from new ways of distributing content to technical innovations such as virtual reality\".",
        "BT is the founding and principal partner of the Wayne Rooney Foundation, which was established to improve the lives of children and young people. The Foundation will run events \"to raise vital funds to support the work of key organisations dedicated to supporting disadvantaged and vulnerable children and young people\". These organisations are four chosen charities which are, Manchester United Foundation, NSPCC, Claire House Children's Hospice and Alder Hey Children's Hospital. The first of these events was Wayne's testimonial match in August 2016 between Manchester United F.C. and Everton F.C. which raised £1.2 million. The match was screened live through BT Sport with BT MyDonate being the official fundraising platform for the testimonial, with both online and text options for donations",
        "which raised £1.2 million. The match was screened live through BT Sport with BT MyDonate being the official fundraising platform for the testimonial, with both online and text options for donations promoted during the match.",
        "On 26 May 2017, it was announced that BT is to sponsor the 2017 British Urban Film Festival (BUFF) and sees BT host every event of the film festival, including the Awards at the BT Tower. BT will also broadcast the awards ceremony on BT.com and will have the opportunity to screen films acquired from the festival on its BT TV store platform.",
        "On 6 September 2017, it was announced that BT had extended its current £20 million four-year sponsorship deal with Scottish Rugby Union that was announced in May 2014, for a further three years beginning from June 2018. The new deal sees BT retain the naming rights to BT Murrayfield Stadium, alongside its role as principal partner of the Scotland national team and Scotland 7s. BT's logo will continue to be displayed on the front of Scotland rugby shirts across the world, in the Six Nations Championship, as well as the summer and autumn test matches. BT will also continue to be promoted at Edinburgh Rugby and Scotstoun Stadium in Glasgow.\n\nHistorical financial performance \nBT's financial results have been as follows:\n\n2008–present\n\n1992–2007\n\nControversies",
        "2008–present\n\n1992–2007\n\nControversies\n\nWorld Wide Web hyperlink patent \nIn 2001, BT discovered it owned a patent () which it believed gave it patent rights on the use of hyperlink technology on the World Wide Web. The corresponding UK patent had already expired, but the US patent was valid until 2006. On 11 February 2002, BT began a court case relating to its claims in a US federal court against the internet service provider Prodigy Communications Corporation.  In the case British Telecommunications plc v. Prodigy, the United States District Court for the Southern District of New York ruled on 22 August 2002 that the BT patent was not applicable to web technology and granted Prodigy's request for summary judgment of non-infringement.",
        "Behavioural targeting",
        "In early 2008 it was announced that BT had entered into a contract (along with Virgin Media and TalkTalk) with the spyware company Phorm (responsible under their 121Media guise for the Apropos rootkit) to intercept and analyse their users' click-stream data and sell the anonymised aggregate information as part of Phorm's OIX advertising service. The practice, known as \"behavioural targeting\" and condemned by critics as \"data pimping\", came under intense fire from various internet communities and other interested-parties who believe that the interception of data without the consent of users and web site owners is illegal under UK law (RIPA). At a more fundamental level, many have argued that the ISPs and Phorm have no right to sell a commodity (a user's data, and the copyrighted content of",
        "web site owners is illegal under UK law (RIPA). At a more fundamental level, many have argued that the ISPs and Phorm have no right to sell a commodity (a user's data, and the copyrighted content of web sites) to which they have no claim of ownership. In response to questions about Phorm and the interception of data by the Webwise system Sir Tim Berners-Lee, the creator of the World Wide Web, indicated his disapproval of the concept and is quoted as saying of his data and web history:",
        "Huawei infrastructure access",
        "Beginning in 2010 the UK intelligence community investigated Huawei, the Chinese supplier of BT's new fibre infrastructure with increasing urgency after the United States, Canada and Australia prevented the company from operating in their countries. Although BT had notified the UK government in 2003 of Huawei's interest in their £10bn network upgrade contract, they did not raise the security implications as BT failed to explain that the Chinese company would have unfettered access to critical infrastructure. On 16 December 2012 the then prime minister David Cameron was supplied with an in-depth report indicating that the intelligence services had very grave doubts regarding Huawei, and that UK governmental, military, and civilian privacy may have been under serious threat.",
        "On 7 June 2013, British lawmakers concluded that BT should not have allowed Huawei access to the UK's communications network without ministerial oversight, saying they were 'deeply shocked' that BT did not inform government that they were allowing Huawei and ZTE, both with ties to the Chinese military, unfettered access to critical national systems. Furthermore, ministers discovered that the agency with the responsibility to ensure Chinese equipment and code was threat-free was entirely staffed by Huawei employees. Subsequently, parliamentarians confirmed that in case of an attack on the UK there was nothing that could be done to stop Chinese infiltration.",
        "By 2016 Huawei had put measures in place to ensure the integrity of UK national security. Specifically their UK work is now overseen by a board that includes directors from GCHQ, the Cabinet Office and the Home Office.\n\nZTE, another Chinese company that supplies extensive network equipment and subscriber hardware used with BT 'Infinity', was also under scrutiny by parliament's intelligence and security committee after the US, Canada, Australia and the European Union declared the company a security risk.\n\nIn 2020 following a government ruling, BT began removing Huawei equipment from its broadband and mobile networks in order to comply with new restrictions on the usage of Huawei equipment. As of 2023, the process is still ongoing.",
        "Alleged complicity with drone strikes in Yemen and Somalia \nIn September 2012, BT entered into a $23 million deal with the US military to provide a key communications cable connecting RAF Croughton, a US military base on UK soil, with Camp Lemonnier, a large US base in Djibouti. Camp Lemonnier is used as a base for American drone attacks in Yemen and Somalia, and has been described by The Economist as \"the most important base for drone operations outside the war zone of Afghanistan.\"",
        "Human rights groups including Reprieve and Amnesty International have criticised the use of armed drones outside declared war zones. Evidence produced by The Bureau of Investigative Journalism and Stanford University's International Human Rights & Conflict Resolution Clinic suggest that drone strikes have caused substantial civilian casualties, and may be illegal under international law.",
        "In 2013, BT was the subject of a complaint by Reprieve to the Department of Business, Innovation and Skills under the OECD Guidelines for Multinational Enterprises, following their refusal to explain whether or not their infrastructure was used to facilitate drone strikes. The subsequent refusal of this complaint was appealed in May 2014, on the basis that the UK National Contact Point's decision did not follow the OECD Guidelines. The issue of bias was also raised, due to the appointment of Lord Ian Livingston as government minister for the department which was processing the complaint: Livingston had occupied a senior position at BT when the cable between RAF Croughton and Camp Lemonnier was originally built.",
        "Overcharging \nIn February 2017, a review of the telecoms market by Ofcom found that BT's landline only contracts provided poor value to customers. Ofcom ordered BT to reduce their prices but stopped short of demanding that customers were compensated. In January 2021, Law firm Mishcon de Reya filed a claim with the Competition Appeal Tribunal against BT worth £600 million, accusing them of historic overcharging on landlines. The class action lawsuit claims BT have increased their prices for line-only services every year since 2009, whilst the wholesale cost for delivering these services has reduced. The claimants suggest that customers could be entitled to compensation of up to £500 each.",
        "Bidding rules violation \nIn 2020, BT was fined £6.3m by the telecoms regulator Ofcom for violating the law on a large public sector deal in Northern Ireland. Under Ofcom's regulations, the BT network shall handle all wholesale customers similarly. In its report, Ofcom found that BT's network violated the rules by failing to supply Eir with the same details on its on-demand fiber-to-the-premises offering as its own rival team.\n\nHistorical documents \nRecords of the Post Office Corporation (Telecommunications division) 19691981 and its predecessors (including Post Office Telegraph and Telephone Service 18641969 and some private telegraph and telephone companies) are Public Records, and are held by BT Archives.\n\nSee also",
        "See also \n\n List of telephone operating companies\n UK telephone area codes (STD codes)\n\nReferences\n\nFurther reading",
        "Baldwin, F.G.C.  The History of the Telephone in the United Kingdom (1925)\n Foreman-Peck, J. \"The development and diffusion of telephone technology in Britain, 1900–1940,\" Transactions of the Newcomen Society, (1991–92).  63, pp165–180.\n Foreman-Peck, J., & Millward, R. Public and private ownership of British industry 1820–1990 (1994). \n Hazlewood, A. \"The origins of the state telephone service in Britain\" Oxford Economic Papers (1953).  5:13–25. in JSTOR\n \n Johannessen, Neil. Ring up Britain: the Early Years of the Telephone in the United Kingdom (British Telecommunications plc, London, 1991)",
        "Johannessen, Neil. Ring up Britain: the Early Years of the Telephone in the United Kingdom (British Telecommunications plc, London, 1991) \n Johnston, S. F. \"The telephone in Scotland.\" in: K. Veitch, ed., Transport and Communications. Publications of the European Ethnological Research Centre; Scottish life and society: a compendium of Scottish ethnology (2009): pp. 716–727 online \n Magill, Frank N. Great Events from History II: Business and Commerce Series, volume 1:1897–1923 (1994) pp 218–23; historiography\n Meyer, Hugo Richard. Public Ownership and the Telephone in Great Britain: Restriction of the Industry by the State and the Municipalities (1907).  online",
        "Meyer, Hugo Richard. Public Ownership and the Telephone in Great Britain: Restriction of the Industry by the State and the Municipalities (1907).  online\n Pitt, D.C. The telecommunications function in the British Post Office. A case study of bureaucratic adaption (Westmead: Saxon House, 1980). \n Robertson, John Henry. The story of the telephone: A history of the telecommunications industry of Britain (1947)",
        "External links \n \n \n BT Archives \n BT Archives online catalogue\n BT Login Links",
        "1846 establishments in the United Kingdom\n1912 establishments in the United Kingdom\n1969 establishments in the United Kingdom\n1981 establishments in the United Kingdom\nBritish royal warrant holders\nCompanies based in the City of London\nTelecommunications companies established in 1846\nTelecommunications companies established in 1912\nTelecommunications companies established in 1969\nTelecommunications companies established in 1981\nBritish companies established in 1846\nBritish companies established in 1912\nBritish companies established in 1969\nBritish companies established in 1981\nCompanies listed on the London Stock Exchange\nInternet service providers of the United Kingdom\nMultinational companies based in the City of London\nTelecommunications companies of the United Kingdom",
        "Companies listed on the London Stock Exchange\nInternet service providers of the United Kingdom\nMultinational companies based in the City of London\nTelecommunications companies of the United Kingdom\n1960s initial public offerings\n1980s initial public offerings\nFormer nationalised industries of the United Kingdom\nHistory of telecommunications in the United Kingdom"
    ],
    [
        "Bubble Bobble\nis a 1986 platform game developed and published by Taito for arcades. It was distributed in the United States by Romstar, and in Europe by Electrocoin. Players control Bub and Bob, two dragons that set out to save their girlfriends from a world known as the Cave of Monsters. In each level, Bub and Bob must defeat each enemy present by trapping them in bubbles and popping, who turn into bonus items when they hit the ground. There are 100 levels total, each becoming progressively more difficult.",
        "Bubble Bobble was designed by Fukio \"MTJ\" Mitsuji. When he joined Taito in 1986, he felt that Taito's game output was of mediocre quality. In response, he decided to make a game that was fun to play and could rejuvenate the company's presence in the industry. Mitsuji hoped his game would appeal to women, specifically couples that visited arcades. As such, he decided to make Bubble Bobble focus largely on its two player co-operative mode. He made bubbles the core mechanic as he thought they would be a fun element that girls would enjoy.",
        "Bubble Bobble became one of Taito's biggest arcade successes, and is credited with inspiring the creation of many similar screen-clear platform games that followed. It was acclaimed by critics for its character design, memorable soundtrack, gameplay, and multiplayer, and is often listed among the greatest games of all time. Bubble Bobble was followed by a long list of sequels and successors for multiple platforms; one of these, Puzzle Bobble, has become successful in its own right and spawned its own line of sequels.\n\nPlot\n\"Baron Von Blubba\" has kidnapped the brothers Bubby and Bobby's girlfriends and turned the brothers into Bubble Dragons, Bub and Bob. Bub and Bob have to finish 100 levels in the Cave of Monsters in order to rescue them.\n\nGameplay",
        "In the game, each player controls one of the two dragons. Players can move along platforms, fall to lower ones, and jump to higher ones and over gaps. Each level is limited to a single screen, with no left/right scrolling; however, if a screen has gaps in its bottom edge, players can fall through these and reappear at the top. Each level has a certain number of enemies that must be defeated in order to advance. The players must blow bubbles to trap the enemies, then burst these bubbles by colliding with them. Each enemy defeated in this manner turns into a food item that can be picked up for extra points. Defeating multiple enemies at once awards higher scores and causes more valuable food items to appear. All bubbles will float for a certain length of time before bursting on their own;",
        "extra points. Defeating multiple enemies at once awards higher scores and causes more valuable food items to appear. All bubbles will float for a certain length of time before bursting on their own; players can jump on these and ride them to otherwise inaccessible areas. Magic items appear from time to time and grant special abilities and advantages when picked up. Special bubbles occasionally appear that can be burst to attack enemies with fire, water, or lightning. Furthermore, if a player collects letter bubbles to spell the word EXTEND, a bonus life is earned and both players immediately advance to the next level.",
        "A player loses one life upon touching any free enemies or their projectiles (rocks, fireballs, lasers, bottles).  Enemies turn \"angry\"—turning pink in color and moving faster—if they escape from a bubble after being left too long or the players spend a certain amount of time on the current level. They return to normal if either player loses a life. After a further time limit expires, an additional invincible enemy appears for each player, actively chasing them using only vertical and horizontal movements. These disappear once the level is cleared, or when a player loses a life. When there is only one enemy left, it immediately becomes angry and remains in this state until defeated.",
        "In the 100th and final level, players face a boss. This is one of the first games to feature multiple endings. Completing Level 100 in single-player mode reveals a message stating that the game has not truly ended and a hint to the player: \"Come here with your friend.\" If two players complete the game, they see a \"happy end\", in which the brothers are transformed to their human selves and reunited with their girlfriends. This ending also includes a code that, when deciphered, allows the game to be played in the faster and more difficult \"super\" mode. If this mode is completed with two players, a second \"happy end\" is displayed in which Super Drunk (the defeated boss) is revealed to be the brothers' parents under the control of some outside influence. The brothers return to normal and are",
        "a second \"happy end\" is displayed in which Super Drunk (the defeated boss) is revealed to be the brothers' parents under the control of some outside influence. The brothers return to normal and are reunited with their parents and girlfriends.",
        "Also, if the player(s) reach levels 20, 30, or 40 without losing a life, a doorway will appear in each of those levels, transporting the player to a secret room and displaying a coded message that, once decoded, gives the player a major hint / spoiler on how to beat the game.",
        "Development and release\nBubble Bobble was designed by Fukio Mitsuji, a Japanese game designer at Taito. A fan of arcade games by Namco, specifically Xevious, Mitsuji felt that Taito's output by comparison were lackluster and of poor quality, hoping that he could help push the company to produce higher-quality arcade titles. His first game was the four-screen racer Super Dead Heat in 1985, followed by the shoot'em up Halley's Comet the same year. After work on these two games was completed, Mitsuji set out to make his next project a platform game, featuring cute characters and a more comical setting compared to his previous works.",
        "Mitsuji wanted the game to be exhilarating and to appeal towards a female audience. Thinking about what kind of things women like to draw or sketch, Mitsuji created an extensive list of over 100 ideas, and after a process of elimination selected bubbles as the core game mechanic. He liked the idea of the screen being filled with bubbles, and thought that popping them all at once would provide a thrilling sensation to the player. His initial idea was to have the player control a robot with a spike on its head to pop bubbles—Mitsuji disliked it for not being \"cool\", instead preferring dinosaurs with ridges along their back. He liked to write down ideas on paper as soon as he thought of them, often flooding his office with stacks of paper filled with potential ideas for game mechanics.",
        "Mitsuji constantly tried to think of new ways to make the game better than it was before, saying to have lost sleep while trying to figure out how he could improve it. He often worked on holidays and late at night to come up with new ideas for the game and to perfect it. Several of the enemies were taken from Chack'n Pop (1983), an older Taito game that is often considered a precursor to Bubble Bobble. Mitsuji intended the game to be played by couples, leading to the creation of the multiple endings, which differ based on player performance.",
        "Bubble Bobble was first published in Japan on June 16, 1986, followed by a wide release in Japan in September 1986 and internationally in October 1986. Alongside Arkanoid, Taito licensed the game to Romstar for distribution in the United States, and to Electrocoin Automatics for Europe.",
        "Ports",
        "Bubble Bobble was ported to many home video game consoles and computers, including the Amstrad CPC, ZX Spectrum, Commodore 64, Apple II, Amiga, Famicom Disk System, Nintendo Entertainment System, MSX2, and Master System—the last of these has two hundred levels as opposed to the arcade version's 100 levels, and was released in Japan as Final Bubble Bobble. A version for the X68000 was developed by Dempa and released in 1994, which includes a gamemode paying homage to Mitsuji's later arcade game Syvalion, titled Sybubblun. Conversions for the Game Boy and Game Boy Color were respectively released in 1991 and 1996, the GBC port being named Classic Bubble Bobble. A version of Bubble Bobble was also produced for the unreleased Taito WOWOW console. In 1996, Taito announced that the source code",
        "in 1991 and 1996, the GBC port being named Classic Bubble Bobble. A version of Bubble Bobble was also produced for the unreleased Taito WOWOW console. In 1996, Taito announced that the source code for Bubble Bobble had been lost, leading to all subsequent home conversions to be reverse-engineered from an original arcade board.",
        "Reception\n\nIn Japan, Game Machine listed Bubble Bobble on their November 1, 1986, issue as the second-most-successful table arcade cabinet of the month, after Taito's Arkanoid. It went on to be the fifth-highest-grossing table arcade game of 1987 in Japan. In the United Kingdom, Bubble Bobble was the top-grossing arcade game for three months in 1987, from April to June. The home conversions were also successful in the United Kingdom, where the game appeared on the sales charts for several years. The ZX Spectrum budget re-release topped the UK charts in July 1991.",
        "The arcade game received positive reviews from Computer and Video Games and Crash. Mean Machines gave the Game Boy port of the game a score of 91%, noting that, while some changes had been made, the game played identical to the original arcade port and \"provides much addiction and challenge\". The four reviewers of Electronic Gaming Monthly stated that the Game Gear version is a faithful conversion of the original which works well in portable form. They particularly praised the simplicity of the gameplay concept and the graphics, and the two-player link option.",
        "Bubble Bobble has been listed by numerous publications among the greatest video games of all time. Your Sinclair magazine ranked the ZX Spectrum version at #58 in their \"Top 100 Games of All Time\" in 1993 based on reader vote. In 1996, GamesMaster rated the game 19th on its \"Top 100 Games of All Time.\" Yahoo! ranked it at #71 in their \"100 Greatest Computer Games Of All Time\" in 2005 for its charming premise and cute character designs. Stuff magazine listed it as part of their \"100 Greatest Games\" in 2008, while GamesTM magazine listed it in their \"Top 100 Games\" in 2010. Stuff.tv ranked it at #47 in their Top 100 Games in 2009, saying \"today’s kids might laugh, but this was gold in 1986\". GamesRadar+ ranked it at #95 in their \"100 Best Games Of All Time\" list in 2011, praising its",
        "it at #47 in their Top 100 Games in 2009, saying \"today’s kids might laugh, but this was gold in 1986\". GamesRadar+ ranked it at #95 in their \"100 Best Games Of All Time\" list in 2011, praising its multiplayer and secrets. GamesRadar+ also labeled it the 24th greatest Nintendo Entertainment System of all time in 2012 for its advancements over other games of its genre and its usage of multiple endings. IGN named it the 23rd best NES game. Hardcore Gaming 101 listed it in their book The 200 Best Video Games of All Time in 2015. Game Informer placed it in their \"Top 300 Games of All Time\" in 2018 for its long-lasting appeal and multiplayer.",
        "Legacy\n\nRe-releases\nThe game has had at least 30 official ports to a large array of computers and consoles throughout the decades.\n\nA remastered version named Bubble Bobble Old & New was made for Game Boy Advance, which also included the original arcade version.\nEurope and North American versions by Empire Interactive\nJapan version by MediaKite\n\nIn October 2005, a version was released for the Xbox, PlayStation 2, and Microsoft Windows as part of the Taito Legends compilation.\n\nAt the end of 2006, a new port for mobile phones in Europe and Japan was released.",
        "At the end of 2006, a new port for mobile phones in Europe and Japan was released.\n\nOn December 24, 2007, the NES version of Bubble Bobble was released  in North America on Nintendo's Virtual Console service for the Wii. The Famicom Disk System version of Bubble Bobble was also released for the Nintendo eShop on October 16, 2013, for the Nintendo 3DS and on January 29, 2014, for the Wii U.\n\nOn November 11, 2016, the game was included in the NES Classic Edition.",
        "Sequels\nRainbow Islands: The Story of Bubble Bobble 2 (1987)\nRainbow Islands Extra Version (1988)\nParasol Stars (1991 originally released for TurboGrafx-16, converted for NES (Europe only), Amiga, Atari ST, and Game Boy (Europe only))\nBubble Bobble Part 2 (1993 Nintendo Entertainment System, Game Boy)\nBubble Bobble II (worldwide) / Bubble Symphony (Europe, Japan, U.S.) (1994 Arcade, Sega Saturn (Japan only))\nBubble Memories: The Story of Bubble Bobble III (1995 Arcade)\n Packy's Treasure Slot (1997 Medal Game)\n Bubble'n Roulette (1998 Medal Game)\n Bubblen No KuruKuru Jump! (1999 Medal Game)\nRainbow Islands: Putty's Party (2000 Bandai WonderSwan)\nBubble Bobble EX (2001 Pachislot)\nBubble Bobble Old & New* (remake, 2002 Game Boy Advance)",
        "Bubblen No KuruKuru Jump! (1999 Medal Game)\nRainbow Islands: Putty's Party (2000 Bandai WonderSwan)\nBubble Bobble EX (2001 Pachislot)\nBubble Bobble Old & New* (remake, 2002 Game Boy Advance)\nBubble Bobble Revolution (2005 Nintendo DS, called Bubble Bobble DS in Japan) \nRainbow Islands Revolution (2005 Nintendo DS)\nBubble Bobble Evolution (2006 PlayStation Portable)\nRainbow Islands Evolution (2007 PlayStation Portable)\nBubble Bobble Plus! (2009, WiiWare on the Wii) also known as  Bubble Bobble Neo! (2009 Xbox Live Arcade on Xbox 360)\nRainbow Islands: Towering Adventure (2009 WiiWare, Xbox Live Arcade)\nBubble Bobble Double (2010 iOS)\nBubble Bobble for Kakao (iOS, Android) - June 15, 2015 (this game was published for KakaoTalk messaging app and fully Taito licensed)",
        "Bubble Bobble Double (2010 iOS)\nBubble Bobble for Kakao (iOS, Android) - June 15, 2015 (this game was published for KakaoTalk messaging app and fully Taito licensed)\nBubble Bobble 4 Friends (2019, Nintendo Switch in Europe; February 20, 2020, in Japan; March 31, 2020, in North America; and November 19, 2020, PlayStation 4 in Japan)\nPuzzle Bobble VR (2021 Oculus Quest)\nMany of the characters and musical themes of Bubble Bobble were used by Taito in a tile-matching video game Puzzle Bobble (also known as Bust-a-Move) and its sequels.",
        "Notes\n\nReferences\n\nExternal links\n\nBubble Bobble for the Atari ST at Atari Mania\n\n \n1986 video games\nAmiga games\nAmstrad CPC games\nApple II games\nArcade video games\nAtari ST games\nCommodore 64 games\nCooperative video games\nDOS games\nFamicom Disk System games\nGame Boy Color games\nGame Boy games\nGame Gear games\nMobile games\nMSX2 games\nNintendo Entertainment System games\nNintendo Switch games\nFM Towns games\nPlatform games\nPlayStation (console) games\nPlayStation 4 games\nRomstar games\nMaster System games\nX68000 games\nVideo games scored by David Whittaker\nVideo games scored by Tim Follin\nVirtual Console games for Wii U\nZX Spectrum games\nSquare Enix franchises\nTaito arcade games\nHamster Corporation games\nNovaLogic games\nVideo games developed in Japan\nMultiplayer and single-player video games"
    ],
    [
        "Bugzilla\nBugzilla is a web-based general-purpose bug tracking system and testing tool originally developed and used by the Mozilla project, and licensed under the Mozilla Public License.\n\nReleased as open-source software by Netscape Communications in 1998, it has been adopted by a variety of organizations for use as a bug tracking system for both free and open-source software and proprietary projects and products. Bugzilla is used, among others, by the Mozilla Foundation, WebKit, Linux kernel, FreeBSD, KDE, Apache, Eclipse and LibreOffice. Red Hat uses it, but is gradually migrating its product to use Jira. It is also self-hosting.\n\nHistory",
        "History \n\nBugzilla was originally devised by Terry Weissman in 1998 for the nascent Mozilla.org project, as an open source application to replace the in-house system then in use at Netscape Communications for tracking defects in the Netscape Communicator suite. Bugzilla was originally written in Tcl, but Weissman decided to port it to Perl before its release as part of Netscape's early open-source code drops, in the hope that more people would be able to contribute to it, given that Perl seemed to be a more popular language at the time.",
        "Bugzilla 2.0 was the result of that port to Perl, and the first version was released to the public via anonymous CVS. In April 2000, Weissman handed over control of the Bugzilla project to Tara Hernandez. Under her leadership, some of the regular contributors were coerced into taking more responsibility, and Bugzilla development became more community-driven. In July 2001, facing distraction from her other responsibilities in Netscape, Hernandez handed control to Dave Miller, who was still in charge .\n\nBugzilla 3.0 was released on May 10, 2007 and brought a refreshed UI, an XML-RPC interface, custom fields and resolutions, mod_perl support, shared saved searches, and improved UTF-8 support, along with other changes.",
        "Bugzilla 4.0 was released on February 15, 2011 and Bugzilla 5.0 was released in July 2015.\n\nTimeline \n\nBugzilla's release timeline:\n\nRequirements \n\nBugzilla's system requirements include:\n\n A compatible database management system\n A suitable release of Perl 5\n An assortment of Perl modules\n A compatible web server\n A suitable mail transfer agent, or any SMTP server\n\nCurrently supported database systems are MySQL, PostgreSQL, Oracle, and SQLite. Bugzilla is usually installed on Linux using the Apache HTTP Server, but any web server that supports CGI such as Lighttpd, Hiawatha, Cherokee can be used. Bugzilla's installation process is command line driven and runs through a series of stages where system requirements and software capabilities are checked.\n\nDesign",
        "Design \n\nWhile the potential exists in the code to turn Bugzilla into a technical support ticket system, task management tool, or project management tool, Bugzilla's developers have chosen to focus on the task of designing a system to track software defects.\n\nZarro Boogs \n\nBugzilla returns the string \"zarro boogs found\" instead of \"0 bugs found\" when a search for bugs returns no results. \"Zarro Boogs\" is intended as a 'buggy' statement itself (a misspelling of \"zero bugs\") and is thus a meta-statement about the nature of software debugging, implying that even when no bugs have been identified, some may exist.\n\nThe following comment is provided in the Bugzilla source code to developers who may be confused by this behaviour:",
        "The following comment is provided in the Bugzilla source code to developers who may be confused by this behaviour:\n\nZarro Boogs Found\nThis is just a goofy way of saying that there were no bugs found matching your query. When asked to explain this message, Terry Weissman (an early Bugzilla developer) had the following to say:",
        "I've been asked to explain this ... way back when, when Netscape released version 4.0 of its browser, we had a release party. Naturally, there had been a big push to try and fix every known bug before the release. Naturally, that hadn't actually happened. (This is not unique to Netscape or to 4.0; the same thing has happened with every software project I've ever seen.) Anyway, at the release party, T-shirts were handed out that said something like \"Netscape 4.0: Zarro Boogs\". Just like the software, the T-shirt had no known bugs. Uh-huh. So, when you query for a list of bugs, and it gets no results, you can think of this as a friendly reminder. Of *course* there are bugs matching your query, they just aren't in the bugsystem yet...\n— Terry Weissman",
        "— Terry Weissman\nFrom The Bugzilla Guide – 2.16.10 Release: Glossary",
        "WONTFIX\n\nWONTFIX is used as a label on issues in Bugzilla and other systems. It indicates that a verified issue will not be addressed for one of several possible reasons including fixing would be too expensive, complicated or risky.\n\nSee also \n\n Comparison of issue-tracking systems\n List of computing mascots\n :Category:Computing mascots\n\nReferences\n\nExternal links \n\nArticles which contain graphical timelines\nBug and issue tracking software\nCross-platform free software\nFree project management software\nFree software programmed in Perl\nMozilla\nSoftware using the Mozilla license"
    ],
    [
        "Bézier curve\nA Bézier curve ( ) is a parametric curve used in computer graphics and related fields. A set of discrete \"control points\" defines a smooth, continuous curve by means of a formula. Usually the curve is intended to approximate a real-world shape that otherwise has no mathematical representation or whose representation is unknown or too complicated. The Bézier curve is named after French engineer Pierre Bézier (1910–1999), who used it in the 1960s for designing curves for the bodywork of Renault cars. Other uses include the design of computer fonts and animation. Bézier curves can be combined to form a Bézier spline, or generalized to higher dimensions to form Bézier surfaces. The Bézier triangle is a special case of the latter.",
        "In vector graphics, Bézier curves are used to model smooth curves that can be scaled indefinitely. \"Paths\", as they are commonly referred to in image manipulation programs, are combinations of linked Bézier curves. Paths are not bound by the limits of rasterized images and are intuitive to modify.",
        "Bézier curves are also used in the time domain, particularly in animation, user interface design and smoothing cursor trajectory in eye gaze controlled interfaces. For example, a Bézier curve can be used to specify the velocity over time of an object such as an icon moving from A to B, rather than simply moving at a fixed number of pixels per step. When animators or interface designers talk about the \"physics\" or \"feel\" of an operation, they may be referring to the particular Bézier curve used to control the velocity over time of the move in question.\n\nThis also applies to robotics where the motion of a welding arm, for example, should be smooth to avoid unnecessary wear.",
        "Invention\nThe mathematical basis for Bézier curves—the Bernstein polynomials—was established in 1912, but the polynomials were not applied to graphics until some 50 years later when mathematician Paul de Casteljau in 1959 developed de Casteljau's algorithm, a numerically stable method for evaluating the curves, and became the first to apply them to computer-aided design at French automaker Citroën. Yet, de Casteljau's method was patented in France but not published until the 1980s while the Bézier polynomials were widely publicised in the 1960s by the French engineer Pierre Bézier, who discovered them independently and used them to design automobile bodies at Renault.",
        "Specific cases\nA Bézier curve is defined by a set of control points P0 through Pn, where n is called the order of the curve (n = 1 for linear, 2 for quadratic, 3 for cubic, etc.). The first and last control points are always the endpoints of the curve; however, the intermediate control points generally do not lie on the curve. The sums in the following sections are to be understood as affine combinations – that is, the coefficients sum to 1.\n\nLinear Bézier curves\nGiven distinct points P0 and P1, a linear Bézier curve is simply a line between those two points. The curve is given by\n\nand is equivalent to linear interpolation. The quantity  represents the displacement vector from the start point to the end point.\n\nQuadratic Bézier curves",
        "and is equivalent to linear interpolation. The quantity  represents the displacement vector from the start point to the end point.\n\nQuadratic Bézier curves\n\nA quadratic Bézier curve is the path traced by the function B(t), given points P0, P1, and P2,\n\n ,\n\nwhich can be interpreted as the linear interpolant of corresponding points on the linear Bézier curves from P0 to P1 and from P1 to P2 respectively. Rearranging the preceding equation yields:\n\n \n\nThis can be written in a way that highlights the symmetry with respect to P1:\n\n \n\nWhich immediately gives the derivative of the Bézier curve with respect to t:",
        "This can be written in a way that highlights the symmetry with respect to P1:\n\n \n\nWhich immediately gives the derivative of the Bézier curve with respect to t:\n\n \n\nfrom which it can be concluded that the tangents to the curve at P0 and P2 intersect at P1. As t increases from 0 to 1, the curve departs from P0 in the direction of P1, then bends to arrive at P2 from the direction of P1.\n\nThe second derivative of the Bézier curve with respect to t is",
        "The second derivative of the Bézier curve with respect to t is\n\nCubic Bézier curves\nFour points P0, P1, P2 and P3 in the plane or in higher-dimensional space define a cubic Bézier curve.\nThe curve starts at P0 going toward P1 and arrives at P3 coming from the direction of P2. Usually, it will not pass through P1 or P2; these points are only there to provide directional information. The distance between P1 and P2 determines \"how far\" and \"how fast\" the curve moves towards P1 before turning towards P2.\n\nWriting BPi,Pj,Pk(t) for the quadratic Bézier curve defined by points Pi, Pj, and Pk, the cubic Bézier curve can be defined as an affine combination of two quadratic Bézier curves:\n\nThe explicit form of the curve is:",
        "The explicit form of the curve is:\n\nFor some choices of P1 and P2 the curve may intersect itself, or contain a cusp.\n\nAny series of 4 distinct points can be converted to a cubic Bézier curve that goes through all 4 points in order.\nGiven the starting and ending point of some cubic Bézier curve, and the points along the curve corresponding to t = 1/3 and t = 2/3, the control points for the original Bézier curve can be recovered.\n\nThe derivative of the cubic Bézier curve with respect to t is\n \n\nThe second derivative of the Bézier curve with respect to t is\n\nGeneral definition\nBézier curves can be defined for any degree n.",
        "The derivative of the cubic Bézier curve with respect to t is\n \n\nThe second derivative of the Bézier curve with respect to t is\n\nGeneral definition\nBézier curves can be defined for any degree n.\n\nRecursive definition\nA recursive definition for the Bézier curve of degree n expresses it as a point-to-point linear combination (linear interpolation) of a pair of corresponding points in two Bézier curves of degree n − 1.\n\nLet  denote the Bézier curve determined by any selection of points P0, P1, ..., Pk. Then to start,\n\nThis recursion is elucidated in the animations below.\n\nExplicit definition\nThe formula can be expressed explicitly as follows (where t0 and (1-t)0 are extended continuously to be 1 throughout [0,1]):\n\nwhere  are the binomial coefficients.\n\nFor example, when n = 5:",
        "where  are the binomial coefficients.\n\nFor example, when n = 5:\n\nTerminology\nSome terminology is associated with these parametric curves. We have\n\nwhere the polynomials\n\nare known as Bernstein basis polynomials of degree n.\n\nt0 = 1, (1 − t)0 = 1, and the binomial coefficient, , is:\n\nThe points Pi are called control points for the Bézier curve. The polygon formed by connecting the Bézier points with lines, starting with P0 and finishing with Pn, is called the Bézier polygon (or control polygon). The convex hull of the Bézier polygon contains the Bézier curve.",
        "Polynomial form\nSometimes it is desirable to express the Bézier curve as a polynomial instead of a sum of less straightforward Bernstein polynomials. Application of the binomial theorem to the definition of the curve followed by some rearrangement will yield\n\nwhere\n\nThis could be practical if  can be computed prior to many evaluations of ; however one should use caution as high order curves may lack numeric stability (de Casteljau's algorithm should be used if this occurs). Note that the empty product is 1.\n\nProperties",
        "The curve begins at  and ends at ; this is the so-called endpoint interpolation property.\n The curve is a line if and only if all the control points are collinear.\n The start and end of the curve is tangent to the first and last section of the Bézier polygon, respectively.\n A curve can be split at any point into two subcurves, or into arbitrarily many subcurves, each of which is also a Bézier curve.",
        "A curve can be split at any point into two subcurves, or into arbitrarily many subcurves, each of which is also a Bézier curve.\n Some curves that seem simple, such as the circle, cannot be described exactly by a Bézier or piecewise Bézier curve; though a four-piece cubic Bézier curve can approximate a circle (see composite Bézier curve), with a maximum radial error of less than one part in a thousand, when each inner control point (or offline point) is the distance  horizontally or vertically from an outer control point on a unit circle. More generally, an n-piece cubic Bézier curve can approximate a circle, when each inner control point is the distance  from an outer control point on a unit circle, where   (i.e. ), and .",
        "Every quadratic Bézier curve is also a cubic Bézier curve, and more generally, every degree n Bézier curve is also a degree m curve for any m > n. In detail, a degree n curve with control points  is equivalent (including the parametrization) to the degree n + 1 curve with control points , where ,  and define , .\n Bézier curves have the variation diminishing property. What this means in intuitive terms is that a Bézier curve does not \"undulate\" more than the polygon of its control points, and may actually \"undulate\" less than that.",
        "There is no local control in degree n Bézier curves—meaning that any change to a control point requires recalculation of and thus affects the aspect of the entire curve, \"although the further that one is from the control point that was changed, the smaller is the change in the curve.\"\n A Bézier curve of order higher than two may intersect itself or have a cusp for certain choices of the control points.",
        "Second-order curve is a parabolic segment\n\nA quadratic Bézier curve is also a segment of a parabola. As a parabola is a conic section, some sources refer to quadratic Béziers as \"conic arcs\". With reference to the figure on the right, the important features of the parabola can be derived as follows:",
        "Tangents to the parabola at the endpoints of the curve (A and B) intersect at its control point (C).\n If D is the midpoint of AB, the tangent to the curve which is perpendicular to CD (dashed cyan line) defines its vertex (V). Its axis of symmetry (dash-dot cyan) passes through V and is perpendicular to the tangent.\n E is either point on the curve with a tangent at 45° to CD (dashed green). If G is the intersection of this tangent and the axis, the line passing through G and perpendicular to CD is the directrix (solid green).\n The focus (F) is at the intersection of the axis and a line passing through E and perpendicular to CD (dotted yellow). The latus rectum is the line segment within the curve (solid yellow).\n\nDerivative\nThe derivative for a curve of order n is",
        "Derivative\nThe derivative for a curve of order n is\n\nConstructing Bézier curves\n\nLinear curves\nLet t denote the fraction of progress (from 0 to 1) the point B(t) has made along its traversal from P0 to P1. For example, when t=0.25, B(t) is one quarter of the way from point P0 to P1. As t varies from 0 to 1, B(t) draws a line from P0 to P1.\n\nQuadratic curves\nFor quadratic Bézier curves one can construct intermediate points Q0 and Q1 such that as t varies from 0 to 1:\n Point Q0(t) varies from P0 to P1 and describes a linear Bézier curve.\n Point Q1(t) varies from P1 to P2 and describes a linear Bézier curve.\n Point B(t) is interpolated linearly between Q0(t) to Q1(t) and describes a quadratic Bézier curve.",
        "Higher-order curves\nFor higher-order curves one needs correspondingly more intermediate points. For cubic curves one can construct intermediate points Q0, Q1, and Q2 that describe linear Bézier curves, and points R0 and R1 that describe quadratic Bézier curves:\n\nFor fourth-order curves one can construct intermediate points Q0, Q1, Q2 and Q3 that describe linear Bézier curves, points R0, R1 and R2 that describe quadratic Bézier curves, and points S0 and S1 that describe cubic Bézier curves:\n\nFor fifth-order curves, one can construct similar intermediate points.\n\nThese representations rest on the process used in De Casteljau's algorithm to calculate Bézier curves.",
        "For fifth-order curves, one can construct similar intermediate points.\n\nThese representations rest on the process used in De Casteljau's algorithm to calculate Bézier curves.\n\nOffsets (or stroking) of Bézier curves\nThe curve at a fixed offset from a given Bézier curve, called an offset or parallel curve in mathematics (lying \"parallel\" to the original curve, like the offset between rails in a railroad track), cannot be exactly formed by a Bézier curve (except in some trivial cases). In general, the two-sided offset curve of a cubic Bézier is a 10th-order algebraic curve and more generally for a Bézier of degree n the two-sided offset curve is an algebraic curve of degree 4n − 2. However, there are heuristic methods that usually give an adequate approximation for practical purposes.",
        "In the field of vector graphics, painting two symmetrically distanced offset curves is called stroking (the Bézier curve or in general a path of several Bézier segments). The conversion from offset curves to filled Bézier contours is of practical importance in converting fonts defined in Metafont, which require stroking of Bézier curves, to the more widely used PostScript type 1 fonts, which only require (for efficiency purposes) the mathematically simpler operation of filling a contour defined by (non-self-intersecting) Bézier curves.",
        "Degree elevation\nA Bézier curve of degree n can be converted into a Bézier curve of degree n + 1 with the same shape. This is useful if software supports Bézier curves only of specific degree. For example, systems that can only work with cubic Bézier curves can implicitly work with quadratic curves by using their equivalent cubic representation.\n\nTo do degree elevation, we use the equality  Each component  is multiplied by (1 − t) and t, thus increasing a degree by one, without changing the value. Here is the example of increasing degree from 2 to 3.\n\nFor arbitrary n we use equalities\n\nTherefore:\n\nintroducing arbitrary  and .\n\nTherefore, new control points are",
        "For arbitrary n we use equalities\n\nTherefore:\n\nintroducing arbitrary  and .\n\nTherefore, new control points are\n\nRepeated degree elevation\nThe concept of degree elevation can be repeated on a control polygon R to get a sequence of control polygons R, R1, R2, and so on. After r degree elevations, the polygon Rr has the vertices P0,r, P1,r, P2,r, ..., Pn+r,r given by \n\nIt can also be shown that for the underlying Bézier curve B,\n\nRational Bézier curves",
        "It can also be shown that for the underlying Bézier curve B,\n\nRational Bézier curves\n\nThe rational Bézier curve adds adjustable weights to provide closer approximations to arbitrary shapes. The numerator is a weighted Bernstein-form Bézier curve and the denominator is a weighted sum of Bernstein polynomials. Rational Bézier curves can, among other uses, be used to represent segments of conic sections exactly, including circular arcs.\n\nGiven n + 1 control points P0, ..., Pn, the rational Bézier curve can be described by\n\nor simply",
        "Given n + 1 control points P0, ..., Pn, the rational Bézier curve can be described by\n\nor simply\n\nThe expression can be extended by using number systems besides reals for the weights. In the complex plane the points {1}, {-1}, and {1} with weights {}, {1}, and {} generate a full circle with radius one. For curves with points and weights on a circle, the weights can be scaled without changing the curve's shape.  Scaling the central weight of the above curve by 1.35508 gives a more uniform parameterization.\n\nApplications\n\nComputer graphics",
        "Applications\n\nComputer graphics\n\nBézier curves are widely used in computer graphics to model smooth curves. As the curve is completely contained in the convex hull of its control points, the points can be graphically displayed and used to manipulate the curve intuitively. Affine transformations such as translation and rotation can be applied on the curve by applying the respective transform on the control points of the curve.",
        "Quadratic and cubic Bézier curves are most common. Higher degree curves are more computationally expensive to evaluate. When more complex shapes are needed, low order Bézier curves are patched together, producing a composite Bézier curve. A composite Bézier curve is commonly referred to as a \"path\" in vector graphics languages (like PostScript), vector graphics standards (like SVG) and vector graphics programs (like Artline, Timeworks Publisher, Adobe Illustrator, CorelDraw, Inkscape, and Allegro). In order to join Bézier curves into a composite Bézier curve without kinks, a property called G1 continuous, it suffices to force the control point at which two constituent Bézier curves meet to lie on the line defined by the two control points on either side.",
        "The simplest method for scan converting (rasterizing) a Bézier curve is to evaluate it at many closely spaced points and scan convert the approximating sequence of line segments. However, this does not guarantee that the rasterized output looks sufficiently smooth, because the points may be spaced too far apart. Conversely it may generate too many points in areas where the curve is close to linear. A common adaptive method is recursive subdivision, in which a curve's control points are checked to see if the curve approximates a line to within a small tolerance. If not, the curve is subdivided parametrically into two segments, 0 ≤ t ≤ 0.5 and 0.5 ≤ t ≤ 1, and the same procedure is applied recursively to each half. There are also forward differencing methods, but great care must be taken to",
        "parametrically into two segments, 0 ≤ t ≤ 0.5 and 0.5 ≤ t ≤ 1, and the same procedure is applied recursively to each half. There are also forward differencing methods, but great care must be taken to analyse error propagation.",
        "Analytical methods where a Bézier is intersected with each scan line involve finding roots of cubic polynomials (for cubic Béziers) and dealing with multiple roots, so they are not often used in practice.",
        "The rasterisation algorithm used in Metafont is based on discretising the curve, so that it is approximated by a sequence of \"rook moves\" that are purely vertical or purely horizontal, along the pixel boundaries. To that end, the plane is first split into eight 45° sectors (by the coordinate axes and the two lines ), then the curve is decomposed into smaller segments such that the direction of a curve segment stays within one sector; since the curve velocity is a second degree polynomial, finding the  values where it is parallel to one of these lines can be done by solving quadratic equations. Within each segment, either horizontal or vertical movement dominates, and the total number of steps in either direction can be read off from the endpoint coordinates; in for example the 0–45°",
        "Within each segment, either horizontal or vertical movement dominates, and the total number of steps in either direction can be read off from the endpoint coordinates; in for example the 0–45° sector horizontal movement to the right dominates, so it only remains to decide between which steps to the right the curve should make a step up.",
        "There is also a modified curve form of Bresenham's line drawing algorithm by Zingl that performs this rasterization by subdividing the curve into rational pieces and calculating the error at each pixel location such that it either travels at a 45° angle or straight depending on compounding error as it iterates through the curve. This reduces the next step calculation to a series of integer additions and subtractions.\n\nAnimation\nIn animation applications, such as Adobe Flash and Synfig, Bézier curves are used to outline, for example, movement. Users outline the wanted path in Bézier curves, and the application creates the needed frames for the object to move along the path.",
        "In 3D animation, Bézier curves are often used to define 3D paths as well as 2D curves for keyframe interpolation. Bézier curves are now very frequently used to control the animation easing in CSS, JavaScript, JavaFx and Flutter SDK.\n\nFonts\nTrueType fonts use composite Bézier curves composed of quadratic Bézier curves. Other languages and imaging tools (such as PostScript, Asymptote, Metafont, and SVG) use composite Béziers composed of cubic Bézier curves for drawing curved shapes. OpenType fonts can use either kind of curve, depending on which font technology underlies the OpenType wrapper.",
        "Font engines, like FreeType, draw the font's curves (and lines) on a pixellated surface using a process known as font rasterization. Typically font engines and vector graphics engines render Bézier curves by splitting them recursively up to the point where the curve is flat enough to be drawn as a series of linear or circular segments. The exact splitting algorithm is implementation dependent, only the flatness criteria must be respected to reach the necessary precision and to avoid non-monotonic local changes of curvature. The \"smooth curve\" feature of charts in Microsoft Excel also uses this algorithm.",
        "Because arcs of circles and ellipses cannot be exactly represented by Bézier curves, they are first approximated by Bézier curves, which are in turn approximated by arcs of circles. This is inefficient as there exists also approximations of all Bézier curves using arcs of circles or ellipses, which can be rendered incrementally with arbitrary precision. Another approach, used by modern hardware graphics adapters with accelerated geometry, can convert exactly all Bézier and conic curves (or surfaces) into NURBS, that can be rendered incrementally without first splitting the curve recursively to reach the necessary flatness condition. This approach also preserves the curve definition under all linear or perspective 2D and 3D transforms and projections.",
        "Robotics\nBézier curves can be used in robotics to produce trajectories of an end-effector due to the virtue of the control polygon’s ability to give a clear indication of whether the path is colliding with any nearby obstacle or object. Furthermore, joint space trajectories can be accurately differentiated using Bézier curves. Consequently, the derivatives of joint space trajectories are used in the calculation of the dynamics and control effort (torque profiles) of the robotic manipulator.\n\nSee also\n Bézier surface\n B-spline\n GEM/4 and GEM/5\n Hermite curve\n NURBS\n String art – Bézier curves are also formed by many common forms of string art, where strings are looped across a frame of nails.\n Variation diminishing property of Bézier curves\n\nNotes\n\nReferences\n\nCitations\n\nSources",
        "Notes\n\nReferences\n\nCitations\n\nSources\n\n \n \n  Excellent discussion of implementation details; available for free as part of the TeX distribution.\n\nFurther reading\n A Primer on Bézier Curves an open source online book explaining Bézier curves and associated graphics algorithms, with interactive graphics\n Cubic Bezier Curves – Under the Hood (video) video showing how computers render a cubic Bézier curve, by Peter Nowell\n From Bézier to Bernstein Feature Column from American Mathematical Society\n \n \n  This book is out of print and freely available from the author.\n \n \n  (60 pages)\n \n \n \n Hovey, Chad (2022).  Formulation and Python Implementation of Bézier and B-Spline Geometry. SAND2022-7702C. (153 pages)",
        "External links\n Computer code\nTinySpline: Open source C-library for NURBS, B-splines and Bézier curves with bindings for various languages\n C++ library to generate Bézier functions at compile time\nSimple Bézier curve implementation via recursive method in Python\n\nGraphic design\nInterpolation\nCurves\nDesign"
    ],
    [
        "Capcom\nis a Japanese video game company. It has created a number of multi-million-selling game franchises, with its most commercially successful being Resident Evil, Monster Hunter, Street Fighter, Mega Man, Devil May Cry, Dead Rising, Ace Attorney, and Marvel vs. Capcom. Mega Man himself serves as the official mascot of the company. Established in 1979, it has become an international enterprise with subsidiaries in East Asia (Hong Kong), Europe (London, England), and North America (San Francisco, California).\n\nHistory\nCapcom's predecessor, I.R.M. Corporation, was founded on May 30, 1979 by Kenzo Tsujimoto, who was still president of Irem Corporation when he founded I.R.M. He worked concomitantly in both companies until leaving the former in 1983.",
        "The original companies that spawned Capcom's Japan branch were I.R.M. and its subsidiary Japan Capsule Computers Co., Ltd., both of which were devoted to the manufacture and distribution of electronic game machines. The two companies underwent a name change to Sanbi Co., Ltd. in September 1981. On June 11, 1983, Tsujimoto established Capcom Co., Ltd. for the purpose of taking over the internal sales department.",
        "In January 1989, Capcom Co., Ltd. merged with Sanbi Co., Ltd., resulting in the current Japan branch. The name Capcom is a clipped compound of \"Capsule Computers\", a term coined by the company for the arcade machines it solely manufactured in its early years, designed to set themselves apart from personal computers that were becoming widespread. \"Capsule\" alludes to how Capcom likened its game software to \"a capsule packed to the brim with gaming fun\", and to the company's desire to protect its intellectual property with a hard outer shell, preventing illegal copies and inferior imitations.",
        "Capcom's first product was the medal game Little League (1983). It released its first arcade video game, Vulgus (May 1984). Starting with the arcade hit 1942 (1984), they began designing games with international markets in mind. The successful 1985 arcade games Commando and Ghosts 'n Goblins have been credited as the products \"that shot [Capcom] to 8-bit silicon stardom\" in the mid-1980s. Starting with Commando (late 1985), Capcom began licensing their arcade games for release on home computers, notably to British software houses Elite Systems and U.S. Gold in the late 1980s.",
        "Beginning with a Nintendo Entertainment System port of 1942 (published in Dec. 1985), the company ventured into the market of home console video games, which would eventually become its main business. The Capcom USA division had a brief stint in the late 1980s as a video game publisher for Commodore 64 and IBM PC DOS computers, although development of these arcade ports was handled by other companies. Capcom went on to create 15 multi-million-selling home video game franchises, with the best-selling being Resident Evil (1996). Their highest-grossing is the fighting game Street Fighter II (1991), driven largely by its success in arcades.",
        "In the late 1980s, Capcom was on the verge of bankruptcy when the development of a strip Mahjong game called Mahjong Gakuen started. It outsold Ghouls 'n Ghosts, the eighth highest-grossing arcade game of 1989 in Japan, and is credited with saving the company from financial crisis.",
        "Capcom has been noted as the last major publisher to be committed to 2D games, though it was not entirely by choice. The company's commitment to the Super Nintendo Entertainment System as its platform of choice caused them to lag behind other leading publishers in developing 3D-capable arcade boards. Also, the 2D animated cartoon-style graphics seen in games such as Darkstalkers: The Night Warriors and X-Men: Children of the Atom proved popular, leading Capcom to adopt them as a signature style and use them in more games.",
        "In 1990, Capcom entered the bowling industry with Bowlingo. It was a coin-operated, electro-mechanical, fully automated mini ten-pin bowling installation. It was smaller than a standard bowling alley, designed to be smaller and cheaper for amusement arcades. Bowlingo drew significant earnings in North America upon release in 1990.\n\nIn 1994, Capcom adapted its Street Fighter series of fighting games into a film of the same name. While commercially successful, it was critically panned. A 2002 adaptation of its Resident Evil series faced similar criticism but was also successful in theaters. The company sees films as a way to build sales for its video games.",
        "Capcom partnered with Nyu Media in 2011 to publish and distribute the Japanese independent (dōjin soft) games that Nyu localized into the English language. The company works with the Polish localization company QLOC to port Capcom's games to other platforms; notably, examples are DmC: Devil May Crys PC version and its PlayStation 4 and Xbox One remasters, Dragon's Dogmas PC version, and Dead Risings version on PlayStation 4, Xbox One, and PC.",
        "In 2012, Capcom came under criticism for controversial sales tactics, such as the implementation of disc-locked content, which requires players to pay for additional content that is already available within the game's files, most notably in Street Fighter X Tekken. The company defended the practice. It has also been criticized for other business decisions, such as not releasing certain games outside of Japan (most notably the Sengoku Basara series), abruptly cancelling anticipated projects (most notably Mega Man Legends 3), and shutting down Clover Studio.",
        "On August 27, 2014, Capcom filed a patent infringement lawsuit against Koei Tecmo Games at the Osaka District Court for 980 million yen in damage. Capcom claimed Koei Tecmo infringed a patent it obtained in 2002 regarding a play feature in video games.",
        "On 2 November 2020, the company reported that its servers were affected by ransomware, scrambling its data, and the threat actors, the Ragnar Locker hacker group, had allegedly stolen 1TB of sensitive corporate data and were blackmailing Capcom to pay them to remove the ransomware. By mid-November, the group began putting information from the hack online, which included contact information for up to 350,000 of the company's employees and partners, as well as plans for upcoming games, indicating that Capcom opted to not pay the group. Capcom affirmed that no credit-card or other sensitive financial information was obtained in the hack.",
        "In 2021, Capcom removed appearances of the Rising Sun Flag from their rerelease of Street Fighter II. Although Capcom did not provide an official explanation for the flag's removal, due to the flag-related controversy, it is speculated that it was done so to avoid offending  segments of the international gaming community.",
        "Artist and author Judy A. Juracek filed a lawsuit in June 2021 against Capcom for copyright infringement. In the court filings, she asserted Capcom had used images from her 1996 book Surfaces in their cover art and other assets for Resident Evil 4, Devil May Cry and other games. This was discovered due to the 2020 Capcom data breach, with several files and images matching those that were included within the book's companion CD-ROM. The court filings noted one image file of a metal surface, named ME0009 in Capcom's files, to have the same exact name on the book's CD-ROM. Juracek was seeking over  in damages and $2,500 to $25,000 in false copyright management for each photograph Capcom used. Before a court date could be made, the matter was settled \"amicably\" in February 2022. It comes on",
        "in damages and $2,500 to $25,000 in false copyright management for each photograph Capcom used. Before a court date could be made, the matter was settled \"amicably\" in February 2022. It comes on the heels of Capcom being accused by Dutch movie director Richard Raaphorst of copying the monster design of his movie Frankenstein's Army into their game Resident Evil Village.",
        "In February 2022, it was reported by Bloomberg that Saudi Arabia's Public Investment Fund had purchased a 5% stake in Capcom, for an approximate value of US$332 million.\n\nIn July 2023, Capcom acquired Tokyo-based computer graphics studio Swordcanes Studio.\n\nCorporate structure",
        "Corporate structure\n\nDevelopment divisions\nIn its beginning few years, Capcom's Japan branch had three development groups referred to as \"Planning Rooms\", led by Tokuro Fujiwara, Takashi Nishiyama and Yoshiki Okamoto. Later, games developed internally were created by several numbered \"Production Studios\", each assigned to different games. Starting in 2002, the development process was reformed to better share technologies and expertise, and the individual studios were gradually restructured into bigger departments responsible for different tasks. While there are self-contained departments for the creation of arcade, pachinko and pachislo, online, and mobile games, the Consumer Games R&D Division is an amalgamation of subsections in charge of game development stages.",
        "Capcom has two internal Consumer Games Development divisions:\n\n Division 1, headed by Jun Takeuchi, develops Resident Evil, Mega Man, Devil May Cry, Dead Rising, and other major franchises (usually targeting global audiences).\n Division 2, headed by Ryozo Tsujimoto, develops Ace Attorney, Onimusha, Sengoku Basara, Ōkami, and other franchises with more traditional IP (usually targeting audiences in Asia) alongside online-focused franchises such as Monster Hunter, Street Fighter, Marvel vs. Capcom, and Lost Planet.",
        "In addition to these teams, Capcom commissions outside development studios to ensure a steady output of titles. However, following poor sales of Dark Void and Bionic Commando, its management has decided to limit outsourcing to sequels and newer versions of installments in existing franchises, reserving the development of original titles for its in-house teams. The production of games, budgets, and platform support are decided on in development approval meetings, attended by the company management and the marketing, sales and quality control departments.\n\nBranches and subsidiaries",
        "Branches and subsidiaries\n\nCapcom Co., Ltd.'s head office building and R&D building are in Chūō-ku, Osaka. The parent company also has a branch office in the Shinjuku Mitsui Building in Nishi-Shinjuku, Shinjuku, Tokyo; and the Ueno Facility, a branch office in Iga, Mie Prefecture.\n\nThe international Capcom Group encompasses 12 subsidiaries in Japan, rest of East Asia, North America, and Europe.\n\nGame-related media\nIn addition to home, online, mobile, arcade, pachinko, and pachislot games, Capcom publishes strategy guides; maintains its own Plaza Capcom arcade centers in Japan; and licenses its franchise and character properties for tie-in products, movies, television series, and stage performances.",
        "Suleputer, an in-house marketing and music label established in cooperation with Sony Music Entertainment Intermedia in 1998, publishes CDs, DVDs, and other media based on Capcom's games. Captivate (renamed from Gamers Day in 2008), an annual private media summit, is traditionally used for new game and business announcements.\n\nGames\n\nCapcom started its Street Fighter franchise in 1987. The series of fighting games are among the most popular in their genre. Having sold more than 50 million copies, it is one of Capcom's flagship franchises. The company also introduced its Mega Man series in 1987, which has sold 40 million copies.",
        "The company released the first entry in its Resident Evil survival horror series in 1996, which become its most successful game series, selling more than 140 million copies. After releasing the second entry in the Resident Evil series, Capcom began a Resident Evil game for PlayStation 2. As it was significantly different from the existing series' games, Capcom decided to spin it into its own series, Devil May Cry. The first three entries were exclusively for PlayStation 2; further entries were released for non-Sony consoles. The entire series has sold almost 30 million copies. Capcom began its Monster Hunter series in 2004, which has sold more than 90 million copies on a variety of consoles.",
        "Although the company often relies on existing franchises, it has also published and developed several titles for the Xbox 360, PlayStation 3, and Wii based on original intellectual property: Lost Planet: Extreme Condition, Dead Rising, Dragon's Dogma, Asura's Wrath, and Zack and Wiki. During this period, Capcom also helped publish several original titles from up-and-coming Western developers, including Remember Me, Dark Void, and Spyborgs, titles other publishers were not willing to gamble on. Other games of note are the titles Ōkami, Ōkamiden, and Ghost Trick: Phantom Detective.",
        "In 2015, the PlayStation 4 version of Ultra Street Fighter IV was pulled from the Capcom Pro Tour due to numerous technical issues and bugs. In 2016, Capcom released Street Fighter V with very limited single player content. At launch, there were stability issues with the game's network that booted players mid-game even when they were not playing in an online mode. Street Fighter V failed to meet its sales target of 2 million in March 2016.\n\nPlatinum Titles\nCapcom compiles a \"Platinum Titles\" list, updated quarterly, of its games that have sold over one million copies. It contains over 100 video games. This table shows the top ten titles, by sold copies, as of June 30, 2023.\n\nSee also\n\nArticles\nCapcom Cup\nCapcom Five\nDreamHack\nEvolution Championship Series",
        "See also\n\nArticles\nCapcom Cup\nCapcom Five\nDreamHack\nEvolution Championship Series\n\nCompanies founded by ex-Capcom employees\n\nReferences\n\nExternal links\nOfficial website\n\n \nCompanies based in Osaka\nCompanies listed on the Tokyo Stock Exchange\nGolden Joystick Award winners\nJapanese brands\nJapanese companies established in 1979\nPinball manufacturers\nPublic Investment Fund\nVideo game companies established in 1979\nVideo game companies of Japan\nVideo game development companies\nVideo game publishers\n1993 initial public offerings"
    ],
    [
        "Car (disambiguation)\nA car is a wheeled motor vehicle used for transporting passengers.\n\nCar(s), CAR(s), or The Car(s) may also refer to:\n\nComputing\n C.a.R. (Z.u.L.), geometry software\n CAR and CDR, commands in LISP computer programming\n Clock with Adaptive Replacement, a page replacement algorithm\n Computer-assisted reporting\n Computer-assisted reviewing\n\nEconomics\n Capital adequacy ratio, a ratio of a bank's capital to its risk\n Cost accrual ratio, an accounting formula\n Cumulative abnormal return\n Cumulative average return, a financial concept related to the time value of money",
        "Film and television\n Cars (franchise), a Disney/Pixar film series\n Cars (film), a 2006 computer-animated film from Disney and Pixar\n The Car (1977 film), an American horror film\n Car, a BBC Two television ident first aired in 1993 (see BBC Two '1991–2001' idents)\n The Car (1997 film), a Malayalam film\n \"The Car\" (The Assistants episode)\n\nLiterature\n Car (magazine), a British auto-enthusiast publication\n The Car (novel), a novel by Gary Paulsen",
        "Literature\n Car (magazine), a British auto-enthusiast publication\n The Car (novel), a novel by Gary Paulsen\n\nMilitary\n Canadian Airborne Regiment, a Canadian Forces formation\n Colt Automatic Rifle, a 5.56mm NATO firearm\n Combat Action Ribbon, a United States military decoration\n U.S. Army Combat Arms Regimental System, a 1950s reorganisation of the regiments of the US Army\n Conflict Armament Research, a UK-based investigative organization that tracks the supply of armaments into conflict-affected areas\n\nMusic\n The Cars, an American band",
        "Music\n The Cars, an American band\n\nAlbums\n Peter Gabriel (1977 album) or Car\n The Cars (album), a 1978 album by The Cars\n Cars (soundtrack), the soundtrack to the 2006 film\n Cars (Now, Now Every Children album), 2009\n Cars, a 2011 album by Kris Delmhorst\n C.A.R. (album), a 2012 album by Serengeti\n The Car (album), a 2022 album by Arctic Monkeys\n\nSongs\n \"The Car\" (song), a song by Jeff Carson\n \"Cars\" (song), a 1979 single by Gary Numan\n \"Car\", a 1994 song by Built to Spill from There's Nothing Wrong with Love\n\nPaintings\n Cars (painting), a series of paintings by Andy Warhol\n The Car (Brack), a 1955 painting by John Brack\n\nPeople\n Car (surname)\n Cars (surname)",
        "Paintings\n Cars (painting), a series of paintings by Andy Warhol\n The Car (Brack), a 1955 painting by John Brack\n\nPeople\n Car (surname)\n Cars (surname)\n\nPlaces\n Car, Azerbaijan, a village\n Čar, a village in Serbia\n Cars, Gironde, France, a commune\n Les Cars, Haute-Vienne, France, a commune\n Central African Republic\n Central Asian Republics\n Cordillera Administrative Region, Philippines\n County Carlow, Ireland, Chapman code\n\nScience\n Canonical anticommutation relation\n Carina (constellation)\n Chimeric antigen receptor, artificial T cell receptors\n Coherent anti-Stokes Raman spectroscopy\n Constitutive androstane receptor\n Cortisol awakening response, on waking from sleep\n Coxsackievirus and adenovirus receptor, a protein",
        "Sports\n Carolina Hurricanes, a National Hockey League team\n Carolina Panthers, a National Football League team\n Club Always Ready, a Bolivian football club from La Paz\n Rugby Africa, formerly known as Confederation of African Rugby\n\nTransportation\n Railroad car\n Canada Atlantic Railway, 1879–1914\n Canadian Atlantic Railway, 1986–1994\n Carlisle railway station's station code\n Car, the cab of an elevator\n Car, a tram, streetcar, or trolley car\n\nOther uses",
        "Other uses\n\nCar\n Car (Greek myth), one or two figures in Greek mythology\n Car language, an Austroasiatic language of the Nicobar Islands in the eastern Indian Ocean\n car, ISO 639-2 and ISO 639-3 codes of the Carib language, spoken by the Kalina people of South America\n Cars (video game), a 2006 video game based on the film\n Chimeric antigen receptor, a type of protein engineered to give T cells the ability to target a specific protein",
        "CAR\n Canadian Aviation Regulations\n Avis Budget Group (Nasdaq: CAR)\n Central apparatus room, an equipment room found at broadcasting facilities\n Children of the American Revolution, a genealogical society\n  or Action Committee for Renewal, a political party of Togo\n Council for Aboriginal Reconciliation, body founded by the Australian Government in 1991 as part of its Reconciliation in Australia policy\n Council for Aboriginal Rights (1951–1980s), Victoria, Australia\n Criminal Appeal Reports, law reports in the United Kingdom\n\nSee also\n\n Carr (disambiguation)\n CARS (disambiguation)\n Le Car (disambiguation)\n iCar"
    ],
    [
        "Central processing unit\nA central processing unit (CPU)—also called a central processor or main processor—is the most important processor in a given computer. Its electronic circuitry executes instructions of a computer program, such as arithmetic, logic, controlling, and input/output (I/O) operations. This role contrasts with that of external components, such as main memory and I/O circuitry, and specialized coprocessors such as graphics processing units (GPUs).",
        "The form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic–logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution (of instructions) by directing the coordinated operations of the ALU, registers, and other components.",
        "Most modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to create additional virtual or logical CPUs.\n\nAn IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC).\n\nArray processors or vector processors have multiple processors that operate in parallel, with no unit considered central. Virtual CPUs are an abstraction of dynamically aggregated computational resources.\n\nHistory",
        "History\n\nEarly computers such as the ENIAC had to be physically rewired to perform different tasks, which caused these machines to be called \"fixed-program computers\". The \"central processing unit\" term has been in use since as early as 1955. Since the term \"CPU\" is generally defined as a device for software (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.",
        "The idea of a stored-program computer had been already present in the design of J. Presper Eckert and John William Mauchly's ENIAC, but was initially omitted so that ENIAC could be finished sooner. On June 30, 1945, before ENIAC was made, mathematician John von Neumann distributed a paper entitled First Draft of a Report on the EDVAC. It was the outline of a stored-program computer that would eventually be completed in August 1949. EDVAC was designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speed computer memory rather than specified by the physical wiring of the computer. This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure",
        "high-speed computer memory rather than specified by the physical wiring of the computer. This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task. With von Neumann's design, the program that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC was not the first stored-program computer; the Manchester Baby, which was a small-scale experimental stored-program computer, ran its first program on 21 June 1948 and the Manchester Mark 1 ran its first program during the night of 16–17 June 1949.",
        "Early CPUs were custom designs used as part of a larger and sometimes distinctive computer. However, this method of designing custom CPUs for a particular application has largely given way to the development of multi-purpose processors produced in large quantities. This standardization began in the era of discrete transistor mainframes and minicomputers, and has rapidly accelerated with the popularization of the integrated circuit (IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of nanometers. Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in electronic devices",
        "of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in electronic devices ranging from automobiles to cellphones, and sometimes even in toys.",
        "While von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, and the design became known as the von Neumann architecture, others before him, such as Konrad Zuse, had suggested and implemented similar ideas. The so-called Harvard architecture of the Harvard Mark I, which was completed before EDVAC, also used a stored-program design using punched paper tape rather than electronic memory. The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded",
        "and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded applications; for instance, the Atmel AVR microcontrollers are Harvard-architecture processors.",
        "Relays and vacuum tubes (thermionic tubes) were commonly used as switching elements; a useful computer requires thousands or tens of thousands of switching devices. The overall speed of a system is dependent on the speed of the switches. Vacuum-tube computers such as EDVAC tended to average eight hours between failures, whereas relay computers—such as the slower but earlier Harvard Mark I—failed very rarely. In the end, tube-based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs. Clock signal frequencies ranging from 100 kHz to 4 MHz were very common at this time, limited largely by the speed of the switching devices",
        "low clock rates compared to modern microelectronic designs. Clock signal frequencies ranging from 100 kHz to 4 MHz were very common at this time, limited largely by the speed of the switching devices they were built with.",
        "Transistor CPUs\n\nThe design complexity of CPUs increased as various technologies facilitated the building of smaller and more reliable electronic devices. The first such improvement came with the advent of the transistor. Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable, and fragile switching elements, like vacuum tubes and relays. With this improvement, more complex and reliable CPUs were built onto one or several printed circuit boards containing discrete (individual) components.",
        "In 1964, IBM introduced its IBM System/360 computer architecture that was used in a series of computers capable of running the same programs with different speeds and performances. This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer. To facilitate this improvement, IBM used the concept of a microprogram (often called \"microcode\"), which still sees widespread use in modern CPUs. The System/360 architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is continued by similar modern computers like the IBM zSeries. In 1965, Digital Equipment Corporation (DEC) introduced another influential computer aimed at the scientific and research markets—the PDP-8.",
        "Transistor-based computers had several distinct advantages over their predecessors. Aside from facilitating increased reliability and lower power consumption, transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay. The increased reliability and dramatically increased speed of the switching elements, which were almost exclusively transistors by this time; CPU clock rates in the tens of megahertz were easily obtained during this period. Additionally, while discrete transistor and IC CPUs were in heavy usage, new high-performance designs like single instruction, multiple data (SIMD) vector processors began to appear. These early experimental designs later gave rise to the era of specialized",
        "heavy usage, new high-performance designs like single instruction, multiple data (SIMD) vector processors began to appear. These early experimental designs later gave rise to the era of specialized supercomputers like those made by Cray Inc and Fujitsu Ltd.",
        "Small-scale integration CPUs\n\nDuring this period, a method of manufacturing many interconnected transistors in a compact space was developed. The integrated circuit (IC) allowed a large number of transistors to be manufactured on a single semiconductor-based die, or \"chip\". At first, only very basic non-specialized digital circuits such as NOR gates were miniaturized into ICs. CPUs based on these \"building block\" ICs are generally referred to as \"small-scale integration\" (SSI) devices. SSI ICs, such as the ones used in the Apollo Guidance Computer, usually contained up to a few dozen transistors. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs.",
        "IBM's System/370, follow-on to the System/360, used SSI ICs rather than Solid Logic Technology discrete-transistor modules. DEC's PDP-8/I and KI10 PDP-10 also switched from the individual transistors used by the PDP-8 and PDP-10 to SSI ICs, and their extremely popular PDP-11 line was originally built with SSI ICs, but was eventually implemented with LSI components once these became practical.",
        "Large-scale integration CPUs",
        "Lee Boysel published influential articles, including a 1967 \"manifesto\", which described how to build the equivalent of a 32-bit mainframe computer from a relatively small number of large-scale integration circuits (LSI). The only way to build LSI chips, which are chips with a hundred or more gates, was to build them using a metal–oxide–semiconductor (MOS) semiconductor manufacturing process (either PMOS logic, NMOS logic, or CMOS logic). However, some companies continued to build processors out of bipolar transistor–transistor logic (TTL) chips because bipolar junction transistors were faster than MOS chips up until the 1970s (a few companies such as Datapoint continued to build processors out of TTL chips until the early 1980s). In the 1960s, MOS ICs were slower and initially considered",
        "than MOS chips up until the 1970s (a few companies such as Datapoint continued to build processors out of TTL chips until the early 1980s). In the 1960s, MOS ICs were slower and initially considered useful only in applications that required low power. Following the development of silicon-gate MOS technology by Federico Faggin at Fairchild Semiconductor in 1968, MOS ICs largely replaced bipolar TTL as the standard chip technology in the early 1970s.",
        "As the microelectronic technology advanced, an increasing number of transistors were placed on ICs, decreasing the number of individual ICs needed for a complete CPU. MSI and LSI ICs increased transistor counts to hundreds, and then thousands. By 1968, the number of ICs required to build a complete CPU had been reduced to 24 ICs of eight different types, with each IC containing roughly 1000 MOSFETs. In stark contrast with its SSI and MSI predecessors, the first LSI implementation of the PDP-11 contained a CPU composed of only four LSI integrated circuits.\n\nMicroprocessors",
        "Since microprocessors were first introduced they have almost completely overtaken all other central processing unit implementation methods. The first commercially available microprocessor, made in 1971, was the Intel 4004, and the first widely used microprocessor, made in 1974, was the Intel 8080. Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures, and eventually produced instruction set compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual success of the ubiquitous personal computer, the term CPU is now applied almost exclusively to microprocessors. Several CPUs (denoted cores) can be combined in a single processing",
        "advent and eventual success of the ubiquitous personal computer, the term CPU is now applied almost exclusively to microprocessors. Several CPUs (denoted cores) can be combined in a single processing chip.",
        "Previous generations of CPUs were implemented as discrete components and numerous small integrated circuits (ICs) on one or more circuit boards. Microprocessors, on the other hand, are CPUs manufactured on a very small number of ICs; usually just one. The overall smaller CPU size, as a result of being implemented on a single die, means faster switching time because of physical factors like decreased gate parasitic capacitance. This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz. Additionally, the ability to construct exceedingly small transistors on an IC has increased the complexity and number of transistors in a single CPU many fold. This widely observed trend is described by Moore's law, which had proven to be a fairly",
        "small transistors on an IC has increased the complexity and number of transistors in a single CPU many fold. This widely observed trend is described by Moore's law, which had proven to be a fairly accurate predictor of the growth of CPU (and other IC) complexity until 2016.",
        "While the complexity, size, construction and general form of CPUs have changed enormously since 1950, the basic design and function has not changed much at all. Almost all common CPUs today can be very accurately described as von Neumann stored-program machines. As Moore's law no longer holds, concerns have arisen about the limits of integrated circuit transistor technology. Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant. These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer, as well as to expand the use of parallelism and other methods that extend the usefulness of the classical von Neumann",
        "researchers to investigate new methods of computing such as the quantum computer, as well as to expand the use of parallelism and other methods that extend the usefulness of the classical von Neumann model.",
        "Operation\nThe fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions that is called a program. The instructions to be executed are kept in some kind of computer memory. Nearly all CPUs follow the fetch, decode and execute steps in their operation, which are collectively known as the instruction cycle.",
        "After the execution of an instruction, the entire process repeats, with the next instruction cycle normally fetching the next-in-sequence instruction because of the incremented value in the program counter. If a jump instruction was executed, the program counter will be modified to contain the address of the instruction that was jumped to and program execution continues normally. In more complex CPUs, multiple instructions can be fetched, decoded and executed simultaneously. This section describes what is generally referred to as the \"classic RISC pipeline\", which is quite common among the simple CPUs used in many electronic devices (often called microcontrollers). It largely ignores the important role of CPU cache, and therefore the access stage of the pipeline.",
        "Some instructions manipulate the program counter rather than producing result data directly; such instructions are generally called \"jumps\" and facilitate program behavior like loops, conditional program execution (through the use of a conditional jump), and existence of functions. In some processors, some other instructions change the state of bits in a \"flags\" register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations. For example, in such processors a \"compare\" instruction evaluates two values and sets or clears bits in the flags register to indicate which one is greater or whether they are equal; one of these flags could then be used by a later jump instruction to determine program flow.",
        "Fetch",
        "Fetch involves retrieving an instruction (which is represented by a number or sequence of numbers) from program memory. The instruction's location (address) in program memory is determined by the program counter (PC; called the \"instruction pointer\" in Intel x86 microprocessors), which stores a number that identifies the address of the next instruction to be fetched. After an instruction is fetched, the PC is incremented by the length of the instruction so that it will contain the address of the next instruction in the sequence. Often, the instruction to be fetched must be retrieved from relatively slow memory, causing the CPU to stall while waiting for the instruction to be returned. This issue is largely addressed in modern processors by caches and pipeline architectures (see below).",
        "Decode\n\nThe instruction that the CPU fetches from memory determines what the CPU will do. In the decode step, performed by binary decoder circuitry known as the instruction decoder, the instruction is converted into signals that control other parts of the CPU.",
        "The way in which the instruction is interpreted is defined by the CPU's instruction set architecture (ISA). Often, one group of bits (that is, a \"field\") within the instruction, called the opcode, indicates which operation is to be performed, while the remaining fields usually provide supplemental information required for the operation, such as the operands. Those operands may be specified as a constant value (called an immediate value), or as the location of a value that may be a processor register or a memory address, as determined by some addressing mode.",
        "In some CPU designs the instruction decoder is implemented as a hardwired, unchangeable binary decoder circuit. In others, a microprogram is used to translate instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses. In some cases the memory that stores the microprogram is rewritable, making it possible to change the way in which the CPU decodes instructions.",
        "Execute\nAfter the fetch and decode steps, the execute step is performed. Depending on the CPU architecture, this may consist of a single action or a sequence of actions. During each action, control signals electrically enable or disable various parts of the CPU so they can perform all or part of the desired operation. The action is then completed, typically in response to a clock pulse. Very often the results are written to an internal CPU register for quick access by subsequent instructions. In other cases results may be written to slower, but less expensive and higher capacity main memory.",
        "For example, if an instruction that performs addition is to be executed, registers containing operands (numbers to be summed) are activated, as are the parts of the arithmetic logic unit (ALU) that perform addition. When the clock pulse occurs, the operands flow from the source registers into the ALU, and the sum appears at its output. On subsequent clock pulses, other components are enabled (and disabled) to move the output (the sum of the operation) to storage (e.g., a register or memory). If the resulting sum is too large (i.e., it is larger than the ALU's output word size), an arithmetic overflow flag will be set, influencing the next operation.\n\nStructure and implementation",
        "Hardwired into a CPU's circuitry is a set of basic operations it can perform, called an instruction set. Such operations may involve, for example, adding or subtracting two numbers, comparing two numbers, or jumping to a different part of a program. Each instruction is represented by a unique combination of bits, known as the machine language opcode. While processing an instruction, the CPU decodes the opcode (via a binary decoder) into control signals, which orchestrate the behavior of the CPU. A complete machine language instruction consists of an opcode and, in many cases, additional bits that specify arguments for the operation (for example, the numbers to be summed in the case of an addition operation). Going up the complexity scale, a machine language program is a collection of",
        "that specify arguments for the operation (for example, the numbers to be summed in the case of an addition operation). Going up the complexity scale, a machine language program is a collection of machine language instructions that the CPU executes.",
        "The actual mathematical operation for each instruction is performed by a combinational logic circuit within the CPU's processor known as the arithmetic–logic unit or ALU. In general, a CPU executes an instruction by fetching it from memory, using its ALU to perform an operation, and then storing the result to memory. Besides the instructions for integer mathematics and logic operations, various other machine instructions exist, such as those for loading data from memory and storing it back, branching operations, and mathematical operations on floating-point numbers performed by the CPU's floating-point unit (FPU).\n\nControl unit",
        "Control unit\n\nThe control unit (CU) is a component of the CPU that directs the operation of the processor. It tells the computer's memory, arithmetic and logic unit and input and output devices how to respond to the instructions that have been sent to the processor.\n\nIt directs the operation of the other units by providing timing and control signals. Most computer resources are managed by the CU. It directs the flow of data between the CPU and the other devices. John von Neumann included the control unit as part of the von Neumann architecture. In modern computer designs, the control unit is typically an internal part of the CPU with its overall role and operation unchanged since its introduction.\n\nArithmetic logic unit",
        "Arithmetic logic unit\n\nThe arithmetic logic unit (ALU) is a digital circuit within the processor that performs integer arithmetic and bitwise logic operations. The inputs to the ALU are the data words to be operated on (called operands), status information from previous operations, and a code from the control unit indicating which operation to perform. Depending on the instruction being executed, the operands may come from internal CPU registers, external memory, or constants generated by the ALU itself.",
        "When all input signals have settled and propagated through the ALU circuitry, the result of the performed operation appears at the ALU's outputs. The result consists of both a data word, which may be stored in a register or memory, and status information that is typically stored in a special, internal CPU register reserved for this purpose.\n\nAddress generation unit",
        "Address generation unit\n\nThe address generation unit (AGU), sometimes also called the address computation unit (ACU), is an execution unit inside the CPU that calculates addresses used by the CPU to access main memory. By having address calculations handled by separate circuitry that operates in parallel with the rest of the CPU, the number of CPU cycles required for executing various machine instructions can be reduced, bringing performance improvements.",
        "While performing various operations, CPUs need to calculate memory addresses required for fetching data from the memory; for example, in-memory positions of array elements must be calculated before the CPU can fetch the data from actual memory locations. Those address-generation calculations involve different integer arithmetic operations, such as addition, subtraction, modulo operations, or bit shifts. Often, calculating a memory address involves more than one general-purpose machine instruction, which do not necessarily decode and execute quickly. By incorporating an AGU into a CPU design, together with introducing specialized instructions that use the AGU, various address-generation calculations can be offloaded from the rest of the CPU, and can often be executed quickly in a single",
        "together with introducing specialized instructions that use the AGU, various address-generation calculations can be offloaded from the rest of the CPU, and can often be executed quickly in a single CPU cycle.",
        "Capabilities of an AGU depend on a particular CPU and its architecture. Thus, some AGUs implement and expose more address-calculation operations, while some also include more advanced specialized instructions that can operate on multiple operands at a time. Some CPU architectures include multiple AGUs so more than one address-calculation operation can be executed simultaneously, which brings further performance improvements due to the superscalar nature of advanced CPU designs. For example, Intel incorporates multiple AGUs into its Sandy Bridge and Haswell microarchitectures, which increase bandwidth of the CPU memory subsystem by allowing multiple memory-access instructions to be executed in parallel.\n\nMemory management unit (MMU)",
        "Memory management unit (MMU)\n\nMany microprocessors (in smartphones and desktop, laptop, server computers) have a memory management unit, translating logical addresses into physical RAM addresses, providing memory protection and paging abilities, useful for virtual memory. Simpler processors, especially microcontrollers, usually don't include an MMU.",
        "Cache\nA CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. A cache is a smaller, faster memory, closer to a processor core, which stores copies of the data from frequently used main memory locations. Most CPUs have different independent caches, including instruction and data caches, where the data cache is usually organized as a hierarchy of more cache levels (L1, L2, L3, L4, etc.).",
        "All modern (fast) CPUs (with few specialized exceptions) have multiple levels of CPU caches. The first CPUs that used a cache had only one level of cache; unlike later level 1 caches, it was not split into L1d (for data) and L1i (for instructions). Almost all current CPUs with caches have a split L1 cache. They also have L2 caches and, for larger processors, L3 caches as well. The L2 cache is usually not split and acts as a common repository for the already split L1 cache. Every core of a multi-core processor has a dedicated L2 cache and is usually not shared between the cores. The L3 cache, and higher-level caches, are shared between the cores and are not split. An L4 cache is currently uncommon, and is generally on dynamic random-access memory (DRAM), rather than on static random-access",
        "higher-level caches, are shared between the cores and are not split. An L4 cache is currently uncommon, and is generally on dynamic random-access memory (DRAM), rather than on static random-access memory (SRAM), on a separate die or chip. That was also the case historically with L1, while bigger chips have allowed integration of it and generally all cache levels, with the possible exception of the last level. Each extra level of cache tends to be bigger and is optimized differently.",
        "Other types of caches exist (that are not counted towards the \"cache size\" of the most important caches mentioned above), such as the translation lookaside buffer (TLB) that is part of the memory management unit (MMU) that most CPUs have.\n\nCaches are generally sized in powers of two: 2, 8, 16 etc. KiB or MiB (for larger non-L1) sizes, although the IBM z13 has a 96 KiB L1 instruction cache.\n\nClock rate",
        "Caches are generally sized in powers of two: 2, 8, 16 etc. KiB or MiB (for larger non-L1) sizes, although the IBM z13 has a 96 KiB L1 instruction cache.\n\nClock rate\n\nMost CPUs are synchronous circuits, which means they employ a clock signal to pace their sequential operations. The clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave. The frequency of the clock pulses determines the rate at which a CPU executes instructions and, consequently, the faster the clock, the more instructions the CPU will execute each second.",
        "To ensure proper operation of the CPU, the clock period is longer than the maximum time needed for all signals to propagate (move) through the CPU. In setting the clock period to a value well above the worst-case propagation delay, it is possible to design the entire CPU and the way it moves data around the \"edges\" of the rising and falling clock signal. This has the advantage of simplifying the CPU significantly, both from a design perspective and a component-count perspective. However, it also carries the disadvantage that the entire CPU must wait on its slowest elements, even though some portions of it are much faster. This limitation has largely been compensated for by various methods of increasing CPU parallelism (see below).",
        "However, architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs. For example, a clock signal is subject to the delays of any other electrical signal. Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase (synchronized) throughout the entire unit. This has led many modern CPUs to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the CPU to malfunction. Another major issue, as clock rates increase dramatically, is the amount of heat that is dissipated by the CPU. The constantly changing clock causes many components to switch regardless of whether they are being used at that time. In general, a component that is switching uses more",
        "is dissipated by the CPU. The constantly changing clock causes many components to switch regardless of whether they are being used at that time. In general, a component that is switching uses more energy than an element in a static state. Therefore, as clock rate increases, so does energy consumption, causing the CPU to require more heat dissipation in the form of CPU cooling solutions.",
        "One method of dealing with the switching of unneeded components is called clock gating, which involves turning off the clock signal to unneeded components (effectively disabling them). However, this is often regarded as difficult to implement and therefore does not see common usage outside of very low-power designs. One notable recent CPU design that uses extensive clock gating is the IBM PowerPC-based Xenon used in the Xbox 360; reducing the power requirements of the Xbox 360.",
        "Clockless CPUs\nAnother method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether. While removing the global clock signal makes the design process considerably more complex in many ways, asynchronous (or clockless) designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs. While somewhat uncommon, entire asynchronous CPUs have been built without using a global clock signal. Two notable examples of this are the ARM compliant AMULET and the MIPS R3000 compatible MiniMIPS.",
        "Rather than totally removing the clock signal, some CPU designs allow certain portions of the device to be asynchronous, such as using asynchronous ALUs in conjunction with superscalar pipelining to achieve some arithmetic performance gains. While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts, it is evident that they do at least excel in simpler math operations. This, combined with their excellent power consumption and heat dissipation properties, makes them very suitable for embedded computers.\n\nVoltage regulator module",
        "Voltage regulator module\n\nMany modern CPUs have a die-integrated power managing module which regulates on-demand voltage supply to the CPU circuitry allowing it to keep balance between performance and power consumption.\n\nInteger range\nEvery CPU represents numerical values in a specific way. For example, some early digital computers represented numbers as familiar decimal (base 10) numeral system values, and others have employed more unusual representations such as ternary (base three). Nearly all modern CPUs represent numbers in binary form, with each digit being represented by some two-valued physical quantity such as a \"high\" or \"low\" voltage.",
        "Related to numeric representation is the size and precision of integer numbers that a CPU can represent. In the case of a binary CPU, this is measured by the number of bits (significant digits of a binary encoded integer) that the CPU can process in one operation, which is commonly called word size, bit width, data path width, integer precision, or integer size. A CPU's integer size determines the range of integer values on which it can it can directly operate. For example, an 8-bit CPU can directly manipulate integers represented by eight bits, which have a range of 256 (28) discrete integer values.",
        "Integer range can also affect the number of memory locations the CPU can directly address (an address is an integer value representing a specific memory location). For example, if a binary CPU uses 32 bits to represent a memory address then it can directly address 232 memory locations. To circumvent this limitation and for various other reasons, some CPUs use mechanisms (such as bank switching) that allow additional memory to be addressed.",
        "CPUs with larger word sizes require more circuitry and consequently are physically larger, cost more and consume more power (and therefore generate more heat). As a result, smaller 4- or 8-bit microcontrollers are commonly used in modern applications even though CPUs with much larger word sizes (such as 16, 32, 64, even 128-bit) are available. When higher performance is required, however, the benefits of a larger word size (larger data ranges and address spaces) may outweigh the disadvantages. A CPU can have internal data paths shorter than the word size to reduce size and cost. For example, even though the IBM System/360 instruction set architecture was a 32-bit instruction set, the System/360 Model 30 and Model 40 had 8-bit data paths in the arithmetic logical unit, so that a 32-bit add",
        "though the IBM System/360 instruction set architecture was a 32-bit instruction set, the System/360 Model 30 and Model 40 had 8-bit data paths in the arithmetic logical unit, so that a 32-bit add required four cycles, one for each 8 bits of the operands, and, even though the Motorola 68000 series instruction set was a 32-bit instruction set, the Motorola 68000 and Motorola 68010 had 16-bit data paths in the arithmetic logical unit, so that a 32-bit add required two cycles.",
        "To gain some of the advantages afforded by both lower and higher bit lengths, many instruction sets have different bit widths for integer and floating-point data, allowing CPUs implementing that instruction set to have different bit widths for different portions of the device. For example, the IBM System/360 instruction set was primarily 32 bit, but supported 64-bit floating-point values to facilitate greater accuracy and range in floating-point numbers. The System/360 Model 65 had an 8-bit adder for decimal and fixed-point binary arithmetic and a 60-bit adder for floating-point arithmetic. Many later CPU designs use similar mixed bit width, especially when the processor is meant for general-purpose use where a reasonable balance of integer and floating-point capability is required.",
        "Parallelism\n\nThe description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take. This type of CPU, usually referred to as subscalar, operates on and executes one instruction on one or two pieces of data at a time, that is less than one instruction per clock cycle ().",
        "This process gives rise to an inherent inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result, the subscalar CPU gets \"hung up\" on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased. This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach scalar performance (one instruction per clock cycle, ). However, the performance is nearly always subscalar (less than one",
        "can operate on only one instruction at a time, can only possibly reach scalar performance (one instruction per clock cycle, ). However, the performance is nearly always subscalar (less than one instruction per clock cycle, ).",
        "Attempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques:\n instruction-level parallelism (ILP), which seeks to increase the rate at which instructions are executed within a CPU (that is, to increase the use of on-die execution resources);\n task-level parallelism (TLP), which purposes to increase the number of threads or processes that a CPU can execute simultaneously.\n\nEach methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU's performance for an application.\n\nInstruction-level parallelism",
        "Instruction-level parallelism\n\nOne of the simplest methods for increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing. This is a technique known as instruction pipelining, and is used in almost all modern general-purpose CPUs. Pipelining allows multiple instruction to be executed at a time by breaking the execution pathway into discrete stages. This separation can be compared to an assembly line, in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired.",
        "Pipelining does, however, introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation; a condition often termed data dependency conflict. Therefore pipelined processors must check for these sorts of conditions and delay a portion of the pipeline if necessary. A pipelined processor can become very nearly scalar, inhibited only by pipeline stalls (an instruction spending more than one clock cycle in a stage).",
        "Improvements in instruction pipelining led to further decreases in the idle time of CPU components. Designs that are said to be superscalar include a long instruction pipeline and multiple identical execution units, such as load–store units, arithmetic–logic units, floating-point units and address generation units. In a superscalar pipeline, instructions are read and passed to a dispatcher, which decides whether or not the instructions can be executed in parallel (simultaneously). If so, they are dispatched to execution units, resulting in their simultaneous execution. In general, the number of instructions that a superscalar CPU will complete in a cycle is dependent on the number of instructions it is able to dispatch simultaneously to execution units.",
        "Most of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher. The dispatcher needs to be able to quickly determine whether instructions can be executed in parallel, as well as dispatch them in such a way as to keep as many execution units busy as possible. This requires that the instruction pipeline is filled as often as possible and requires significant amounts of CPU cache. It also makes hazard-avoiding techniques like branch prediction, speculative execution, register renaming, out-of-order execution and transactional memory crucial to maintaining high levels of performance. By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait",
        "high levels of performance. By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed. Speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes. Out-of-order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies. Also in case of single instruction stream, multiple data stream, a case when a lot of data from the same type has to be processed, modern processors can disable parts of the pipeline so that when a single instruction is executed many times, the CPU skips the fetch and",
        "when a lot of data from the same type has to be processed, modern processors can disable parts of the pipeline so that when a single instruction is executed many times, the CPU skips the fetch and decode phases and thus greatly increases performance on certain occasions, especially in highly monotonous program engines such as video creation software and photo processing.",
        "When a fraction of the CPU is superscalar, the part that is not suffers a performance penalty due to scheduling stalls. The Intel P5 Pentium had two superscalar ALUs which could accept one instruction per clock cycle each, but its FPU could not. Thus the P5 was integer superscalar but not floating point superscalar. Intel's successor to the P5 architecture, P6, added superscalar abilities to its floating-point features.",
        "Simple pipelining and superscalar design increase a CPU's ILP by allowing it to execute instructions at rates surpassing one instruction per clock cycle. Most modern CPU designs are at least somewhat superscalar, and nearly all general purpose CPUs designed in the last decade are superscalar. In later years some of the emphasis in designing high-ILP computers has been moved out of the CPU's hardware and into its software interface, or instruction set architecture (ISA). The strategy of the very long instruction word (VLIW) causes some ILP to become implied directly by the software, reducing the CPU’s work in boosting ILP and thereby reducing design complexity.\n\nTask-level parallelism",
        "Task-level parallelism\n\nAnother strategy of achieving performance is to execute multiple threads or processes in parallel. This area of research is known as parallel computing. In Flynn's taxonomy, this strategy is known as multiple instruction stream, multiple data stream (MIMD).",
        "One technology used for this purpose is multiprocessing (MP). The initial type of this technology is known as symmetric multiprocessing (SMP), where a small number of CPUs share a coherent view of their memory system. In this scheme, each CPU has additional hardware to maintain a constantly up-to-date view of memory. By avoiding stale views of memory, the CPUs can cooperate on the same program and programs can migrate from one CPU to another. To increase the number of cooperating CPUs beyond a handful, schemes such as non-uniform memory access (NUMA) and directory-based coherence protocols were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple",
        "were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors. When the processors and their interconnect are all implemented on a single chip, the technology is known as chip-level multiprocessing (CMP) and the single chip as a multi-core processor.",
        "It was later recognized that finer-grain parallelism existed with a single program. A single program might have several threads (or functions) that could be executed separately or in parallel. Some of the earliest examples of this technology implemented input/output processing such as direct memory access as a separate thread from the computation thread. A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel. This technology is known as multi-threading (MT). The approach is considered more cost-effective than multiprocessing, as only a small number of components within a CPU are replicated to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system",
        "than multiprocessing, as only a small number of components within a CPU are replicated to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system including the caches are shared among multiple threads. The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT. One type of MT that was implemented is known as temporal multithreading, where one thread is executed until it is stalled waiting for data to return from external memory. In this scheme, the CPU would then quickly context switch to another thread which is ready to run, the switch often done in one CPU clock cycle, such as the",
        "for data to return from external memory. In this scheme, the CPU would then quickly context switch to another thread which is ready to run, the switch often done in one CPU clock cycle, such as the UltraSPARC T1. Another type of MT is simultaneous multithreading, where instructions from multiple threads are executed in parallel within one CPU clock cycle.",
        "For several decades from the 1970s to early 2000s, the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining, caches, superscalar execution, out-of-order execution, etc. This trend culminated in large, power-hungry CPUs such as the Intel Pentium 4. By the early 2000s, CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques.",
        "CPU designers then borrowed ideas from commercial computing markets such as transaction processing, where the aggregate performance of multiple programs, also known as throughput computing, was more important than the performance of a single thread or process.\n\nThis reversal of emphasis is evidenced by the proliferation of dual and more core processor designs and notably, Intel's newer designs resembling its less superscalar P6 architecture. Late designs in several processor families exhibit CMP, including the x86-64 Opteron and Athlon 64 X2, the SPARC UltraSPARC T1, IBM POWER4 and POWER5, as well as several video game console CPUs like the Xbox 360's triple-core PowerPC design, and the PlayStation 3's 7-core Cell microprocessor.\n\nData parallelism",
        "A less common but increasingly important paradigm of processors (and indeed, computing in general) deals with data parallelism. The processors discussed earlier are all referred to as some type of scalar device. As the name implies, vector processors deal with multiple pieces of data in the context of one instruction. This contrasts with scalar processors, which deal with one piece of data for every instruction. Using Flynn's taxonomy, these two schemes of dealing with data are generally referred to as single instruction stream, multiple data stream (SIMD) and single instruction stream, single data stream (SISD), respectively. The great utility in creating processors that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a",
        "data stream (SISD), respectively. The great utility in creating processors that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a dot product) to be performed on a large set of data. Some classic examples of these types of tasks include multimedia applications (images, video and sound), as well as many types of scientific and engineering tasks. Whereas a scalar processor must complete the entire process of fetching, decoding and executing each instruction and value in a set of data, a vector processor can perform a single operation on a comparatively large set of data with one instruction. This is only possible when the application tends to require many steps which apply one operation to a large set of data.",
        "Most early vector processors, such as the Cray-1, were associated almost exclusively with scientific research and cryptography applications. However, as multimedia has largely shifted to digital media, the need for some form of SIMD in general-purpose processors has become significant. Shortly after inclusion of floating-point units started to become commonplace in general-purpose processors, specifications for and implementations of SIMD execution units also began to appear for general-purpose processors. Some of these early SIMD specifications – like HP's Multimedia Acceleration eXtensions (MAX) and Intel's MMX – were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with",
        "(MAX) and Intel's MMX – were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with floating-point numbers. Progressively, developers refined and remade these early designs into some of the common modern SIMD specifications, which are usually associated with one instruction set architecture (ISA). Some notable modern examples include Intel's Streaming SIMD Extensions (SSE) and the PowerPC-related AltiVec (also known as VMX).",
        "Hardware performance counter\n\nMany modern architectures (including embedded ones) often include hardware performance counters (HPC), which enables low-level (instruction-level) collection, benchmarking, debugging or analysis of running software metrics. HPC may also be used to discover and analyze unusual or suspicious activity of the software, such as return-oriented programming (ROP) or sigreturn-oriented programming (SROP) exploits etc. This is usually done by software-security teams to assess and find malicious binary programs.",
        "Many major vendors (such as IBM, Intel, AMD, and Arm etc.) provide software interfaces (usually written in C/C++) that can be used to collected data from CPUs registers in order to get metrics. Operating system vendors also provide software like perf (Linux) to record, benchmark, or trace CPU events running kernels and applications.\n\nVirtual CPUs\n\nCloud computing can involve subdividing CPU operation into virtual central processing units (vCPUs).",
        "Virtual CPUs\n\nCloud computing can involve subdividing CPU operation into virtual central processing units (vCPUs).\n\nA host is the virtual equivalent of a physical machine, on which a virtual system is operating. When there are several physical machines operating in tandem and managed as a whole, the grouped computing and memory resources form a cluster. In some systems, it is possible to dynamically add and remove from a cluster. Resources available at a host and cluster level can be partitioned into resources pools with fine granularity.\n\nPerformance",
        "The performance or speed of a processor depends on, among many other factors, the clock rate (generally given in multiples of hertz) and the instructions per clock (IPC), which together are the factors for the instructions per second (IPS) that the CPU can perform.",
        "Many reported IPS values have represented \"peak\" execution rates on artificial instruction sequences with few branches, whereas realistic workloads consist of a mix of instructions and applications, some of which take longer to execute than others. The performance of the memory hierarchy also greatly affects processor performance, an issue barely considered in IPS calculations. Because of these problems, various standardized tests, often called \"benchmarks\" for this purpose such as SPECinthave been developed to attempt to measure the real effective performance in commonly used applications.",
        "Processing performance of computers is increased by using multi-core processors, which essentially is plugging two or more individual processors (called cores in this sense) into one integrated circuit. Ideally, a dual core processor would be nearly twice as powerful as a single core processor. In practice, the performance gain is far smaller, only about 50%, due to imperfect software algorithms and implementation. Increasing the number of cores in a processor (i.e. dual-core, quad-core, etc.) increases the workload that can be handled. This means that the processor can now handle numerous asynchronous events, interrupts, etc. which can take a toll on the CPU when overwhelmed. These cores can be thought of as different floors in a processing plant, with each floor handling a different",
        "asynchronous events, interrupts, etc. which can take a toll on the CPU when overwhelmed. These cores can be thought of as different floors in a processing plant, with each floor handling a different task. Sometimes, these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information.",
        "Due to specific capabilities of modern CPUs, such as simultaneous multithreading and uncore, which involve sharing of actual CPU resources while aiming at increased utilization, monitoring performance levels and hardware use gradually became a more complex task. As a response, some CPUs implement additional hardware logic that monitors actual use of various parts of a CPU and provides various counters accessible to software; an example is Intel's Performance Counter Monitor technology.\n\nSee also",
        "See also\n\n Addressing mode\n AMD Accelerated Processing Unit\n Complex instruction set computer\n Computer bus\n Computer engineering\n CPU core voltage\n CPU socket\n Data processing unit\n Digital signal processor\n Graphics processing unit\n Comparison of instruction set architectures\n Protection ring\n Reduced instruction set computer\n Stream processing\n True Performance Index\n Tensor Processing Unit\n Wait state\n\nNotes\n\nReferences\n\nExternal links\n\n .\n 25 Microchips that shook the world – an article by the Institute of Electrical and Electronics Engineers.\n\n \nDigital electronics\nElectronic design\nElectronic design automation"
    ],
    [
        "Character encoding\nCharacter encoding is the process of assigning numbers to graphical characters, especially the written characters of human language, allowing them to be stored, transmitted, and transformed using digital computers. The numerical values that make up a character encoding are known as \"code points\" and collectively comprise a \"code space\", a \"code page\", or a \"character map\".",
        "Early character codes associated with the optical or electrical telegraph could only represent a subset of the characters used in written languages, sometimes restricted to upper case letters, numerals and some punctuation only. The low cost of digital representation of data in modern computer systems allows more elaborate character codes (such as Unicode) which represent most of the characters used in many written languages. Character encoding using internationally accepted standards permits worldwide interchange of text in electronic form.",
        "History",
        "The history of character codes illustrates the evolving need for machine-mediated character-based symbolic information over a distance, using once-novel electrical means. The earliest codes were based upon manual and hand-written encoding and cyphering systems, such as Bacon's cipher, Braille, international maritime signal flags, and the 4-digit encoding of Chinese characters for a Chinese telegraph code (Hans Schjellerup, 1869). With the adoption of electrical and electro-mechanical techniques these earliest codes were adapted to the new capabilities and limitations of the early machines. The earliest well-known electrically transmitted character code, Morse code, introduced in the 1840s, used a system of four \"symbols\" (short signal, long signal, short space, long space) to generate",
        "The earliest well-known electrically transmitted character code, Morse code, introduced in the 1840s, used a system of four \"symbols\" (short signal, long signal, short space, long space) to generate codes of variable length. Though some commercial use of Morse code was via machinery, it was often used as a manual code, generated by hand on a telegraph key and decipherable by ear, and persists in amateur radio and aeronautical use.  Most codes are of fixed per-character length or variable-length sequences of fixed-length codes (e.g. Unicode).",
        "Common examples of character encoding systems include Morse code, the Baudot code, the American Standard Code for Information Interchange (ASCII) and Unicode. Unicode, a well-defined and extensible encoding system, has supplanted most earlier character encodings, but the path of code development to the present is fairly well known.",
        "The Baudot code, a five-bit encoding, was created by Émile Baudot in 1870, patented in 1874, modified by Donald Murray in 1901, and standardized by CCITT as International Telegraph Alphabet No. 2 (ITA2) in 1930. The name baudot has been erroneously applied to ITA2 and its many variants. ITA2 suffered from many shortcomings and was often improved by many equipment manufacturers, sometimes creating compatibility issues. In 1959 the U.S. military defined its Fieldata code, a six-or seven-bit code, introduced by the U.S. Army Signal Corps. While Fieldata addressed many of the then-modern issues (e.g. letter and digit codes arranged for machine collation), it fell short of its goals and was short-lived. In 1963 the first ASCII code was released (X3.4-1963) by the ASCII committee (which",
        "(e.g. letter and digit codes arranged for machine collation), it fell short of its goals and was short-lived. In 1963 the first ASCII code was released (X3.4-1963) by the ASCII committee (which contained at least one member of the Fieldata committee, W. F. Leubbert), which addressed most of the shortcomings of Fieldata, using a simpler code. Many of the changes were subtle, such as collatable character sets within certain numeric ranges. ASCII63 was a success, widely adopted by industry, and with the follow-up issue of the 1967 ASCII code (which added lower-case letters and fixed some \"control code\" issues) ASCII67 was adopted fairly widely. ASCII67's American-centric nature was somewhat addressed in the European ECMA-6 standard.",
        "Herman Hollerith invented punch card data encoding in the late 19th century to analyze census data. Initially, each hole position represented a different data element, but later, numeric information was encoded by numbering the lower rows 0 to 9, with a punch in a column representing its row number. Later alphabetic data was encoded by allowing more than one punch per column. Electromechanical tabulating machines represented date internally by the timing of pulses relative to the motion of the cards through the machine. When IBM went to electronic processing, starting with the IBM 603 Electronic Multiplier, it used a variety of binary encoding schemes that were tied to the punch card code.",
        "IBM's Binary Coded Decimal (BCD) was a six-bit encoding scheme used by IBM as early as 1953 in its 702 and 704 computers, and in its later 7000 Series and 1400 series, as well as in associated peripherals. Since the punched card code then in use only allowed digits, upper-case English letters and a few special characters, six bits were sufficient. BCD extended existing simple four-bit numeric encoding to include alphabetic and special characters, mapping it easily to punch-card encoding which was already in widespread use. IBMs codes were used primarily with IBM equipment; other computer vendors of the era had their own character codes, often six-bit, but usually had the ability to read tapes produced on IBM equipment. BCD was the precursor of IBM's Extended Binary-Coded Decimal",
        "vendors of the era had their own character codes, often six-bit, but usually had the ability to read tapes produced on IBM equipment. BCD was the precursor of IBM's Extended Binary-Coded Decimal Interchange Code (usually abbreviated as EBCDIC), an eight-bit encoding scheme developed in 1963 for the IBM System/360 that featured a larger character set, including lower case letters.",
        "In trying to develop universally interchangeable character encodings, researchers in the 1980s faced the dilemma that, on the one hand, it seemed necessary to add more bits to accommodate additional characters, but on the other hand, for the users of the relatively small character set of the Latin alphabet (who still constituted the majority of computer users), those additional bits were a colossal waste of then-scarce and expensive computing resources (as they would always be zeroed out for such users).  In 1985, the average personal computer user's hard disk drive could store only about 10 megabytes, and it cost approximately US$250 on the wholesale market (and much higher if purchased separately at retail), so it was very important at the time to make every bit count.",
        "The compromise solution that was eventually found and  was to break the assumption (dating back to telegraph codes) that each character should always directly correspond to a particular sequence of bits. Instead, characters would first be mapped to a universal intermediate representation in the form of abstract numbers called code points. Code points would then be represented in a variety of ways and with various default numbers of bits per character (code units) depending on context. To encode code points higher than the length of the code unit, such as above 256 for eight-bit units, the solution was to implement variable-length encodings where an escape sequence would signal that subsequent bits should be parsed as a higher code point.",
        "Terminology \nInformally, the terms \"character encoding\", \"character map\", \"character set\" and \"code page\" are often used interchangeably. Historically, the same standard would specify a repertoire of characters and how they were to be encoded into a stream of code units — usually with a single character per code unit. However, due to the emergence of more sophisticated character encodings, the distinction between these terms has become important.",
        "A character is a minimal unit of text that has semantic value.\n A character set is a collection of elements used to represent text. For example, the Latin alphabet and Greek alphabet are both character sets.\n A coded character set is a character set mapped to set of unique numbers. For historical reasons, this is also often referred to as a code page.\n A character repertoire is the set of characters that can be represented by a particular coded character set. The repertoire may be closed, meaning that no additions are allowed without creating a new standard (as is the case with ASCII and most of the ISO-8859 series); or it may be open, allowing additions (as is the case with Unicode and to a limited extent Windows code pages).",
        "A code point is a value or position of a character in a coded character set.\n A code space is the range of numerical values spanned by a coded character set.\n A code unit is the minimum bit combination that can represent a character in a character encoding (in computer science terms, it is the word size of the character encoding). For example, common code units include 7-bit, 8-bit, 16-bit, and 32-bit. In some encodings, some characters are encoded using multiple code units; such an encoding is referred to as a variable-width encoding.",
        "Code pages\n\n\"Code page\" is a historical name for a coded character set.\n\nOriginally, a code page referred to a specific page number in the IBM standard character set manual, which would define a particular character encoding. Other vendors, including Microsoft, SAP, and Oracle Corporation, also published their own sets of code pages; the most well-known code page suites are \"Windows\" (based on Windows-1252) and \"IBM\"/\"DOS\" (based on code page 437).\n\nDespite no longer referring to specific page numbers in a standard, many character encodings are still referred to by their code page number; likewise, the term \"code page\" is often still used to refer to character encodings in general.",
        "The term \"code page\" is not used in Unix or Linux, where \"charmap\" is preferred, usually in the larger context of locales. IBM's Character Data Representation Architecture (CDRA) designates entities with coded character set identifiers (CCSIDs), each of which is variously called a \"charset\", \"character set\", \"code page\", or \"CHARMAP\".\n\nCode units\nThe code unit size is equivalent to the bit measurement for the particular encoding:\n A code unit in US-ASCII consists of 7 bits;\n A code unit in UTF-8, EBCDIC and GB 18030 consists of 8 bits;\n A code unit in UTF-16 consists of 16 bits;\n A code unit in UTF-32 consists of 32 bits.",
        "Code points\nA code point is represented by a sequence of code units. The mapping is defined by the encoding. Thus, the number of code units required to represent a code point depends on the encoding:\n UTF-8: code points map to a sequence of one, two, three or four code units.\n UTF-16: code units are twice as long as 8-bit code units. Therefore, any code point with a scalar value less than U+10000 is encoded with a single code unit. Code points with a value U+10000 or higher require two code units each. These pairs of code units have a unique term in UTF-16: \"Unicode surrogate pairs\".\n UTF-32: the 32-bit code unit is large enough that every code point is represented as a single code unit.",
        "UTF-32: the 32-bit code unit is large enough that every code point is represented as a single code unit.\n GB 18030: multiple code units per code point are common, because of the small code units. Code points are mapped to one, two, or four code units.",
        "Characters\n\nExactly what constitutes a character varies between character encodings.\n\nFor example, for letters with diacritics, there are two distinct approaches that can be taken to encode them: they can be encoded either as a single unified character (known as a precomposed character), or as separate characters that combine into a single glyph. The former simplifies the text handling system, but the latter allows any letter/diacritic combination to be used in text. Ligatures pose similar problems.",
        "Exactly how to handle glyph variants is a choice that must be made when constructing a particular character encoding. Some writing systems, such as Arabic and Hebrew, need to accommodate things like graphemes that are joined in different ways in different contexts, but represent the same semantic character.",
        "Unicode encoding model\nUnicode and its parallel standard, the ISO/IEC 10646 Universal Character Set, together constitute a unified standard for character encoding. Rather than mapping characters directly to bytes, Unicode separately defines a coded character set that maps characters to unique natural numbers (code points), how those code points are mapped to a series of fixed-size natural numbers (code units), and finally how those units are encoded as a stream of octets (bytes). The purpose of this decomposition is to establish a universal set of characters that can be encoded in a variety of ways. To describe this model precisely, Unicode uses its own set of terminology to describe its process:",
        "An abstract character repertoire (ACR) is the full set of abstract characters that a system supports. Unicode has an open repertoire, meaning that new characters will be added to the repertoire over time.\n\nA coded character set (CCS) is a function that maps characters to code points (each code point represents one character). For example, in a given repertoire, the capital letter \"A\" in the Latin alphabet might be represented by the code point 65, the character \"B\" by 66, and so on. Multiple coded character sets may share the same character repertoire; for example ISO/IEC 8859-1 and IBM code pages 037 and 500 all cover the same repertoire but map them to different code points.",
        "A character encoding form (CEF) is the mapping of code points to code units to facilitate storage in a system that represents numbers as bit sequences of fixed length (i.e. practically any computer system). For example, a system that stores numeric information in 16-bit units can only directly represent code points 0 to 65,535 in each unit, but larger code points (say, 65,536 to 1.4 million) could be represented by using multiple 16-bit units. This correspondence is defined by a CEF.",
        "A character encoding scheme (CES) is the mapping of code units to a sequence of octets to facilitate storage on an octet-based file system or transmission over an octet-based network. Simple character encoding schemes include UTF-8, UTF-16BE, UTF-32BE, UTF-16LE, and UTF-32LE; compound character encoding schemes, such as UTF-16, UTF-32 and ISO/IEC 2022, switch between several simple schemes by using a byte order mark or escape sequences; compressing schemes try to minimize the number of bytes used per code unit (such as SCSU and BOCU).",
        "Although UTF-32BE and UTF-32LE are simpler CESes, most systems working with Unicode use either UTF-8, which is backward compatible with fixed-length ASCII and maps Unicode code points to variable-length sequences of octets, or UTF-16BE, which is backward compatible with fixed-length UCS-2BE and maps Unicode code points to variable-length sequences of 16-bit words. See comparison of Unicode encodings for a detailed discussion.\n\nFinally, there may be a higher-level protocol which supplies additional information to select the particular variant of a Unicode character, particularly where there are regional variants that have been 'unified' in Unicode as the same character. An example is the XML attribute xml:lang.",
        "The Unicode model uses the term \"character map\" for other systems which directly assign a sequence of characters to a sequence of bytes, covering all of the CCS, CEF and CES layers.\n\nUnicode code points\nIn Unicode, a character can be referred to as 'U+' followed by its codepoint value in hexadecimal. The range of valid code points (the codespace) for the Unicode standard is U+0000 to U+10FFFF, inclusive, divided in 17 planes, identified by the numbers 0 to 16. Characters in the range U+0000 to U+FFFF are in plane 0, called the Basic Multilingual Plane (BMP). This plane contains most commonly-used characters. Characters in the range U+10000 to U+10FFFF in the other planes are called supplementary characters.\n\nThe following table shows examples of code point values:",
        "The following table shows examples of code point values:\n\nExample\nConsider a string of the letters \"ab̲c𐐀\"—that is, a string containing a Unicode combining character () as well a supplementary character (). This string has several Unicode representations which are logically equivalent, yet while each is suited to a diverse set of circumstances or range of requirements:\n Four composed characters:\n, , , \n Five graphemes:\n, , , , \n Five Unicode code points:\n, , , , \n Five UTF-32 code units (32-bit integer values):\n, , , , \n Six UTF-16 code units (16-bit integers)\n, , , , , \n Nine UTF-8 code units (8-bit values, or bytes)\n, , , , , , , ,",
        "Note in particular that 𐐀 is represented with either one 32-bit value (UTF-32), two 16-bit values (UTF-16), or four 8-bit values (UTF-8). Although each of those forms uses the same total number of bits (32) to represent the glyph, it is not obvious how the actual numeric byte values are related.\n\nTranscoding \nAs a result of having many character encoding methods in use (and the need for backward compatibility with archived data), many computer programs have been developed to translate data between character encoding schemes, a process known as transcoding. Some of these are cited below.",
        "Cross-platform:\n Web browsers – most modern web browsers feature automatic character encoding detection. On Firefox 3, for example, see the View/Character Encoding submenu.\n iconv – a program and standardized API to convert encodings\n luit – a program that converts encoding of input and output to programs running interactively\n International Components for Unicode – A set of C and Java libraries to perform charset conversion. uconv can be used from ICU4C.\n\nWindows:\n Encoding.Convert – .NET API\n MultiByteToWideChar/WideCharToMultiByte – to convert from ANSI to Unicode & Unicode to ANSI",
        "See also \n Percent-encoding\n Alt code\n Character encodings in HTML\n :Category:Character encoding – articles related to character encoding in general\n :Category:Character sets – articles detailing specific character encodings\n Hexadecimal representations\n Mojibake – character set mismap\n Mojikyō – a system (\"glyph set\") that includes over 100,000 Chinese character drawings, modern and ancient, popular and obscure\n Presentation layer\n TRON, part of the TRON project, is an encoding system that does not use Han Unification; instead, it uses \"control codes\" to switch between 16-bit \"planes\" of characters.\n Universal Character Set characters\n Charset sniffing – used in some applications when character encoding metadata is not available\n\nCommon character encodings",
        "ISO 646\n ASCII\n EBCDIC\n ISO 8859:\n ISO 8859-1 Western Europe\n ISO 8859-2 Western and Central Europe\n ISO 8859-3 Western Europe and South European (Turkish, Maltese plus Esperanto)\n ISO 8859-4 Western Europe and Baltic countries (Lithuania, Estonia, Latvia and Lapp)\n ISO 8859-5 Cyrillic alphabet\n ISO 8859-6 Arabic\n ISO 8859-7 Greek\n ISO 8859-8 Hebrew\n ISO 8859-9 Western Europe with amended Turkish character set\n ISO 8859-10 Western Europe with rationalised character set for Nordic languages, including complete Icelandic set\n ISO 8859-11 Thai\n ISO 8859-13 Baltic languages plus Polish\n ISO 8859-14 Celtic languages (Irish Gaelic, Scottish, Welsh)\n ISO 8859-15 Added the Euro sign and other rationalisations to ISO 8859-1",
        "ISO 8859-11 Thai\n ISO 8859-13 Baltic languages plus Polish\n ISO 8859-14 Celtic languages (Irish Gaelic, Scottish, Welsh)\n ISO 8859-15 Added the Euro sign and other rationalisations to ISO 8859-1\n ISO 8859-16 Central, Eastern and Southern European languages (Albanian, Bosnian, Croatian, Hungarian, Polish, Romanian, Serbian and Slovenian, but also French, German, Italian and Irish Gaelic)\n CP437, CP720, CP737, CP850, CP852, CP855, CP857, CP858, CP860, CP861, CP862, CP863, CP865, CP866, CP869, CP872\n MS-Windows character sets:\n Windows-1250 for Central European languages that use Latin script, (Polish, Czech, Slovak, Hungarian, Slovene, Serbian, Croatian, Bosnian, Romanian and Albanian)\n Windows-1251 for Cyrillic alphabets\n Windows-1252 for Western languages\n Windows-1253 for Greek",
        "Windows-1251 for Cyrillic alphabets\n Windows-1252 for Western languages\n Windows-1253 for Greek\n Windows-1254 for Turkish\n Windows-1255 for Hebrew\n Windows-1256 for Arabic\n Windows-1257 for Baltic languages\n Windows-1258 for Vietnamese\n Mac OS Roman\n KOI8-R, KOI8-U, KOI7\n MIK\n ISCII\n TSCII\n VISCII\n JIS X 0208 is a widely deployed standard for Japanese character encoding that has several encoding forms.\n Shift JIS (Microsoft Code page 932 is a dialect of Shift_JIS)\n EUC-JP\n ISO-2022-JP\n JIS X 0213 is an extended version of JIS X 0208.\n Shift_JIS-2004\n EUC-JIS-2004\n ISO-2022-JP-2004\n Chinese Guobiao\n GB 2312\n GBK (Microsoft Code page 936)\n GB 18030\n Taiwan Big5 (a more famous variant is Microsoft Code page 950)\n Hong Kong HKSCS\n Korean",
        "EUC-JIS-2004\n ISO-2022-JP-2004\n Chinese Guobiao\n GB 2312\n GBK (Microsoft Code page 936)\n GB 18030\n Taiwan Big5 (a more famous variant is Microsoft Code page 950)\n Hong Kong HKSCS\n Korean\n KS X 1001 is a Korean double-byte character encoding standard\n EUC-KR\n ISO-2022-KR\n Unicode (and subsets thereof, such as the 16-bit 'Basic Multilingual Plane')\n UTF-8\n UTF-16\n UTF-32\n ANSEL or ISO/IEC 6937",
        "References\n\nFurther reading\n\nExternal links \n\nCharacter sets registered by Internet Assigned Numbers Authority (IANA)\nCharacters and encodings, by Jukka Korpela\nUnicode Technical Report #17: Character Encoding Model\nDecimal, Hexadecimal Character Codes in HTML Unicode – Encoding converter\nThe Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky (Oct 10, 2003)\n\n \nEncoding"
    ],
    [
        "Character encodings in HTML\nWhile Hypertext Markup Language (HTML) has been in use since 1991, HTML 4.0 from December 1997 was the first standardized version where international characters were given reasonably complete treatment. When an HTML document includes special characters outside the range of seven-bit ASCII, two goals are worth considering: the information's integrity, and universal browser display.\n\nSpecifying the document's character encoding\nThere are two general ways to specify which character encoding is used in the document.",
        "Specifying the document's character encoding\nThere are two general ways to specify which character encoding is used in the document.\n\nFirst, the web server can include the character encoding or \"charset\" in the Hypertext Transfer Protocol (HTTP) Content-Type header, which would typically look like this:\nContent-Type: text/html; charset=utf-8\nThis method gives the HTTP server a convenient way to alter document's encoding according to content negotiation; certain HTTP server software can do it, for example Apache with the module mod_charset_lite.\n\nSecond, a declaration can be included within the document itself.\n\nFor HTML it is possible to include this information inside the head element near the top of the document:\n\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">",
        "For HTML it is possible to include this information inside the head element near the top of the document:\n\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n\nHTML5 also allows the following syntax to mean exactly the same:\n\n<meta charset=\"utf-8\">\n\nXHTML documents have a third option: to express the character encoding via XML declaration, as follows:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>",
        "XHTML documents have a third option: to express the character encoding via XML declaration, as follows:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\nWith this second approach, because the character encoding cannot be known until the declaration is parsed, there is a problem knowing which character encoding is used in the document up to and including the declaration itself. If the character encoding is an ASCII extension then the content up to and including the declaration itself should be pure ASCII and this will work correctly. For character encodings that are not ASCII extensions (i.e. not a superset of ASCII), such as UTF-16BE and UTF-16LE, a processor of HTML, such as a web browser, should be able to parse the declaration in some cases through the use of heuristics.",
        "Encoding detection algorithm \nAs of HTML5 the recommended charset is UTF-8. An \"encoding sniffing algorithm\" is defined in the specification to determine the character encoding of the document based on multiple sources of input, including:\n Explicit user instruction\n An explicit meta tag within the first 1024 bytes of the document\n A byte order mark (BOM) within the first three bytes of the document\n The HTTP Content-Type or other transport layer information\n Analysis of the document bytes looking for specific sequences or ranges of byte values, and other tentative detection mechanisms.",
        "Characters outside of the printable ASCII range (32 to 126) usually appear incorrectly. This presents few problems for English-speaking users, but other languages regularly—in some cases, always—require characters outside that range. In Chinese, Japanese, and Korean (CJK) language environments where there are several different multi-byte encodings in use, auto-detection is also often employed. Finally, browsers usually permit the user to override incorrect charset label manually as well.",
        "It is increasingly common for multilingual websites and websites in non-Western languages to use UTF-8, which allows use of the same encoding for all languages. UTF-16 or UTF-32, which can be used for all languages as well, are less widely used because they can be harder to handle in programming languages that assume a byte-oriented ASCII superset encoding, and they are less efficient for text with a high frequency of ASCII characters, which is usually the case for HTML documents.",
        "Successful viewing of a page is not necessarily an indication that its encoding is specified correctly. If the page's creator and reader are both assuming some platform-specific character encoding, and the server does not send any identifying information, then the reader will nonetheless see the page as the creator intended, but other readers on different platforms or with different native languages will not see the page as intended.",
        "Permitted encodings\nThe WHATWG Encoding Standard, referenced by recent HTML standards (the current WHATWG HTML Living Standard, as well as the formerly competing W3C HTML 5.0 and 5.1) specifies a list of encodings which browsers must support. The HTML standards forbid support of other encodings. The Encoding Standard further stipulates that new formats, new protocols (even when existing formats are used) and authors of new documents are required to use UTF-8 exclusively.\n\nBesides UTF-8, the following encodings are explicitly listed in the HTML standard itself, with reference to the Encoding Standard:\n\nThe following additional encodings are listed in the Encoding Standard, and support for them is therefore also required:",
        "The following additional encodings are listed in the Encoding Standard, and support for them is therefore also required:\n\nThe following encodings are listed as explicit examples of forbidden encodings:",
        "The following encodings are listed as explicit examples of forbidden encodings:\n\nThe standard also defines a \"replacement\" decoder, which maps all content labelled as certain encodings to the replacement character (�), refusing to process it at all. This is intended to prevent attacks (e.g. cross site scripting) which may exploit a difference between the client and server in what encodings are supported in order to mask malicious content. Although the same security concern applies to ISO-2022-JP and UTF-16, which also allow sequences of ASCII bytes to be interpreted differently, this approach was not seen as feasible for them since they are comparatively more frequently used in deployed content. The following encodings receive this treatment:\n\nCharacter references",
        "Character references\n\nIn addition to native character encodings, characters can also be encoded as character references, which can be numeric character references (decimal or hexadecimal) or character entity references. Character entity references are also sometimes referred to as named entities, or HTML entities for HTML. HTML's usage of character references derives from SGML.\n\nHTML character references\n\nA numeric character reference in HTML refers to a character by its Universal Character Set/Unicode code point, and uses the format\n\n&#nnnn;\nor\n&#xhhhh;",
        "HTML character references\n\nA numeric character reference in HTML refers to a character by its Universal Character Set/Unicode code point, and uses the format\n\n&#nnnn;\nor\n&#xhhhh;\n\nwhere nnnn is the code point in decimal form, and hhhh is the code point in hexadecimal form. The x must be lowercase in XML documents. The nnnn or hhhh may be any number of digits and may include leading zeros. The hhhh may mix uppercase and lowercase, though uppercase is the usual style.",
        "Not all web browsers or email clients used by receivers of HTML documents, or text editors used by authors of HTML documents, will be able to render all HTML characters. Most modern software is able to display most or all of the characters for the user's language, and will draw a box or other clear indicator for characters they cannot render.\n\nFor codes from 0 to 127, the original 7-bit ASCII standard set, most of these characters can be used without a character reference. Codes from 160 to 255 can all be created using character entity names. Only a few higher-numbered codes can be created using entity names, but all can be created by decimal number character reference.",
        "Character entity references can also have the format &name; where name is a case-sensitive alphanumeric string. For example, \"λ\" can also be encoded as &lambda; in an HTML document. The character entity references &lt;, &gt;, &quot; and &amp; are predefined in HTML and SGML, because <, >, \"  and & are already used to delimit markup. This notably did not include XML's &apos; (') entity prior to HTML5. For a list of all named HTML character entity references along with the versions in which they were introduced, see List of XML and HTML character entity references.",
        "Unnecessary use of HTML character references may significantly reduce HTML readability. If the character encoding for a web page is chosen appropriately, then HTML character references are usually only required for markup delimiting characters as mentioned above, and for a few special characters (or none at all if a native Unicode encoding like UTF-8 is used). Incorrect HTML entity escaping may also open up security vulnerabilities for injection attacks such as cross-site scripting. If HTML attributes are left unquoted, certain characters, most importantly whitespace, such as space and tab, must be escaped using entities. Other languages related to HTML have their own methods of escaping characters.",
        "XML character references\nUnlike traditional HTML with its large range of character entity references, in XML there are only five predefined character entity references. These are used to escape characters that are markup sensitive in certain contexts:\n\nAll other character entity references have to be defined before they can be used. For example, use of &eacute; (which gives é, Latin lower-case E with acute accent, U+00E9 in Unicode) in an XML document will generate an error unless the entity has already been defined. XML also requires that the x in hexadecimal numeric references be in lowercase: for example &#xA1b rather than &#XA1b. XHTML, which is an XML application, supports the HTML entity set, along with XML's predefined entities.",
        "See also \n Charset sniffing – used by many browsers when character encoding metadata is not available\n Unicode and HTML\n Language code\n List of XML and HTML character entity references\n\nReferences\n\nExternal links \n Online HTML entity encoder & decoder tool\n Character entity references in HTML4\n The Definitive Guide to Web Character Encoding\n HTML Entity Encoding chapter of Browser Security Handbook – more information about current browsers and their entity handling\n The Open Web Application Security Project's wiki article on cross-site scripting (XSS)\n\nHTML\nWorld Wide Web Consortium standards"
    ],
    [
        "Cipher\nIn cryptography, a cipher (or cypher) is an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure. An alternative, less common term is encipherment. To encipher or encode is to convert information into cipher or code. In common parlance, \"cipher\" is synonymous with \"code\", as they are both a set of steps that encrypt a message; however, the concepts are distinct in cryptography, especially classical cryptography.",
        "Codes generally substitute different length strings of characters in the output, while ciphers generally substitute the same number of characters as are input. A code maps one meaning with another. Words and phrases can be coded as letters or numbers. Codes typically have direct meaning from input to key. Codes primarily function to save time. Ciphers are algorithmic. The given input must follow the cipher's process to be solved. Ciphers are commonly used to encrypt written information.",
        "Codes operated by substituting according to a large codebook which linked a random string of characters or numbers to a word or phrase.  For example, \"UQJHSE\" could be the code for \"Proceed to the following coordinates.\" When using a cipher the original information is known as plaintext, and the encrypted form as ciphertext. The ciphertext message contains all the information of the plaintext message, but is not in a format readable by a human or computer without the proper mechanism to decrypt it.",
        "The operation of a cipher usually depends on a piece of auxiliary information, called a key (or, in traditional NSA parlance, a cryptovariable). The encrypting procedure is varied depending on the key, which changes the detailed operation of the algorithm. A key must be selected before using a cipher to encrypt a message. Without knowledge of the key, it should be extremely difficult, if not impossible, to decrypt the resulting ciphertext into readable plaintext.",
        "Most modern ciphers can be categorized in several ways\n By whether they work on blocks of symbols usually of a fixed size (block ciphers), or on a continuous stream of symbols (stream ciphers).\n By whether the same key is used for both encryption and decryption (symmetric key algorithms), or if a different key is used for each (asymmetric key algorithms). If the algorithm is symmetric, the key must be known to the recipient and sender and to no one else. If the algorithm is an asymmetric one, the enciphering key is different from, but closely related to, the deciphering key. If one key cannot be deduced from the other, the asymmetric key algorithm has the public/private key property and one of the keys may be made public without loss of confidentiality.",
        "Etymology\nOriginating from the Arabic word for zero صفر (sifr), the word \"cipher\" spread to Europe as part of the Arabic numeral system during the Middle Ages. The Roman numeral system lacked the concept of zero, and this limited advances in mathematics. In this transition, the word was adopted into Medieval Latin as cifra, and then into Middle French as cifre. This eventually led to the English word cipher (minority spelling cypher). One theory for how the term came to refer to encoding is that the concept of zero was confusing to Europeans, and so the term came to refer to a message or communication that was not easily understood.",
        "The term cipher was later also used to refer to any Arabic digit, or to calculation using them, so encoding text in the form of Arabic numerals is literally converting the text to \"ciphers\".\n\nVersus codes\n\nIn casual contexts, \"code\" and \"cipher\" can typically be used interchangeably, however, the technical usages of the words refer to different concepts. Codes contain meaning; words and phrases are assigned to numbers or symbols, creating a shorter message. \n\nAn example of this is the commercial telegraph code which was used to shorten long telegraph messages which resulted from entering into commercial contracts using exchanges of telegrams.",
        "An example of this is the commercial telegraph code which was used to shorten long telegraph messages which resulted from entering into commercial contracts using exchanges of telegrams.\n\nAnother example is given by whole word ciphers, which allow the user to replace an entire word with a symbol or character, much like the way written Japanese utilizes Kanji (meaning Chinese characters in Japanese) characters to supplement the native Japanese characters representing syllables. An example using English language with Kanji could be to replace \"The quick brown fox jumps over the lazy dog\" by \"The quick brown 狐 jumps 上 the lazy 犬\". Stenographers sometimes use specific symbols to abbreviate whole words.",
        "Ciphers, on the other hand, work at a lower level: the level of individual letters, small groups of letters, or, in modern schemes, individual bits and blocks of bits. Some systems used both codes and ciphers in one system, using superencipherment to increase the security. In some cases the terms codes and ciphers are used synonymously with substitution and transposition, respectively.\n\nHistorically, cryptography was split into a dichotomy of codes and ciphers, while coding had its own terminology analogous to that of ciphers: \"encoding, codetext, decoding\" and so on.",
        "Historically, cryptography was split into a dichotomy of codes and ciphers, while coding had its own terminology analogous to that of ciphers: \"encoding, codetext, decoding\" and so on.\n\nHowever, codes have a variety of drawbacks, including susceptibility to cryptanalysis and the difficulty of managing a cumbersome codebook. Because of this, codes have fallen into disuse in modern cryptography, and ciphers are the dominant technique.\n\nTypes\nThere are a variety of different types of encryption. Algorithms used earlier in the history of cryptography are substantially different from modern methods, and modern ciphers can be classified according to how they operate and whether they use one or two keys.\n\nHistorical",
        "Historical\n\nThe Caesar Cipher is one of the earliest known cryptographic systems. Julius Caesar used a cipher that shifts the letters in the alphabet in place by three and wrapping the remaining letters to the front to write to Marcus Tullius Cicero in approximately 50 BC.[11]\n\nHistorical pen and paper ciphers used in the past are sometimes known as classical ciphers. They include simple substitution ciphers (such as ROT13) and transposition ciphers (such as a Rail Fence Cipher). For example, \"GOOD DOG\" can be encrypted as \"PLLX XLP\" where \"L\" substitutes for \"O\", \"P\" for \"G\", and \"X\" for \"D\" in the message.  Transposition of the letters \"GOOD DOG\" can result in \"DGOGDOO\". These simple ciphers and examples are easy to crack, even without plaintext-ciphertext pairs.",
        "William Shakespeare often used the concept of ciphers in his writing to symbolize nothingness. In Shakespeare's Henry V, he relates one of the accounting methods that brought the Arabic Numeral system and zero to Europe, to the human imagination. The actors who perform this play were not at the battles of Henry V's reign, so they represent absence. In another sense, ciphers are important to people who work with numbers, but they do not hold value. Shakespeare used this concept to outline how those who counted and identified the dead from the battles used that information as a political weapon, furthering class biases and xenophobia.",
        "In the 1640s, the Parliamentarian commander, Edward Montagu, 2nd Earl of Manchester, developed ciphers to send coded messages to his allies during the English Civil War.\n\nSimple ciphers were replaced by polyalphabetic substitution ciphers (such as the Vigenère) which changed the substitution alphabet for every letter.  For example, \"GOOD DOG\" can be encrypted as \"PLSX TWF\" where \"L\", \"S\", and \"W\" substitute for \"O\".  With even a small amount of known or estimated plaintext, simple polyalphabetic substitution ciphers and letter transposition ciphers designed for pen and paper encryption are easy to crack. It is possible to create a secure pen and paper cipher based on a one-time pad though, but the usual disadvantages of one-time pads apply.",
        "During the early twentieth century, electro-mechanical machines were invented to do encryption and decryption using transposition, polyalphabetic substitution, and a kind of \"additive\" substitution.  In rotor machines, several rotor disks provided polyalphabetic substitution, while plug boards provided another substitution. Keys were easily changed by changing the rotor disks and the plugboard wires. Although these encryption methods were more complex than previous schemes and required machines to encrypt and decrypt, other machines such as the British Bombe were invented to crack these encryption methods.\n\nModern\nModern encryption methods can be divided by two criteria: by type of key used, and by type of input data.",
        "Modern\nModern encryption methods can be divided by two criteria: by type of key used, and by type of input data.\n\nBy type of key used ciphers are divided into:\n symmetric key algorithms (Private-key cryptography), where one same key is used for encryption and decryption, and\n asymmetric key algorithms (Public-key cryptography), where two different keys are used for encryption and decryption.",
        "In a symmetric key algorithm (e.g., DES and AES), the sender and receiver must have a shared key set up in advance and kept secret from all other parties; the sender uses this key for encryption, and the receiver uses the same key for decryption. The design of AES (Advanced Encryption System) was beneficial because it aimed to overcome the flaws in the design of the DES (Data encryption standard). AES's designer's claim that the common means of modern cipher cryptanalytic attacks are ineffective against AES due to its design structure.[12] \n\nCiphers can be distinguished into two types by the type of input data:\n block ciphers, which encrypt block of data of fixed size, and\n stream ciphers, which encrypt continuous streams of data.",
        "Ciphers can be distinguished into two types by the type of input data:\n block ciphers, which encrypt block of data of fixed size, and\n stream ciphers, which encrypt continuous streams of data.\n\nKey size and vulnerability\nIn a pure mathematical attack, (i.e., lacking any other information to help break a cipher) two factors above all count:",
        "Computational power available, i.e., the computing power which can be brought to bear on the problem. It is important to note that average performance/capacity of a single computer is not the only factor to consider. An adversary can use multiple computers at once, for instance, to increase the speed of exhaustive search for a key (i.e., \"brute force\" attack) substantially.\n Key size, i.e., the size of key used to encrypt a message. As the key size increases, so does the complexity of exhaustive search to the point where it becomes impractical to crack encryption directly.\nSince the desired effect is computational difficulty, in theory one would choose an algorithm and desired difficulty level, thus decide the key length accordingly.",
        "An example of this process can be found at Key Length which uses multiple reports to suggest that a symmetrical cipher with 128 bits, an asymmetric cipher with 3072 bit keys, and an elliptic curve cipher with 256 bits, all have similar difficulty at present.\n\nClaude Shannon proved, using information theory considerations, that any theoretically unbreakable cipher must have keys which are at least as long as the plaintext, and used only once: one-time pad.\n\nSee also\n Autokey cipher\n Cover-coding\n Encryption software\n List of ciphertexts\n Steganography\n Telegraph code\n\nNotes",
        "References\n Richard J. Aldrich, GCHQ: The Uncensored Story of Britain's Most Secret Intelligence Agency, HarperCollins July 2010.\n Helen Fouché Gaines, \"Cryptanalysis\", 1939, Dover. \n Ibrahim A. Al-Kadi, \"The origins of cryptology: The Arab contributions\", Cryptologia, 16(2) (April 1992) pp. 97–126.\n David Kahn, The Codebreakers - The Story of Secret Writing () (1967)\n David A. King, The ciphers of the monks - A forgotten number notation of the Middle Ages, Stuttgart: Franz Steiner, 2001 ()\n Abraham Sinkov, Elementary Cryptanalysis: A Mathematical Approach, Mathematical Association of America, 1966. \n William Stallings, ''Cryptography and Network Security, principles and practices, 4th Edition",
        "William Stallings, ''Cryptography and Network Security, principles and practices, 4th Edition\n \n \"Ciphers vs. Codes (Article) | Cryptography.\" Khan Academy, Khan Academy, https://www.khanacademy.org/computing/computer-science/cryptography/ciphers/a/ciphers-vs-codes. \n Caldwell, William Casey. \"Shakespeare's Henry V and the Ciphers of History.\" SEL Studies in English Literature, 1500-1900, vol. 61, no. 2, 2021, pp. 241–68. EBSCOhost, .\n Luciano, Dennis, and Gordon Prichett. \"Cryptology: From Caesar Ciphers to Public-Key Cryptosystems.\" The College Mathematics Journal, vol. 18, no. 1, 1987, pp. 2–17. JSTOR, https://doi.org/10.2307/2686311. Accessed 19 Feb. 2023.",
        "Ho Yean Li, et al. \"Heuristic Cryptanalysis of Classical and Modern Ciphers.\" 2005 13th IEEE International Conference on Networks Jointly Held with the 2005 IEEE 7th Malaysia International Conf on Communic, Networks, 2005. Jointly Held with the 2005 IEEE 7th Malaysia International Conference on Communication., 2005 13th IEEE International Conference on, Networks and Communications, vol. 2, Jan. 2005. EBSCOhost, .",
        "External links\n\n Kish cypher\n\nCryptography"
    ],
    [
        "Code\nIn communications and information processing, code is a system of rules to convert information—such as a letter, word, sound, image, or gesture—into another form, sometimes shortened or secret, for communication through a communication channel or storage in a storage medium. An early example is an invention of language, which enabled a person, through speech, to communicate what they thought, saw, heard, or felt to others. But speech limits the range of communication to the distance a voice can carry and limits the audience to those present when the speech is uttered. The invention of writing, which converted spoken language into visual symbols, extended the range of communication across space and time.",
        "The process of encoding converts information from a source into symbols for communication or storage. Decoding is the reverse process, converting code symbols back into a form that the recipient understands, such as English or/and Spanish.\n\nOne reason for coding is to enable communication in places where ordinary plain language, spoken or written, is difficult or impossible. For example, semaphore, where the configuration of flags held by a signaler or the arms of a semaphore tower encodes parts of the message, typically individual letters, and numbers. Another person standing a great distance away can interpret the flags and reproduce the words sent.\n\nTheory",
        "Theory \n\nIn information theory and computer science, a code is usually considered as an algorithm that uniquely represents symbols from some source alphabet, by encoded strings, which may be in some other target alphabet. An extension of the code for representing sequences of symbols over the source alphabet is obtained by concatenating the encoded strings.\n\nBefore giving a mathematically precise definition, this is a brief example. The mapping\n\nis a code, whose source alphabet is the set  and whose target alphabet is the set . Using the extension of the code, the encoded string 0011001 can be grouped into codewords as 0 011 0 01, and these in turn can be decoded to the sequence of source symbols acab.",
        "Using terms from formal language theory, the precise mathematical definition of this concept is as follows: let S and T be two finite sets, called the source and target alphabets, respectively. A code  is a total function mapping each symbol from S to a sequence of symbols over T. The extension  of , is a homomorphism of  into , which naturally maps each sequence of source symbols to a sequence of target symbols.\n\nVariable-length codes \n\nIn this section, we consider codes that encode each source (clear text) character by a code word from some dictionary, and concatenation of such code words give us an encoded string. Variable-length codes are especially useful when clear text characters have different probabilities; see also entropy encoding.",
        "A prefix code is a code with the \"prefix property\": there is no valid code word in the system that is a prefix (start) of any other valid code word in the set. Huffman coding is the most known algorithm for deriving prefix codes. Prefix codes are widely referred to as \"Huffman codes\" even when the code was not produced by a Huffman algorithm. Other examples of prefix codes are country calling codes, the country and publisher parts of ISBNs, and the Secondary Synchronization Codes used in the UMTS WCDMA 3G Wireless Standard.\n\nKraft's inequality characterizes the sets of codeword lengths that are possible in a prefix code. Virtually any uniquely decodable one-to-many code, not necessarily a prefix one, must satisfy Kraft's inequality.\n\nError-correcting codes",
        "Error-correcting codes \n\nCodes may also be used to represent data in a way more resistant to errors in transmission or storage. This so-called error-correcting code works by including carefully crafted redundancy with the stored (or transmitted) data. Examples include Hamming codes, Reed–Solomon, Reed–Muller, Walsh–Hadamard, Bose–Chaudhuri–Hochquenghem, Turbo, Golay, algebraic geometry codes, low-density parity-check codes, and space–time codes.\nError detecting codes can be optimised to detect burst errors, or random errors.\n\nExamples\n\nCodes in communication used for brevity \n\nA cable code replaces words (e.g. ship or invoice) with shorter words, allowing the same information to be sent with fewer characters, more quickly, and less expensively.",
        "Codes can be used for brevity. When telegraph messages were the state of the art in rapid long-distance communication, elaborate systems of commercial codes that encoded complete phrases into single mouths (commonly five-minute groups) were developed, so that telegraphers became conversant with such \"words\" as BYOXO (\"Are you trying to weasel out of our deal?\"), LIOUY (\"Why do you not answer my question?\"), BMULD (\"You're a skunk!\"), or AYYLU (\"Not clearly coded, repeat more clearly.\"). Code words were chosen for various reasons: length, pronounceability, etc. Meanings were chosen to fit perceived needs: commercial negotiations, military terms for military codes, diplomatic terms for diplomatic codes, any and all of the preceding for espionage codes. Codebooks and codebook publishers",
        "perceived needs: commercial negotiations, military terms for military codes, diplomatic terms for diplomatic codes, any and all of the preceding for espionage codes. Codebooks and codebook publishers proliferated, including one run as a front for the American Black Chamber run by Herbert Yardley between the First and Second World Wars. The purpose of most of these codes was to save on cable costs. The use of data coding for data compression predates the computer era; an early example is the telegraph Morse code where more-frequently used characters have shorter representations. Techniques such as Huffman coding are now used by computer-based algorithms to compress large data files into a more compact form for storage or transmission.",
        "Character encodings",
        "Character encodings are representations of textual data. A given character encoding may be associated with a specific character set (the collection of characters which it can represent), though some character sets have multiple character encodings and vice versa. Character encodings may be broadly grouped according to the number of bytes required to represent a single character: there are single-byte encodings, multibyte (also called wide) encodings, and variable-width (also called variable-length) encodings. The earliest character encodings were single-byte, the best-known example of which is ASCII. ASCII remains in use today, for example in HTTP headers. However, single-byte encodings cannot model character sets with more than 256 characters. Scripts that require large character sets",
        "is ASCII. ASCII remains in use today, for example in HTTP headers. However, single-byte encodings cannot model character sets with more than 256 characters. Scripts that require large character sets such as Chinese, Japanese and Korean must be represented with multibyte encodings. Early multibyte encodings were fixed-length, meaning that although each character was represented by more than one byte, all characters used the same number of bytes (\"word length\"), making them suitable for decoding with a lookup table. The final group, variable-width encodings, is a subset of multibyte encodings. These use more complex encoding and decoding logic to efficiently represent large character sets while keeping the representations of more commonly used characters shorter or maintaining backward",
        "These use more complex encoding and decoding logic to efficiently represent large character sets while keeping the representations of more commonly used characters shorter or maintaining backward compatibility properties. This group includes UTF-8, an encoding of the Unicode character set; UTF-8 is the most common encoding of text media on the Internet.",
        "Genetic code \n\nBiological organisms contain genetic material that is used to control their function and development. This is DNA, which contains units named genes from which messenger RNA is derived. This in turn produces proteins through a genetic code in which a series of triplets (codons) of four possible nucleotides can be translated into one of twenty possible amino acids. A sequence of codons results in a corresponding sequence of amino acids that form a protein molecule; a type of codon called a stop codon signals the end of the sequence.\n\nGödel code \nIn mathematics, a Gödel code was the basis for the proof of Gödel's incompleteness theorem. Here, the idea was to map mathematical notation to a natural number (using a Gödel numbering).",
        "Gödel code \nIn mathematics, a Gödel code was the basis for the proof of Gödel's incompleteness theorem. Here, the idea was to map mathematical notation to a natural number (using a Gödel numbering).\n\nOther \nThere are codes using colors, like traffic lights, the color code employed to mark the nominal value of the electrical resistors or that of the trashcans devoted to specific types of garbage (paper, glass, organic, etc.).\n\nIn marketing, coupon codes can be used for a financial discount or rebate when purchasing a product from a (usual internet) retailer.\n\nIn military environments, specific sounds with the cornet are used for different uses: to mark some moments of the day, to command the infantry on the battlefield, etc.",
        "In military environments, specific sounds with the cornet are used for different uses: to mark some moments of the day, to command the infantry on the battlefield, etc.\n\nCommunication systems for sensory impairments, such as sign language for deaf people and braille for blind people, are based on movement or tactile codes.\n\nMusical scores are the most common way to encode music.\n\nSpecific games have their own code systems to record the matches, e.g. chess notation.\n\nCryptography \nIn the history of cryptography, codes were once common for ensuring the confidentiality of communications, although ciphers are now used instead.",
        "Cryptography \nIn the history of cryptography, codes were once common for ensuring the confidentiality of communications, although ciphers are now used instead.\n\nSecret codes intended to obscure the real messages, ranging from serious (mainly espionage in military, diplomacy, business, etc.) to trivial (romance, games) can be any kind of imaginative encoding: flowers, game cards, clothes, fans, hats, melodies, birds, etc., in which the sole requirement is the pre-agreement on the meaning by both the sender and the receiver.",
        "Other examples \nOther examples of encoding include:\nEncoding (in cognition) - a basic perceptual process of interpreting incoming stimuli; technically speaking, it is a complex, multi-stage process of converting relatively objective sensory input (e.g., light, sound) into a subjectively meaningful experience.\nA content format - a specific encoding format for converting a specific type of data to information.\nText encoding uses a markup language to tag the structure and other features of a text to facilitate processing by computers. (See also Text Encoding Initiative.)\nSemantics encoding of formal language A informal language B is a method of representing all terms (e.g. programs or descriptions) of language A using language B.",
        "Semantics encoding of formal language A informal language B is a method of representing all terms (e.g. programs or descriptions) of language A using language B.\nData compression transforms a signal into a code optimized for transmission or storage, generally done with a codec.\nNeural encoding - the way in which information is represented in neurons.\nMemory encoding - the process of converting sensations into memories.\nTelevision encoding: NTSC, PAL and SECAM",
        "Other examples of decoding include:\n Decoding (computer science)\n Decoding methods, methods in communication theory for decoding codewords sent over a noisy channel\n Digital signal processing, the study of signals in a digital representation and the processing methods of these signals\n Digital-to-analog converter, the use of analog circuit for decoding operations\n Word decoding, the use of phonics to decipher print patterns and translate them into the sounds of language\n\nCodes and acronyms\nAcronyms and abbreviations can be considered codes, and in a sense, all languages and writing systems are codes for human thought.",
        "Codes and acronyms\nAcronyms and abbreviations can be considered codes, and in a sense, all languages and writing systems are codes for human thought.\n\nInternational Air Transport Association airport codes are three-letter codes used to designate airports and used for bag tags. Station codes are similarly used on railways but are usually national, so the same code can be used for different stations if they are in different countries.\n\nOccasionally, a code word achieves an independent existence (and meaning) while the original equivalent phrase is forgotten or at least no longer has the precise meaning attributed to the code word. For example, '30' was widely used in journalism to mean \"end of story\", and has been used in other contexts to signify \"the end\".\n\nSee also",
        "See also \n\n Asemic writing\n Cipher\n Code (semiotics)\n Equipment codes\n Quantum error correction\n Semiotics\n Universal language\n\nReferences\n\nFurther reading\n \n\n \nSignal processing"
    ],
    [
        "Computer data storage\nComputer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.\n\nThe central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally, the fast technologies are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".",
        "Even the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.",
        "Functionality",
        "Without a significant amount of memory, a computer would merely be able to perform fixed operations and immediately output the result. It would have to be reconfigured to change its behavior. This is acceptable for devices such as desk calculators, digital signal processors, and other specialized devices. Von Neumann machines differ in having a memory in which they store their operating instructions and data. Such computers are more versatile in that they do not need to have their hardware reconfigured for each new program, but can simply be reprogrammed with new in-memory instructions; they also tend to be simpler to design, in that a relatively simple processor may keep state between successive computations to build up complex procedural results. Most modern computers are von Neumann",
        "also tend to be simpler to design, in that a relatively simple processor may keep state between successive computations to build up complex procedural results. Most modern computers are von Neumann machines.",
        "Data organization and representation \nA modern digital computer represents data using the binary numeral system. Text, numbers, pictures, audio, and nearly any other form of information can be converted into a string of bits, or binary digits, each of which has a value of 0 or 1. The most common unit of storage is the byte, equal to 8 bits. A piece of information can be handled by any computer or device whose storage space is large enough to accommodate the binary representation of the piece of information, or simply data. For example, the complete works of Shakespeare, about 1250 pages in print, can be stored in about five megabytes (40 million bits) with one byte per character.",
        "Data are encoded by assigning a bit pattern to each character, digit, or multimedia object. Many standards exist for encoding (e.g. character encodings like ASCII, image encodings like JPEG, and video encodings like MPEG-4).",
        "By adding bits to each encoded unit, redundancy allows the computer to detect errors in coded data and correct them based on mathematical algorithms. Errors generally occur in low probabilities due to random bit value flipping, or \"physical bit fatigue\", loss of the physical bit in the storage of its ability to maintain a distinguishable value (0 or 1), or due to errors in inter or intra-computer communication. A random bit flip (e.g. due to random radiation) is typically corrected upon detection. A bit or a group of malfunctioning physical bits (the specific defective bit is not always known; group definition depends on the specific storage device) is typically automatically fenced out, taken out of use by the device, and replaced with another functioning equivalent group in the device,",
        "group definition depends on the specific storage device) is typically automatically fenced out, taken out of use by the device, and replaced with another functioning equivalent group in the device, where the corrected bit values are restored (if possible). The cyclic redundancy check (CRC) method is typically used in communications and storage for error detection. A detected error is then retried.",
        "Data compression methods allow in many cases (such as a database) to represent a string of bits by a shorter bit string (\"compress\") and reconstruct the original string (\"decompress\") when needed. This utilizes substantially less storage (tens of percent) for many types of data at the cost of more computation (compress and decompress when needed). Analysis of the trade-off between storage cost saving and costs of related computations and possible delays in data availability is done before deciding whether to keep certain data compressed or not.\n\nFor security reasons, certain types of data (e.g. credit card information) may be kept encrypted in storage to prevent the possibility of unauthorized information reconstruction from chunks of storage snapshots.\n\nHierarchy of storage",
        "Hierarchy of storage \n\nGenerally, the lower a storage is in the hierarchy, the lesser its bandwidth and the greater its access latency is from the CPU. This traditional division of storage to primary, secondary, tertiary, and off-line storage is also guided by cost per bit.\n\nIn contemporary usage, memory is usually semiconductor storage read-write random-access memory, typically DRAM (dynamic RAM) or other forms of fast but temporary storage. Storage consists of storage devices and their media not directly accessible by the CPU (secondary or tertiary storage), typically hard disk drives, optical disc drives, and other devices slower than RAM but non-volatile (retaining contents when powered down).",
        "Historically, memory has, depending on technology, been called central memory, core memory, core storage, drum, main memory, real storage, or internal memory. Meanwhile, slower persistent storage devices have been referred to as secondary storage, external memory, or auxiliary/peripheral storage.\n\nPrimary storage \n\nPrimary storage (also known as main memory, internal memory, or prime memory), often referred to simply as memory, is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in a uniform manner.",
        "Historically, early computers used delay lines, Williams tubes, or rotating magnetic drums as primary storage. By 1954, those unreliable methods were mostly replaced by magnetic-core memory. Core memory remained dominant until the 1970s, when advances in integrated circuit technology allowed semiconductor memory to become economically competitive.",
        "This led to modern random-access memory (RAM). It is small-sized, light, but quite expensive at the same time. The particular types of RAM used for primary storage are volatile, meaning that they lose the information when not powered. Besides storing opened programs, it serves as disk cache and write buffer to improve both reading and writing performance. Operating systems borrow RAM capacity for caching so long as it's not needed by running software. Spare memory can be utilized as RAM drive for temporary high-speed data storage.",
        "As shown in the diagram, traditionally there are two more sub-layers of the primary storage, besides main large-capacity RAM:\n Processor registers are located inside the processor. Each register typically holds a word of data (often 32 or 64 bits). CPU instructions instruct the arithmetic logic unit to perform various calculations or other operations on this data (or with the help of it). Registers are the fastest of all forms of computer data storage.",
        "Processor cache is an intermediate stage between ultra-fast registers and much slower main memory. It was introduced solely to improve the performance of computers. Most actively used information in the main memory is just duplicated in the cache memory, which is faster, but of much lesser capacity. On the other hand, main memory is much slower, but has a much greater storage capacity than processor registers. Multi-level hierarchical cache setup is also commonly used—primary cache being smallest, fastest and located inside the processor; secondary cache being somewhat larger and slower.",
        "Main memory is directly or indirectly connected to the central processing unit via a memory bus. It is actually two buses (not on the diagram): an address bus and a data bus. The CPU firstly sends a number through an address bus, a number called memory address, that indicates the desired location of data. Then it reads or writes the data in the memory cells using the data bus. Additionally, a memory management unit (MMU) is a small device between CPU and RAM recalculating the actual memory address, for example to provide an abstraction of virtual memory or other tasks.",
        "As the RAM types used for primary storage are volatile (uninitialized at start up), a computer containing only such storage would not have a source to read instructions from, in order to start the computer. Hence, non-volatile primary storage containing a small startup program (BIOS) is used to bootstrap the computer, that is, to read a larger program from non-volatile secondary storage to RAM and start to execute it. A non-volatile technology used for this purpose is called ROM, for read-only memory (the terminology may be somewhat confusing as most ROM types are also capable of random access).",
        "Many types of \"ROM\" are not literally read only, as updates to them are possible; however it is slow and memory must be erased in large portions before it can be re-written. Some embedded systems run programs directly from ROM (or similar), because such programs are rarely changed. Standard computers do not store non-rudimentary programs in ROM, and rather, use large capacities of secondary storage, which is non-volatile as well, and not as costly.\n\nRecently, primary storage and secondary storage in some uses refer to what was historically called, respectively, secondary storage and tertiary storage.\n\nSecondary storage",
        "Recently, primary storage and secondary storage in some uses refer to what was historically called, respectively, secondary storage and tertiary storage.\n\nSecondary storage \n\nSecondary storage (also known as external memory or auxiliary storage) differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfer the desired data to primary storage. Secondary storage is non-volatile (retaining data when its power is shut off). Modern computer systems typically have two orders of magnitude more secondary storage than primary storage because secondary storage is less expensive.",
        "In modern computers, hard disk drives (HDDs) or solid-state drives (SSDs) are usually used as secondary storage. The access time per byte for HDDs or SSDs is typically measured in milliseconds (thousandths of a second), while the access time per byte for primary storage is measured in nanoseconds (billionths of a second). Thus, secondary storage is significantly slower than primary storage. Rotating optical storage devices, such as CD and DVD drives, have even longer access times. Other examples of secondary storage technologies include USB flash drives, floppy disks, magnetic tape, paper tape, punched cards, and RAM disks.",
        "Once the disk read/write head on HDDs reaches the proper placement and the data, subsequent data on the track are very fast to access. To reduce the seek time and rotational latency, data are transferred to and from disks in large contiguous blocks. Sequential or block access on disks is orders of magnitude faster than random access, and many sophisticated paradigms have been developed to design efficient algorithms based on sequential and block access. Another way to reduce the I/O bottleneck is to use multiple disks in parallel to increase the bandwidth between primary and secondary memory.",
        "Secondary storage is often formatted according to a file system format, which provides the abstraction necessary to organize data into files and directories, while also providing metadata describing the owner of a certain file, the access time, the access permissions, and other information.\n\nMost computer operating systems use the concept of virtual memory, allowing the utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks (pages) to a swap file or page file on secondary storage, retrieving them later when needed. If a lot of pages are moved to slower secondary storage, the system performance is degraded.\n\nTertiary storage",
        "Tertiary storage \n\nTertiary storage or tertiary memory is a level below secondary storage. Typically, it involves a robotic mechanism which will mount (insert) and dismount removable mass storage media into a storage device according to the system's demands; such data are often copied to secondary storage before use. It is primarily used for archiving rarely accessed information since it is much slower than secondary storage (e.g. 5–60 seconds vs. 1–10 milliseconds). This is primarily useful for extraordinarily large data stores, accessed without human operators. Typical examples include tape libraries and optical jukeboxes.",
        "When a computer needs to read information from the tertiary storage, it will first consult a catalog database to determine which tape or disc contains the information. Next, the computer will instruct a robotic arm to fetch the medium and place it in a drive. When the computer has finished reading the information, the robotic arm will return the medium to its place in the library.\n\nTertiary storage is also known as nearline storage because it is \"near to online\". The formal distinction between online, nearline, and offline storage is:",
        "Tertiary storage is also known as nearline storage because it is \"near to online\". The formal distinction between online, nearline, and offline storage is:\n\n Online storage is immediately available for I/O.\n Nearline storage is not immediately available, but can be made online quickly without human intervention.\n Offline storage is not immediately available, and requires some human intervention to become online.\n\nFor example, always-on spinning hard disk drives are online storage, while spinning drives that spin down automatically, such as in massive arrays of idle disks (MAID), are nearline storage. Removable media such as tape cartridges that can be automatically loaded, as in tape libraries, are nearline storage, while tape cartridges that must be manually loaded are offline storage.",
        "Off-line storage \n\nOff-line storage is computer data storage on a medium or a device that is not under the control of a processing unit. The medium is recorded, usually in a secondary or tertiary storage device, and then physically removed or disconnected. It must be inserted or connected by a human operator before a computer can access it again. Unlike tertiary storage, it cannot be accessed without human interaction.",
        "Off-line storage is used to transfer information since the detached medium can easily be physically transported. Additionally, it is useful for cases of disaster, where, for example, a fire destroys the original data, a medium in a remote location will be unaffected, enabling disaster recovery. Off-line storage increases general information security since it is physically inaccessible from a computer, and data confidentiality or integrity cannot be affected by computer-based attack techniques. Also, if the information stored for archival purposes is rarely accessed, off-line storage is less expensive than tertiary storage.",
        "In modern personal computers, most secondary and tertiary storage media are also used for off-line storage. Optical discs and flash memory devices are the most popular, and to a much lesser extent removable hard disk drives; older examples include floppy disks and Zip disks. In enterprise uses, magnetic tape cartridges are predominant; older examples include open-reel magnetic tape and punched cards.\n\nCharacteristics of storage",
        "Characteristics of storage \n\nStorage technologies at all levels of the storage hierarchy can be differentiated by evaluating certain core characteristics as well as measuring characteristics specific to a particular implementation. These core characteristics are volatility, mutability, accessibility, and addressability. For any particular implementation of any storage technology, the characteristics worth measuring are capacity and performance.",
        "Volatility \nNon-volatile memory retains the stored information even if not constantly supplied with electric power. It is suitable for long-term storage of information. Volatile memory requires constant power to maintain the stored information. The fastest memory technologies are volatile ones, although that is not a universal rule. Since the primary storage is required to be very fast, it predominantly uses volatile memory.",
        "Dynamic random-access memory is a form of volatile memory that also requires the stored information to be periodically reread and rewritten, or refreshed, otherwise it would vanish. Static random-access memory is a form of volatile memory similar to DRAM with the exception that it never needs to be refreshed as long as power is applied; it loses its content when the power supply is lost.\n\nAn uninterruptible power supply (UPS) can be used to give a computer a brief window of time to move information from primary volatile storage into non-volatile storage before the batteries are exhausted. Some systems, for example EMC Symmetrix, have integrated batteries that maintain volatile storage for several minutes.",
        "Mutability \n Read/write storage or mutable storage  Allows information to be overwritten at any time. A computer without some amount of read/write storage for primary storage purposes would be useless for many tasks. Modern computers typically use read/write storage also for secondary storage.\n Slow write, fast read storage  Read/write storage which allows information to be overwritten multiple times, but with the write operation being much slower than the read operation. Examples include CD-RW and SSD.\n Write once storage  Write once read many (WORM) allows the information to be written only once at some point after manufacture. Examples include semiconductor programmable read-only memory and CD-R.",
        "Write once storage  Write once read many (WORM) allows the information to be written only once at some point after manufacture. Examples include semiconductor programmable read-only memory and CD-R.\n Read only storage  Retains the information stored at the time of manufacture. Examples include mask ROM ICs and CD-ROM.",
        "Accessibility \n Random access Any location in storage can be accessed at any moment in approximately the same amount of time. Such characteristic is well suited for primary and secondary storage. Most semiconductor memories, flash memories and hard disk drives provide random access, though both semiconductor and flash memories have minimal latency when compared to hard disk drives, as no mechanical parts need to be moved.\n Sequential access The accessing of pieces of information will be in a serial order, one after the other; therefore the time to access a particular piece of information depends upon which piece of information was last accessed. Such characteristic is typical of off-line storage.",
        "Addressability \n Location-addressable  Each individually accessible unit of information in storage is selected with its numerical memory address. In modern computers, location-addressable storage usually limits to primary storage, accessed internally by computer programs, since location-addressability is very efficient, but burdensome for humans.\n File addressable Information is divided into files of variable length, and a particular file is selected with human-readable directory and file names. The underlying device is still location-addressable, but the operating system of a computer provides the file system abstraction to make the operation more understandable. In modern computers, secondary, tertiary and off-line storage use file systems.",
        "Content-addressable Each individually accessible unit of information is selected based on the basis of (part of) the contents stored there. Content-addressable storage can be implemented using software (computer program) or hardware (computer device), with hardware being faster but more expensive option. Hardware content addressable memory is often used in a computer's CPU cache.",
        "Capacity \n Raw capacity The total amount of stored information that a storage device or medium can hold. It is expressed as a quantity of bits or bytes (e.g. 10.4 megabytes).\n Memory storage density The compactness of stored information. It is the storage capacity of a medium divided with a unit of length, area or volume (e.g. 1.2 megabytes per square inch).",
        "Performance \n Latency The time it takes to access a particular location in storage. The relevant unit of measurement is typically nanosecond for primary storage, millisecond for secondary storage, and second for tertiary storage. It may make sense to separate read latency and write latency (especially for non-volatile memory) and in case of sequential access storage, minimum, maximum and average latency.\n Throughput The rate at which information can be read from or written to the storage. In computer data storage, throughput is usually expressed in terms of megabytes per second (MB/s), though bit rate may also be used. As with latency, read rate and write rate may need to be differentiated. Also accessing media sequentially, as opposed to randomly, typically yields maximum throughput.",
        "Granularity The size of the largest \"chunk\" of data that can be efficiently accessed as a single unit, e.g. without introducing additional latency.\nReliability The probability of spontaneous bit value change under various conditions, or overall failure rate.\nUtilities such as hdparm and sar can be used to measure IO performance in Linux.",
        "Energy use \n Storage devices that reduce fan usage automatically shut-down during inactivity, and low power hard drives can reduce energy consumption by 90 percent.\n 2.5-inch hard disk drives often consume less power than larger ones. Low capacity solid-state drives have no moving parts and consume less power than hard disks. Also, memory may use more power than hard disks. Large caches, which are used to avoid hitting the memory wall, may also consume a large amount of power.\n\nSecurity \nFull disk encryption, volume and virtual disk encryption, andor file/folder encryption is readily available for most storage devices.",
        "Security \nFull disk encryption, volume and virtual disk encryption, andor file/folder encryption is readily available for most storage devices.\n\nHardware memory encryption is available in Intel Architecture, supporting Total Memory Encryption (TME) and page granular memory encryption with multiple keys (MKTME). and in SPARC M7 generation since October 2015.\n\nVulnerability and reliability \n\nDistinct types of data storage have different points of failure and various methods of predictive failure analysis.\n\nVulnerabilities that can instantly lead to total loss are head crashing on mechanical hard drives and failure of electronic components on flash storage.\n\nError detection",
        "Vulnerabilities that can instantly lead to total loss are head crashing on mechanical hard drives and failure of electronic components on flash storage.\n\nError detection \n\nImpending failure on hard disk drives is estimable using S.M.A.R.T. diagnostic data that includes the hours of operation and the count of spin-ups, though its reliability is disputed.\n\nFlash storage may experience downspiking transfer rates as a result of accumulating errors, which the flash memory controller attempts to correct.\n\nThe health of optical media can be determined by measuring correctable minor errors, of which high counts signify deteriorating and/or low-quality media. Too many consecutive minor errors can lead to data corruption. Not all vendors and models of optical drives support error scanning.",
        "Storage media \n, the most commonly used data storage media are semiconductor, magnetic, and optical, while paper still sees some limited usage. Some other fundamental storage technologies, such as all-flash arrays (AFAs) are proposed for development.\n\nSemiconductor \n\nSemiconductor memory uses semiconductor-based integrated circuit (IC) chips to store information. Data are typically stored in metal–oxide–semiconductor (MOS) memory cells. A semiconductor memory chip may contain millions of memory cells, consisting of tiny MOS field-effect transistors (MOSFETs) and/or MOS capacitors. Both volatile and non-volatile forms of semiconductor memory exist, the former using standard MOSFETs and the latter using floating-gate MOSFETs.",
        "In modern computers, primary storage almost exclusively consists of dynamic volatile semiconductor random-access memory (RAM), particularly dynamic random-access memory (DRAM). Since the turn of the century, a type of non-volatile floating-gate semiconductor memory known as flash memory has steadily gained share as off-line storage for home computers. Non-volatile semiconductor memory is also used for secondary storage in various advanced electronic devices and specialized computers that are designed for them.\n\nAs early as 2006, notebook and desktop computer manufacturers started using flash-based solid-state drives (SSDs) as default configuration options for the secondary storage either in addition to or instead of the more traditional HDD.\n\nMagnetic",
        "Magnetic \n\nMagnetic storage uses different patterns of magnetization on a magnetically coated surface to store information. Magnetic storage is non-volatile. The information is accessed using one or more read/write heads which may contain one or more recording transducers. A read/write head only covers a part of the surface so that the head or medium or both must be moved relative to another in order to access data. In modern computers, magnetic storage will take these forms:\n\n Magnetic disk;\n Floppy disk, used for off-line storage;\n Hard disk drive, used for secondary storage.\n Magnetic tape, used for tertiary and off-line storage;\n Carousel memory (magnetic rolls).\n\nIn early computers, magnetic storage was also used as:",
        "In early computers, magnetic storage was also used as:\n\n Primary storage in a form of magnetic memory, or core memory, core rope memory, thin-film memory and/or twistor memory;\n Tertiary (e.g. NCR CRAM) or off line storage in the form of magnetic cards;\n Magnetic tape was then often used for secondary storage.\n\nMagnetic storage does not have a definite limit of rewriting cycles like flash storage and re-writeable optical media, as altering magnetic fields causes no physical wear. Rather, their life span is limited by mechanical parts.\n\nOptical",
        "Optical \n\nOptical storage, the typical optical disc, stores information in deformities on the surface of a circular disc and reads this information by illuminating the surface with a laser diode and observing the reflection. Optical disc storage is non-volatile. The deformities may be permanent (read only media), formed once (write once media) or reversible (recordable or read/write media). The following forms are in common use :",
        "CD, CD-ROM, DVD, BD-ROM: Read only storage, used for mass distribution of digital information (music, video, computer programs);\n CD-R, DVD-R, DVD+R, BD-R: Write once storage, used for tertiary and off-line storage;\n CD-RW, DVD-RW, DVD+RW, DVD-RAM, BD-RE: Slow write, fast read storage, used for tertiary and off-line storage;\n Ultra Density Optical or UDO is similar in capacity to BD-R or BD-RE and is slow write, fast read storage used for tertiary and off-line storage.",
        "Magneto-optical disc storage is optical disc storage where the magnetic state on a ferromagnetic surface stores information. The information is read optically and written by combining magnetic and optical methods. Magneto-optical disc storage is non-volatile, sequential access, slow write, fast read storage used for tertiary and off-line storage.\n\n3D optical data storage has also been proposed.\n\nLight induced magnetization melting in magnetic photoconductors has also been proposed for high-speed low-energy consumption magneto-optical storage.\n\nPaper",
        "Light induced magnetization melting in magnetic photoconductors has also been proposed for high-speed low-energy consumption magneto-optical storage.\n\nPaper \n\nPaper data storage, typically in the form of paper tape or punched cards, has long been used to store information for automatic processing, particularly before general-purpose computers existed. Information was recorded by punching holes into the paper or cardboard medium and was read mechanically (or later optically) to determine whether a particular location on the medium was solid or contained a hole. Barcodes make it possible for objects that are sold or transported to have some computer-readable information securely attached.",
        "Relatively small amounts of digital data (compared to other digital data storage) may be backed up on paper as a matrix barcode for very long-term storage, as the longevity of paper typically exceeds even magnetic data storage.\n\nOther storage media or substrates \n Vacuum-tube memory A Williams tube used a cathode-ray tube, and a Selectron tube used a large vacuum tube to store information. These primary storage devices were short-lived in the market, since the Williams tube was unreliable, and the Selectron tube was expensive.\n\n Electro-acoustic memory Delay-line memory used sound waves in a substance such as mercury to store information. Delay-line memory was dynamic volatile, cycle sequential read/write storage, and was used for primary storage.",
        "Optical tape is a medium for optical storage, generally consisting of a long and narrow strip of plastic, onto which patterns can be written and from which the patterns can be read back. It shares some technologies with cinema film stock and optical discs, but is compatible with neither. The motivation behind developing this technology was the possibility of far greater storage capacities than either magnetic tape or optical discs.",
        "Phase-change memory uses different mechanical phases of phase-change material to store information in an X–Y addressable matrix and reads the information by observing the varying electrical resistance of the material. Phase-change memory would be non-volatile, random-access read/write storage, and might be used for primary, secondary and off-line storage. Most rewritable and many write-once optical disks already use phase-change material to store information.",
        "Holographic data storage stores information optically inside crystals or photopolymers. Holographic storage can utilize the whole volume of the storage medium, unlike optical disc storage, which is limited to a small number of surface layers. Holographic storage would be non-volatile, sequential-access, and either write-once or read/write storage. It might be used for secondary and off-line storage. See Holographic Versatile Disc (HVD).\n\n Molecular memory stores information in polymer that can store electric charge. Molecular memory might be especially suited for primary storage. The theoretical storage capacity of molecular memory is 10 terabits per square inch (16 Gbit/mm2).\n\n Magnetic photoconductors store magnetic information, which can be modified by low-light illumination.",
        "Magnetic photoconductors store magnetic information, which can be modified by low-light illumination.\n\n DNA stores information in DNA nucleotides. It was first done in 2012, when researchers achieved a ratio of 1.28 petabytes per gram of DNA. In March 2017 scientists reported that a new algorithm called a DNA fountain achieved 85% of the theoretical limit, at 215 petabytes per gram of DNA.\n\nRelated technologies\n\nRedundancy \n\nWhile a group of bits malfunction may be resolved by error detection and correction mechanisms (see above), storage device malfunction requires different solutions. The following solutions are commonly used and valid for most storage devices:",
        "Device mirroring (replication) – A common solution to the problem is constantly maintaining an identical copy of device content on another device (typically of the same type). The downside is that this doubles the storage, and both devices (copies) need to be updated simultaneously with some overhead and possibly some delays. The upside is the possible concurrent reading of the same data group by two independent processes, which increases performance. When one of the replicated devices is detected to be defective, the other copy is still operational and is being utilized to generate a new copy on another device (usually available operational in a pool of stand-by devices for this purpose).",
        "Redundant array of independent disks (RAID) – This method generalizes the device mirroring above by allowing one device in a group of devices to fail and be replaced with the content restored (Device mirroring is RAID with n=2). RAID groups of n=5 or n=6 are common. n>2 saves storage, when compared with n=2, at the cost of more processing during both regular operation (with often reduced performance) and defective device replacement.",
        "Device mirroring and typical RAID are designed to handle a single device failure in the RAID group of devices. However, if a second failure occurs before the RAID group is completely repaired from the first failure, then data can be lost. The probability of a single failure is typically small. Thus the probability of two failures in the same RAID group in time proximity is much smaller (approximately the probability squared, i.e., multiplied by itself). If a database cannot tolerate even such a smaller probability of data loss, then the RAID group itself is replicated (mirrored). In many cases such mirroring is done geographically remotely, in a different storage array, to handle recovery from disasters (see disaster recovery above).",
        "Network connectivity \nA secondary or tertiary storage may connect to a computer utilizing computer networks. This concept does not pertain to the primary storage, which is shared between multiple processors to a lesser degree.",
        "Direct-attached storage (DAS) is a traditional mass storage, that does not use any network. This is still a most popular approach. This retronym was coined recently, together with NAS and SAN.\n Network-attached storage (NAS) is mass storage attached to a computer which another computer can access at file level over a local area network, a private wide area network, or in the case of online file storage, over the Internet. NAS is commonly associated with the NFS and CIFS/SMB protocols.",
        "Storage area network (SAN) is a specialized network, that provides other computers with storage capacity. The crucial difference between NAS and SAN, is that NAS presents and manages file systems to client computers, while SAN provides access at block-addressing (raw) level, leaving it to attaching systems to manage data or file systems within the provided capacity. SAN is commonly associated with Fibre Channel networks.",
        "Robotic storage \nLarge quantities of individual magnetic tapes, and optical or magneto-optical discs may be stored in robotic tertiary storage devices. In tape storage field they are known as tape libraries, and in optical storage field optical jukeboxes, or optical disk libraries per analogy. The smallest forms of either technology containing just one drive device are referred to as autoloaders or autochangers.",
        "Robotic-access storage devices may have a number of slots, each holding individual media, and usually one or more picking robots that traverse the slots and load media to built-in drives. The arrangement of the slots and picking devices affects performance. Important characteristics of such storage are possible expansion options: adding slots, modules, drives, robots. Tape libraries may have from 10 to more than 100,000 slots, and provide terabytes or petabytes of near-line information. Optical jukeboxes are somewhat smaller solutions, up to 1,000 slots.",
        "Robotic storage is used for backups, and for high-capacity archives in imaging, medical, and video industries. Hierarchical storage management is a most known archiving strategy of automatically migrating long-unused files from fast hard disk storage to libraries or jukeboxes. If the files are needed, they are retrieved back to disk.\n\nSee also\n\nPrimary storage topics \n Aperture (computer memory)\n Dynamic random-access memory (DRAM)\n Memory latency\n Mass storage\n Memory cell (disambiguation) \n Memory management\n Memory leak\n Virtual memory\n Memory protection\n Page address register\n Stable storage\n Static random-access memory (SRAM)",
        "Secondary, tertiary and off-line storage topics \n Cloud storage\n Hybrid cloud storage\n Data deduplication\n Data proliferation\n Data storage tag used for capturing research data\n Disk utility\n File system\n List of file formats\n Global filesystem\n Flash memory\n Geoplexing\n Information repository\n Noise-predictive maximum-likelihood detection\n Object(-based) storage\n Removable media\n Solid-state drive\n Spindle\n Virtual tape library\n Wait state\n Write buffer\n Write protection\n\nData storage conferences \n Storage Networking World\n Storage World Conference\n\nNotes\n\nReferences\n\nFurther reading \n \n Memory & storage, Computer history museum\n\n \nComputer architecture"
    ],
    [
        "Computer programming\nComputer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.",
        "Auxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code. While these are sometimes considered programming, often the term software development is used for this larger overall process – with the terms programming, implementation, and coding reserved for the writing and editing of code per se. Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process.\n\nHistory",
        "History\n\nProgrammable devices have existed for centuries. As early as the 9th century, a programmable music sequencer was invented by the Persian Banu Musa brothers, who described an automated mechanical flute player in the Book of Ingenious Devices. In 1206, the Arab engineer Al-Jazari invented a programmable drum machine where a musical mechanical automaton could be made to play different rhythms and drum patterns, via pegs and cams. In 1801, the Jacquard loom could produce entirely different weaves by changing the \"program\" – a series of pasteboard cards with holes punched in them.",
        "Code-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.\n\nThe first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine. However, Charles Babbage had already written his first program for the Analytical Engine in 1837.",
        "In the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form. Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers. However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.",
        "Machine language\nMachine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.\n\nCompiler languages",
        "Compiler languages\n\nHigh-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware. \nThe first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'. FORTRAN, the first widely used high-level language to have a functional implementation, came out in 1957, and many other languages were soon developed—in particular, COBOL aimed at commercial data processing, and Lisp for computer research.",
        "These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.\n\nSource code entry\n\nPrograms were mostly entered using punched cards or paper tape. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards.\n\nModern programming",
        "Modern programming\n\nQuality requirements\n\nWhatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:",
        "Reliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms and minimization of programming mistakes, such as mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off-by-one errors).\nRobustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of needed resources such as memory, operating system services, and network connections, user error, and unexpected power outages.",
        "Usability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical, and sometimes hardware elements that improve the clarity, intuitiveness, cohesiveness and completeness of a program's user interface.",
        "Portability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware and operating system, and availability of platform-specific compilers (and sometimes libraries) for the language of the source code.",
        "Maintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or to customize, fix bugs and security holes, or adapt it to new environments. Good practices during initial development make the difference in this regard. This quality may not be directly apparent to the end user but it can significantly affect the fate of a program over the long term.",
        "Efficiency/performance: Measure of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating memory leaks. This is often discussed under the shadow of a chosen programming language. Although the language certainly affects performance, even slower languages, such as Python, can execute programs instantly from a human perspective. Speed, resource usage, and performance are important for programs that bottleneck the system, but efficient use of programmer time is also important and is related to cost: more hardware may be cheaper.",
        "Readability of source code\nIn computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.\n\nReadability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.",
        "Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:\nDifferent indent styles (whitespace)\nComments\nDecomposition\nNaming conventions for objects (such as variables, classes, functions, procedures, etc.)\n\nThe presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.",
        "Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.",
        "Algorithmic complexity\nThe academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.",
        "Methodologies\nThe first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.",
        "Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.\n\nA similar technique used for database design is Entity-Relationship Modeling (ER Modeling).\n\nImplementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.",
        "Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.\n\nMeasuring language usage\nIt is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).",
        "Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use.  New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).\n\nDebugging",
        "Debugging\n\nDebugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.",
        "After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, when a bug in a compiler can make it crash when parsing some large source file, a simplification of the test case that results in only few lines from the original source file can be sufficient to reproduce the same crash. Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear. Scripting and breakpointing is also part of this process.",
        "Debugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.\n\nProgramming languages",
        "Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from \"low-level\" to \"high-level\"; \"low-level\" languages are typically more machine-oriented and faster to execute, whereas \"high-level\" languages are more abstract and",
        "form an approximate spectrum from \"low-level\" to \"high-level\"; \"low-level\" languages are typically more machine-oriented and faster to execute, whereas \"high-level\" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in \"high-level\" languages than in \"low-level\" ones.",
        "Programming languages are essential for software development. They are the building blocks for all software, from the simplest applications to the most sophisticated ones.",
        "Allen Downey, in his book How To Think Like A Computer Scientist, writes:\n\nThe details look different in different languages, but a few basic instructions appear in just about every language:\nInput: Gather data from the keyboard, a file, or some other device.\nOutput: Display data on the screen or send data to a file or other device.\nArithmetic: Perform basic arithmetical operations like addition and multiplication.\nConditional Execution: Check for certain conditions and execute the appropriate sequence of statements.\nRepetition: Perform some action repeatedly, usually with some variation.",
        "Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.\n\nProgrammers\n\nComputer programmers are those who write computer software. Their jobs usually involve:\n\nPrototyping\nCoding\nDebugging\nDocumentation\nIntegration\nMaintenance\nRequirements analysis\nSoftware architecture\nSoftware testing\nSpecification\nAlthough programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language.\n\nSee also",
        "See also\n\nACCU\nAssociation for Computing Machinery\nComputer networking\nHello world program\nInstitution of Analysts and Programmers\nNational Coding Week\nObject hierarchy\nProgramming best practices\nSystem programming\nComputer programming in the punched card era\nThe Art of Computer Programming\nWomen in computing\nTimeline of women in computing\n\nReferences\n\nSources",
        "References\n\nSources\n\nFurther reading\n A.K. Hartmann, Practical Guide to Computer Simulations, Singapore: World Scientific (2009)\n A. Hunt, D. Thomas, and W. Cunningham, The Pragmatic Programmer. From Journeyman to Master, Amsterdam: Addison-Wesley Longman (1999)\n Brian W. Kernighan, The Practice of Programming, Pearson (1999)\n Weinberg, Gerald M., The Psychology of Computer Programming, New York: Van Nostrand Reinhold (1971)\n Edsger W. Dijkstra, A Discipline of Programming, Prentice-Hall (1976)\n O.-J. Dahl, E.W.Dijkstra, C.A.R. Hoare, Structured Programming, Academic Press (1972)\n David Gries, The Science of Programming, Springer-Verlag (1981)\n\nExternal links\n\n \n\n \nProgramming"
    ],
    [
        "Computer science\nComputer science is the study of computation, information, and automation. Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). Though more often considered an academic discipline, computer science is closely related to computer programming.\n\nAlgorithms and data structures are central to computer science.",
        "The theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and for preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and",
        "interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.",
        "The fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.\n\nHistory\n\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.",
        "Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his",
        "Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885,",
        "notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics, and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point",
        "and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the  Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine, on which commands could be typed and the results printed automatically. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine,",
        "was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".",
        "During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental",
        "lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct",
        "The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.",
        "Etymology",
        "Although first proposed in 1956, the term \"computer science\" appears in a 1959 article in Communications of the ACM,\nin which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.",
        "His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur",
        "treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.",
        "In the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in",
        "(French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"",
        "A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology,",
        "of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.",
        "Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.",
        "The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.",
        "The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\n\nPhilosophy",
        "Epistemology of computer science",
        "Despite the word \"science\" in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975,  It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences",
        "that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.",
        "Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.",
        "Paradigms of computer science",
        "A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial",
        "in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).",
        "Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.",
        "Fields",
        "As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.",
        "CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.",
        "Theoretical computer science\n\nTheoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\n\nTheory of computation",
        "Theory of computation\n\nAccording to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\n\nThe famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.",
        "The famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.\n\nInformation and coding theory",
        "Information and coding theory\n\nInformation theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\n\nData structures and algorithms",
        "Data structures and algorithms\n\nData structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\n\nProgramming language theory and formal methods\n\nProgramming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.",
        "Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development",
        "errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.",
        "Applied computer science\n\nComputer graphics and visualization\n\nComputer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\n\nImage and sound processing",
        "Image and sound processing\n\nInformation can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier – whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.\n\nComputational science, finance and engineering",
        "Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.",
        "Social computing and human–computer interaction\n\nSocial computing is an area that is concerned with the intersection of social behavior and computational systems. Human–computer interaction research develops theories, principles, and guidelines for user interface designers.\n\nSoftware engineering",
        "Software engineering\n\nSoftware engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.\n\nArtificial intelligence",
        "Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan",
        "the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.",
        "Computer systems\n\nComputer architecture and organization\n\nComputer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term \"architecture\" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.\n\nConcurrent, parallel and distributed computing",
        "Concurrent, parallel and distributed computing\n\nConcurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.\n\nComputer networks\n\nThis branch of computer science aims to manage networks between computers worldwide.\n\nComputer security and cryptography",
        "Computer networks\n\nThis branch of computer science aims to manage networks between computers worldwide.\n\nComputer security and cryptography\n\nComputer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.\n\nHistorical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked. Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.",
        "Databases and data mining\n\nA database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.",
        "Discoveries\nThe philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:\n Gottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent \"anything\".\n All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as \"on/off\", \"magnetized/de-magnetized\", \"high-voltage/low-voltage\", etc.).",
        "Alan Turing's insight: there are only five actions that a computer has to perform in order to do \"anything\".\n Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:\n move left one location;\n move right one location;\n read symbol at current location;\n print 0 at current location;\n print 1 at current location.\n\n Corrado Böhm and Giuseppe Jacopini's insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do \"anything\".",
        "Corrado Böhm and Giuseppe Jacopini's insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do \"anything\".\n\n Only three rules are needed to combine any set of basic instructions into more complex ones:\nsequence: first do this, then do that;\n  selection: IF such-and-such is the case, THEN do this, ELSE do that;\n repetition: WHILE such-and-such is the case, DO this.\n The three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming).\n\nProgramming paradigms",
        "Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\n Functional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.\n Imperative programming, a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.",
        "Object-oriented programming, a programming paradigm based on the concept of \"objects\", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another.\n Service-oriented programming, a programming paradigm that uses \"services\" as the unit of computer work, to design and implement integrated business applications and mission critical software programs",
        "Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.\n\nResearch\n\nConferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.\n\nEducation",
        "Education\n\nComputer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students. In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11–16-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.",
        "In the US, with 14,000 school districts deciding the curriculum, provision was fractured. According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science. According to a 2021 report, only 51% of high schools in the US offer computer science.\n\nIsrael, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.\n\nSee also\n\n Glossary of computer science\n List of computer scientists\n List of computer science awards\n List of pioneers in computer science\n Outline of computer science\n\nNotes\n\nReferences\n\nFurther reading",
        "Glossary of computer science\n List of computer scientists\n List of computer science awards\n List of pioneers in computer science\n Outline of computer science\n\nNotes\n\nReferences\n\nFurther reading\n\n \n \n \n \n \n \n \n \n \n Peter J. Denning. Is computer science science?, Communications of the ACM, April 2005.\n Peter J. Denning, Great principles in computing curricula, Technical Symposium on Computer Science Education, 2004.\n\nExternal links\n\n DBLP Computer Science Bibliography\n Association for Computing Machinery\n Institute of Electrical and Electronics Engineers"
    ],
    [
        "Computing\nComputing is any goal-oriented activity requiring, benefiting from, or creating computing machinery. It includes the study and experimentation of algorithmic processes, and development of both hardware and software. Computing has scientific, engineering, mathematical, technological and social aspects. Major computing disciplines include computer engineering, computer science, cybersecurity, data science, information systems, information technology, digital art and software engineering.\n\nThe term computing is also synonymous with counting and calculating. In earlier times, it was used in reference to the action performed by mechanical computing machines, and before that, to human computers.\n\nHistory",
        "History \n\nThe history of computing is longer than the history of computing hardware and includes the history of methods intended for pen and paper (or for chalk and slate) with or without the aid of tables. Computing is intimately tied to the representation of numbers, though mathematical concepts necessary for computing existed before numeral systems. The earliest known tool for use in computation is the abacus, and it is thought to have been invented in Babylon circa between 2700–2300 BC. Abaci, of a more modern design, are still used as calculation tools today.",
        "The first recorded proposal for using digital electronics in computing was the 1931 paper \"The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena\" by C. E. Wynn-Williams. Claude Shannon's 1938 paper \"A Symbolic Analysis of Relay and Switching Circuits\" then introduced the idea of using electronics for Boolean algebraic operations.",
        "The concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947. In 1953, the University of Manchester built the first transistorized computer, the Manchester Baby. However, early junction transistors were relatively bulky devices that were difficult to mass-produce, which limited them to a number of specialised applications. The metal–oxide–silicon field-effect transistor (MOSFET, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959.  The MOSFET made it possible to build high-density integrated circuits, leading to what is known as the computer revolution or microcomputer",
        "by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959.  The MOSFET made it possible to build high-density integrated circuits, leading to what is known as the computer revolution or microcomputer revolution.",
        "Computer \n\nA computer is a machine that manipulates data according to a set of instructions called a computer program. The program has an executable form that the computer can use directly to execute the instructions. The same program in its human-readable source code form, enables a programmer to study and develop a sequence of steps known as an algorithm. Because the instructions can be carried out in different types of computers, a single set of source instructions converts to machine instructions according to the CPU type.",
        "The execution process carries out the instructions in a computer program. Instructions express the computations performed by the computer. They trigger sequences of simple actions on the executing machine. Those actions produce effects according to the semantics of the instructions.\n\nComputer hardware \n\nComputer hardware includes the physical parts of a computer, including central processing unit, memory and input/output. Computational logic and computer architecture are key topics in the field of computer hardware.\n\nComputer software",
        "Computer software \n\nComputer software, or just software, is a collection of computer programs and related data, which provides instructions to a computer. Software refers to one or more computer programs and data held in the storage of the computer. It is a set of programs, procedures, algorithms, as well as its documentation concerned with the operation of a data processing system. Program software performs the function of the program it implements, either by directly providing instructions to the computer hardware or by serving as input to another piece of software. The term was coined to contrast with the old term hardware (meaning physical devices). In contrast to hardware, software is intangible.",
        "Software is also sometimes used in a more narrow sense, meaning application software only.\n\nSystem software\n\nSystem software, or systems software, is computer software designed to operate and control computer hardware, and to provide a platform for running application software. System software includes operating systems, utility software, device drivers, window systems, and firmware. Frequently used development tools such as compilers, linkers, and debuggers are classified as system software. System software and middleware manage and integrate a computer's capabilities, but typically do not directly apply them in the performance of tasks that benefit the user, unlike application software.\n\nApplication software",
        "Application software \n\nApplication software, also known as an application or an app, is computer software designed to help the user perform specific tasks. Examples include enterprise software, accounting software, office suites, graphics software and media players. Many application programs deal principally with documents. Apps may be bundled with the computer and its system software, or may be published separately. Some users are satisfied with the bundled apps and need never install additional applications. The system software manages the hardware and serves the application, which in turn serves the user.",
        "Application software applies the power of a particular computing platform or system software to a particular purpose. Some apps, such as Microsoft Office, are developed in multiple versions for several different platforms; others have narrower requirements and are generally referred to by the platform they run on. For example, a geography application for Windows or an Android application for education or Linux gaming. Applications that run only on one platform and increase the desirability of that platform due to the popularity of the application, known as killer applications.\n\nComputer network",
        "Computer network \n\nA computer network, often simply referred to as a network, is a collection of hardware components and computers interconnected by communication channels that allow sharing of resources and information. When at least one process in one device is able to send or receive data to or from at least one process residing in a remote device, the two devices are said to be in a network. Networks may be classified according to a wide variety of characteristics such as the medium used to transport the data, communications protocol used, scale, topology, and organizational scope.",
        "Communications protocols define the rules and data formats for exchanging information in a computer network, and provide the basis for network programming. One well-known communications protocol is Ethernet, a hardware and link layer standard that is ubiquitous in local area networks. Another common protocol is the Internet Protocol Suite, which defines a set of protocols for internetworking, i.e. for data communication between multiple networks, host-to-host data transfer, and application-specific data transmission formats.\n\nComputer networking is sometimes considered a sub-discipline of electrical engineering, telecommunications, computer science, information technology or computer engineering, since it relies upon the theoretical and practical application of these disciplines.",
        "Internet \n\nThe Internet is a global system of interconnected computer networks that use the standard Internet Protocol Suite (TCP/IP) to serve billions of users. This includes millions of private, public, academic, business, and government networks, ranging in scope from local to global. These networks are linked by a broad array of electronic, wireless and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents of the World Wide Web and the infrastructure to support email.\n\nComputer programming",
        "Computer programming \n\nComputer programming is the process of writing, testing, debugging, and maintaining the source code and documentation of computer programs. This source code is written in a programming language, which is an artificial language that is often more restrictive than natural languages, but easily translated by the computer. Programming is used to invoke some desired behavior (customization) from the machine.",
        "Writing high-quality source code requires knowledge of both the computer science domain and the domain in which the application will be used. The highest-quality software is thus often developed by a team of domain experts, each a specialist in some area of development. However, the term programmer may apply to a range of program quality, from hacker to open source contributor to professional. It is also possible for a single programmer to do most or all of the computer programming needed to generate the proof of concept to launch a new killer application.\n\nComputer programmer",
        "Computer programmer \n\nA programmer, computer programmer, or coder is a person who writes computer software. The term computer programmer can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (C, C++, Java, Lisp, Python etc.) is often prefixed to the above titles, and those who work in a web environment often prefix their titles with Web. The term programmer can be used to refer to a software developer, software engineer, computer scientist, or software analyst. However, members of these professions typically possess other software engineering skills, beyond programming.",
        "Computer industry \n\nThe computer industry is made up of businesses involved in developing computer software, designing computer hardware and computer networking infrastructures, manufacturing computer components and providing information technology services, including system administration and maintenance.\n\nThe software industry includes businesses engaged in development, maintenance and publication of software. The industry also includes software services, such as training, documentation, and consulting.\n\nSub-disciplines of computing\n\nComputer engineering",
        "Sub-disciplines of computing\n\nComputer engineering \n\nComputer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration, rather than just software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering includes not only the design of hardware within its own domain, but also the interactions between hardware and the context in which it operates.",
        "Software engineering",
        "Software engineering (SE) is the application of a systematic, disciplined and quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches. That is, the application of engineering to software. It is the act of using insights to conceive, model and scale a solution to a problem. The first reference to the term is the 1968 NATO Software Engineering Conference, and was intended to provoke thought regarding the perceived software crisis at the time. Software development, a widely used and more generic term, does not necessarily subsume the engineering paradigm. The generally accepted concepts of Software Engineering as an engineering discipline have been specified in the Guide to the Software Engineering Body of Knowledge",
        "subsume the engineering paradigm. The generally accepted concepts of Software Engineering as an engineering discipline have been specified in the Guide to the Software Engineering Body of Knowledge (SWEBOK). The SWEBOK has become an internationally accepted standard in ISO/IEC TR 19759:2015.",
        "Computer science \n\nComputer science or computing science (abbreviated CS or Comp Sci) is the scientific and practical approach to computation and its applications. A computer scientist specializes in the theory of computation and the design of computational systems.",
        "Its subfields can be divided into practical techniques for its implementation and application in computer systems, and purely theoretical areas. Some, such as computational complexity theory, which studies fundamental properties of computational problems, are highly abstract, while others, such as computer graphics, emphasize real-world applications. Others focus on the challenges in implementing computations. For example, programming language theory studies approaches to the description of computations, while the study of computer programming investigates the use of programming languages and complex systems. The field of human–computer interaction focuses on the challenges in making computers and computations useful, usable, and universally accessible to humans.",
        "Cybersecurity \nThe field of cybersecurity pertains to the protection of computer systems and networks. This includes information and data privacy, preventing disruption of IT services and prevention of theft of and damage to hardware, software and data.\n\nData science \nData science is a field that uses scientific and computing tools to extract information and insights from data, driven by the increasing volume and availability of data. Data mining, big data, statistics and machine learning are all interwoven with data science.\n\nInformation systems",
        "Information systems \n\nInformation systems (IS) is the study of complementary networks of hardware and software (see information technology) that people and organizations use to collect, filter, process, create, and distribute data. The ACM's Computing Careers describes IS as: \n\nThe study of IS bridges business and computer science, using the theoretical foundations of information and computation to study various business models and related algorithmic processes within a computer science discipline.  The field of Computer Information Systems (CIS) studies computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society while IS emphasizes functionality over design.\n\nInformation technology",
        "Information technology \n\nInformation technology (IT) is the application of computers and telecommunications equipment to store, retrieve, transmit and manipulate data, often in the context of a business or other enterprise. The term is commonly used as a synonym for computers and computer networks, but also encompasses other information distribution technologies such as television and telephones. Several industries are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, e-commerce and computer services.\n\nResearch and emerging technologies",
        "Research and emerging technologies\n\nDNA-based computing and quantum computing are areas of active research for both computing hardware and software, such as the development of quantum algorithms. Potential infrastructure for future technologies includes DNA origami on photolithography and quantum antennae for transferring information between ion traps. By 2011, researchers had entangled 14 qubits. Fast digital circuits, including those based on Josephson junctions and rapid single flux quantum technology, are becoming more nearly realizable with the discovery of nanoscale superconductors.",
        "Fiber-optic and photonic (optical) devices, which already have been used to transport data over long distances, are starting to be used by data centers, along with CPU and semiconductor memory components. This allows the separation of RAM from CPU by optical interconnects. IBM has created an integrated circuit with both electronic and optical information processing in one chip. This is denoted CMOS-integrated nanophotonics (CINP). One benefit of optical interconnects is that motherboards, which formerly required a certain kind of system on a chip (SoC), can now move formerly dedicated memory and network controllers off the motherboards, spreading the controllers out onto the rack. This allows standardization of backplane interconnects and motherboards for multiple types of SoCs, which",
        "and network controllers off the motherboards, spreading the controllers out onto the rack. This allows standardization of backplane interconnects and motherboards for multiple types of SoCs, which allows more timely upgrades of CPUs.",
        "Another field of research is spintronics. Spintronics can provide computing power and storage, without heat buildup. Some research is being done on hybrid chips, which combine photonics and spintronics. There is also research ongoing on combining plasmonics, photonics, and electronics.",
        "Cloud computing \nCloud computing is a model that allows for the use of computing resources, such as servers or applications, without the need for interaction between the owner of these resources and the end user. It is typically offered as a service, making it an example of Software as a Service, Platforms as a Service, and Infrastructure as a Service, depending on the functionality offered. Key characteristics include on-demand access, broad network access, and the capability of rapid scaling. It allows individual users or small business to benefit from economies of scale.",
        "One area of interest in this field is its potential to support energy efficiency. Allowing thousands of instances of computation to occur on one single machine instead of thousands of individual machines could help save energy. It could also ease the transition to renewable energy source, since it would suffice to power one server farm with renewable energy, rather than millions of homes and offices. \n\nHowever, this centralized computing model poses several challenges, especially in security and privacy. Current legislation does not sufficiently protect users from companies mishandling their data on company servers. This suggests potential for further legislative regulations on cloud computing and tech companies.",
        "Quantum computing",
        "Quantum computing is an area of research that brings together the disciplines of computer science, information theory, and quantum physics. While the idea of information as part of physics is relatively new, there appears to be a strong tie between information theory and quantum mechanics. Whereas traditional computing operates on a binary system of ones and zeros, quantum computing uses qubits. Qubits are capable of being in a superposition, i.e. in both states of one and zero, simultaneously. Thus, the value of the qubit is not between 1 and 0, but changes depending on when it is measured. This trait of qubits is known as quantum entanglement, and is the core idea of quantum computing that allows quantum computers to do large scale computations. Quantum computing is often used for",
        "This trait of qubits is known as quantum entanglement, and is the core idea of quantum computing that allows quantum computers to do large scale computations. Quantum computing is often used for scientific research in cases where traditional computers do not have the computing power to do the necessary calculations, such in molecular modeling. Large molecules and their reactions are far too complex for traditional computers to calculate, but the computational power of quantum computers could provide a tool to perform such calculations.",
        "See also\n Artificial intelligence\n Computational thinking\n Creative computing\n Electronic data processing\n Enthusiast computing\n Index of history of computing articles\n Instruction set architecture\n Lehmer sieve\n List of computer term etymologies\n Mobile computing\n Scientific computing\n\nReferences\n\nExternal links\n\nFOLDOC: the Free On-Line Dictionary Of Computing"
    ],
    [
        "Conditional\nConditional (if then) may refer to:\n\nCausal conditional, if X then Y, where X is a cause of Y\nConditional probability, the probability of an event A given that another event B has occurred\nConditional proof, in logic: a proof that asserts a conditional, and proves that the antecedent leads to the consequent\nStrict conditional, in philosophy, logic, and mathematics\nMaterial conditional, in propositional calculus, or logical calculus in mathematics\nRelevance conditional, in relevance logic\nConditional (computer programming), a statement or expression in computer programming languages\nA conditional expression in computer programming languages such as ?:\nConditions in a contract",
        "Grammar and linguistics\nConditional mood (or conditional tense), a verb form in many languages\nConditional sentence, a sentence type used to refer to hypothetical situations and their consequences\nIndicative conditional, a conditional sentence expressing \"if A then B\" in a natural language\nCounterfactual conditional, a conditional sentence indicating what would be the case if its antecedent were true\n\nOther \n \"Conditional\" (Laura Mvula song)\n Conditional jockey, an apprentice jockey in British or Irish National Hunt racing\n Conditional short-circuit current\n Conditional Value-at-Risk\n\nSee also\nCondition (disambiguation)\nConditional statement (disambiguation)"
    ],
    [
        "Control character\nIn computing and telecommunication, a control character or non-printing character (NPC) is a code point in a character set that does not represent a written character or symbol. They are used as in-band signaling to cause effects other than the addition of a symbol to the text. All other characters are mainly graphic characters, also known as printing characters (or printable characters), except perhaps for \"space\" characters. In the ASCII standard there are 33 control characters, such as code 7, , which rings a terminal bell.\n\nHistory\n\nProcedural signs in Morse code are a form of control character.",
        "History\n\nProcedural signs in Morse code are a form of control character.\n\nA form of control characters were introduced in the 1870 Baudot code: NUL and DEL.\nThe 1901 Murray code added the carriage return (CR) and line feed (LF), and other versions of the Baudot code included other control characters.\n\nThe bell character (BEL), which rang a bell to alert operators, was also an early teletype control character.\n\nSome control characters have also been called \"format effectors\".\n\nIn ASCII",
        "There were quite a few control characters defined (33 in ASCII, and the ECMA-48 standard adds 32 more). This was because early terminals had very primitive mechanical or electrical controls that made any kind of state-remembering API quite expensive to implement, thus a different code for each and every function looked like a requirement. It quickly became possible and inexpensive to interpret sequences of codes to perform a function, and device makers found a way to send hundreds of device instructions. Specifically, they used ASCII code 2710 (escape), followed by a series of characters called a \"control sequence\" or \"escape sequence\". The mechanism was invented by Bob Bemer, the father of ASCII. For example, the sequence of code 2710, followed by the printable characters \"[2;10H\", would",
        "a \"control sequence\" or \"escape sequence\". The mechanism was invented by Bob Bemer, the father of ASCII. For example, the sequence of code 2710, followed by the printable characters \"[2;10H\", would cause a Digital Equipment Corporation VT100 terminal to move its cursor to the 10th cell of the 2nd line of the screen. Several standards exist for these sequences, notably ANSI X3.64. But the number of non-standard variations in use is large, especially among printers, where technology has advanced far faster than any standards body can possibly keep up with.",
        "All entries in the ASCII table below code 3210 (technically the C0 control code set) are of this kind, including CR and LF used to separate lines of text. The code 12710 (DEL) is also a control character. Extended ASCII sets defined by ISO 8859 added the codes 12810 through 15910 as control characters. This was primarily done so that if the high bit was stripped, it would not change a printing character to a C0 control code. This second set is called the C1 set.\n\nThese 65 control codes were carried over to Unicode. Unicode added more characters that could be considered controls, but it makes a distinction between these \"Formatting characters\" (such as the zero-width non-joiner) and the 65 control characters.",
        "The Extended Binary Coded Decimal Interchange Code (EBCDIC) character set contains 65 control codes, including all of the ASCII control codes plus additional codes which are mostly used to control IBM peripherals.",
        "The control characters in ASCII still in common use include:\n 0x00 (null, , , ), originally intended to be an ignored character, but now used by many programming languages including C to mark the end of a string.\n 0x07 (bell, , , ), which may cause the device to emit a warning such as a bell or beep sound or the screen flashing.\n 0x08 (backspace, , , ), may overprint the previous character.\n 0x09 (horizontal tab, , , ), moves the printing position right to the next tab stop.\n 0x0A (line feed, , , ), moves the print head down one line, or to the left edge and down. Used as the end of line marker in most UNIX systems and variants.\n 0x0B (vertical tab, , , ), vertical tabulation.",
        "0x0B (vertical tab, , , ), vertical tabulation.\n 0x0C (form feed, , , ), to cause a printer to eject paper to the top of the next page, or a video terminal to clear the screen.\n 0x0D (carriage return, , , ), moves the printing position to the start of the line, allowing overprinting. Used as the end of line marker in Classic Mac OS, OS-9, FLEX (and variants). A  pair is used by CP/M-80 and its derivatives including DOS and Windows, and by Application Layer protocols such as FTP, SMTP, and HTTP.\n 0x1A (Control-Z, , ). Acts as an end-of-file for the Windows text-mode file i/o.\n 0x1B (escape, ,  (GCC only), ). Introduces an escape sequence.",
        "Control characters may be described as doing something when the user inputs them, such as code 3 (End-of-Text character, ETX, ) to interrupt the running process, or code 4 (End-of-Transmission character, EOT, ), used to end text input on Unix or to exit a Unix shell. These uses usually have little to do with their use when they are in text being output.\n\nIn Unicode\n\nIn Unicode, \"Control-characters\" are U+0000—U+001F (C0 controls), U+007F (delete), and U+0080—U+009F (C1 controls). Their General Category is \"Cc\". Formatting codes are distinct, in General Category \"Cf\". The Cc control characters have no Name in Unicode, but are given labels such as \"<control-001A>\" instead.",
        "Display\nThere are a number of techniques to display non-printing characters, which may be illustrated with the bell character in ASCII encoding:\n Code point: decimal 7, hexadecimal 0x07\n An abbreviation, often three capital letters: BEL\n A special character condensing the abbreviation: Unicode U+2407 (␇), \"symbol for bell\"\n An ISO 2047 graphical representation: Unicode U+237E (⍾), \"graphic for bell\"\n Caret notation in ASCII, where code point 00xxxxx is represented as a caret followed by the capital letter at code point 10xxxxx: ^G\n An escape sequence, as in C/C++ character string codes: , , , etc.",
        "How control characters map to keyboards",
        "ASCII-based keyboards have a key labelled \"Control\", \"Ctrl\", or (rarely) \"Cntl\" which is used much like a shift key, being pressed in combination with another letter or symbol key. In one implementation, the control key generates the code 64 places below the code for the (generally) uppercase letter it is pressed in combination with (i.e., subtract 0x40 from ASCII code value of the (generally) uppercase letter). The other implementation is to take the ASCII code produced by the key and bitwise AND it with 0x1F, forcing bits 5 to 7 to zero. For example, pressing \"control\" and the letter \"g\" (which is 0110 0111 in binary), produces the code 7 (BELL, 7 in base ten, or 0000 0111 in binary). The NULL character (code 0) is represented by Ctrl-@, \"@\" being the code immediately before \"A\" in the",
        "is 0110 0111 in binary), produces the code 7 (BELL, 7 in base ten, or 0000 0111 in binary). The NULL character (code 0) is represented by Ctrl-@, \"@\" being the code immediately before \"A\" in the ASCII character set. For convenience, some terminals accept Ctrl-Space as an alias for Ctrl-@. In either case, this produces one of the 32 ASCII control codes between 0 and 31. Neither approach works to produce  the DEL character because of its special location in the table and its value (code 12710), Ctrl-? is sometimes used for this character.",
        "When the control key is held down, letter keys produce the same control characters regardless of the state of the shift or caps lock keys.  In other words, it does not matter whether the key would have produced an upper-case or a lower-case letter.  The interpretation of the control key with the space, graphics character, and digit keys (ASCII codes 32 to 63) vary between systems.  Some will produce the same character code as if the control key were not held down.  Other systems translate these keys into control characters when the control key is held down.  The interpretation of the control key with non-ASCII (\"foreign\") keys also varies between systems.",
        "Control characters are often rendered into a printable form known as caret notation by printing a caret (^) and then the ASCII character that has a value of the control character plus 64.  Control characters generated using letter keys are thus displayed with the upper-case form of the letter.  For example, ^G represents code 7, which is generated by pressing the G key when the control key is held down.\n\nKeyboards also typically have a few single keys which produce control character codes. For example, the key labelled \"Backspace\" typically produces code 8, \"Tab\" code 9, \"Enter\" or \"Return\" code 13 (though some keyboards might produce code 10 for \"Enter\").",
        "Many keyboards include keys that do not correspond to any ASCII printable or control character, for example cursor control arrows and word processing functions.  The associated keypresses are communicated to computer programs by one of four methods: appropriating otherwise unused control characters; using some encoding other than ASCII; using multi-character control sequences; or using an additional mechanism outside of generating characters. \"Dumb\" computer terminals typically use control sequences.  Keyboards attached to stand-alone personal computers made in the 1980s typically use one (or both) of the first two methods.  Modern computer keyboards generate scancodes that identify the specific physical keys that are pressed; computer software then determines how to handle the keys that",
        "both) of the first two methods.  Modern computer keyboards generate scancodes that identify the specific physical keys that are pressed; computer software then determines how to handle the keys that are pressed, including any of the four methods described above.",
        "The design purpose\n\nThe control characters were designed to fall into a few groups: printing and display control, data structuring, transmission control, and miscellaneous.",
        "Printing and display control\nPrinting control characters were first used to control the physical mechanism of printers, the earliest output device.  An early example of this idea was the use of Figures (FIGS) and Letters (LTRS) in Baudot code to shift between two code pages. A later, but still early, example was the out-of-band ASA carriage control characters.  Later, control characters were integrated into the stream of data to be printed.\nThe carriage return character (CR), when sent to such a device, causes it to put the character at the edge of the paper at which writing begins (it may, or may not, also move the printing position to the next line).",
        "The line feed character (LF/NL) causes the device to put the printing position on the next line. It may (or may not), depending on the device and its configuration, also move the printing position to the start of the next line (which would be the leftmost position for left-to-right scripts, such as the alphabets used for Western languages, and the rightmost position for right-to-left scripts such as the Hebrew and Arabic alphabets).\nThe vertical and horizontal tab characters (VT and HT/TAB) cause the output device to move the printing position to the next tab stop in the direction of reading.\nThe form feed character (FF/NP) starts a new sheet of paper, and may or may not move to the start of the first line.",
        "The form feed character (FF/NP) starts a new sheet of paper, and may or may not move to the start of the first line.\nThe backspace character (BS) moves the printing position one character space backwards. On printers, including hard-copy terminals, this is most often used so the printer can overprint characters to make other, not normally available, characters. On video terminals and other electronic output devices, there are often software (or hardware) configuration choices that allow a destructive backspace (e.g., a BS, SP, BS sequence), which erases, or a non-destructive one, which does not.\nThe shift in and shift out characters (SI and SO) selected alternate character sets, fonts, underlining, or other printing modes. Escape sequences were often used to do the same thing.",
        "With the advent of computer terminals that did not physically print on paper and so offered more flexibility regarding screen placement, erasure, and so forth, printing control codes were adapted. Form feeds, for example, usually cleared the screen, there being no new paper page to move to. More complex escape sequences were developed to take advantage of the flexibility of the new terminals, and indeed of newer printers. The concept of a control character had always been somewhat limiting, and was extremely so when used with new, much more flexible, hardware. Control sequences (sometimes implemented as escape sequences) could match the new flexibility and power and became the standard method. However, there were, and remain, a large variety of standard sequences to choose from.",
        "Data structuring\nThe separators (File, Group, Record, and Unit: FS, GS, RS and US) were made to structure data, usually on a tape, in order to simulate punched cards.\nEnd of medium (EM) warns that the tape (or other recording medium) is ending.\nWhile many systems use CR/LF and TAB for structuring data, it is possible to encounter the separator control characters in data that needs to be structured.  The separator control characters are not overloaded; there is no general use of them except to separate data into structured groupings.  Their numeric values are contiguous with the space character, which can be considered a member of the group, as a word separator.",
        "For example, the RS separator is used by  (JSON Text Sequences) to encode a sequence of JSON elements. Each sequence item starts with a RS character and ends with a line feed. This allows to serialize open-ended JSON sequences. It is one of the JSON streaming protocols.\n\nTransmission control\nThe transmission control characters were intended to structure a data stream, and to manage re-transmission or graceful failure, as needed, in the face of transmission errors.",
        "The start of heading (SOH) character was to mark a non-data section of a data stream—the part of a stream containing addresses and other housekeeping data. The start of text character (STX) marked the end of the header, and the start of the textual part of a stream. The end of text character (ETX) marked the end of the data of a message. A widely used convention is to make the two characters preceding ETX a checksum or CRC for error-detection purposes. The end of transmission block character (ETB) was used to indicate the end of a block of data, where data was divided into such blocks for transmission purposes.",
        "The escape character (ESC) was intended to \"quote\" the next character, if it was another control character it would print it instead of performing the control function. It is almost never used for this purpose today. Various printable characters are used as visible \"escape characters\", depending on context.",
        "The substitute character (SUB) was intended to request a translation of the next character from a printable character to another value, usually by setting bit 5 to zero. This is handy because some media (such as sheets of paper produced by typewriters) can transmit only printable characters. However, on MS-DOS systems with files opened in text mode, \"end of text\" or \"end of file\" is marked by this Ctrl-Z character, instead of the Ctrl-C or Ctrl-D, which are common on other operating systems.",
        "The cancel character (CAN) signaled that the previous element should be discarded. The negative acknowledge character (NAK) is a definite flag for, usually, noting that reception was a problem, and, often, that the current element should be sent again.  The acknowledge character (ACK) is normally used as a flag to indicate no problem detected with current element.",
        "When a transmission medium is half duplex (that is, it can transmit in only one direction at a time), there is usually a master station that can transmit at any time, and one or more slave stations that transmit when they have permission. The enquire character (ENQ) is generally used by a master station to ask a slave station to send its next message. A slave station indicates that it has completed its transmission by sending the end of transmission character (EOT).",
        "The device control codes (DC1 to DC4) were originally generic, to be implemented as necessary by each device. However, a universal need in data transmission is to request the sender to stop transmitting when a receiver is temporarily unable to accept any more data. Digital Equipment Corporation invented a convention which used 19 (the device control 3 character (DC3), also known as control-S, or XOFF) to \"S\"top transmission, and 17 (the device control 1 character (DC1), a.k.a. control-Q, or XON) to start transmission. It has become so widely used that most don't realize it is not part of official ASCII. This technique, however implemented, avoids additional wires in the data cable devoted only to transmission management, which saves money. A sensible protocol for the use of such",
        "of official ASCII. This technique, however implemented, avoids additional wires in the data cable devoted only to transmission management, which saves money. A sensible protocol for the use of such transmission flow control signals must be used, to avoid potential deadlock conditions, however.",
        "The data link escape character (DLE) was intended to be a signal to the other end of a data link that the following character is a control character such as STX or ETX.  For example a packet may be structured in the following way (DLE) <STX> <PAYLOAD> (DLE) <ETX>.\n\nMiscellaneous codes\nCode 7 (BEL) is intended to cause an audible signal in the receiving terminal.",
        "Miscellaneous codes\nCode 7 (BEL) is intended to cause an audible signal in the receiving terminal.\n\nMany of the ASCII control characters were designed for devices of the time that are not often seen today. For example, code 22, \"synchronous idle\" (SYN), was originally sent by synchronous modems (which have to send data constantly) when there was no actual data to send. (Modern systems typically use a start bit to announce the beginning of a transmitted word— this is a feature of asynchronous communication.  Synchronous communication links were more often seen with mainframes, where they were typically run over corporate leased lines to connect a mainframe to another mainframe or perhaps a minicomputer.)",
        "Code 0 (ASCII code name NUL) is a special case. In paper tape, it is the case when there are no holes.  It is convenient to treat this as a fill character with no meaning otherwise.  Since the position of a NUL character has no holes punched, it can be replaced with any other character at a later time, so it was typically used to reserve space, either for correcting errors or for inserting information that would be available at a later time or in another place. In computing it is often used for padding in fixed length records and more commonly, to mark the end of a string.",
        "Code 127 (DEL, a.k.a. \"rubout\") is likewise a special case. Its 7-bit code is all-bits-on in binary, which essentially erased a character cell on a paper tape when overpunched. Paper tape was a common storage medium when ASCII was developed, with a computing history dating back to WWII code breaking equipment at Biuro Szyfrów. Paper tape became obsolete in the 1970s, so this clever aspect of ASCII rarely saw any use after that. Some systems (such as the original Apples) converted it to a backspace.  But because its code is in the range occupied by other printable characters, and because it had no official assigned glyph, many computer equipment vendors used it as an additional printable character (often an all-black \"box\" character useful for erasing text by overprinting with ink).",
        "Non-erasable programmable ROMs are typically implemented as arrays of fusible elements, each representing a bit, which can only be switched one way, usually from one to zero. In such PROMs, the DEL and NUL characters can be used in the same way that they were used on punched tape: one to reserve meaningless fill bytes that can be written later, and the other to convert written bytes to meaningless fill bytes. For PROMs that switch one to zero, the roles of NUL and DEL are reversed; also, DEL will only work with 7-bit characters, which are rarely used today; for 8-bit content, the character code 255, commonly defined as a nonbreaking space character, can be used instead of DEL.\n\nMany file systems do not allow control characters in filenames, as they may have reserved functions.",
        "Many file systems do not allow control characters in filenames, as they may have reserved functions.\n\nSee also\n , HJKL as arrow keys, used on ADM-3A terminal\n C0 and C1 control codes\n Escape sequence\n In-band signaling\n Whitespace character\n\nNotes and references\n\nExternal links \n\n ISO IR 1 C0 Set of ISO 646 (PDF)"
    ],
    [
        "Country code\nA country code is a short alphanumeric identification code for countries and dependent areas. Its primary use is in data processing and communications. Several identification systems have been developed.\n\nThe term country code frequently refers to ISO 3166-1 alpha-2, as well as the telephone country code, which is embodied in the E.164 recommendation by the International Telecommunication Union (ITU).\n\nISO 3166-1\nThe standard ISO 3166-1 defines short identification codes for most countries and dependent areas:\nISO 3166-1 alpha-2: two-letter code\nISO 3166-1 alpha-3: three-letter code\nISO 3166-1 numeric: three-digit code",
        "The two-letter codes are used as the basis for other codes and applications, for example,\nfor ISO 4217 currency codes\nwith deviations, for country code top-level domain names (ccTLDs) on the Internet: list of Internet TLDs.\nOther applications are defined in ISO 3166-1 alpha-2.",
        "ITU country codes\nIn telecommunication, a country code, or international subscriber dialing (ISD) code, is a telephone number prefix used in international direct dialing (IDD) and for destination routing of telephone calls to a country other than the caller's. A country or region with an autonomous telephone administration must apply for membership in the International Telecommunication Union (ITU) to participate in the international public switched telephone network (PSTN). County codes are defined by the ITU-T section of the ITU in standards E.123 and E.164.",
        "Country codes constitute the international telephone numbering plan, and are dialed only when calling a telephone number in another country. They are dialed before the national telephone number. International calls require at least one additional prefix to be dialing before the country code, to connect the call to international circuits, the international call prefix. When printing telephone numbers this is indicated by a plus-sign (+) in front of a complete international telephone number, per recommendation E164 by the ITU.",
        "Other country codes\nEuropean Union:\nBefore the 2004 EU enlargement the EU used the UN Road Traffic Conventions license plate codes. Since then, it has used the ISO 3166-1 alpha-2 code, but with two modifications:\nEL for Greece (instead of GR)\n(formerly) UK for United Kingdom (instead of GB)\nThe Nomenclature des unités territoriales statistiques (Nomenclature of territorial units for statistics, NUTS) of the European Union, mostly focusing on subdivisions of the EU member states\nFIFA (Fédération Internationale de Football Association) assigns a three-letter code (dubbed FIFA Trigramme) to each of its member and non-member countries: List of FIFA country codes",
        "FIFA (Fédération Internationale de Football Association) assigns a three-letter code (dubbed FIFA Trigramme) to each of its member and non-member countries: List of FIFA country codes\nFederal Information Processing Standard (FIPS) 10-4 defined two-letter codes used by the U.S. government and in the CIA World Factbook: list of FIPS country codes. On September 2, 2008, FIPS 10-4 was one of ten standards withdrawn by NIST as a Federal Information Processing Standard.\n The Bureau of Transportation Statistics, part of the United States Department of Transportation (US DOT), maintains its own list of codes, so-called World Area Codes (WAC), for state and country codes.\n GOST 7.67: country codes in Cyrillic from the GOST standards committee",
        "GOST 7.67: country codes in Cyrillic from the GOST standards committee\nFrom the International Civil Aviation Organization (ICAO):\nThe national prefixes used in aircraft registration numbers\nLocation prefixes in four-character ICAO airport codes\nInternational Olympic Committee (IOC) three-letter codes used in sporting events: list of IOC country codes\nFrom the International Telecommunication Union (ITU):\nthe E.212 mobile country codes (MCC), for mobile/wireless phone addresses,\nthe first few characters of call signs of radio stations (maritime, aeronautical, amateur radio, broadcasting, and so on) define the country: the ITU prefix,\nITU letter codes for member-countries,",
        "the first few characters of call signs of radio stations (maritime, aeronautical, amateur radio, broadcasting, and so on) define the country: the ITU prefix,\nITU letter codes for member-countries,\nITU prefix - amateur and experimental stations - The International Telecommunication Union (ITU) assigns national telecommunication prefixes for amateur and experimental radio use, so that operators can be identified by their country of origin. These prefixes are legally administered by the national entity to which prefix ranges are assigned.\nThree-digit codes used to identify countries in maritime mobile radio transmissions, known as maritime identification digits\nLicense plates for automobiles:",
        "Three-digit codes used to identify countries in maritime mobile radio transmissions, known as maritime identification digits\nLicense plates for automobiles:\nUnder the 1949 and 1968 United Nations Road Traffic Conventions (distinguishing signs of vehicles in international traffic): List of international license plate codes.\nDiplomatic license plates in the United States, assigned by the U.S. State Department.",
        "Diplomatic license plates in the United States, assigned by the U.S. State Department.\nNorth Atlantic Treaty Organization (NATO) used two-letter codes of its own: list of NATO country codes. They were largely borrowed from the FIPS 10-4 codes mentioned below. In 2003 the eighth edition of the Standardisation Agreement (STANAG) adopted the ISO 3166 three-letter codes with one exception (the code for Macedonia). With the ninth edition, NATO is transitioning to four- and six-letter codes based on ISO 3166 with a few exceptions and additions\nUnited Nations Development Programme (UNDP) also has its own list of trigram country codes\n World Intellectual Property Organization (WIPO): WIPO ST.3 gives two-letter codes to countries and regional intellectual property organizations",
        "World Intellectual Property Organization (WIPO): WIPO ST.3 gives two-letter codes to countries and regional intellectual property organizations\nWorld Meteorological Organization (WMO) maintains a list of country codes, used in reporting meteorological observations\n UIC (the International Union of Railways): UIC Country Codes",
        "The developers of ISO 3166 intended that in time it would replace other coding systems.\n\nOther codings\nCountry identities may be encoded in the following coding systems:\nThe initial digits of International Standard Book Numbers (ISBN) are group identifiers for countries, areas, or language regions.\nThe first three digits of GS1 Company Prefixes used to identify products, for example, in barcodes, designate (national) numbering agencies.\n\nLists of country codes by country\nA -\nB -\nC -\nD–E -\nF -\nG -\nH–I -\nJ–K -\nL -\nM -\nN -\nO–Q -\nR -\nS -\nT -\nU–Z\n\nSee also\nList of ISO 3166 country codes\nISO 639 language codes\nLanguage code\nNumbering scheme\n\nReferences\n\nExternal links\nComparison of various systems\nAnother comparison: \nA comparison with ISO, IFS and others with notes\nUnited Nations Region Codes",
        "References\n\nExternal links\nComparison of various systems\nAnother comparison: \nA comparison with ISO, IFS and others with notes\nUnited Nations Region Codes\n\n \nGeocodes"
    ],
    [
        "Disney's Magical Mirror Starring Mickey Mouse\nDisney's Magical Mirror Starring Mickey Mouse is a point-and-click adventure video game developed by Nintendo and Capcom and published by Nintendo for the GameCube and licensed from Disney.\n\nGameplay\nThe game uses a simple point-and-click mechanic, similar to Pac-Man 2: The New Adventures, which involves using a cursor to guide Mickey Mouse to various locations. Mickey will react to what the player does and what he encounters in the game by expressing curiosity, getting mad, falling down, running away, standing his ground, or other actions. At certain points, the player is able to have Mickey perform a special move that generally involves having him stomp on an onscreen enemy.",
        "Mini-games, such as having Mickey fly an airplane or ski down a mountain, are available throughout the game. Special souvenirs may be uncovered as well, which are displayed in Mickey's room at the end of the game, such as Pluto's collar or Minnie's bow. The objective of the game is to find all the pieces of the mirror so that Mickey goes back home.",
        "Plot",
        "One night when Mickey is fast asleep, he falls into a dream where a mischievous ghost traps a dream vision of himself inside a magic mirror. Stuck in a large mansion within an alternate universe that strangely resembles his own house, Mickey yearns to get back through the mirror to the real world in order to wake up from his dreamlike state; however, the ghost destroys the mirror and the pieces shrink and fly off to different areas around the house, which turns the magic mirror into a normal mirror. The player must direct Mickey to outwit and pull gags in order to get past enemies, obstacles, and the aforementioned ghost and recover the twelve broken mirror pieces he needs to go home again and search for twelve magic star containers (needed to pull gags) and items needed to help him",
        "and the aforementioned ghost and recover the twelve broken mirror pieces he needs to go home again and search for twelve magic star containers (needed to pull gags) and items needed to help him throughout his quest. Whenever he finds a piece, it will fly back to the mirror, return to its normal size, and put itself back in place.",
        "After repairing the mirror, Mickey prepares to leave, but the ghost stops him, revealing that it only brought him here so it can have someone to play with. The player could either choose to stay or go. Choosing to stay will make the ghost run off, leaving Mickey stuck in the alternate world until he reenters the mirror room where the player can choose to stay or leave again. If the player chooses to leave, Mickey says goodbye to the ghost and begins to go home, but the ghost decides to go with him (only if the player has collected all the mirror pieces). After Mickey wakes up, he goes downstairs to get something to eat. If the player repairs the mirror with all twelve pieces found, a model of the ghost is shown hanging on the ceiling fan and the ghost's laughter is heard, implying that",
        "to get something to eat. If the player repairs the mirror with all twelve pieces found, a model of the ghost is shown hanging on the ceiling fan and the ghost's laughter is heard, implying that the ghost is now residing in Mickey's house.",
        "Development\nThe game was teased at Nintendo's Space World 2001 presentation with a series of screenshots, which was assumed to be a platformer like much the vein of Disney's Magical Quest on the Super NES. The title was formally announced at the 2002 Electronic Entertainment Expo.\n\nThe game's introduction sequence is loosely based on Thru the Mirror, a 1936 Mickey Mouse cartoon; Mickey's animations are replicated from the short. An alternate scene later in the game, where Mickey grows to a tremendous size then shrinks to a minuscule size, was also replicated as in the cartoon.\n\nReception",
        "Reception\n\nThe game received \"mixed\" reviews according to the review aggregation website Metacritic. For the most part, the player is given no instructions and cut scenes are limited to watching Mickey get chased or falling through to the next area. In Japan, Famitsu gave it a score of 27 out of 40.\n\nGameSpot said, \"The incredibly slow pacing and monotonous puzzles will override the Disney entertainment factor for the young as well.\" IGN wrote, \"It's just plain boring and often tedious. Most kids will find the game as just that, and for the monetary investment, it's not recommended.\" GameSpy wrote, \"Geared towards a younger audience, Mickey is mind-numbingly simple, and it's a shame that such impressive visuals had to go to waste on such a disposable game.\"",
        "See also\n Disney's Hide and Sneak\n List of Disney video games\n\nNotes\n\nReferences\n\nExternal links\n\n2002 video games\nDisney games by Capcom\nDisney video games\nGameCube games\nGameCube-only games\nGames with GameCube-GBA connectivity\nMickey Mouse video games\nNintendo games\nPoint-and-click adventure games\nSingle-player video games\nVideo games about dreams\nVideo games about ghosts\nVideo games about magic\nVideo games about parallel universes\nVideo games developed in Japan"
    ],
    [
        "ECPP\nECPP may refer to:\n\nElliptic curve primality proving\nEnvironmental Crime Prevention Program"
    ],
    [
        "Etoys (programming language)\nEtoys is a child-friendly computer environment and object-oriented prototype-based programming language for use in education.\n\nEtoys is a media-rich authoring environment with a scripted object model for many different objects that runs on different platforms and is free and open source.",
        "History \n Squeak was originally developed at Apple in 1996 by Dan Ingalls.\n Squeak is a Smalltalk implementation, object-oriented, class-based, and reflective, derived from Smalltalk-80 at Apple Computer.  It was developed by some of the original Smalltalk-80 developers, including Dan Ingalls, Ted Kaehler, and Alan Kay.  The team also included Scott Wallace and John Maloney.\n Squeak 4.0 is released under the MIT License, with some of the original Apple parts remaining under the Apache License. Contributions are required to be under MIT.\n “Back to the Future: the story of Squeak, a practical Smalltalk written in itself” by Dan Ingalls, Ted Kaehler, John Maloney, Scott Wallace, Alan Kay.  Paper presented at OOPSLA, Atlanta, Georgia, 1997 by Dan Ingalls.",
        "Squeak migrated to Disney Imagineering Research in 1996.\nEtoys development began and was directed by Alan Kay at Disney to support constructionist learning, influenced by Seymour Papert and the Logo programming language.\nThe original Etoys development team at Disney included: Scott Wallace, Ted Kaehler, John Maloney, Dan Ingalls.\nEtoys influenced the development of another Squeak-based educational programming environment known as Scratch.  Scratch was developed at MIT, after Mitchell Resnick invited John Maloney of the original Etoys development team to come to MIT.\nEtoys migrated to Viewpoints Research, Inc., incorporated in 2001,  to improve education for the world’s children and advance the state of systems research and personal computing.",
        "Etoys migrated to Viewpoints Research, Inc., incorporated in 2001,  to improve education for the world’s children and advance the state of systems research and personal computing.\nIn 2006-2007, Etoys built in Squeak was used by the OLPC project, on their OLPC XO-1 educational machine.  It is preinstalled on all of the XO-1 laptops.\n“Etoys for One Laptop Per Child”, paper by Bert Freudenberg, Yoshiki Ohshima, Scott Wallace, January 2009.  Paper presented at the Seventh Annual International Conference on Creating, Computing, Connecting, and Collaborating through Computing, Kyoto University, Kyoto, Japan, January 2009.",
        "In 2009, the Squeakland Foundation was created by Viewpoints Research, Inc., as an initial step in launching the foundation to continue encouraging development and use of Etoys as an educational medium.\nViewpoints Research Inc. supported Squeakland Foundation in 2009-2010, and in January 2010, the Squeakland Foundation was launched as a separate entity.",
        "Motivation and influences\nEtoys development was inspired and directed by Alan Kay and his work to advance and support constructionist learning. Primary influences include Seymour Papert and the Logo programming language, a dialect of Lisp optimized for educational use; work done at Xerox Palo Alto Research Center, PARC; Smalltalk, HyperCard, StarLogo and NetLogo. The drag and drop tile-based approach is very similar to AgentSheets. Scott Wallace is the main author.  Promotion and development of the main Squeak version of Etoys is co-ordinated by the Viewpoints Research Institute, a U.S. educational non-profit.",
        "Etoys was a major influence on a similar Squeak-based programming environment known as Scratch.  Scratch was designed with Etoys code in the early 21st century by the MIT Media Lab, initially targeted at after-school computer clubs.\n\nFeatures\n\nThe Etoys system is based on the idea of programmable virtual entities behaving on the computer screen.\n\nEtoys provides a media-rich authoring environment with a simple, powerful scripted object model for many kinds of objects created by end-users. It includes 2D and 3D graphics, images, text, particles, presentations, web-pages, videos, sound and MIDI, the ability to share desktops with other Etoy users in real-time, so many forms of immersive mentoring and play can be done over the Internet.",
        "It is multilingual, and has been used successfully in United States, Europe, South America, Japan, Korea, India, Nepal, Ethiopia, and Russia .\n\nVersions\nAll Etoys versions are based on object-oriented programming languages. Squeak Etoys runs on more than 20 platforms bit-identically. Versions exist written in three programming languages. The original and most widely used is based on Squeak, a dialect of Smalltalk. The second is also based on Squeak, but uses the optional Tweak programming environment instead of Squeak's default Morphic environment. The third is based on Python and is named PataPata . PataPata has been abandoned by its author.",
        "In 2006 and; 2007, the Squeak Morphic version was adapted for distribution on the OLPC XO-1 educational machine, sometimes known as the $100 laptop.  Viewpoints Research Institute participates in the One Laptop per Child association, and Etoys is pre-installed on all XO-1 laptops.\n\nThe licensing is free and open source.\n\nAs of 2010, Etoys 4 conforms to the requirements of free and open source systems, such as the various Linux distributions.\n\nIn 1996, Apple had released Squeak under their \"Squeak license\", which did not qualify as fully free software, due to the presence of an indemnity clause. The source code was available and modification was permitted.",
        "In May 2006, Apple relicensed the Squeak core under the Apache 2.0 license, thanks to Steve Jobs, Dan Ingalls, and Alan Kay.  Viewpoints Research collected written relicensing agreements from several hundred contributors under the MIT license, and all code in Etoys not explicitly covered by a relicensing agreement was removed, rewritten, or reverted to an earlier version, mostly by Yoshiki Ohshima.  Squeak Etoys is now completely free and open source.\n\nReferences\n\nExternal links\nSqueakland — Etoys official site\nEtoysIllinois — a multiLingual collection of more than educational projects and curricular materials (hosted by the University of Illinois at Urbana-Champaign)",
        "Smalltalk programming language family\nEducational programming languages\nFree educational software\nVisual programming languages\n1996 software\nPedagogic integrated development environments\nApple Inc. software\nDisney technology\nFormerly proprietary software"
    ],
    [
        "File archiver\nA file archiver is a computer program that combines a number of files together into one archive file, or a series of archive files, for easier transportation or storage. File archivers may employ lossless data compression in their archive formats to reduce the size of the archive.\n\nBasic archivers just take a list of files and concatenate their contents sequentially into archives. The archive files need to store metadata, at least the names and lengths of the original files, if proper reconstruction is possible. More advanced archivers store additional metadata, such as the original timestamps, file attributes or access control lists.",
        "The process of making an archive file is called archiving or packing. Reconstructing the original files from the archive is termed unarchiving, unpacking or extracting.\n\nHistory\nAn early archiver was the Multics command archive, descended from the CTSS command of the same name, which was a basic archiver and performed no compression. Multics also had a \"tape_archiver\" command, abbreviated ta, which was perhaps the forerunner of the Unix command tar.",
        "Unix archivers\nThe Unix tools ar, tar, and cpio act as archivers but not compressors. Users of the Unix tools use additional compression tools, such as gzip, bzip2, or xz, to compress the archive file after packing or remove compression before unpacking the archive file. The filename extensions are successively added at each step of this process. For example, archiving a collection of files with tar and then compressing the resulting archive file with gzip results a file with .tar.gz extension.",
        "This approach has two goals:\n It follows the Unix philosophy that each program should accomplish a single task to perfection, as opposed to attempting to accomplish everything with one tool. As compression technology progresses, users may use different compression programs without having to modify or abandon their archiver.\n The archives use solid compression. When the files are combined, the compressor can exploit redundancy across several archived files and achieve better compression than a compressor that compresses each files individually.",
        "This approach, however, has disadvantages too:\n Extracting or modifying one file is difficult. Extracting one file requires decompressing an entire archive, which can be time- and space-consuming. Modifying one means the file needs to be put back into archive and the archive recompressed again. This operation requires additional time and disk space.\n The archive becomes damage-prone. If the area holding shared data for several files is damaged, all those files are lost.",
        "The archive becomes damage-prone. If the area holding shared data for several files is damaged, all those files are lost.\n It is impossible to take advantage of redundancy between files unless the compression window is larger than the size of an individual file. For example, gzip uses DEFLATE, which typically operates with a 32768-byte window, whereas bzip2 uses a Burrows–Wheeler transform roughly 27 times bigger. xz defaults to 8 MiB but supports significantly larger windows.",
        "Windows archivers\nThe built-in archiver of Microsoft Windows as well as third-party archiving software, such as WinRAR and 7-zip, often use a graphical user interface. They also offer an optional command-line interface, while Windows itself does not. Windows archivers perform both archiving and compression. Solid compression may or may not be offered, depending on the product: Windows itself does not support it; WinRAR and 7-zip offer it as an option that can be turned on or off.\n\nSee also\n Comparison of file archivers\n Archive format\n List of archive formats\n Comparison of archive formats\n\nReferences\n\nExternal links\n \n\nComputer storage systems\nComputer file systems\nComputer archives\n \nUtility software types"
    ],
    [
        "Flight controller\nFlight controllers are personnel who aid space flight by working in such Mission Control Centers as NASA's Mission Control Center or ESA's European Space Operations Centre. Flight controllers work at computer consoles and use telemetry to monitor various technical aspects of a space mission in real-time. Each controller is an expert in a specific area and constantly communicates with additional experts in the \"back room\". The flight director, who leads the flight controllers, monitors the activities of a team of flight controllers, and has overall responsibility for success and safety.",
        "This article primarily discusses NASA's flight controllers at the Johnson Space Center (JSC) in Houston. The various national and commercial flight control facilities have their own teams, which may be described on their own pages.",
        "NASA's flight controllers",
        "The room where the flight controllers work was called the mission operations control room (MOCR, pronounced \"moh-ker\"), and now is called the flight control room (FCR, pronounced \"ficker\"). The controllers are experts in individual systems, and make recommendations to the flight director involving their areas of responsibility. Any controller may call for an abort if the circumstances require it. Before significant events, the flight director will \"go around the room\", polling each controller for a go/no go decision, a procedure also known as a launch status check. If all factors are good, each controller calls for a go, but if there is a problem requiring a hold or an abort, the call is no go. Another form of this is stay/no stay, when the spacecraft has completed a maneuver and has now",
        "controller calls for a go, but if there is a problem requiring a hold or an abort, the call is no go. Another form of this is stay/no stay, when the spacecraft has completed a maneuver and has now \"parked\" in relation to another body, including spacecraft, orbiting the Earth or the Moon, or the lunar landings.",
        "Controllers in MOCR/FCR are supported by the \"backrooms\", teams of flight controllers located in other parts of the building or even at remote facilities. The backroom was formerly called the staff support room (SSR), and is now called the multi-purpose support room (MPSR, pronounced \"mipser\"). Backroom flight controllers are responsible for the details of their assigned system and for making recommendations for actions needed for that system. \"Frontroom\" flight controllers are responsible for integrating the needs of their system into the larger needs of the vehicle and working with the rest of the flight control team to develop a cohesive plan of action, even if that plan is not necessarily in the best interests of the system they are responsible for. Within the chain of command of the",
        "of the flight control team to develop a cohesive plan of action, even if that plan is not necessarily in the best interests of the system they are responsible for. Within the chain of command of the MCC, information and recommendations flow from the backroom to the frontroom to Flight, and then, potentially, to the on board crew. Generally, a MOCR/FCR flight control team is made up of the more seasoned flight controllers than the SSR/MPSR, though senior flight controllers cycle back to support in the backroom periodically. One example of the usefulness of this system occurred during the descent of the Apollo 11 Lunar Module Eagle, when \"1202\" and \"1201\" program alarms came from the LM. GUIDO Steve Bales, not sure whether to call for an abort, trusted the experts in the guidance backroom,",
        "of the Apollo 11 Lunar Module Eagle, when \"1202\" and \"1201\" program alarms came from the LM. GUIDO Steve Bales, not sure whether to call for an abort, trusted the experts in the guidance backroom, especially Jack Garman, who told him that the problem was a computer overload, but could be ignored if it was intermittent. Bales called \"Go!\", Flight Director Gene Kranz accepted the call and the mission continued to success. Without the support of the backroom, a controller might make a bad call based on faulty memory or information not readily available to the person on the console. The nature of quiescent operations aboard the International Space Station (ISS) today is such that the full team is not required for 24/7/365 support. FCR flight controllers accept responsibility for operations",
        "of quiescent operations aboard the International Space Station (ISS) today is such that the full team is not required for 24/7/365 support. FCR flight controllers accept responsibility for operations without MPSR support most of the time, and the MPSR is only staffed for high-intensity periods of activity, such as joint Shuttle/ISS missions.",
        "The flight controllers in the FCR and MPSR are further supported by hardware and software designers, analysts and engineering specialists in other parts of the building or remote facilities. These extended support teams have more detailed analysis tools and access to development and test data that is not readily accessible to the flight control team. These support teams were referred to by the name of their room in Mission Control, the mission operations integration room (MOIR), and are now collectively referred to by the name of their current location, the mission evaluation room (MER). While the flight controllers and their backrooms are responsible for real-time decision making, the MOIR/MER provides the detailed data and history needed to solve longer-term issues.",
        "Uncrewed U.S. space missions also have flight controllers but are managed from separate organizations, either the Jet Propulsion Laboratory or the Johns Hopkins University Applied Physics Laboratory for deep-space missions or Goddard Space Flight Center for near-Earth missions.\n\nEach flight controller has a unique call sign, which describes the position's responsibilities. The call sign and responsibility refer to the particular console, not just the person, since missions are managed around the clock and with each shift change a different person takes over the console.",
        "Flight controller responsibilities have changed over time, and continue to evolve. New controllers are added, and tasks are reassigned to other controllers to keep up with changing technical systems. For example, the EECOM handled command and service module communication systems through Apollo 10, which was afterward assigned to a new position called INCO.\n\nResponsibility \nFlight controllers are responsible for the success of the mission and for the lives of the astronauts under their watch. The Flight Controllers' Creed states that they must \"always be aware that suddenly and unexpectedly we may find ourselves in a role where our performance has ultimate consequences.\" Well-known actions taken by flight controllers include:",
        "The Apollo 11 Lunar Module computer was overloaded because the astronauts forgot to switch off their upper-stage radar before switching on the downward-pointing radar. Guidance officer Steve Bales had only a few seconds to determine whether it was safe to land anyway or to abort the mission mere feet above the Moon. Bales was later honored for his role in the mission, when he was selected to accept the NASA Group Achievement Award from President Richard Nixon on behalf of the Apollo 11 mission operations team.",
        "During the launch of Apollo 12, the Saturn V was struck by lightning which knocked out all telemetry and multiple command module systems. Seconds before mission abort, EECOM controller John Aaron determined that switching to the backup electrical power distribution telemetry conditioning would reveal the true nature of the issue.",
        "During Space Shuttle mission STS-51-F, a main engine failed during ascent to orbit. Subsequently, indications were received of a second engine beginning to fail, which would have caused a mission abort, possibly including loss of the shuttle. Booster officer Jenny Howard Stein determined that the anomalous readings on the second engine were a sensor error and not an engine problem. At her direction the crew inhibited the sensor, which saved the mission and possibly the crew.",
        "Common flight control positions \n\nThere are some positions that have and will serve the same function in every vehicle's flight control team. The group of individuals serving in those positions may be different, but they will be called the same thing and serve the same function.\n\nFlight director \n\nLeads the flight control team. Flight has overall operational responsibility for missions and payload operations and for all decisions regarding safe, expedient flight. This person monitors the other flight controllers, remaining in constant verbal communication with them via intercom channels called \"loops\".\n\nFlight operations directorate (FOD)",
        "Flight operations directorate (FOD) \n\nIs a representative of the senior management chain at JSC, and is there to help the flight director make those decisions that have no safety-of-flight consequences, but may have cost or public perception consequences. The FOD cannot overrule the flight director during a mission.  The former mission operations directorate (MOD) position was renamed FOD when the flight crew operations directorate (FCOD) was merged back with MOD beginning in August 2014.\n\nSpacecraft communicator (CAPCOM)",
        "Spacecraft communicator (CAPCOM)\n\nGenerally, only the spacecraft communicator communicates directly with the crew of a crewed space flight. The acronym dates back to Project Mercury when the spacecraft was originally termed a \"capsule.\" NASA felt it important for all communication with the astronauts in space to pass through a single individual in the Mission Control Center. That role was first designated the capsule communicator or CAPCOM and was filled by another astronaut, often one of the backup- or support-crew members. NASA believes that an astronaut is most able to understand the situation in the spacecraft and pass information in the clearest way.",
        "For long-duration missions there is more than one CAPCOM, each assigned to a different shift team. After control of U.S. spaceflights moved to the Johnson Space Center in the early 1960s, each CAPCOM used the radio call-sign Houston. When non-astronauts are communicating directly with the spacecraft, CAPCOM acts as the communications controller.\n\n, due to the shrinking size of the astronaut corps at the end of the Shuttle program, fewer astronauts are available to perform CAPCOM duties, so non-astronauts from the space flight training and flight controller branches also function as CAPCOM during ISS missions, while the role was filled solely by astronauts for the Apollo and Shuttle missions. Astronauts still take the CAPCOM position during critical events such as docking and EVA.",
        "In the context of potential crewed missions to Mars, NASA Ames Research Center has conducted field trials of advanced computer-support for astronaut and remote science teams, to test the possibilities for automating CAPCOM.\n\nFlight surgeon\n\nThe flight surgeon directs all medical activities during the mission – monitors crew health via telemetry, provides crew consultation, and advises the flight director. A private communication channel can be established between astronauts and the flight surgeon, to provide doctor–patient confidentiality.\n\nPublic affairs officer (PAO)",
        "Public affairs officer (PAO)\n\nProvides mission commentary to supplement and explain air-to-ground transmissions and flight control operations to the news media and the public. The individual filling this role is often referred to colloquially as The Voice of Mission Control.\n\nApollo flight control positions \n\nThe flight control positions used during the Apollo era were predominantly identical to the positions used for the Mercury and Gemini vehicles. This was because of the similarity of the vehicle design of the capsules used for the three programs.\n\nBooster systems engineer",
        "Booster systems engineer\n\nThe booster systems engineer monitored and evaluated performance of propulsion-related aspects of the launch vehicle during prelaunch and ascent. During the Apollo program there were three booster positions, who worked only until trans-lunar injection (TLI); after that, their consoles were vacated. Booster had the power to send an abort command to the spacecraft. All booster technicians were employed at the Marshall Space Flight Center and reported to JSC for the launches.\n\nControl officer\n\nThe control officer was responsible for the Apollo Lunar Module guidance, navigation, and control systems – essentially the equivalent of the GNC for the Command and Service Module.\n\nElectrical, environmental, and consumables manager (EECOM)",
        "Electrical, environmental, and consumables manager (EECOM)\n\nThe EECOM monitored cryogenic levels for fuel cells, and cabin cooling systems; electrical distribution systems; cabin pressure control systems; and vehicle lighting systems. EECOM originally stood for electrical, environmental and communication systems. The Apollo EECOM was responsible for CSM communications through Apollo 10. Afterward the communication task was moved to a new console named INCO.",
        "Perhaps the most famous NASA EECOMs are Seymour \"Sy\" Liebergot, the EECOM on duty at the time of the oxygen tank explosion on Apollo 13, and John Aaron, who designed the drastically reduced power budget for its return. Aaron also saved the Apollo 12 mission by realizing that using the backup power supply for telemetry of analog capsule sensors would allow diagnosis of all the seemingly-unrelated problems caused by a lightning strike.\n\nFlight activities officer (FAO)\n\nThe FAO planned and supported crew activities, checklists, procedures and schedules.\n\nFlight director",
        "Flight activities officer (FAO)\n\nThe FAO planned and supported crew activities, checklists, procedures and schedules.\n\nFlight director\n\nThe flight directors held overall control of all of the individual positions in the MOCR. Some Apollo era directors were:\n Gene Kranz, White Flight. Apollo missions 7, 9, 11, 13, 15, 16 and 17.\n Glynn Lunney, Black Flight. Apollo missions 7, 8, 10, 11, 13, 14 and 15.\n Gerry Griffin, Gold Flight. Apollo missions 7, 9, 11, 12, 13, 14, 15, 16 and 17. \n Milt Windler, Maroon Flight. Apollo missions 8, 10, 12, 13, 14 and 15.\n Clifford E. Charlesworth, Green Flight. Apollo missions 8, 11 and 12.\n M. P. (Pete) Frank, Orange Flight. Apollo missions 9, 12, 16, 17, and Apollo–Soyuz.\n\nFlight dynamics officer (FDO or FIDO)",
        "Flight dynamics officer (FDO or FIDO)\n\nResponsible for the flight path of the space vehicle, both atmospheric and orbital. During lunar missions the FDO was also responsible for the lunar trajectory. The FDO monitored vehicle performance during the powered flight phase and assessed abort modes, calculated orbital maneuvers and resulting trajectories, and monitored vehicle flight profile and energy levels during reentry.\n\nGuidance officer (GUIDANCE or GUIDO)\n\nThe guidance officer monitored on board navigational systems and on board guidance computer software. Responsible for determining the position of the spacecraft in space. One well-known guidance officer was Steve Bales, who gave the go call when the Apollo 11 guidance computer came close to overloading during the first lunar descent.",
        "Guidance, navigation, and controls systems engineer (GNC)\n\nThe GNC monitored all vehicle guidance, navigation, and control systems. Also responsible for propulsion systems such as the service propulsion system and reaction control system (RCS).\n\nIntegrated communications officer (INCO)\n\nThe INCO was responsible for all data, voice and video communications systems, including monitoring the configuration of in-flight communications and instrumentation systems. Duties also included monitoring the telemetry link between the vehicle and the ground, and overseeing the uplink command and control processes. The position was formed from the combination of LEM and CSM communicator positions.\n\nNetwork",
        "Network\n\nSupervised the network of ground stations that relayed telemetry and communications from the spacecraft.\n\nOrganization and procedures officer (O&P)\n\nSupervised the application of mission rules and established techniques to the conduct of the flight.\n\nRetrofire officer (RETRO)\n\nDrew up abort plans and was responsible for determination of retrofire times. During lunar missions the RETRO planned and monitored Trans Earth Injection (TEI) maneuvers, where the Apollo Service Module fired its engine to return to Earth from the Moon.\n\nTelemetry, electrical, EVA mobility unit officer (TELMU)\n\nMonitored the lunar module electrical and environmental systems, plus lunar astronaut spacesuits. Essentially the equivalent of the EECOM for the lunar module.",
        "Monitored the lunar module electrical and environmental systems, plus lunar astronaut spacesuits. Essentially the equivalent of the EECOM for the lunar module.\n\nShuttle and Space Station flight controllers \nNASA currently has a group of flight controllers at the Johnson Space Center in Houston for the International Space Station (ISS). The Space Shuttle flight control team (as well as those for the earlier Gemini, Apollo, and Skylab programs) were also based there. Console manning for short-duration and extended operations differed in operational philosophy.",
        "The Space Shuttle (and prior program) flight controllers worked relatively brief periods: The several minutes of ascent, the few days the vehicle was in orbit, and reentry. The duration of operations for Space Shuttle flight controllers was short and time-critical. A failure on the Shuttle could leave flight controllers little time for talking, putting pressure on them to respond quickly to potential failures. The Space Shuttle flight controllers generally had limited capability to send commands to the shuttle for system reconfigurations.",
        "In contrast, the ISS flight controllers work 24 hours a day, 365 days a year. This allows the ISS flight controllers time to discuss off-nominal telemetry. The ISS flight controllers have the opportunity to interface with many groups and engineering experts. The mentality of an ISS flight controller is to preempt a failure. Telemetry is closely monitored for any signatures that may begin to indicate future catastrophic failures. Generally, ISS flight controllers take a prophylactic approach to space vehicle operations. There are command capabilities that ISS flight controllers use to preclude a potential failure.",
        "Shuttle flight control positions (1981-2011)\nMany Apollo program mission control positions were carried forward to the Space Shuttle program. However, other positions were eliminated or redefined, and new positions were added.\n\nPositions remaining generally the same:\n Booster\n FAO\n FDO\n Guidance (became guidance and procedures officer, or GPO)\n GNC\n INCO (became instrumentation and communications officer)\n\nPositions eliminated or modified:\n RETRO\n EECOM (duties split up)\n TELMU\n CONTROL\n\nAfter retirement of the Space Shuttle in 2011, the operational concept of flight control of a launched crewed vehicle was used as the basis for the Boeing CST-100 Commercial Crew vehicle starting in 2019.\n\nAssembly and checkout officer (ACO)/payloads",
        "Assembly and checkout officer (ACO)/payloads\n\nResponsible for all Space Shuttle-based activities related to construction and operation of the Space Station, including logistics and transfer items stored in a multi-purpose logistics module (MPLM) or Spacehab. Also responsible for all Shuttle payloads, from Spacehab to the Hubble Space Telescope to deployable satellites. On Shuttle missions that did not dock with the ISS, this position was known as payloads.\n\nBooster systems engineer (BOOSTER)\n\nMonitored and evaluated performance of propulsion-related aspects of the launch vehicle during prelaunch and ascent, including the main engines and solid rocket boosters.\n\nData processing system engineer (DPS)",
        "Data processing system engineer (DPS)\n\nResponsible for data processing systems in a space flight. This included monitoring the on board General Purpose Computers (GPCs), flight-critical, launch and payload data buses, the multi-function electronic display system (MEDS), solid-state mass memory (SSMM) units, flight critical and payload multiplexer/de-multiplexer (MDM) units, master timing unit (MTU), backup flight control (BFC) units and system-level software.\n\nThe Space Shuttle general purpose computers were a critical subsystem, and the vehicle cannot fly without them.\n\nEmergency, environmental, and consumables management (EECOM)",
        "The Space Shuttle general purpose computers were a critical subsystem, and the vehicle cannot fly without them.\n\nEmergency, environmental, and consumables management (EECOM)\n\nEECOM's revamped Space Shuttle responsibilities included the atmospheric pressure control and revitalization systems, the cooling systems (air, water, and freon), and the supply/waste water system.\n\nMPSR positions\n Life support – monitored atmospheric pressure control systems, // maintenance and management, air cooling equipment, waste water systems,\n Thermal – monitored water and refrigerant coolant loop systems, supply water maintenance\n\nEECOM's critical function was to maintain the systems, such as atmosphere and thermal control, that keep the crew alive.",
        "EECOM's critical function was to maintain the systems, such as atmosphere and thermal control, that keep the crew alive.\n\nElectrical generation and integrated lighting systems engineer (EGIL)\n\nMonitored cryogenic levels for the fuel cells, electrical generation and distribution systems on the spacecraft, and vehicle lighting. This was a portion of the job formerly done by EECOM.\n\nMPSR positions\n EPS – provided expert support monitoring of the fuel cells, cryo system, and electrical bus system\n\nExtravehicular activity officer (EVA)\n\nResponsible for all spacesuit and spacewalking-related tasks, equipment and plans when the EVA took place from the shuttle.\n\nFlight activities officer (FAO)",
        "Extravehicular activity officer (EVA)\n\nResponsible for all spacesuit and spacewalking-related tasks, equipment and plans when the EVA took place from the shuttle.\n\nFlight activities officer (FAO)\n\nPlanned and supported crew activities, checklists, procedures, schedules, attitude maneuvers and timelines.",
        "MPSR positions\n Attitude and pointing officer (Pointing) – Generated and maintained the attitude timeline, monitored the executions of all attitude maneuvers, provided attitude maneuver inputs for the crew, generated star pairs and attitudes for IMU aligns.\n Message and timeline support (MATS) – Created messages based on MCC inputs, created the execute package, monitored crew activities and assessed impacts to the timeline.\n Orbital communications officer (OCA) – Transferred electronic messages to the crew, synced the crews e-mail, uplinked and downlinked files for the crew.\n Timeline – Generated the pre-flight timelines for the flight plan, monitored in-flight crew activities, coordinated activities with other flight controllers.\n\nFlight dynamics officer (FDO or FIDO)",
        "Flight dynamics officer (FDO or FIDO)\n\nResponsible for the flight path of the Space Shuttle, both atmospheric and orbital. FDO monitored vehicle performance during the powered flight phase and assessed abort modes, calculated orbital maneuvers and resulting trajectories, and monitored vehicle flight profile and energy levels during re-entry.",
        "MPSR positions\n Abort support (ascent only) – provided expert support during the powered flight portion of an RTLS or TAL\n ARD support (ascent only) – maintained the abort region determinator processor which is used to predict trajectory capabilities during powered flight\n Ascent support team (ascent only) – monitored the winds and weather at the launch site, help compute day-of-launch updates\n Dynamics – maintained the inputs to the mission operation computer for all processors\n Entry console – provided expert support for entry, approach, and landing\n Entry support team (ascent and entry) – monitored the winds and weather at the various potential landing sites, prepare trajectory adjustments",
        "Entry support team (ascent and entry) – monitored the winds and weather at the various potential landing sites, prepare trajectory adjustments\n Landing support officer (LSO) team – maintained the airspace at any landing site, dispatch Search and Rescue teams if needed, act as first liaison in case of a landing outside the US\n Nav support team – responsible for maintaining the on-board navigation (telemetry) and the ground navigation (tracking)\n Profile support (rendezvous only) – assisted the FDO with rendezvous profile evaluation and determination\n Range safety team (ascent only) – tracked the falling external tank and solid rocket boosters\n Targeting (ascent only) – provided expert support for abort to orbit (ATO) or abort once around (AOA) trajectories",
        "Targeting (ascent only) – provided expert support for abort to orbit (ATO) or abort once around (AOA) trajectories\n Track – coordinated tracking site data flow and data requests\n Weather – a member of the spaceflight meteorology group who provided worldwide weather data",
        "Ground controller (GC)\n\nDirected maintenance and operation activities affecting Mission Control hardware, software and support facilities; coordinated space flight tracking and data network, and Tracking and Data Relay Satellite system with Goddard Space Flight Center.\n\nGuidance, navigation, and controls systems engineer (GNC)\n\nMonitored all shuttle guidance, navigation and control systems.\n\nMPSR positions\n GNC Support: Provided support to the orbit GNC officer during the orbit phase of flight.\n Control: Provided support to the ascent/entry GNC officer during those phases of flight.\n Sensors: Provided support to the ascent/entry GNC officer during those phases of flight.\n\nInstrumentation and communications officer (INCO)",
        "Instrumentation and communications officer (INCO)\n\nResponsible for all data, voice and video communications systems, including monitoring the configuration of in-flight communications and instrumentation systems. Duties also included monitoring the telemetry link between the vehicle and the ground, and overseeing the uplink command and control processes. The INCO was the only position that uplinked commands to the orbiter. This position was a direct evolution of the integrated communications officer from the Apollo program.",
        "MPSR positions\n RF COMM: MPSR lead and responsible for the Ku-band and S-band communication systems.\n INST: Responsible for uplinking commands and telemetry flows.\n DATA COMM: Responsible for recording and downlinking telemetry that was not streamed live and the FM communication system.",
        "Mechanical, maintenance, arm, and crew systems (MMACS)",
        "Responsible for Space Shuttle structural and mechanical systems, monitoring auxiliary power units and hydraulic systems, managing payload bay door, external tank umbilical door, vent door, radiator deploy/stow, Ku-band antenna deploy/stow, and payload retention latch operations, landing gear/deceleration systems (landing gear deploy, tires, brakes/antiskid, and drag chute deploy), and monitoring the orbiter docking system. MMACS also followed use of on board crew hardware and in-flight equipment maintenance. This represented another portion of the job formerly done by EECOM, with additional responsibilities added by the specific requirements of Space Shuttle operations. The MMACS officer served as the point of contact for PDRS, Booster, and EVA during periods in a mission when these",
        "responsibilities added by the specific requirements of Space Shuttle operations. The MMACS officer served as the point of contact for PDRS, Booster, and EVA during periods in a mission when these positions did not require constant staffing.",
        "MPSR positions\n MECH – provided expert support monitoring of mechanical, hydraulic, and landing gear systems\n MECH 2 – provided extra support during the dynamic ascent and entry phases of flight\n IFM – In-flight maintenance support\n Crew systems/escape – responsible for operations of on board crew hardware and the crew's launch and entry suits\n Photo/TV – responsible for the \"loose\" camera operation and maintenance, such as still cameras and camcorders, and the integration of video into and out of the orbiter's TV monitors\n\nPayload deployment and retrieval system (PDRS)\n\nResponsible for Space Shuttle remote manipulator system (RMS) or \"robot arm\".\n\nPropulsion engineer (PROP)",
        "Payload deployment and retrieval system (PDRS)\n\nResponsible for Space Shuttle remote manipulator system (RMS) or \"robot arm\".\n\nPropulsion engineer (PROP)\n\nManaged the reaction control thrusters and orbital maneuvering engines during all phases of flight, monitored fuel usage and propellant tank status, and calculated optimal sequences for thruster firings.\n\nMPSR Positions\nOMS & RCS Engine Officer (OREO): Monitored health of shuttle on-orbit engines\nConsumables: Monitored fuel usage and mass properties through the mission\n\nRendezvous (RNDZ)\n\nResponsible for activities such as trajectory operations related to the rendezvous and docking/capture with another spacecraft, including Mir, the ISS, and satellites such as the Hubble Space Telescope.\n\nTrajectory officer (TRAJ)",
        "Trajectory officer (TRAJ)\n\nAssisted the FDO during time-critical operations, responsible for maintaining the various processors that helped determine the shuttle's current and potential trajectories. A FDO was certified as a TRAJ first. Shares the FCR with FDO.\n\nTransoceanic abort landing communicator (TALCOM)",
        "Transoceanic abort landing communicator (TALCOM)\n\nOne of the few members of Shuttle Mission Control not physically present in Houston. If an emergency had occurred, such as loss of one or more main engine during a Space Shuttle launch, requiring the shuttle to land at one of the contingency landing sites in Africa, Europe or the Middle East, TALCOM would have assumed the role of CAPCOM providing communications with astronauts aboard the crippled orbiter. Like CAPCOM, the TALCOM role was filled by an astronaut. Three astronauts were deployed to the alternate landing sites in Zaragoza Air Base and Moron Air Base in Spain, and Istres Air Base in France. These astronauts flew aboard weather reconnaissance aircraft to provide support at the selected landing site.",
        "ISS flight control positions to 2010 \nThe International Space Station flight control positions used by NASA in Houston are different from those used by previous NASA programs. These differences exist primarily to stem the potential confusion that might otherwise follow from conflicting use of the same name in two different rooms during the same operations, such as when the space shuttle was conducting mated operations with the space station. There are also differences in the control positions because of differences in the operation of the two. The following is a list of those flight controllers located in Mission Control Center – Houston. There are several other control centers which house dozens of other flight controllers that support the vastly complex vehicle.",
        "Positions formerly used but eliminated or modified:\n\n Assembly and checkout officer (ACO) – Retired position at end of shuttle. Was responsible for the integration of assembly and activation tasks for all ISS systems and elements. Coordinated with station and shuttle flight controllers on the execution of these operations. Was also the front room position to ACO Transfer who was responsible for the exchange of cargo between the shuttle and the ISS.\n Cargo integration officer (CIO) – Former front room position that answered for ISO and PLUTO\n Station duty officer (SDO) – During early phases of ISS, when the vehicle was free-flying (no shuttle present) and uncrewed, the SDO and GC were the only positions on duty, and would call in the appropriate personnel if any problems arose.",
        "Starting in 2001, the ISS flight control room has consolidated six of the below positions into just two, to reduce staffing during low-activity periods. This concept is known as Gemini. After Assembly complete, the Gemini concept was eliminated in the realignment of the core ISS flight control positions.\n TITAN (Telemetry, Information Transfer, and Attitude Navigation) is responsible for Communication & Tracking (CATO), Command & Data Handling (ODIN), and Motion Control Systems (ADCO).",
        "TITAN (Telemetry, Information Transfer, and Attitude Navigation) is responsible for Communication & Tracking (CATO), Command & Data Handling (ODIN), and Motion Control Systems (ADCO).\n ATLAS (Atmosphere, Thermal, Lighting and Articulation Specialist) is responsible for Thermal Control (THOR), Environmental Control & Life Support (ECLSS), and Electrical Power Systems (PHALCON). ATLAS is also responsible for monitoring Robotics (ROBO) and Mechanical Systems (OSO) heaters, as those consoles are not supported during the majority of Gemini shifts.",
        "Attitude determination and control officer (ADCO)\n\nWorks in partnership with Russian controllers to determine and manage the station's orientation, controlled by the on board motion control systems. This position also plans and calculates future orientations and maneuvers for the station and is responsible for docking the ISS with other vehicles.",
        "MPSR positions\n HawkI – Pronounced (Hawk-eye) – provides expert support monitoring of all US GNC systems, leaving the ADCO to coordinate with other flight controllers and MCC-M. Hawki is actually a strung-together set of common engineering abbreviations for quantities that affect or reflect ISS attitude, primarily chosen because they fit well enough to make a name:\n H – Momentum.\n α – angular rate.\n ω – angular velocity.\n k – kinetic energy.\n I – moment of inertia.\n\nBiomedical engineer (BME)",
        "Biomedical engineer (BME)\n\nThe BME monitors health-related station systems and Crew Health Care Systems (CHeCS) equipment.  The BME provides technical and operational support for CHeCS and all other medical operations activities.  Along with the SURGEON, the BME serves as a Medical Operations Branch representative to the USOS Flight Control Team.\n\nCommunication and tracking officer (CATO)\n\nResponsible for management and operations of the U.S. communication systems, including audio, video, telemetry and commanding systems.\n\nEnvironmental control and life support system (ECLSS)",
        "Responsible for management and operations of the U.S. communication systems, including audio, video, telemetry and commanding systems.\n\nEnvironmental control and life support system (ECLSS)\n\nResponsible for the assembly and operation of systems related to atmosphere control and supply, atmosphere revitalization, cabin air temperature and humidity control, circulation, fire detection and suppression, water collection and processing and crew hygiene equipment, among other areas.\n\nMPSR Position\nACE (Atmosphere and Consumables Engineer)\n\nExtravehicular activity officer (EVA)\n\nResponsible for all spacesuit and spacewalking-related tasks, equipment and plans when the EVA takes place from the ISS.\n\nInventory and stowage officer (ISO)",
        "Extravehicular activity officer (EVA)\n\nResponsible for all spacesuit and spacewalking-related tasks, equipment and plans when the EVA takes place from the ISS.\n\nInventory and stowage officer (ISO)\n\nResponsible for the daily tracking and inventory of all US cargo on the ISS. ISO is the integrator for all cargo that is delivered to and from the ISS for ATV, HTV, Dragon, and Cygnus vehicles.\n\nIntegration systems engineer (ISE)",
        "Integration systems engineer (ISE)\n\nA specialist position, the ISE is the systems liaison between ISS and visiting vehicles that are berthed to the US side of ISS. The ISE flight control is responsible for the safety of the ISS such that the visiting vehicle can safely approach, berth, and integrate with the ISS. This includes HTV, Dragon, Cygnus, and even special missions like the deployment of Bigelow Expandable Activity Module (BEAM). ISE works closely with VVO.\n\nOnboard data interfaces and networks (ODIN)",
        "Onboard data interfaces and networks (ODIN)\n\nThe ODIN is responsible for the Command and Data Handling (C&DH) system, the Portable Computer System (PCS) computers, the Caution & Warning (C&W) system, overall responsibility for commanding, and interfaces with International Partner avionics systems. The C&DH system consists of the Multiplexer/DeMultiplexers (MDMs) which are the ISS computers. Core software in each MDM (not User Application Software), the MIL-STD-1553 data busses, Automated Payload Switches (APSs), fiber optic network, Payload Ethernet Hub Gateway (PEHG), and the Ethernet network. This does not include the Ops LAN, Station Support Computers (SSC), or file server.\n\nMPSR positions\n Resource avionics engineer (RAVEN)\n\nOperations planner (OPSPLAN)",
        "MPSR positions\n Resource avionics engineer (RAVEN)\n\nOperations planner (OPSPLAN)\n\nLeads the coordination, development and maintenance of the station's short-term plan, including crew and ground activities. The plan includes the production and uplink of the on board station plan and the coordination and maintenance of the on board inventory and stowage listings.\n\nMPSR positions\n Real time planning engineer (RPE)\n Real time planning engineer support (RPE-Support)\n Orbital communications adapter officer (OCA)\n on board data file and procedures officer (ODF)\n\nOperations support officer (OSO)",
        "Operations support officer (OSO)\n\nCharged with those logistics support functions that address on-orbit maintenance, support data and documentation, logistics information systems, maintenance data collection and maintenance analysis. The OSO is also responsible for mechanical systems—such as those used to attach new modules or truss sections to the vehicle during assembly.\n\nMPSR position\n OSO Support\n\nPlug-in port utilization officer (PLUTO)",
        "The name PLUTO is inherited from the flight controller's original role, which was to maintain and coordinate changes to the U.S. segment of the electrical plug-in plan (PiP). The PiP is the tracking of portable electronic equipment, making sure equipment connected is compatible and does not violate constraints, and will not overdraw the power source. Along with this, PLUTO is responsible for maintaining the OPSLAN (Operations Local Area Network) and the JSL (Joint Station LAN). PLUTO has remote desktop administration and monitoring capability to the network from the ground. The PLUTO is also responsible for certain Station Developmental Test Objectives, or SDTOs during the mission. This includes programming the Wireless Instrumentation System (WIS) and also remote desktop commanding for",
        "responsible for certain Station Developmental Test Objectives, or SDTOs during the mission. This includes programming the Wireless Instrumentation System (WIS) and also remote desktop commanding for ROBONAUT activities.",
        "MPSR position\n Hydra\n\nPower, heating, articulation, lighting control officer (PHALCON)\n\nManages the power generation, storage, and power distribution capabilities.\n\nRemote interface officer (RIO)",
        "Formerly known as the Russian interface officer. Responsible for integrating operations between MCC-Houston (MCC-H) and the other International Partner (IP) Control Centers. RIO is a FCR-1 position in MCC-Houston and works closely in conjunction with the Houston Support Group (HSG) teams located at the IP Control Centers:\n HSG-Moscow (HSG-M): Team of NASA Flight Controllers working with Russian Flight Controllers at MCC-Moscow (MCC-M). Responsible for integrating operations between MCC-H and MCC-M. HSG-M also has taken over operations of the US segment of ISS during Hurricanes Lili and Rita (2002 and 2005, respectively). MSFC in Huntsville took over ISS Backup Control Center operations from HSG-M in 2008.",
        "HSG-Columbus (HSG-C): Small team of NASA Flight Controllers that were responsible for integrating MCC-H and Columbus Control Center (COL-CC) operations at Oberpfaffenhofen, near Munich, Germany. Following completion of Columbus commissioning in August 2008, this team discontinued operations.\n HSG-ATV (HSG-A): Small team of NASA Flight Controllers responsible for integrating MCC-H and Autonomous Transfer Vehicle (ATV) operations at ATV-CC near Toulouse, France. Following completion of the ATV5 mission in February 2015, this team discontinued operations.",
        "HSG-Japan (HSG-J): Small team of NASA Flight Controllers responsible for integrating MCC-H and Japanese Experiment Module (JEM) and H-II Transfer Vehicle (HTV) operations at the Space Station Integration and Promotions Center (SSIPC) at Tsukuba, Japan near Tokyo. This team discontinued permanent operations in October 2008, but afterwards did temporarily support the STS-127 and HTV-1 missions.",
        "Robotics operations systems officer (ROBO)\n\nResponsible for the operations of the Canadian Mobile Servicing System (MSS), which includes a mobile base system, station robotic arm, station robotic hand or special purpose dexterous manipulator. (Call sign: ROBO) represents a joint NASA-Canadian Space Agency team of specialists to plan and execute robotic operations.\n MPSR Position: MSS Systems\n MPSR Position: MSS Task\n\nThermal operations and resources (THOR)\n\nResponsible for the assembly and operation of multiple station subsystems which collect, distribute, and reject waste heat from critical equipment and payloads.\n\nTrajectory operations officer (TOPO)",
        "Responsible for the assembly and operation of multiple station subsystems which collect, distribute, and reject waste heat from critical equipment and payloads.\n\nTrajectory operations officer (TOPO)\n\nResponsible for the station trajectory. The TOPO works in partnership with Russian controllers, ADCO, and the U.S. Space Command to maintain data regarding the station's orbital position. TOPO plans all station orbital maneuvers.\n\nVisiting vehicle officer (VVO)\n\nA specialist position, the VVO is the guidance and navigation liaison between the ISS and \"visiting vehicles\" such as Progress, Soyuz or Dragon.",
        "Visiting vehicle officer (VVO)\n\nA specialist position, the VVO is the guidance and navigation liaison between the ISS and \"visiting vehicles\" such as Progress, Soyuz or Dragon.\n\nISS flight control positions 2010–present \nAfter \"Assembly Complete\" in 2010 (which was the name given to the phase following the completion of the ISS assembly using the Space Shuttle), the core ISS flight control positions were realigned and the Gemini manning concept eliminated. While the other specialty positions – ADCO, BME, EVA, ISO, ISE, OPSPLAN, OSO, PLUTO, RIO, ROBO, TOPO, and VVO – remain the same, the new core positions are:\n\nCommunications RF on board networks utilization specialist (CRONUS)",
        "Communications RF on board networks utilization specialist (CRONUS) \n\nThis is a combination of the previous ODIN and CATO positions. Responsibilities for this group include the control and monitoring of on-board command and data systems (i.e. computers). Video cameras, both on board and external, are managed by CRONUS. The Caution And Warning System is also used to alert the crew and flight controllers to serious and dangerous safety situations. Communication radios, both for space-to-ground communication (S-Band and Ku-Band) and space-to-space communication (C2V2) are operated by CRONUS.\n MPSR Position: RAPTOR\n\nEnvironmental and Thermal operating systems (ETHOS)\n\nThis consists of the ECLSS system responsibilities as well as the internal thermal control systems from THOR.",
        "Environmental and Thermal operating systems (ETHOS)\n\nThis consists of the ECLSS system responsibilities as well as the internal thermal control systems from THOR.\n\n MPSR position: TREC\n\nStation power, articulation, thermal, and analysis (SPARTAN)\n\nThis consists of the electrical power (old PHALCON) and external thermal control systems from THOR.\n\n MPSR position: SPOC",
        "This consists of the electrical power (old PHALCON) and external thermal control systems from THOR.\n\n MPSR position: SPOC\n\nExploration Flight Test-1 (2014)\nThe Orion program, later to become part of the Artemis program, had its own flight control team, mostly derived from Space Shuttle flight control positions. The Primary Team sat in the main flight control room (FCR), while the Support Team sat in the various multi-purpose support rooms (MPSRs) nearby. All positions named below had similar responsibilities to their Shuttle counterparts. The Orion flight control team operated out of the Blue FCR, which had previously been used in the early days of ISS. As this was an uncrewed mission, the CAPCOM and SURGEON were not needed on console.",
        "Command and Data Handling (C&DH)\nResponsible for the command and data handling system, including the flight control module, the on-board storage module, the Orion data network and portions of the power and data units.\n Support Team position: C&DH Support\n\nEmergency, Environmental, and Consumables manager (EECOM)\nResponsible for spacecraft pressure control and active thermal control systems.\n Support Team position: EECOM Support\n\nElectrical Power System Officer (EPS)\nResponsible for spacecraft electrical and mechanical systems.\n Support Team position: EPS MPSR",
        "Electrical Power System Officer (EPS)\nResponsible for spacecraft electrical and mechanical systems.\n Support Team position: EPS MPSR\n\nFlight Dynamics Officer (FDO)\nResponsible for the pre-launch heading alignment update and orbit and entry trajectory predictions.\n Support Team position: DOD-H\n Support Team position: Landing Support Officer (LSO) - responsible for relaying mission status and milestones to external interfaces, such as the State Department, the Department of Defense, the recovery team, NASA Headquarters and others as required\n Support Team position: Trajectory Officer (TRAJ)\n Support Team position: Weather - responsible for providing landing and contingency weather forecasts, as well as sea state information; located in a supporting room in the Mission Control Center",
        "Flight Director (FLIGHT)\nThe responsible authority for the spacecraft between liftoff and the post-splashdown handover to the Orion recovery team. Provide a go or no-go decision for Orion's launch to the Mission Management Team and provide recommendations on operations outside the flight rules as needed.\n\nFlight Operations Directorate (FOD)\nFormerly known as MOD (Mission Operations Directorate), the console position representing the Directorate was renamed when the directorate itself was, taking the Apollo-era name of Flight Operations Directorate.\n\nGround Control Officer (GC)\nResponsible for ground data systems and data flows that interface with the Mission Control Center.",
        "Ground Control Officer (GC)\nResponsible for ground data systems and data flows that interface with the Mission Control Center.\n\nGuidance Navigation and Control Officer (GNC)\nResponsible for operations of the navigation hardware, including inertial measurement units, barometric altimeters and the GPS receiver and antennas.\n Support Team position: GNC Support\n Support Team position: Nav",
        "Guidance Officer (GUIDANCE)\nResponsible for the onboard navigation performance; launch vehicle and onboard navigation state vector quality assessments; guidance performance monitoring and associated flight test objectives evaluations; ground navigation processing and best state vector source determination; and the Mission Control Center contingency state vector update command.\n Support Team position: Pointing\n\nInstrumentation and Communications Officer (INCO)\nResponsible for Orion's communication systems, development flight instrumentation systems, video systems and recovery beacon; INCO sends all nominal and contingency commands to Orion.\n Support Team position: INCO Support\n\nPublic Affairs Officer (PAO)",
        "Public Affairs Officer (PAO)\n\nPropulsion Officer (PROP)\nResponsible for the propulsion system hardware and software.\n Support Team position: PROP Support\n\nCommercial Crew, 2019-present \n\nWhile SpaceX manages its own Mission Control Center for Dragon 2 vehicles in Hawthorne, CA, the management of the Boeing CST-100 Starliner launch, orbit, and entry operations is controlled from various flight control rooms in MCC Houston (MCC-H) collectively known as MCC-CST. The concept of operations in MCC-CST is derived from the Space Shuttle flight control room, and the following positions are largely unchanged from Shuttle responsibilities: CAPCOM, EECOM, FAO, FDO, Flight, FOD, GC, GNC, INCO, PAO, PROP, RNDZ, Surgeon, and TRAJ.",
        "Some positions were formerly Shuttle back-room positions: Crew Systems, Nav, Pointing, Profile, Timeline, and Weather\n\nAnd the positions newly invented for CST-100: CDH, FDF, MPO (combining the Shuttle's EGIL and MMACS controllers), Recovery, SDO, and Tablet\n\nNewly invented position for SpaceX Crew Dragon 2 is CORE (replaces CAPCOM among other roles)\n\nCommand and Data Handling (CDH)\n\nCDH is responsible for monitoring the health and status of the CST-100 avionics systems including the on board computers, display units, keyboards, on board data bus, wireless networks, tablets, on board software, data services for payloads and more.\n\nCrew Systems and Photo/TV",
        "Crew Systems and Photo/TV\n\nThe roles and responsibilities for CREW SYSTEMS include developing operations products supporting crew and cargo integration and being hardware experts for flight crew equipment that deal with crew escape, human habitability, productivity, and well-being. The docking system centerline camera and digital imagery experts from Photo/ TV group will be relied upon for direct mission support and training.\n\nEmergency, Environmental, and Consumables Management (EECOM)",
        "Emergency, Environmental, and Consumables Management (EECOM)\n\nEECOM is responsible for the environmental control and life support systems; monitoring and control of the active thermal control subsystems; atmosphere; suits; consumables management and reporting; cooling services for payloads and ingress/egress support. EECOM leads an integrated team response to emergencies (fire/cabin leak/toxic atmosphere/loss of cooling), and to internal and EECOM system leaks.\n\nFlight Data File (FDF)",
        "Flight Data File (FDF)\n\nFDF manages the development and publication of FDF books for use by the crew and flight controllers. FDF provides real-time support for crew procedures and other FDF related activities. Duties include coordinating technical changes to procedures with flight directors, flight controllers, crew, and international partners. The FDF develops software requirements for procedures tools.\n\nFlight Activities Officer (FAO)",
        "Flight Activities Officer (FAO)\n\nFAO leads the coordination and integration of the crew activities, ground activities and attitude timeline into an integrated flight plan that meets the mission requirements defined by the program. For missions to the International Space Station (ISS), the FAO also works with the ISS Operations Planner to integrate CST-100 vehicle operations and preparations into the station timeline during both the joint-mission timeframe and during quiescent operations.\n\nFlight Dynamics Officer (FDO)\n\nFDO is responsible for pre-mission planning and real-time execution of all CST-100 trajectory operations, including launch, undocking re-entry and landing.\n\nGround Control (GC)",
        "FDO is responsible for pre-mission planning and real-time execution of all CST-100 trajectory operations, including launch, undocking re-entry and landing.\n\nGround Control (GC)\n\nThe GC team is responsible for the ground systems infrastructure and ground communications necessary to perform planning, training, testing, execution and evaluation of human spaceflight mission operations at the Johnson Space Center Mission Control Center for Boeing CST Mission Operations (MCC-CST).\n\nGuidance, Navigation, and Control (GNC)\n\nGNC manages guidance, navigation and control hardware, and associated software during all phases of flight, including GPS, attitude controllers, the Vision-based Electro-optical Sensor Tracking Assemblies (VESTAs) and more.\n\nIntegrated Communications Officer (INCO)",
        "Integrated Communications Officer (INCO)\n\nINCO is responsible for monitoring the health and status of the communications avionics including the Space-to-Ground (S/G) and Space-to-Space (S/S) systems, handheld radio communication, command encryption, audio systems and associated loose equipment functionality such as handheld microphones and headsets.\n\nMechanical and Power Officer (MPO)\n\nMPO is responsible for the CST-100 spacecraft electrical, mechanical, structural, and landing and recovery systems. These systems include batteries, solar arrays, power converters, interior lighting, vehicle structure, thermal protection, parachutes, airbags, crew hardware and more.\n\nNavigation (NAV)",
        "Navigation (NAV)\n\nNAV is responsible for ensuring both the on board and ground segments of the CST-100 navigation system is operating properly. NAV monitors performance of the on board navigation hardware and software, sensor status and performance, acceptability of sensor data, navigation convergence, VESTA performance and the VESTA Ground Station. NAV supports the GNC officer for issues related to relative and inertial navigation hardware, and for inertial navigation performance. NAV supports FDO for relative navigation performance monitoring and troubleshooting.\n\nPublic Affairs Officer (PAO)",
        "Public Affairs Officer (PAO)\n\nPAO duties will be shared between NASA and Boeing. PAO coordinates news media events between the news media and the crew and/or Mission Control, and provides mission commentary to supplement and explain air-to-ground transmissions and flight control operations to the news media and the public.\n\nPointing\n\nThe Pointing console is responsible the integration of all CST-100 Tracking and Data Relay Satellite (TDRS) communication requirements, communication predictions and unique target lines-of-sight analysis for payloads and on board systems. Pointing also provides attitude optimization to support unique pointing requirements, as needed.\n\nProfile",
        "Profile\n\nThe Profile console monitors the CST-100 relative trajectory and translational maneuvers to ensure performance within defined limits. Profile assists in monitoring the progress of crew and automated procedures related to rendezvous and proximity operations. Profile monitors vehicle compliance with applicable flight rules and provides to the Rendezvous position a go/no-go recommendation prior to Authority to Proceed (ATP) points. Profile maintains awareness of potential vehicle automated responses to failure conditions and the resulting abort trajectories.\n\nPropulsion (PROP)",
        "Propulsion (PROP)\n\nPROP is responsible for all aspects of the operation and management of the propulsion system hardware and software used during all phases of flight. This includes thruster performance and propellant usage, translation burns and attitude control maneuvers, and consumables budgeting, management and reporting.\n\nRecovery\n\nThe Recovery position is responsible for planning CST-100 recovery and executing recovery operations once the vehicle has landed.\n\nRendezvous (RNDZ)",
        "Recovery\n\nThe Recovery position is responsible for planning CST-100 recovery and executing recovery operations once the vehicle has landed.\n\nRendezvous (RNDZ)\n\nThe Rendezvous position monitors the CST-100 during integrated operations with the space station and ensures that all space station trajectory safety requirements are satisfied. Rendezvous is the primary interface to the space station Visiting Vehicle Officer (VVO) and monitors relative navigation, guidance, and trajectory performance in the proximity operations, docking, separation and flyaround phases of flight.\n\nStarliner Duty Officer (SDO)",
        "Starliner Duty Officer (SDO)\n\nFor the OFT mission only, the SDO is responsible for monitoring the CST-100 while it is docked to the space station in a quiescent configuration while the remainder of the CST-100 Flight Control Team is on-call. The SDO is responsible for leading the ground and crew response to Starliner events that result in cautions or warnings on the space station.\n\nTablet\n\nThe Tablet position monitors crew usage of the tablet devices and provides assistance/advice to the crew as needed.\n\nTimeline",
        "Tablet\n\nThe Tablet position monitors crew usage of the tablet devices and provides assistance/advice to the crew as needed.\n\nTimeline\n\nThe Timeline position assists the FAO in all aspects of preflight mission planning and coordination, and in real-time planning and replanning operations. TIMELINE generates the pre-flight timelines for the flight plan, monitors in-flight crew activities and coordinates activities with other flight controllers.\n\nTrajectory (TRAJ)",
        "Trajectory (TRAJ)\n\nTRAJ tracks the spacecraft's position in orbit to support acquisitions, plotting, external notifications, conjunction screening, and debris conjunction message evaluation and notification. TRAJ coordinates trajectory planning and events with the mission planning team, and is the primary member of the team responsible for running the CST-100 simulation to accomplish replanning and position update tasks.\n\nWeather",
        "Weather\n\nThe Weather officer provides weather forecasts and real-time weather observations for launch and landing operations to the mission management community, Flight Director, and flight control team. The Weather officer manages meteorological forecasting models and computer systems that access and assemble radar and satellite imagery, and provides mission-critical inputs to the flight director for go-for-launch and go-for-deorbit decisions.\n\nCrew Operations and Resources Engineer (CORE) \nCORE or Crew Operations and Resources Engineer is SpaceX Crew Dragon 2 specific position that replaces CAPCOM and is the point person between Mission Control and crew on board the Dragon 2 spacecraft.\n\nSee also",
        "Space centers and mission control centers\n Baikonur Cosmodrome (launch control center for the Russian Federal Space Agency, Tyuratam, Kazakhstan)\n Beijing Aerospace Command and Control Center (mission control center for the China National Space Administration, Beijing, People's Republic of China)\n Christopher C. Kraft Jr. Mission Control Center (mission control center for NASA, Houston, Texas)\n John H. Chapman Space Centre, the Canadian Space Agency Robotics Misison Control centre, Longueuil, Quebec\n European Space Operations Centre (mission control center for the European Space Agency, Darmstadt, Germany)\n German Space Operations Center (mission control center for the German Aerospace Center, including Columbus Control Centre for the European Space Agency, Oberpfaffenhofen, Germany)",
        "German Space Operations Center (mission control center for the German Aerospace Center, including Columbus Control Centre for the European Space Agency, Oberpfaffenhofen, Germany)\n Guiana Space Centre (launch control center for the European Space Agency, the French space agency CNES, and the commercial Arianespace, Kourou, French Guiana)\n Kennedy Space Center (NASA launch center, Cape Canaveral, Florida)\n Johnson Space Center (NASA field center, Houston, Texas)\n Payload Operations and Integration Center (Marshall Space Flight Center, Huntsville, AL)\n RKA Mission Control Center (mission control center for the Russian Federal Space Agency, Korolyov, Russia)\n SpaceX Headquarters and Mission Control Center (mission control center for SpaceX Dragon 2, Hawthorne, California)",
        "SpaceX Headquarters and Mission Control Center (mission control center for SpaceX Dragon 2, Hawthorne, California)\n Tanegashima Space Center (launch control center for JAXA, Tanegashima Island, Japan)\n Tsukuba Space Center (mission control center for JAXA, Tsukuba, Japan)",
        "Current and former NASA human space flight programs\n Mercury program\n Gemini program\n Apollo program\nSkylab\n Space Shuttle program\n International Space Station\n\nFormer NASA flight controllers\n John Aaron, EECOM\n Steve Bales, GUIDO\n Jay Greene, FDO, Range Safety, Flight Director\n John Hodge, Flight Director\n Christopher C. Kraft, Jr., Flight Director\n Eugene Kranz, Flight Director\n Sy Liebergot, EECOM\n Glynn Lunney, FDO, Flight Director\n Roger Balettie, FDO\n William (Bill) Gravett, PHALCON (Apollo), EGIL (STS)\n\nReferences\n\nSources\n\nExternal links \n National Park Service mission control page\n Space Station Live!",
        "References\n\nSources\n\nExternal links \n National Park Service mission control page\n Space Station Live!\n\nFlight Dynamics Officer (FDO)\n Former FDO Roger Balettie\n What is a FDO?\n What is the FDO looking at?\n Former FDO Chris Edelen\n Former Targeting Michael Grabois, behind the scenes at a launch from the MPSR"
    ],
    [
        "IBM AIX\nAIX (Advanced Interactive eXecutive, pronounced  ,) is a series of proprietary Unix operating systems developed and sold by IBM for several of its computer platforms.\n\nBackground \nOriginally released for the IBM RT PC RISC workstation in 1986, AIX has supported a wide variety of hardware platforms, including the IBM RS/6000 series and later Power and PowerPC-based systems, IBM System i, System/370 mainframes, PS/2 personal computers, and the Apple Network Server. It is currently supported on IBM Power Systems alongside IBM i and Linux.",
        "AIX is based on UNIX System V with 4.3BSD-compatible extensions. It is certified to the UNIX 03 and UNIX V7 marks of the Single UNIX Specification, beginning with AIX versions 5.3 and 7.2 TL5 respectively. Older versions were previously certified to the UNIX 95 and UNIX 98 marks.\n\nAIX was the first operating system to have a journaling file system, and IBM has continuously enhanced the software with features such as processor, disk and network virtualization, dynamic hardware resource allocation (including fractional processor units), and reliability engineering ported from its mainframe designs.\n\nHistory",
        "History\n\nUnix started life at AT&T's Bell Labs research center in the early 1970s, running on DEC minicomputers. By 1976, the operating system was in use at various academic institutions, including Princeton, where Tom Lyon and others ported it to the S/370, to run as a guest OS under VM/370. This port would later grow out to become UTS, a mainframe Unix offering by IBM's competitor Amdahl Corporation.\nIBM's own involvement in Unix can be dated to 1979, when it assisted Bell Labs in doing its own Unix port to the 370 (to be used as a build host for the 5ESS switch's software). In the process, IBM made modifications to the TSS/370 hypervisor to better support Unix.",
        "It took until 1985 for IBM to offer its own Unix on the S/370 platform, IX/370, which was developed by Interactive Systems Corporation and intended by IBM to compete with Amdahl UTS. The operating system offered special facilities for interoperating with PC/IX, Interactive/IBM's version of Unix for IBM PC compatible hardware, and was licensed at $10,000 per sixteen concurrent users.\n\nAIX Version 1, introduced in 1986 for the IBM RT PC workstation, was based on UNIX System V Releases 1 and 2. In developing AIX, IBM and Interactive Systems Corporation (whom IBM contracted) also incorporated source code from 4.2 and 4.3 BSD UNIX.",
        "Among other variants, IBM later produced AIX Version 3 (also known as AIX/6000), based on System V Release 3, for their POWER-based RS/6000 platform. Since 1990, AIX has served as the primary operating system for the RS/6000 series (later renamed IBM eServer pSeries, then IBM System p, and now IBM Power Systems). AIX Version 4, introduced in 1994, added symmetric multiprocessing with the introduction of the first RS/6000 SMP servers and continued to evolve through the 1990s, culminating with AIX 4.3.3 in 1999. Version 4.1, in a slightly modified form, was also the standard operating system for the Apple Network Server systems sold by Apple Computer to complement the Macintosh line.",
        "In the late 1990s, under Project Monterey, IBM and the Santa Cruz Operation planned to integrate AIX and UnixWare into a single 32-bit/64-bit multiplatform UNIX with particular emphasis on running on Intel IA-64 (Itanium) architecture CPUs.  A beta test version of AIX 5L for IA-64 systems was released, but according to documents released in the SCO v. IBM lawsuit, less than forty licenses for the finished Monterey Unix were ever sold before the project was terminated in 2002. In 2003, the SCO Group alleged that (among other infractions) IBM had misappropriated licensed source code from UNIX System V Release 4 for incorporation into AIX; SCO subsequently withdrew IBM's license to develop and distribute AIX.  IBM maintains that their license was irrevocable, and continued to sell and",
        "UNIX System V Release 4 for incorporation into AIX; SCO subsequently withdrew IBM's license to develop and distribute AIX.  IBM maintains that their license was irrevocable, and continued to sell and support the product until the litigation was adjudicated.",
        "AIX was a component of the 2003 SCO v. IBM lawsuit, in which the SCO Group filed a lawsuit against IBM, alleging IBM contributed SCO's intellectual property to the Linux codebase.  The SCO Group, who argued they were the rightful owners of the copyrights covering the Unix operating system, attempted to revoke IBM's license to sell or distribute the AIX operating system.  In March 2010, a jury returned a verdict finding that Novell, not the SCO Group, owns the rights to Unix.",
        "AIX 6 was announced in May 2007, and it ran as an open beta from June 2007 until the general availability (GA) of AIX 6.1 on November 9, 2007.  Major new features in AIX 6.1 included full role-based access control, workload partitions (which enable application mobility), enhanced security (Addition of AES encryption type for NFS v3 and v4), and Live Partition Mobility on the POWER6 hardware.",
        "AIX 7.1 was announced in April 2010, and an open beta ran until general availability of AIX 7.1 in September 2010. Several new features, including better scalability, enhanced clustering and management capabilities were added. AIX 7.1 includes a new built-in clustering capability called Cluster Aware AIX. AIX is able to organize multiple LPARs through the multipath communications channel to neighboring CPUs, enabling very high-speed communication between processors. This enables multi-terabyte memory address range and page table access to support global petabyte shared memory space for AIX POWER7 clusters so that software developers can program a cluster as if it were a single system, without using message passing (i.e. semaphore-controlled Inter-process Communication). AIX administrators",
        "clusters so that software developers can program a cluster as if it were a single system, without using message passing (i.e. semaphore-controlled Inter-process Communication). AIX administrators can use this new capability to cluster a pool of AIX nodes. By default, AIX V7.1 pins kernel memory and includes support to allow applications to pin their kernel stack. Pinning kernel memory and the kernel stack for applications with real-time requirements can provide performance improvements by ensuring that the kernel memory and kernel stack for an application is not paged out.",
        "AIX 7.2 was announced in October 2015, and released in December 2015. The principal feature of AIX 7.2 is the Live Kernel Update capability, which allows OS fixes to replace the entire AIX kernel with no impact to applications, by live migrating workloads to a temporary surrogate AIX OS partition while the original OS partition is patched. AIX 7.2 was also restructured to remove obsolete components. The networking component, bos.net.tcp.client was repackaged to allow additional installation flexibility. Unlike AIX 7.1, AIX 7.2 is only supported on systems based on POWER7 or later processors.\n\nIn January 2023, IBM moved development of AIX to its Indian subsidiary.\n\nSupported hardware platforms",
        "In January 2023, IBM moved development of AIX to its Indian subsidiary.\n\nSupported hardware platforms\n\nIBM RT PC\nThe original AIX (sometimes called AIX/RT) was developed for the IBM RT PC workstation by IBM in conjunction with Interactive Systems Corporation, who had previously ported UNIX System III to the IBM PC for IBM as PC/IX. According to its developers, the AIX source (for this initial version) consisted of one million lines of code. Installation media consisted of eight 1.2M floppy disks.  The RT was based on the IBM ROMP microprocessor, the first commercial RISC chip. This was based on a design pioneered at IBM Research (the IBM 801).",
        "One of the novel aspects of the RT design was the use of a microkernel, called Virtual Resource Manager (VRM).  The keyboard, mouse, display, disk drives and network were all controlled by a microkernel.  One could \"hotkey\" from one operating system to the next using the Alt-Tab key combination. Each OS in turn would get possession of the keyboard, mouse and display. Besides AIX v2, the PICK OS also included this microkernel.",
        "Much of the AIX v2 kernel was written in the PL/8 programming language, which proved troublesome during the migration to AIX v3.  AIX v2 included full TCP/IP networking, as well as SNA and two networking file systems: NFS, licensed from Sun Microsystems, and Distributed Services (DS).  DS had the distinction of being built on top of SNA, and thereby being fully compatible with DS on  and on midrange systems running OS/400 through IBM i.  For the graphical user interfaces, AIX v2 came with the X10R3 and later the X10R4 and X11 versions of the X Window System from MIT, together with the Athena widget set.  Compilers for Fortran and C were available.\n\nIBM PS/2 series",
        "IBM PS/2 series\n\nAIX PS/2 (also known as AIX/386) was developed by Locus Computing Corporation under contract to IBM. AIX PS/2, first released in October 1988, ran on IBM PS/2 personal computers with Intel 386 and compatible processors.",
        "The product was announced in September 1988 with a baseline tag price of $595, although some utilities like uucp were included in a separate Extension package priced at $250. nroff and troff for AIX were also sold separately in a Text Formatting System package priced at $200. The TCP/IP stack for AIX PS/2 retailed for another $300. The X Window System package was priced at $195, and featured a graphical environment called the AIXwindows Desktop, based on IXI's X.desktop. The C and FORTRAN compilers each had a price tag of $275. Locus also made available their DOS Merge virtual machine environment for AIX, which could run MS DOS 3.3 applications inside AIX; DOS Merge was sold separately for another $250. IBM also offered a $150 AIX PS/2 DOS Server Program, which provided file server and",
        "for AIX, which could run MS DOS 3.3 applications inside AIX; DOS Merge was sold separately for another $250. IBM also offered a $150 AIX PS/2 DOS Server Program, which provided file server and print server services for client computers running PC DOS 3.3.",
        "The last version of PS/2 AIX is 1.3. It was released in 1992 and announced to add support for non-IBM (non-microchannel) computers as well. Support for PS/2 AIX ended in March 1995.",
        "IBM mainframes",
        "In 1988, IBM announced AIX/370, also developed by Locus Computing.  AIX/370 was IBM's fourth attempt to offer Unix-like functionality for their mainframe line, specifically the System/370 (the prior versions were a TSS/370-based Unix system developed jointly with AT&T c.1980, a VM/370-based system named VM/IX developed jointly with Interactive Systems Corporation c.1984, and a VM/370-based version of TSS/370 named IX/370 which was upgraded to be compatible with UNIX System V).  AIX/370 was released in 1990 with functional equivalence to System V Release 2 and 4.3BSD as well as IBM enhancements.  With the introduction of the ESA/390 architecture, AIX/370 was replaced by AIX/ESA in 1991, which was based on OSF/1, and also ran on the System/390 platform.  This development effort was made",
        "With the introduction of the ESA/390 architecture, AIX/370 was replaced by AIX/ESA in 1991, which was based on OSF/1, and also ran on the System/390 platform.  This development effort was made partly to allow IBM to compete with Amdahl UTS.  Unlike AIX/370, AIX/ESA ran both natively as the host operating system, and as a guest under VM.  AIX/ESA, while technically advanced, had little commercial success, partially because UNIX functionality was added as an option to the existing mainframe operating system, MVS, as MVS/ESA SP Version 4 Release 3 OpenEdition in 1994, and continued as an integral part of MVS/ESA SP Version 5, OS/390 and z/OS, with the name eventually changing from OpenEdition to Unix System Services. IBM also provided OpenEdition in VM/ESA Version 2 through z/VM.",
        "IA-64 systems\nAs part of Project Monterey, IBM released a beta test version of AIX 5L for the IA-64 (Itanium) architecture in 2001, but this never became an official product due to lack of interest.\n\nApple Network Servers\nThe Apple Network Server (ANS) systems were PowerPC-based systems designed by Apple Computer to have numerous high-end features that standard Apple hardware did not have, including swappable hard drives, redundant power supplies, and external monitoring capability. These systems were more or less based on the Power Macintosh hardware available at the time but were designed to use AIX (versions 4.1.4 or 4.1.5) as their native operating system in a specialized version specific to the ANS called AIX for Apple Network Servers.",
        "AIX was only compatible with the Network Servers and was not ported to standard Power Macintosh hardware. It should not be confused with A/UX, Apple's earlier version of Unix for 68k-based Macintoshes.\n\nPOWER ISA/PowerPC/Power ISA-based systems\n\nThe release of AIX version 3 (sometimes called AIX/6000) coincided with the announcement of the first POWER1-based IBM RS/6000 models in 1990.",
        "AIX v3 innovated in several ways on the software side. It was the first operating system to introduce the idea of a journaling file system, JFS, which allowed for fast boot times by avoiding the need to ensure the consistency of the file systems on disks (see fsck) on every reboot. Another innovation was shared libraries which avoid the need for static linking from an application to the libraries it used. The resulting smaller binaries used less of the hardware RAM to run, and used less disk space to install. Besides improving performance, it was a boon to developers: executable binaries could be in the tens of kilobytes instead of a megabyte for an executable statically linked to the C library. AIX v3 also scrapped the microkernel of AIX v2, a contentious move that resulted in v3",
        "could be in the tens of kilobytes instead of a megabyte for an executable statically linked to the C library. AIX v3 also scrapped the microkernel of AIX v2, a contentious move that resulted in v3 containing no PL/8 code and being somewhat more \"pure\" than v2.",
        "Other notable subsystems included:",
        "IRIS GL, a 3D rendering library, the progenitor of OpenGL. IRIS GL was licensed by IBM from SGI in 1987, then still a fairly small company, which had sold only a few thousand machines at the time. SGI also provided the low-end graphics card for the RS/6000, capable of drawing 20,000 gouraud-shaded triangles per second. The high-end graphics card was designed by IBM, a follow-on to the mainframe-attached IBM 5080, capable of rendering 990,000 vectors per second.\n PHIGS, another 3D rendering API, popular in automotive CAD/CAM circles, and at the core of CATIA.\n Full implementation of version 11 of the X Window System, together with Motif as the recommended widget toolkit and window manager.",
        "Full implementation of version 11 of the X Window System, together with Motif as the recommended widget toolkit and window manager.\n Network file systems: NFS from Sun; AFS, the Andrew File System; and DFS, the Distributed File System.\n NCS, the Network Computing System, licensed from Apollo Computer (later acquired by HP).\n DPS on-screen display system. This was notable as a \"plan B\" in case the X11+Motif combination failed in the marketplace. However, it was highly proprietary, supported only by Sun, NeXT, and IBM. This cemented its failure in the marketplace in the face of the open systems challenge of X11+Motif and its lack of 3D capability.",
        "In addition, AIX applications can run in the PASE subsystem under IBM i.\n\nSource code \nIBM formerly made the AIX for RS/6000 source code available to customers for an additional fee; in 1991, IBM customers could order the AIX 3.0 source code for a one-time charge of US$60,000; subsequently, IBM released the AIX 3.1 source code in 1992,  and AIX 3.2 in 1993. These source code distributions excluded certain files (authored by third-parties) which IBM did not have rights to redistribute, and also excluded layered products such as the MS-DOS emulator and the C compiler. Furthermore, in order to be able to license the AIX source code, the customer first had to procure source code license agreements with AT&T and the University of California, Berkeley.\n\nVersions",
        "Versions\n\nPOWER/PowerPC/Power ISA releases",
        "AIX V7.3, December 10, 2021\n Requires POWER8 or newer CPUs\n AIX V7.2, October 5, 2015\n Live update for Interim Fixes, Service Packs and Technology Levels replaces the entire AIX kernel without impacting applications\n Flash based filesystem caching\n Cluster Aware AIX automation with repository replacement mechanism\n SRIOV-backed VNIC, or dedicated VNIC virtualized network adapter support \n RDSv3 over RoCE adds support of the Oracle RDSv3 protocol over the Mellanox Connect RoCE adapters\n Supports secure boot on POWER9 systems.\n Requires POWER7 or newer CPUs\n AIX V7.1, September 10, 2010\n Support for 256 cores / 1024 threads in a single LPAR\n The ability to run AIX V5.2 or V5.3 inside of a Workload Partition\n An XML profile based system configuration management utility",
        "Support for 256 cores / 1024 threads in a single LPAR\n The ability to run AIX V5.2 or V5.3 inside of a Workload Partition\n An XML profile based system configuration management utility\n Support for export of Fibre Channel adapters to WPARs\n VIOS disk support in a WPAR\n Cluster Aware AIX\n AIX Event infrastructure\n Role-based access control (RBAC) with domain support for multi-tenant environments\n Requires POWER4 or newer CPUs\n AIX V6.1, November 9, 2007\n Workload Partitions (WPARs) operating system-level virtualization\n Live Application Mobility\n Live Partition Mobility\n Security\n Role Based Access Control RBAC\n AIX Security Expert a system and network security hardening tool\n Encrypting JFS2 filesystem\n Trusted AIX\n Trusted Execution",
        "Live Partition Mobility\n Security\n Role Based Access Control RBAC\n AIX Security Expert a system and network security hardening tool\n Encrypting JFS2 filesystem\n Trusted AIX\n Trusted Execution\n Integrated Electronic Service Agent for auto error reporting\n Concurrent Kernel Maintenance\n Kernel exploitation of POWER6 storage keys\n ProbeVue dynamic tracing\n Systems Director Console for AIX\n Integrated filesystem snapshot\n Requires POWER4 or newer CPUs\n AIX 6 withdrawn from Marketing effective April 2016 and from Support effective April 2017\n AIX 5L 5.3, August 13, 2004, end of support April 30, 2012\n NFS Version 4\n Advanced Accounting\n Virtual SCSI\n Virtual Ethernet\n Exploitation of Simultaneous multithreading (SMT)\n Micro-Partitioning enablement\n POWER5 exploitation\n JFS2 quotas",
        "NFS Version 4\n Advanced Accounting\n Virtual SCSI\n Virtual Ethernet\n Exploitation of Simultaneous multithreading (SMT)\n Micro-Partitioning enablement\n POWER5 exploitation\n JFS2 quotas\n Ability to shrink a JFS2 filesystem\n Kernel scheduler has been enhanced to dynamically increase and decrease the use of virtual processors.\n AIX 5L 5.2, October 18, 2002, end of support April 30, 2009\n Ability to run on the IBM BladeCenter JS20 with the PowerPC 970\n Minimum level required for POWER5 hardware\n MPIO for Fibre Channel disks\n iSCSI Initiator software\n Participation in Dynamic LPAR\n Concurrent I/O (CIO) feature introduced for JFS2 released in Maintenance Level 01 in May 2003\n AIX 5L 5.1, May 4, 2001, end of support April 1, 2006",
        "Participation in Dynamic LPAR\n Concurrent I/O (CIO) feature introduced for JFS2 released in Maintenance Level 01 in May 2003\n AIX 5L 5.1, May 4, 2001, end of support April 1, 2006\n Ability to run on an IA-64 architecture processor, although this never went beyond beta.\n Minimum level required for POWER4 hardware and the last release that worked on the Micro Channel architecture\n 64-bit kernel, installed but not activated by default\n JFS2\n Ability to run in a Logical Partition on POWER4\n The L stands for Linux affinity\n Trusted Computing Base (TCB)\n Support for mirroring with striping\n AIX 4.3.3, September 17, 1999\n Online backup function\n Workload Manager (WLM)\n Introduction of topas utility\n AIX 4.3.2, October 23, 1998\n AIX 4.3.1, April 24, 1998",
        "AIX 4.3.3, September 17, 1999\n Online backup function\n Workload Manager (WLM)\n Introduction of topas utility\n AIX 4.3.2, October 23, 1998\n AIX 4.3.1, April 24, 1998\n First TCSEC security evaluation, completed December 18, 1998\n AIX 4.3, October 31, 1997\n Ability to run on 64-bit architecture CPUs\n IPv6\n Web-based System Manager\n AIX 4.2.1, April 25, 1997\n NFS Version 3\n Y2K-compliant \n AIX 4.2, May 17, 1996\n AIX 4.1.5, November 8, 1996\n AIX 4.1.4, October 20, 1995\n AIX 4.1.3, July 7, 1995\n CDE 1.0 became the default GUI environment, replacing the AIXwindows Desktop.\n AIX 4.1.1, October 28, 1994\n AIX 4.1, August 12, 1994\n AIX Ultimedia Services introduced (multimedia drivers and applications)\n AIX 4.0, 1994\n Run on RS/6000 systems with PowerPC processors and PCI busses.",
        "AIX 4.1, August 12, 1994\n AIX Ultimedia Services introduced (multimedia drivers and applications)\n AIX 4.0, 1994\n Run on RS/6000 systems with PowerPC processors and PCI busses.\n AIX 3.2.5, October 15, 1993\n AIX 3.2 1992\n AIX 3.1, (General Availability) February 1990\n Journaled File System (JFS) filesystem type\n AIXwindows Desktop (based on X.desktop from IXI Limited)\n AIX 3.0 1989 (Early Access) \n LVM (Logical Volume Manager) was incorporated into OSF/1, and in 1995 for HP-UX, and the Linux LVM implementation is similar to the HP-UX LVM implementation.\n SMIT was introduced.",
        "IBM System/370 releases\n AIX/370 Version 1 Release 1\n Announced March 15, 1988\n Available February 16, 1989\n Does not run in XA, ESA or z mode\n\n AIX/370 Version 1 Release 2.1\n Announced February 5, 1991\n Available February February 22, 1991\n Withdrawn December 31, 1992\n Does not run in XA, ESA or z mode\n\n AIX/ESA Version 2 Release 1\n Announced March 31, 1992\n Available June 26, 1992\n Withdrawn Jun 19, 1993\n Runs only in S/370-ESA mode\n AIX/ESA Version 2 Release 2\n Announced December 15, 1992\n Available February 26, 1993\n Withdrawn Jun 19, 1993\n Runs only in S/370-ESA mode",
        "IBM PS/2 releases\n AIX PS/2 v1.3, October 1992\n Withdrawn from sale in US, March 1995\n Patches supporting IBM ThinkPad 750C family of notebook computers, 1994\n Patches supporting non PS/2 hardware and systems, 1993\n AIX PS/2 v1.2.1, May 1991\n AIX PS/2 v1.2, March 1990\n AIX PS/2 v1.1, March 1989\n\nIBM RT releases\n AIX RT v2.2.1, March 1991\n AIX RT v2.2, March 1990\n AIX RT v2.1, March 1989\n X-Windows included on installation media \n AIX RT v1.1, 1986\n AIX RT v1.0, 1985\n\nUser interfaces\n\nThe default shell was Bourne shell up to AIX version 3, but was changed to KornShell (ksh88) in version 4 for XPG4 and POSIX compliance.",
        "User interfaces\n\nThe default shell was Bourne shell up to AIX version 3, but was changed to KornShell (ksh88) in version 4 for XPG4 and POSIX compliance.\n\nGraphical\nThe Common Desktop Environment (CDE) is AIX's default graphical user interface. As part of Linux Affinity and the free AIX Toolbox for Linux Applications (ATLA), open-source KDE Plasma Workspaces and GNOME desktop are also available.\n\nSystem Management Interface Tool",
        "SMIT is the System Management Interface Tool for AIX. It allows a user to navigate a menu hierarchy of commands, rather than using the command line. Invocation is typically achieved with the command smit. Experienced system administrators make use of the F6 function key which generates the command line that SMIT will invoke to complete it.\nSMIT also generates a log of commands that are performed in the smit.script file. The smit.script file automatically records the commands with the command flags and parameters used. The smit.script file can be used as an executable shell script to rerun system configuration tasks. SMIT also creates the smit.log file, which contains additional detailed information that can be used by programmers in extending the SMIT system.",
        "smit and smitty refer to the same program, though smitty invokes the text-based version, while smit will invoke an X Window System based interface if possible; however, if smit determines that X Window System capabilities are not present, it will present the text-based version instead of failing.  Determination of X Window System capabilities is typically performed by checking for the existence of the DISPLAY variable.\n\nDatabase\nObject Data Manager (ODM) is a database of system information integrated into AIX, analogous to the registry in Microsoft Windows. A good understanding of the ODM is essential for managing AIX systems.",
        "Data managed in ODM is stored and maintained as objects with associated attributes. Interaction with ODM is possible via application programming interface (API) library for programs, and command-line utilities such as odmshow, odmget, odmadd, odmchange and odmdelete for shell scripts and users.  SMIT and its associated AIX commands can also be used to query and modify information in the ODM. ODM is stored on disk using Berkeley DB files.\n\nExample of information stored in the ODM database are:\nNetwork configuration\nLogical volume management configuration\nInstalled software information\nInformation for logical devices or software drivers\nList of all AIX supported devices\nPhysical hardware devices installed and their configuration\nMenus, screens and commands that SMIT uses",
        "See also\n AOS, IBM's educational-market port of 4.3BSD\n IBM PowerHA SystemMirror (formerly HACMP)\n List of Unix systems\n nmon\n Operating systems timeline\n Service Update Management Assistant\n Vital Product Data (VPD)\n\nReferences\n\nExternal links\n IBM AIX\n\nIBM operating systems\nPower ISA operating systems\nPowerPC operating systems\nIBM Aix\nObject-oriented database management systems\n1986 software"
    ],
    [
        "Kolmogorov complexity\nIn algorithmic information theory (a subfield of computer science and mathematics), the Kolmogorov complexity of an object, such as a piece of text, is the length of a shortest computer program (in a predetermined programming language) that produces the object as output. It is a measure of the computational resources needed to specify the object, and is also known as algorithmic complexity, Solomonoff–Kolmogorov–Chaitin complexity, program-size complexity, descriptive complexity, or algorithmic entropy. It is named after Andrey Kolmogorov, who first published on the subject in 1963 and is a generalization of classical information theory.",
        "The notion of Kolmogorov complexity can be used to state and prove impossibility results akin to Cantor's diagonal argument, Gödel's incompleteness theorem, and Turing's halting problem.\nIn particular, no program P computing a lower bound for each text's Kolmogorov complexity can return a value essentially larger than P's own length (see section ); hence no single program can compute the exact Kolmogorov complexity for infinitely many texts.\n\nDefinition\nConsider the following two strings of 32 lowercase letters and digits:\n\n  abababababababababababababababab , and\n  4c1j5b2p0cv4w1x8rx2y39umgw5q85s7",
        "Definition\nConsider the following two strings of 32 lowercase letters and digits:\n\n  abababababababababababababababab , and\n  4c1j5b2p0cv4w1x8rx2y39umgw5q85s7\n\nThe first string has a short English-language description, namely \"write ab 16 times\", which consists of 17 characters. The second one has no obvious simple description (using the same character set) other than writing down the string itself, i.e., \"write 4c1j5b2p0cv4w1x8rx2y39umgw5q85s7\" which has 38 characters. Hence the operation of writing the first string can be said to have \"less complexity\" than writing the second.",
        "More formally, the complexity of a string is the length of the shortest possible description of the string in some fixed universal description language (the sensitivity of complexity relative to the choice of description language is discussed below). It can be shown that the Kolmogorov complexity of any string cannot be more than a few bytes larger than the length of the string itself. Strings like the abab example above, whose Kolmogorov complexity is small relative to the string's size, are not considered to be complex.",
        "The Kolmogorov complexity can be defined for any mathematical object, but for simplicity the scope of this article is restricted to strings. We must first specify a description language for strings.  Such a description language can be based on any computer programming language, such as Lisp, Pascal, or Java.  If P is a program which outputs a string x, then P is a description of x. The length of the description is just the length of P as a character string, multiplied by the number of bits in a character (e.g., 7 for ASCII).",
        "We could, alternatively, choose an encoding for Turing machines, where an encoding is a function which associates to each Turing Machine M a bitstring <M>. If M is a Turing Machine which, on input w, outputs string x, then the concatenated string <M> w is a description of x. For theoretical analysis, this approach is more suited for constructing detailed formal proofs and is generally preferred in the research literature. In this article, an informal approach is discussed.\n\nAny string s has at least one description. For example, the second string above is output by the pseudo-code:\n\n function GenerateString2()\n     return \"4c1j5b2p0cv4w1x8rx2y39umgw5q85s7\"\n\nwhereas the first string is output by the (much shorter) pseudo-code:\n\n function GenerateString1()\n     return \"ab\" × 16",
        "function GenerateString2()\n     return \"4c1j5b2p0cv4w1x8rx2y39umgw5q85s7\"\n\nwhereas the first string is output by the (much shorter) pseudo-code:\n\n function GenerateString1()\n     return \"ab\" × 16\n\nIf a description d(s) of a string s is of minimal length (i.e., using the fewest bits), it is called a minimal description of s, and the length of d(s) (i.e. the number of bits in the minimal description) is the Kolmogorov complexity of s, written K(s). Symbolically,\n\nK(s) = |d(s)|.\n\nThe length of the shortest description will depend on the choice of description language; but the effect of changing languages is bounded (a result called the invariance theorem).\n\nInvariance theorem",
        "Invariance theorem\n\nInformal treatment\nThere are some description languages which are optimal, in the following sense: given any description of an object in a description language, said description may be used in the optimal description language with a constant overhead. The constant depends only on the languages involved, not on the description of the object, nor the object being described.\n\nHere is an example of an optimal description language. A description will have two parts:\n\n The first part describes another description language.\n The second part is a description of the object in that language.",
        "The first part describes another description language.\n The second part is a description of the object in that language.\n\nIn more technical terms, the first part of a description is a computer program (specifically: a compiler for the object's language, written in the description language), with the second part being the input to that computer program which produces the object as output.\n\nThe invariance theorem follows: Given any description language L, the optimal description language is at least as efficient as L, with some constant overhead.",
        "The invariance theorem follows: Given any description language L, the optimal description language is at least as efficient as L, with some constant overhead.\n\nProof: Any description D in L can be converted into a description in the optimal language by first describing L as a computer program P (part 1), and then using the original description D as input to that program (part 2). The\ntotal length of this new description D′ is (approximately):\n\n|D′ | = |P| + |D|\n\nThe length of P is a constant that doesn't depend on D. So, there is at most a constant overhead, regardless of the object described. Therefore, the optimal language is universal up to this additive constant.",
        "A more formal treatment\nTheorem: If K1 and K2 are the complexity functions relative to Turing complete description languages L1 and L2, then there is a constant c – which depends only on the languages L1 and L2 chosen – such that\n\n∀s.  −c ≤ K1(s) − K2(s) ≤ c.\n\nProof: By symmetry, it suffices to prove that there is some constant c such that for all strings s\n\nK1(s) ≤ K2(s) + c.\n\nNow, suppose there is a program in the language L1 which acts as an interpreter for L2:\n\n function InterpretLanguage(string p)\n\nwhere p is a program in L2. The interpreter is characterized by the following property:\n\n Running InterpretLanguage on input p returns the result of running p.",
        "function InterpretLanguage(string p)\n\nwhere p is a program in L2. The interpreter is characterized by the following property:\n\n Running InterpretLanguage on input p returns the result of running p.\n\nThus, if P is a program in L2 which is a minimal description of s, then InterpretLanguage(P) returns the string s. The length of this description of s is the sum of\n\n The length of the program InterpretLanguage, which we can take to be the constant c.\n The length of P which by definition is K2(s).\n\nThis proves the desired upper bound.\n\nHistory and context\nAlgorithmic information theory is the area of computer science that studies Kolmogorov complexity and other complexity measures on strings (or other data structures).",
        "History and context\nAlgorithmic information theory is the area of computer science that studies Kolmogorov complexity and other complexity measures on strings (or other data structures).\n\nThe concept and theory of Kolmogorov Complexity is based on a crucial theorem first discovered by Ray Solomonoff, who published it in 1960, describing it in \"A Preliminary Report on a General Theory of Inductive Inference\" as part of his invention of algorithmic probability. He gave a more complete description in his 1964 publications, \"A Formal Theory of Inductive Inference,\" Part 1 and Part 2 in Information and Control.",
        "Andrey Kolmogorov later independently published this theorem in Problems Inform. Transmission in 1965. Gregory Chaitin also presents this theorem in J. ACM – Chaitin's paper was submitted October 1966 and revised in December 1968, and cites both Solomonoff's and Kolmogorov's papers.",
        "The theorem says that, among algorithms that decode strings from their descriptions (codes), there exists an optimal one. This algorithm, for all strings, allows codes as short as allowed by any other algorithm up to an additive constant that depends on the algorithms, but not on the strings themselves. Solomonoff used this algorithm and the code lengths it allows to define a \"universal probability\" of a string on which inductive inference of the subsequent digits of the string can be based. Kolmogorov used this theorem to define several functions of strings, including complexity, randomness, and information.",
        "When Kolmogorov became aware of Solomonoff's work, he acknowledged Solomonoff's priority. For several years, Solomonoff's work was better known in the Soviet Union than in the Western World. The general consensus in the scientific community, however, was to associate this type of complexity with Kolmogorov, who was concerned with randomness of a sequence, while Algorithmic Probability became associated with Solomonoff, who focused on prediction using his invention of the universal prior probability distribution. The broader area encompassing descriptional complexity and probability is often called Kolmogorov complexity. The computer scientist Ming Li considers this an example of the Matthew effect: \"...to everyone who has, more will be given...\"",
        "There are several other variants of Kolmogorov complexity or algorithmic information. The most widely used one is based on self-delimiting programs, and is mainly due to Leonid Levin (1974).\n\nAn axiomatic approach to Kolmogorov complexity based on Blum axioms (Blum 1967) was introduced by Mark Burgin in the paper presented for publication by Andrey Kolmogorov.\n\nBasic results\nIn the following discussion, let K(s) be the complexity of the string s.\n\nIt is not hard to see that the minimal description of a string cannot be too much larger than the string itself — the program GenerateString2 above that outputs s is a fixed amount larger than s.\n\nTheorem: There is a constant c such that\n\n∀s. K(s) ≤ |s| + c.\n\nUncomputability of Kolmogorov complexity\n\nA naive attempt at a program to compute K",
        "Theorem: There is a constant c such that\n\n∀s. K(s) ≤ |s| + c.\n\nUncomputability of Kolmogorov complexity\n\nA naive attempt at a program to compute K \n\nAt first glance it might seem trivial to write a program which can compute K(s) for any s, such as the following:\n\n function KolmogorovComplexity(string s)\n     for i = 1 to infinity:\n         for each string p of length exactly i\n             if isValidProgram(p) and evaluate(p) == s\n                 return i",
        "function KolmogorovComplexity(string s)\n     for i = 1 to infinity:\n         for each string p of length exactly i\n             if isValidProgram(p) and evaluate(p) == s\n                 return i\n\nThis program iterates through all possible programs (by iterating through all possible strings and only considering those which are valid programs), starting with the shortest. Each program is executed to find the result produced by that program, comparing it to the input s. If the result matches then the length of the program is returned.",
        "However this will not work because some of the programs p tested will not terminate, e.g. if they contain infinite loops. There is no way to avoid all of these programs by testing them in some way before executing them due to the non-computability of the halting problem.\n\nWhat is more, no program at all can compute the function K, be it ever so sophisticated. This is proven in the following.\n\nFormal proof of uncomputability of K\n\nTheorem: There exist strings of arbitrarily large Kolmogorov complexity. Formally: for each natural number n, there is a string s with K(s) ≥ n.\n\nProof: Otherwise all of the infinitely many possible finite strings could be generated by the finitely many programs with a complexity below n bits.",
        "Proof: Otherwise all of the infinitely many possible finite strings could be generated by the finitely many programs with a complexity below n bits.\n\nTheorem:  K is not a computable function. In other words, there is no program which takes any string s as input and produces the integer K(s) as output.\n\nThe following proof by contradiction uses a simple Pascal-like language to denote programs; for sake of proof simplicity assume its description (i.e. an interpreter) to have a length of  bits.\nAssume for contradiction there is a program\n\n function KolmogorovComplexity(string s)\n\nwhich takes as input a string s and returns K(s). All programs are of finite length so, for sake of proof simplicity, assume it to be  bits.\nNow, consider the following program of length  bits:",
        "which takes as input a string s and returns K(s). All programs are of finite length so, for sake of proof simplicity, assume it to be  bits.\nNow, consider the following program of length  bits:\n\n function GenerateComplexString()\n     for i = 1 to infinity:\n         for each string s of length exactly i\n             if KolmogorovComplexity(s) ≥ 8000000000\n                 return s",
        "function GenerateComplexString()\n     for i = 1 to infinity:\n         for each string s of length exactly i\n             if KolmogorovComplexity(s) ≥ 8000000000\n                 return s\n\nUsing KolmogorovComplexity as a subroutine, the program tries every string, starting with the shortest, until it returns a string with Kolmogorov complexity at least  bits, i.e. a string that cannot be produced by any program shorter than  bits. However, the overall length of the above program that produced s is only  bits, which is a contradiction. (If the code of KolmogorovComplexity is shorter, the contradiction remains. If it is longer, the constant used in GenerateComplexString can always be changed appropriately.)",
        "The above proof uses a contradiction similar to that of the Berry paradox: \"The smallest positive integer that cannot be defined in fewer than twenty English words\". It is also possible to show the non-computability of K by reduction from the non-computability of the halting problem H, since K and H are Turing-equivalent.\n\nThere is a corollary, humorously called the \"full employment theorem\" in the programming language community, stating that there is no perfect size-optimizing compiler.\n\nChain rule for Kolmogorov complexity\n\nThe chain rule for Kolmogorov complexity states that\n\nK(X,Y) = K(X) + K(Y|X) + O(log(K(X,Y))).",
        "Chain rule for Kolmogorov complexity\n\nThe chain rule for Kolmogorov complexity states that\n\nK(X,Y) = K(X) + K(Y|X) + O(log(K(X,Y))).\n\nIt states that the shortest program that reproduces X and Y is no more than a logarithmic term larger than a program to reproduce X and a program to reproduce Y given X. Using this statement, one can define an analogue of mutual information for Kolmogorov complexity.\n\nCompression\nIt is straightforward to compute upper bounds for K(s) – simply compress the string s with some method, implement the corresponding decompressor in the chosen language, concatenate the decompressor to the compressed string, and measure the length of the resulting string – concretely, the size of a self-extracting archive in the given language.",
        "A string s is compressible by a number c if it has a description whose length does not exceed |s| − c bits. This is equivalent to saying that K(s) ≤ |s| − c.  Otherwise, s is incompressible by c. A string incompressible by 1 is said to be simply incompressible – by the pigeonhole principle, which applies because every compressed string maps to only one uncompressed string, incompressible strings must exist, since there are 2n bit strings of length n, but only 2n − 1 shorter strings, that is, strings of length less than n, (i.e. with length 0, 1, ..., n − 1).",
        "For the same reason, most strings are complex in the sense that they cannot be significantly compressed – their K(s) is not much smaller than |s|, the length of s in bits. To make this precise, fix a value of n. There are 2n bitstrings of length n. The uniform probability distribution on the space of these bitstrings assigns exactly equal weight 2−n to each string of length n.\n\nTheorem: With the uniform probability distribution on the space of bitstrings of length n, the probability that a string is incompressible by c is at least 1 − 2−c+1 + 2−n.\n\nTo prove the theorem, note that the number of descriptions of length not exceeding n − c is given by the geometric series:\n\n 1 + 2 + 22 + ... + 2n − c = 2n−c+1 − 1.\n\nThere remain at least\n\n 2n − 2n−c+1 + 1",
        "1 + 2 + 22 + ... + 2n − c = 2n−c+1 − 1.\n\nThere remain at least\n\n 2n − 2n−c+1 + 1\n\nbitstrings of length n that are incompressible by c.  To determine the probability, divide by 2n.\n\nChaitin's incompleteness theorem",
        "There remain at least\n\n 2n − 2n−c+1 + 1\n\nbitstrings of length n that are incompressible by c.  To determine the probability, divide by 2n.\n\nChaitin's incompleteness theorem\n\nBy the above theorem (), most strings are complex in the sense that they cannot be described in any significantly \"compressed\" way. However, it turns out that the fact that a specific string is complex cannot be formally proven, if the complexity of the string is above a certain threshold. The precise formalization is as follows. First, fix a particular axiomatic system S for the natural numbers. The axiomatic system has to be powerful enough so that, to certain assertions  A about complexity of strings, one can associate a formula FA in S. This association must have the following property:",
        "If FA is provable from the axioms of S, then the corresponding assertion A must be true. This \"formalization\" can be achieved based on a Gödel numbering.\n\nTheorem: There exists a constant L (which only depends on S and on the choice of description language) such that there does not exist a string s for which the statementK(s) ≥  L         (as formalized in S)\n\ncan be proven within S.",
        "can be proven within S.\n\nProof Idea: The proof of this result is modeled on a self-referential construction used in Berry's paradox. We firstly obtain a program which enumerates the proofs within S and we specify a procedure P which takes as an input an integer L and prints the strings x which are within proofs within S of the statement K(x) ≥ L. By then setting L to greater than the length of this procedure P, we have that the required length of a program to print x as stated in K(x) ≥ L as being at least L is then less than the amount L since the string x was printed by the procedure P. This is a contradiction. So it is not possible for the proof system S to prove K(x) ≥ L for L arbitrarily large, in particular, for L larger than the length of the procedure P, (which is finite).",
        "Proof:\n\nWe can find an effective enumeration of all the formal proofs in S by some procedure\n\n function NthProof(int n)\n\nwhich takes as input n and outputs some proof. This function enumerates all proofs. Some of these are proofs for formulas we do not care about here, since every possible proof in the language of S is produced for some n. Some of these are complexity formulas of the form K(s) ≥ n where s and n are constants in the language of S. There is a procedure\n\n function NthProofProvesComplexityFormula(int n)\n\nwhich determines whether the nth proof actually proves a complexity formula K(s) ≥ L. The strings s, and the integer L in turn, are computable by procedure:\n\n function StringNthProof(int n)\n\n function ComplexityLowerBoundNthProof(int n)\n\nConsider the following procedure:",
        "function StringNthProof(int n)\n\n function ComplexityLowerBoundNthProof(int n)\n\nConsider the following procedure:\n\n function GenerateProvablyComplexString(int n)\n     for i = 1 to infinity:\n         if NthProofProvesComplexityFormula(i) and ComplexityLowerBoundNthProof(i) ≥ n             return StringNthProof(i)\n\nGiven an n, this procedure tries every proof until it finds a string and a proof in the formal system S of the formula K(s) ≥ L for some L ≥ n; if no such proof exists, it loops forever.\n\nFinally, consider the program consisting of all these procedure definitions, and a main call:\n\n GenerateProvablyComplexString(n0)",
        "Finally, consider the program consisting of all these procedure definitions, and a main call:\n\n GenerateProvablyComplexString(n0)\n\nwhere the constant n0 will be determined later on. The overall program length can be expressed as U+log2(n0), where U is some constant and log2(n0) represents the length of the integer value n0, under the reasonable assumption that it is encoded in binary digits. We will choose n0 to be greater than the program length, that is, such that n0 > U+log2(n0).  This is clearly true for n0 sufficiently large, because the left hand side grows linearly in n0 whilst the right hand side grows logarithmically in n0 up to the fixed constant U.",
        "Then no proof of the form \"K(s)≥L\" with L≥n0 can be obtained in S, as can be seen by an indirect argument:\nIf ComplexityLowerBoundNthProof(i) could return a value ≥n0, then the loop inside GenerateProvablyComplexString would eventually terminate, and that procedure would return a string s such that\n\nThis is a contradiction, Q.E.D.\n\nAs a consequence, the above program, with the chosen value of n0, must loop forever.\n\nSimilar ideas are used to prove the properties of Chaitin's constant.\n\nMinimum message length",
        "Minimum message length\n\nThe minimum message length principle of statistical and inductive inference and machine learning was developed by C.S. Wallace and D.M. Boulton in 1968. MML is Bayesian (i.e. it incorporates prior beliefs) and information-theoretic. It has the desirable properties of statistical invariance (i.e. the inference transforms with a re-parametrisation, such as from polar coordinates to Cartesian coordinates), statistical consistency (i.e. even for very hard problems, MML will converge to any underlying model) and efficiency (i.e. the MML model will converge to any true underlying model about as quickly as is possible). C.S. Wallace and D.L. Dowe (1999) showed a formal connection between MML and algorithmic information theory (or Kolmogorov complexity).",
        "Kolmogorov randomnessKolmogorov randomness defines a string (usually of bits) as being random if and only if every computer program that can produce that string is at least as long as the string itself.  To make this precise, a universal computer (or universal Turing machine) must be specified, so that \"program\" means a program for this universal machine. A random string in this sense is \"incompressible\" in that it is impossible to \"compress\" the string into a program that is shorter than the string itself. For every universal computer, there is at least one algorithmically random string of each length.  Whether a particular string is random, however, depends on the specific universal computer that is chosen. This is because a universal computer can have a particular string hard-coded in",
        "length.  Whether a particular string is random, however, depends on the specific universal computer that is chosen. This is because a universal computer can have a particular string hard-coded in itself, and a program running on this universal computer can then simply refer to this hard-coded string using a short sequence of bits (i.e. much shorter than the string itself).",
        "This definition can be extended to define a notion of randomness for infinite sequences from a finite alphabet. These algorithmically random sequences can be defined in three equivalent ways. One way uses an effective analogue of measure theory; another uses effective martingales.  The third way defines an infinite sequence to be random if the prefix-free Kolmogorov complexity of its initial segments grows quickly enough — there must be a constant c such that the complexity of an initial segment of length n is always at least n−c.  This definition, unlike the definition of randomness for a finite string, is not affected by which universal machine is used to define prefix-free Kolmogorov complexity.",
        "Relation to entropy\nFor dynamical systems, entropy rate and algorithmic complexity of the trajectories are related by a theorem of Brudno, that the equality  holds for almost all .\n\nIt can be shown that for the output of Markov information sources, Kolmogorov complexity is related to the entropy of the information source. More precisely, the Kolmogorov complexity of the output of a Markov information source, normalized by the length of the output, converges almost surely (as the length of the output goes to infinity) to the entropy of the source.\n\nConditional versions\n\nThe conditional Kolmogorov complexity of two strings  is, roughly speaking, defined as the Kolmogorov complexity of x given y as an auxiliary input to the procedure.",
        "Conditional versions\n\nThe conditional Kolmogorov complexity of two strings  is, roughly speaking, defined as the Kolmogorov complexity of x given y as an auxiliary input to the procedure.\n\nThere is also a length-conditional complexity , which is the complexity of x given the length of x'' as known/input.\n\nSee also\n Berry paradox\n Code golf\n Data compression\n Descriptive complexity theory\n Grammar induction\n Inductive reasoning\n Kolmogorov structure function\n Levenshtein distance\n Solomonoff's theory of inductive inference\n Sample entropy\n\nNotes\n\nReferences\n\nFurther reading",
        "Notes\n\nReferences\n\nFurther reading\n\nExternal links\n The Legacy of Andrei Nikolaevich Kolmogorov\n Chaitin's online publications\n Solomonoff's IDSIA page\n Generalizations of algorithmic information by J. Schmidhuber\n \n  Tromp's lambda calculus computer model offers a concrete definition of K()]\n Universal AI based on Kolmogorov Complexity  by M. Hutter:  \n David Dowe's Minimum Message Length (MML) and Occam's razor pages.\n \n\nComputability theory\nDescriptive complexity\nMeasures of complexity\nComputational complexity theory"
    ],
    [
        "Live-variable analysis\nIn compilers, live variable analysis (or simply liveness analysis) is a classic data-flow analysis to calculate the variables that are live at each point in the program. A variable is live at some point if it holds a value that may be needed in the future, or equivalently if its value may be read before the next time the variable is written to.\n\nExample \nConsider the following program:\n\n b = 3\n c = 5\n a = f(b * c)\n\nThe set of live variables between lines 2 and 3 is {b, c} because both are used in the multiplication on line 3. But the set of live variables after line 1 is only {b}, since variable c is updated later, on line 2. The value of variable a is not used in this code.",
        "Note that the assignment to a may be eliminated as a is not used later, but there is insufficient information to justify removing all of line 3 as f may have side effects (printing b * c, perhaps).\n\nExpression in terms of dataflow equations",
        "Expression in terms of dataflow equations \n\nLiveness analysis is a \"backwards may\" analysis. The analysis is done in a backwards order, and the dataflow confluence operator is set union. In other words, if applying liveness analysis to a function with a particular number of logical branches within it, the analysis is performed starting from the end of the function working towards the beginning (hence \"backwards\"), and a variable is considered live if any of the branches moving forward within the function might potentially (hence \"may\") need the variable's current value. This is in contrast to a \"backwards must\" analysis which would instead enforce this condition on all branches moving forward.",
        "The dataflow equations used for a given basic block s and exiting block f in live variable analysis are the following:\n\n: The set of variables that are used in s before any assignment in the same basic block.\n\n: The set of variables that are assigned a value in s (in many books, KILL (s) is also defined as the set of variables assigned a value in s before any use, but this does not change the solution of the dataflow equation):",
        "The in-state of a block is the set of variables that are live at the start of the block. Its out-state is the set of variables that are live at the end of it. The out-state is the union of the in-states of the block's successors. The transfer function of a statement is applied by making the variables that are written dead, then making the variables that are read live.\n\nSecond example \n\nThe in-state of b3 only contains b and d, since c has been written. The out-state of b1 is the union of the in-states of b2 and b3. The definition of c in b2 can be removed, since c is not live immediately after the statement.",
        "Solving the data flow equations starts with initializing all in-states and out-states to the empty set. The work list is initialized by inserting the exit point (b3) in the work list (typical for backward flow). Its computed in-state differs from the previous one, so its predecessors b1 and b2 are inserted and the process continues. The progress is summarized in the table below.\n\nNote that b1 was entered in the list before b2, which forced processing b1 twice (b1 was re-entered as predecessor of b2). Inserting b2 before b1 would have allowed earlier completion.",
        "Note that b1 was entered in the list before b2, which forced processing b1 twice (b1 was re-entered as predecessor of b2). Inserting b2 before b1 would have allowed earlier completion.\n\nInitializing with the empty set is an optimistic initialization: all variables start out as dead. Note that the out-states cannot shrink from one iteration to the next, although the out-state can be smaller than the in-state. This can be seen from the fact that after the first iteration the out-state can only change by a change of the in-state. Since the in-state starts as the empty set, it can only grow in further iterations.\n\nReferences\n\nCompiler optimizations\nData-flow analysis\nStatic program analysis"
    ],
    [
        "MacFormat\nMacFormat is the UK's biggest computer magazine aimed at Macintosh users. It published 13 issues per year. It is published by Future plc, and has been since 1993.\n\nContent\nThe main content of this magazine includes news from major Apple events such as the WWDC or the Macworld Expo, features, detailed tutorials and reviews of the latest accessories and apps. Until 2012, the magazine included a free cover disc filled with Mac software mentioned in the magazine. In previous years, MacFormat came with programs on a free 3½-inch (88.9 mm) Floppy disk, CD or CD/DVD option as reflected the state of cheap removable media in that era.\n\nEditorial team\n Editor: Rob Mead-Green\n Managing Art Editor: Paul Blachford\n Operations Editor: Jo Membery\n\nReferences\n\nExternal links",
        "Editorial team\n Editor: Rob Mead-Green\n Managing Art Editor: Paul Blachford\n Operations Editor: Jo Membery\n\nReferences\n\nExternal links\n \n\n1993 establishments in the United Kingdom\nComputer magazines published in the United Kingdom\nMacintosh magazines\nMagazines established in 1993\nMass media in Bath, Somerset\nMonthly magazines published in the United Kingdom"
    ],
    [
        "Marabunta (software)\nMarabunta is a fully distributed software application for anonymous P2P. The main goal is the fight against internet censorship and assuring the freedom of speech. It is a peer-to-peer platform for information exchange among nodes in an anonymous way based on several communication algorithms called \"Order and Chaos\" which can be found in massive social organizations such as ant colonies.\n\nThe project was founded at the University of Zaragoza, Spain, developed and promoted by students of computing engineering although development teams and users from many different places have shown interest, perhaps attracted by the ideological aims of the project. The software is available in Spanish and English, the website is also available in English.",
        "Marabunta uses the Qt graphical widget toolkit, allowing it to be used on both Linux and Microsoft Windows. Released under the GNU General Public License, Marabunta is free software.\n\nPurpose \n\nMarabunta is an implementation of the ideas explained in the \"Free Nets project \" and it has been developed with these ideas in mind:",
        "Purpose \n\nMarabunta is an implementation of the ideas explained in the \"Free Nets project \" and it has been developed with these ideas in mind:\n\n Avoiding censure: Communication between people avoiding central servers is allowed, so it is free from censure attacks, which are commonly launched by many governments and corporations that want to control the communications.\n Anonymity: A degree of anonymous communication is made possible, such that information can be accessed without knowledge of its original source.\n Motives and development: Marabunta is the first project of this kind developed in Spain (first version released in 2005). Still, the software has potential use by people anywhere in the world, especially in countries with little or no freedom of speech.\n\nFeatures",
        "Features \n\nThere are many potential services that can be run on top of Marabunta. Text message interchange is the first service. It could be taken as a platform for telegram distribution, where each computer in the net works as a host and as a server.\n\n As a host: messages are sent, active nodes are sought, etc.\n As a server: text messages and requests to increase the connectivity between nodes are routed to the network using a broadcast forwarding method.\n\nThere are four message distribution lists, so receivers only receive messages sent to the list they are interested in: General, Technology, Philosophy, and Politics.",
        "There are four message distribution lists, so receivers only receive messages sent to the list they are interested in: General, Technology, Philosophy, and Politics.\n\nContent filters are allowed, so only messages with certain patterns are displayed. This is specially useful when searching for some specific information because Marabunta just selects potentially interesting messages.\n\nConnections \n\nAll generated traffic uses the UDP/IP protocols. Avoiding setting up connections between nodes lets more traffic flow in the network and the operational redundancy of every node can be used. Moreover, the UDP protocol could be seen as increasing the anonymity in the net because there is no need to validate source hosts to receive a datagram.\n\nPort-forwarding on NATs",
        "Port-forwarding on NATs \n\nMarabunta does not support UDP hole punching, so users behind NATs have to establish a port forwarding route to let the router know to which port and node of the internal net it should forward the arriving datagrams.\n\nReferences\n\nExternal links \n  Marabunta home page  (Software does display in Spanish by default. To set in English : run the software, click the tab \"Opciones Generales\", area \"Seleccion de Idioma\" (bottom of the window), change from \"Castellano\" to \"English\").",
        "Anonymity networks\nAnonymous file sharing networks\nCross-platform free software\nFree communication software\nFree file sharing software\nFree instant messaging clients\nFree multilingual software\nInternet privacy software\nPeer-to-peer computing\nFile sharing software that uses Qt\nWindows file sharing software\nFile sharing software for Linux\nFree software programmed in C++"
    ],
    [
        "MessagePad\nThe MessagePad is a discontinued series of personal digital assistant devices developed by Apple Computer for the Newton platform in 1993.  Some electronic engineering and the manufacture of Apple's MessagePad devices was undertaken in Japan by Sharp. The devices are based on the ARM 610 RISC processor and all featured handwriting recognition software and were developed and marketed by Apple. The devices run Newton OS.",
        "History\nThe development of the Newton MessagePad first began with Apple's former senior vice president of research and development, Jean-Louis Gassée; his team included Steve Capps, co-writer of macOS Finder, and an employed engineer named Steve Sakoman. The development of the Newton MessagePad operated in secret until it was eventually revealed to the Apple Board of Directors in late 1990.\n\nWhen Gassée resigned from his position due to a significant disagreement with the board, seeing how his employer was treated, Sakoman also stopped developing the MessagePad on March 2, 1990.",
        "When Gassée resigned from his position due to a significant disagreement with the board, seeing how his employer was treated, Sakoman also stopped developing the MessagePad on March 2, 1990.\n\nBill Atkinson, an Apple Executive responsible for the company's Lisa graphical interface, invited Steve Capps, John Sculley, Andy Hertzfeld, Susan Kare, and Marc Porat to a meeting on March 11, 1990. There, they brainstormed a way of saving the MessagePad. Sculley suggested adding new features, including libraries, museums, databases, or institutional archives features, allowing customers to navigate through various window tabs or opened galleries/stacks. The Board later approved his suggestion; he then gave Newton it is official and full backing.",
        "The first MessagePad was unveiled by Sculley on the 29th of May 1992 at the summer Consumer Electronics Show (CES) in Chicago. Sculley caved in to pressure to unveil the product early because the Newton did not officially ship for another 14 months on August 2, 1993, starting at a price of . Over 50,000 units were sold by late November 1993.\n\nDetails\n\nScreen and input\nWith the MessagePad 120 with Newton OS 2.0, the Newton Keyboard by Apple became available, which can also be used via the dongle on Newton devices with a Newton InterConnect port, most notably the Apple MessagePad 2000/2100 series, as well as the Apple eMate 300.",
        "Newton devices featuring Newton OS 2.1 or higher can be used with the screen turned horizontally (\"landscape\") as well as vertically (\"portrait\"). A change of a setting rotates the contents of the display by 90, 180 or 270 degrees. Handwriting recognition still works properly with the display rotated, although display calibration is needed when rotation in any direction is used for the first time or when the Newton device is reset.\n\nHandwriting recognition",
        "In initial versions (Newton OS 1.x) the handwriting recognition gave extremely mixed results for users and was sometimes inaccurate. The original handwriting recognition engine was called Calligrapher, and was licensed from a Russian company called Paragraph International. Calligrapher's design was quite sophisticated; it attempted to learn the user's natural handwriting, using a database of known words to make guesses as to what the user was writing, and could interpret writing anywhere on the screen, whether hand-printed, in cursive, or a mix of the two. By contrast, Palm Pilot's Graffiti had a less sophisticated design than Calligrapher, but was sometimes found to be more accurate and precise due to its reliance on a fixed, predefined stroke alphabet. The stroke alphabet used letter",
        "had a less sophisticated design than Calligrapher, but was sometimes found to be more accurate and precise due to its reliance on a fixed, predefined stroke alphabet. The stroke alphabet used letter shapes which resembled standard handwriting, but which were modified to be both simple and very easy to differentiate. Palm Computing also released two versions of Graffiti for Newton devices.  The Newton version sometimes performed better and could also show strokes as they were being written as input was done on the display itself, rather than on a silkscreen area.",
        "For editing text, Newton had a very intuitive system for handwritten editing, such as scratching out words to be deleted, circling text to be selected, or using written carets to mark inserts.",
        "Later releases of the Newton operating system retained the original recognizer for compatibility, but added a hand-printed-text-only (not cursive) recognizer, called \"Rosetta\", which was developed by Apple, included in version 2.0 of the Newton operating system, and refined in Newton 2.1. Rosetta is generally considered a significant improvement and many reviewers, testers, and most users consider the Newton 2.1 handwriting recognition software better than any of the alternatives even 10 years after it was introduced.  Recognition and computation of handwritten horizontal and vertical formulas such as \"1 + 2 =\" was also under development but never released. However, users wrote similar programs which could evaluate mathematical formulas using the Newton OS Intelligent Assistant, a unique",
        "such as \"1 + 2 =\" was also under development but never released. However, users wrote similar programs which could evaluate mathematical formulas using the Newton OS Intelligent Assistant, a unique part of every Newton device.",
        "The handwriting recognition and parts of the user interface for the Newton are best understood in the context of the broad history of pen computing, which is quite extensive.",
        "A vital feature of the Newton handwriting recognition system is the modeless error correction. That is, correction done in situ without using a separate window or widget, using a minimum of gestures. If a word is recognized improperly, the user could double-tap the word and a list of alternatives would pop up in a menu under the stylus. Most of the time, the correct word will be in the list. If not, a button at the bottom of the list allows the user to edit individual characters in that word. Other pen gestures could do such things as transpose letters (also in situ). The correction popup also allowed the user to revert to the original, un-recognized letter shapes - this would be useful in note-taking scenarios if there was insufficient time to make corrections immediately. To conserve",
        "allowed the user to revert to the original, un-recognized letter shapes - this would be useful in note-taking scenarios if there was insufficient time to make corrections immediately. To conserve memory and storage space, alternative recognition hypotheses would not be saved indefinitely. If the user returned to a note a week later, for example, they would only see the best match. Error correction in many current handwriting systems provides such functionality but adds more steps to the process, greatly increasing the interruption to a user's workflow that a given correction requires.",
        "User interface",
        "Text could also be entered by tapping with the stylus on a small on-screen pop-up QWERTY virtual keyboard, although more layouts were developed by users. Newton devices could also accept free-hand \"Sketches\", \"Shapes\", and \"Ink Text\", much like a desktop computer graphics tablet. With \"Shapes\", Newton could recognize that the user was attempting to draw a circle, a line, a polygon, etc., and it would clean them up into perfect vector representations (with modifiable control points and defined vertices) of what the user was attempting to draw. \"Shapes\" and \"Sketches\" could be scaled or deformed once drawn. \"Ink text\" captured the user's free-hand writing but allowed it to be treated somewhat like recognized text when manipulating for later editing purposes (\"ink text\" supported word wrap,",
        "once drawn. \"Ink text\" captured the user's free-hand writing but allowed it to be treated somewhat like recognized text when manipulating for later editing purposes (\"ink text\" supported word wrap, could be formatted to be bold, italic, etc.). At any time a user could also direct their Newton device to recognize selected \"ink text\" and turn it into recognized text (deferred recognition). A Newton note (or the notes attached to each contact in Names and each Dates calendar or to-do event) could contain any mix of interleaved text, Ink Text, Shapes, and Sketches.",
        "While the Newton offered handwriting recognition training and would clean up sketches into vector shapes, both were unreliable and required much rewriting and redrawing. The most reliable application of the Newton was collecting and organizing address and phone numbers. While handwritten messages could be stored, they could not be easily filed, sorted or searched. While the technology was a probable cause for the failure of the device (which otherwise met or exceeded expectations), the technology has been instrumental in producing the future generation of handwriting software that realizes the potential and promise that began in the development of Newton-Apple's Ink Handwriting Recognition.",
        "Connectivity",
        "The MessagePad 100 series of devices used Macintosh's proprietary serial ports—round Mini-DIN 8 connectors. The MessagePad 2000/2100 models (as well as the eMate 300) have a small, proprietary Newton InterConnect port. However, the development of the Newton hardware/software platform was canceled by Steve Jobs on February 27, 1998, so the InterConnect port, while itself very advanced, can only be used to connect a serial dongle. A prototype multi-purpose InterConnect device containing serial, audio in, audio out, and other ports was also discovered. In addition, all Newton devices have infrared connectivity, initially only the Sharp ASK protocol, but later also IrDA, though the Sharp ASK protocol was kept in for compatibility reasons. Unlike the Palm Pilot, all Newton devices are equipped",
        "connectivity, initially only the Sharp ASK protocol, but later also IrDA, though the Sharp ASK protocol was kept in for compatibility reasons. Unlike the Palm Pilot, all Newton devices are equipped with a standard PC Card expansion slot (two on the 2000/2100). This allows native modem and even Ethernet connectivity; Newton users have also written drivers for 802.11b wireless networking cards and ATA-type flash memory cards (including the popular CompactFlash format), as well as for Bluetooth cards. Newton can also dial a phone number through the built-in speaker of the Newton device by simply holding a telephone handset up to the speaker and transmitting the appropriate tones. Fax and printing support is also built in at the operating system level, although it requires peripherals such as",
        "holding a telephone handset up to the speaker and transmitting the appropriate tones. Fax and printing support is also built in at the operating system level, although it requires peripherals such as parallel adapters, PCMCIA cards, or serial modems, the most notable of which is the lightweight Newton Fax Modem released by Apple in 1993. It is powered by 2 AA batteries, and can also be used with a power adapter. It provides data transfer at 2,400 bit/s, and can also send and receive fax messages at 9,600 and 4,800 bit/s respectively.",
        "Power options\nThe original Apple MessagePad and MessagePad 100 used four AAA batteries. They were eventually replaced by AA batteries with the release of the Apple MessagePad 110.",
        "The use of 4 AA NiCd (MessagePad 110, 120 and 130) and 4x AA NiMH cells (MP2x00 series, eMate 300) give a runtime of up to 30 hours (MP2100 with two 20 MB Linear Flash memory PC Cards, no backlight usage) and up to 24 hours with backlight on. While adding more weight to the handheld Newton devices than AAA batteries or custom battery packs, the choice of an easily replaceable/rechargeable cell format gives the user a still unsurpassed runtime and flexibility of power supply. This, together with the flash memory used as internal storage starting with the Apple MessagePad 120 (if all cells lost their power, no data was lost due to the non-volatility of this storage), gave birth to the slogan \"Newton never dies, it only gets new batteries\".",
        "Later efforts and improvements\nThe Apple MessagePad 2000/2100, with a vastly improved handwriting recognition system, 162 MHz StrongARM SA-110 RISC processor, Newton OS 2.1, and a better, clearer, backlit screen, attracted critical plaudits.\n\neMate 300",
        "eMate 300\n\nThe eMate 300 was a Newton device in a laptop form factor offered to schools in 1997 as an inexpensive ($799 US, originally sold to education markets only) and durable computer for classroom use. However, in order to achieve its low price, the eMate 300 did not have all the speed and features of the contemporary MessagePad equivalent, the MessagePad 2000. The eMate was cancelled along with the rest of the Newton products in 1998. It is the only Newton device to use the ARM710 microprocessor (running at 25 MHz), have an integrated keyboard, use Newton OS 2.2 (officially numbered 2.1), and its batteries are officially irreplaceable, although several users replaced them with longer-lasting ones without any damage to the eMate hardware whatsoever.\n\nPrototypes",
        "Prototypes\n\nMany prototypes of additional Newton devices were spotted. Most notable was a Newton tablet or \"slate\", a large, flat screen that could be written on. Others included a \"Kids Newton\" with side handgrips and buttons, \"VideoPads\" which would have incorporated a video camera and screen on their flip-top covers for two-way communications, the \"Mini 2000\" which would have been very similar to a Palm Pilot, and the NewtonPhone developed by Siemens, which incorporated a handset and a keyboard.",
        "Market reception\nFourteen months after Sculley demoed it at the May 1992, Chicago CES, the MessagePad was first offered for sale on August 2, 1993, at the Boston Macworld Expo. The hottest item at the show, it cost $900. 50,000 MessagePads were sold in the device's first three months on the market.\n\nThe original Apple MessagePad and MessagePad 100 were limited by the very short lifetime of their inadequate AAA batteries.",
        "The original Apple MessagePad and MessagePad 100 were limited by the very short lifetime of their inadequate AAA batteries.\n\nLater versions of Newton OS offered improved handwriting recognition, quite possibly a leading reason for the continued popularity of the devices among Newton users. Even given the age of the hardware and software, Newtons still demand a sale price on the used market far greater than that of comparatively aged PDAs produced by other companies. In 2006, CNET compared an Apple MessagePad 2000 to a Samsung Q1, and the Newton was declared better. In 2009, CNET compared an Apple MessagePad 2000 to an iPhone 3GS, and the Newton was declared more innovative at its time of release.",
        "A chain of dedicated Newton-only stores called Newton Source, independently run by Stephen Elms, existed from 1994 until 1998. Locations included New York, Los Angeles, San Francisco, Chicago and Boston. The Westwood Village, California, near UCLA featured the trademark red and yellow light bulb Newton logo in neon. The stores provided an informative educational venue to learn about the Newton platform in a hands on relaxed fashion. The stores had no traditional computer retail counters and featured oval desktops where interested users could become intimately involved with the Newton product range. The stores were a model for the later Apple Stores.\n\nNewton device models",
        "{| class=\"wikitable\"\n|+\n!Brand\n| colspan=\"2\" |Apple Computer \n|Sharp \n|Siemens\n| colspan=\"2\" |Apple \n|Sharp \n|Apple Computer \n|Digital Ocean \n|Motorola\n|Harris \n|Digital Ocean \n| colspan=\"4\" |Apple \n| colspan=\"3\" |Harris\n|Siemens\n|Schlumberger\n|-\n!Device\n|OMP (Original Newton MessagePad)\n|Newton \"Dummy\"\n|ExpertPad PI-7000\n|Notephone.[better source needed]\n|MessagePad 100\n|MessagePad 110\n|Sharp ExpertPad PI-7100\n|MessagePad 120\n|Tarpon\n|Marco\n|SuperTech 2000\n|Seahorse\n|MessagePad 130\n|eMate 300\n|MessagePad 2000\n|MessagePad 2100\n|Access Device 2000\n|Access Device, GPS\n|Access Device, Wireline\n|Online Terminal, also known as Online Access Device(OAD)\n|Watson \n|-\n!Introduced\n|August 3, 1993 (US) December 1993 (Germany)\n|?\n|August 3, 1993 (US and Japan)\n|1993?\n| colspan=\"2\" |March 1994",
        "|Online Terminal, also known as Online Access Device(OAD)\n|Watson \n|-\n!Introduced\n|August 3, 1993 (US) December 1993 (Germany)\n|?\n|August 3, 1993 (US and Japan)\n|1993?\n| colspan=\"2\" |March 1994\n|April 1994\n|October 1994 (Germany), January 1995 (US)\n| colspan=\"2\" |January 1995 (US)\n|August 1995 (US)\n|January 1996 (US)\n|March 1996\n| colspan=\"2\" |March 1997\n|November 1997\n| colspan=\"3\" |1998\n|Announced 1997\n|?\n|-\n!Discontinued\n| colspan=\"3\" |March 1994\n|?\n| colspan=\"2\" |April 1995\n|late 1994\n|June 1996\n|?\n|?\n|?\n|?\n|April 1997\n| colspan=\"3\" |February 1998\n|\n|\n|\n|\n|\n|-\n!Code name\n|Junior\n|\n|?\n|?\n|Junior\n|Lindy\n|?\n|Gelato\n|?\n|?\n|?\n|?\n|Dante\n|?\n|Q\n|?\n|\n|\n|\n|\n|\n|-\n!Model No.\n|H1000\n|\n|?\n|?\n|H1000\n|H0059\n|?\n|H0131\n|?\n|?\n|?\n|?\n|H0196\n|H0208\n|H0136\n|H0149\n|\n|\n|\n|\n|\n|-\n!Processor",
        "|Junior\n|\n|?\n|?\n|Junior\n|Lindy\n|?\n|Gelato\n|?\n|?\n|?\n|?\n|Dante\n|?\n|Q\n|?\n|\n|\n|\n|\n|\n|-\n!Model No.\n|H1000\n|\n|?\n|?\n|H1000\n|H0059\n|?\n|H0131\n|?\n|?\n|?\n|?\n|H0196\n|H0208\n|H0136\n|H0149\n|\n|\n|\n|\n|\n|-\n!Processor\n| colspan=\"13\" |ARM 610 (20 MHz)\n|ARM 710a (25 MHz)\n| colspan=\"7\" |StrongARM SA-110 (162 MHz)\n|-\n!ROM\n| colspan=\"7\" |4 MB\n| colspan=\"2\" |4 MB (OS 1.3) or 8 MB (OS 2.0) \n|5 MB\n|4 MB\n| colspan=\"5\" |8 MB \n|\n|\n|\n|\n|\n|-\n!System Memory (RAM)\n| colspan=\"5\" |490 KB* SRAM\n|544 KB SRAM\n|490 KB* SRAM\n| colspan=\"2\" |639/687 KB DRAM\n|544 KB SRAM\n|639 KB DRAM\n| colspan=\"2\" |1199 KB DRAM\n|1 MB DRAM (Upgradable) \n|1 MB DRAM \n|4 MB DRAM \n| colspan=\"3\" |1 MB DRAM\n|?\n|1 MB DRAM\n|-\n!User Storage \n| colspan=\"5\" |150 KB* SRAM\n|480 KB SRAM\n|150 KB* SRAM\n| colspan=\"2\" |385/1361 KB Flash RAM\n|480 KB SRAM",
        "|1 MB DRAM \n|4 MB DRAM \n| colspan=\"3\" |1 MB DRAM\n|?\n|1 MB DRAM\n|-\n!User Storage \n| colspan=\"5\" |150 KB* SRAM\n|480 KB SRAM\n|150 KB* SRAM\n| colspan=\"2\" |385/1361 KB Flash RAM\n|480 KB SRAM\n|385 KB Flash RAM\n| colspan=\"2\" |1361 KB Flash RAM\n|2 MB Flash RAM(Upgradable) \n| colspan=\"5\" |4 MB Flash RAM\n|?\n|4 MB Flash RAM\n|-\n!Total RAM\n| colspan=\"5\" |640 KB \n|1 MB \n|640 KB\n| colspan=\"2\" |1.0/2.0 MB \n| colspan=\"2\" |1 MB \n| colspan=\"2\" |2.5 MB \n|3 MB (Upgradable via Internal Expansion)\n|5 MB \n|8 MB\n| colspan=\"3\" |5 MB\n|?\n|5 MB\n|-\n!Display\n| colspan=\"5\" |336 × 240 (B&W)\n|320 × 240 (B&W)\n|336 × 240 (B&W)\n|320 × 240 (B&W)\n|320 × 240 (B&W) w/ backlight\n|320 × 240 (B&W)\n| colspan=\"3\" |320 × 240 (B&W) w/ backlight\n| colspan=\"6\" |480 × 320 grayscale (16 shades) w/ backlight\n|",
        "|336 × 240 (B&W)\n|320 × 240 (B&W)\n|320 × 240 (B&W) w/ backlight\n|320 × 240 (B&W)\n| colspan=\"3\" |320 × 240 (B&W) w/ backlight\n| colspan=\"6\" |480 × 320 grayscale (16 shades) w/ backlight\n|\n|480 × 320 greyscale (16 shades) w/ backlight\n|-\n!Newton OS version\n| colspan=\"3\" |1.0 to 1.05, or 1.10 to 1.11\n|1.11\n| colspan=\"2\" |1.2 or 1.3\n|1.3\n| colspan=\"2\" |1.3 or 2.0\n| colspan=\"2\" |1.3\n| colspan=\"2\" |2.0\n|2.1 (2.2)\n| colspan=\"2\" |2.1\n| colspan=\"5\" |2.1\n|-\n!Newton OS languages\n|English or German\n|\n|English or Japanese\n|German\n|English, German or French\n|English or French\n|English or Japanese\n|English, German or French\n| colspan=\"4\" |English\n|English or German\n| colspan=\"2\" |English\n|English or German\n| colspan=\"3\" |English\n|German\n|French\n|-\n!Connectivity",
        "|English or Japanese\n|English, German or French\n| colspan=\"4\" |English\n|English or German\n| colspan=\"2\" |English\n|English or German\n| colspan=\"3\" |English\n|German\n|French\n|-\n!Connectivity\n| colspan=\"3\" |RS-422, LocalTalk & SHARP ASK Infrared\n|Modem and Telephone dock Attachment\n| colspan=\"4\" |RS-422, LocalTalk & SHARP ASK Infrared\n|RS-422, LocalTalk & SHARP ASK Infrared\n|RS-422, LocalTalk, Infrared, ARDIS Network\n|RS-232, LocalTalk WLAN, V.22bis modem, Analog/Digital Cellular, CDPD, RAM, ARDIS, Trunk Radio\n|RS-232, LocalTalk, CDPD, WLAN, Optional dGPS, GSM, or IR via modular attachments\n|RS-422, LocalTalk & SHARP ASK Infrared\n|IrDA, headphone port, Interconnect port, LocalTalk, Audio I/O, Autodock\n|Dual-mode IR;IrDA & SHARP ASK, LocalTalk, Audio I/O, Autodock, Phone I/O",
        "|RS-422, LocalTalk & SHARP ASK Infrared\n|IrDA, headphone port, Interconnect port, LocalTalk, Audio I/O, Autodock\n|Dual-mode IR;IrDA & SHARP ASK, LocalTalk, Audio I/O, Autodock, Phone I/O\n|Dual-mode IR; IrDA & SHARP ASK, LocalTalk, Audio I/O, Autodock\n| colspan=\"3\" |Dual-mode IR;IrDA & SHARP ASK, LocalTalk, Audio I/O, Autodock, Phone I/O\n|?\n|Dual-mode IR;IrDA & SHARP ASK, LocalTalk, Audio I/O, Autodock, Phone I/O\n|-\n!PCMCIA\n| colspan=\"13\" |1 PCMCIA-slot II, 5v or 12v\n|1 PCMCIA-slot I/II/III, 5v\n| colspan=\"2\" |2 PCMCIA-slot II, 5v or 12v\n| colspan=\"2\" |1 PCMCIA-slot II, 5v or 12v\n|1 PCMCIA-slot II, 5v or 12v, 2nd slot Propriety Rado Card\n| colspan=\"2\" |1 PCMCIA-slot II, 5v or 12v, 1 Smart Card Reader\n|-\n!Power\n| colspan=\"5\" |4 AAA or NiCd rechargeable or external power supply",
        "|1 PCMCIA-slot II, 5v or 12v, 2nd slot Propriety Rado Card\n| colspan=\"2\" |1 PCMCIA-slot II, 5v or 12v, 1 Smart Card Reader\n|-\n!Power\n| colspan=\"5\" |4 AAA or NiCd rechargeable or external power supply\n|4 AA or NiCd rechargeable or external power supply \n|4 AAA or NiCd rechargeable or external power supply\n|4 AA or NiCd rechargeable or external power supply\n| colspan=\"2\" |NiCd battery pack or external power supply\n|4 AA or NiCd rechargeable or external power supply\n|NiCd battery pack or external power supply\n|4 AA or NiCd rechargeable or external power supply\n|NiMH battery pack (built-in) or external power supply\n| colspan=\"2\" |4 AA or NiMH rechargeable or external power supply\n| colspan=\"3\" |Custom NiMH rechargeable or external power supply\n|? Unknown, but likely external power supply",
        "| colspan=\"2\" |4 AA or NiMH rechargeable or external power supply\n| colspan=\"3\" |Custom NiMH rechargeable or external power supply\n|? Unknown, but likely external power supply\n|4 AA or NiMH rechargeable or external power supply\n|-\n!Dimensions\n(HxWxD)\n|\n|\n|",
        "(lid open)\n| colspan=\"2\" |\n|\n\n|\n\n(lid open)\n|\n\n|\n\n|\n\n|?\n|\n|\n|\n\n| colspan=\"2\" |\n|?\n|?\n|?\n|9 x 14.5 x 5.1 inches\n(23 x 37 x 13 cm)\n|?\n|-\n!Weight\n|\n|\n|\nwith batteries installed\n|\n|\n| with batteries installed\n|\nwith batteries installed\n|with batteries installed\n|\n|\n|?\n|\n|\n\nwith batteries installed\n|\n| colspan=\"2\" |\n|?\n|?\n|?\n|?\n|?\n|}\n*  Varies with installed OS",
        "with batteries installed\n|\n| colspan=\"2\" |\n|?\n|?\n|?\n|?\n|?\n|}\n*  Varies with installed OS\n\nNotes: The eMate 300 actually has ROM chips silk screened with 2.2 on them. Stephanie Mak on her website discusses this:\nIf one removes all patches to the eMate 300 (by replacing the ROM chip, and then putting in the original one again, as the eMate and the MessagePad 2000/2100 devices erase their memory completely after replacing the chip), the result will be the Newton OS saying that this is version 2.2.00. Also, the Original MessagePad and the MessagePad 100 share the same model number, as they only differ in the ROM chip version. (The OMP has OS versions 1.0 to 1.05, or 1.10 to 1.11, while the MP100 has 1.3 that can be upgraded with various patches.)\n\nTimeline",
        "Timeline\n\nThird party licenses\nThe Newton OS was also licensed to a number of third party developers including Sharp and Motorola who developed additional PDA devices based on the Newton platform. Motorola added wireless connectivity, as well as made a unique two-part design, and shipped additional software with its Newton device, called the Marco. Sharp developed a line of Newton devices called the ExpertPad PI-7000/7100; those were the same as Apple's MessagePad and MessagePad 100, the only difference is the physical design (the ExpertPads feature a screen lid, which Apple added in 1994 with the release of the MessagePad 110) and the naming.\n\nOther uses",
        "Other uses\n\nThere were a number of projects that used the Newton as a portable information device in cultural settings such as museums. For example, Visible Interactive created a walking tour in San Francisco's Chinatown but the most significant effort took place in Malaysia at the Petronas Discovery Center, known as Petrosains.\n\nIn 1995, an exhibit design firm, DMCD Inc., was awarded the contract to design a new  science museum in the Petronas Towers in Kuala Lumpur. A major factor in the award was the concept that visitors would use a Newton device to access additional information, find out where they were in the museum, listen to audio, see animations, control robots and other media, and to bookmark information for printout at the end of the exhibit.",
        "The device became known as the ARIF, a Malay word for \"wise man\" or \"seer\" and it was also an acronym for A Resourceful Informative Friend. Some 400 ARIFS were installed and over 300 are still in use today.  The development of the ARIF system was extremely complex and required a team of hardware and software engineers, designers, and writers. ARIF is an ancestor of the PDA systems used in museums today and it boasted features that have not been attempted since.",
        "Anyway & Company firm was involved with the Petronas Discovery Center project back in 1998 and NDAs were signed which prevents getting to know more information about this project.  It was confirmed that they purchased of MP2000u or MP2100's by this firm on the behalf of the project under the name of \"Petrosains Project Account\". By 1998 they had invested heavily into the R&D of this project with the Newton at the center. After Apple officially cancelled the Newton in 1998 they had to acquire as many Newtons as possible for this project. It was estimated initially 1000 Newtons, but later readjusted the figure to possibly 750 Newtons. They placed an “Internet Call” for Newtons. They purchased them in large and small quantities.",
        "The Newton was also used in healthcare applications, for example in collecting data directly from patients. Newtons were used as electronic diaries, with patients entering their symptoms and other information concerning their health status on a daily basis. The compact size of the device and its ease of use made it possible for the electronic diaries to be carried around and used in the patients' everyday life setting. This was an early example of electronic patient-reported outcomes (ePRO).\n\nSee also\n Newton (platform)\n Newton OS\n eMate 300\n NewtonScript\n Orphaned technology\n Pen computing\n\nReferences",
        "See also\n Newton (platform)\n Newton OS\n eMate 300\n NewtonScript\n Orphaned technology\n Pen computing\n\nReferences\n\nBibliography\n Apple's press release on the debut of the MessagePad 2100: \n Apple's overview of features & limitations of Newton Connection Utilities: \n Newton overview at Newton Source archived from Apple: \n Newton FAQ: \n Newton Gallery: \n Birth of the Newton: \n The Newton Hall of Fame: People behind the Newton: \n Pen Computing's Why did Apple kill the Newton?: \n Pen Computing's Newton Notes column archive: \n A.I. Magazine article by Yaeger on Newton HWR design, algorithms, & quality: \n Associated slides: \n Info on Newton HWR from Apple's HWR Technical Lead: \n Notes on the History of Pen-based Computing: \n This links to:\n\nExternal links",
        "External links\n\nAdditional resources and information\n Defying Gravity: The Making of Newton, by Kounalakis & Menuez (Hardcover)\n Hardcover: 192 pages\n Publisher: Beyond Words Publishing (October 1993)\n \n \n \n Complete Developer's manual for the StrongARM SA-110\n Beginner's overview of the StrongARM SA-110 Microprocessor\n\nReviews\n MessagePad 2000 review at \"The History and Macintosh Society\"\n Prof. Wittmann's collection of Newton & MessagePad reviews\n\nApple Newton\nProducts introduced in 1993\nApple Inc. personal digital assistants"
    ],
    [
        "Mickey's Racing Adventure\nMickey's Racing Adventure is a racing video game developed by Rare and published by Nintendo for the Game Boy Color in 1999. It was followed by Mickey's Speedway USA in 2001.\n\nGameplay\nMickey's Racing Adventure is a single-player racing game with adventure elements. It is played from an isometric perspective and races consist of land or water tracks.\n\nDevelopment and release\nMickey's Racing Adventure was developed by Rare over the course of approximately six months. It is the company's second Game Boy Color game and its extra mini-games are based on classics such as Loco-Motion. It supports the Game Boy Color's infrared port to transmit data between machines. The game was released in November 1999.\n\nReception",
        "Mickey's Racing Adventure received positive reviews from critics. IGN reviewer Craig Harris felt that it was Rare's \"first real quality Game Boy Color-exclusive title\" after their \"atrocious\" Conker's Pocket Tales, while GameSpot praised the number of tracks and characters to choose from, stating that Mickey's Racing Adventure \"shows how Game Boy racers should be done\". N64 Magazine said that the game successfully combines the exploration aspects of Rare's Diddy Kong Racing with the racing style of R.C. Pro-Am for the Nintendo Entertainment System, but criticised the lack of a multiplayer mode. Game Informer gave the game an overall score of 8 out of 10 noting the game being well designed, especially for a Disney game, and commenting that the game has plenty of racing courses, characters,",
        "Game Informer gave the game an overall score of 8 out of 10 noting the game being well designed, especially for a Disney game, and commenting that the game has plenty of racing courses, characters, power ups and vehicle upgrades concluding \"Mickey's Racing Adventure won't take you an exorbitant amount of time to finish, but you’ll like it all the way to the end\".",
        "References\n\nExternal links\n\n1999 video games\nGame Boy Color games\nGame Boy Color-only games\nMickey Mouse video games\nNintendo games\nRacing video games\nRare (company) games\nSingle-player video games\nVideo games scored by David Wise\nVideo games developed in the United Kingdom"
    ],
    [
        "Naive set theory\nNaive set theory is any of several theories of sets used in the discussion of the foundations of mathematics.\nUnlike axiomatic set theories, which are defined using formal logic, naive set theory is defined informally, in natural language. It describes the aspects of mathematical sets familiar in discrete mathematics (for example Venn diagrams and symbolic reasoning about their Boolean algebra), and suffices for the everyday use of set theory concepts in contemporary mathematics.\n\nSets are of great importance in mathematics; in modern formal treatments, most mathematical objects (numbers, relations, functions, etc.) are defined in terms of sets. Naive set theory suffices for many purposes, while also serving as a stepping stone towards more formal treatments.",
        "Method\nA naive theory in the sense of \"naive set theory\" is a non-formalized theory, that is, a theory that uses natural language to describe sets and operations on sets. The words and, or, if ... then, not, for some, for every are treated as in ordinary mathematics. As a matter of convenience, use of naive set theory and its formalism prevails even in higher mathematics – including in more formal settings of set theory itself.\n\nThe first development of set theory was a naive set theory. It was created at the end of the 19th century by Georg Cantor as part of his study of infinite sets and developed by Gottlob Frege in his Grundgesetze der Arithmetik.",
        "Naive set theory may refer to several very distinct notions. It may refer to\n Informal presentation of an axiomatic set theory, e.g. as in Naive Set Theory by Paul Halmos.\n Early or later versions of Georg Cantor's theory and other informal systems.\n Decidedly inconsistent theories (whether axiomatic or not), such as a theory of Gottlob Frege that yielded Russell's paradox, and theories of Giuseppe Peano and Richard Dedekind.\n\nParadoxes\nThe assumption that any property may be used to form a set, without restriction, leads to paradoxes.  One common example is Russell's paradox: there is no set consisting of \"all sets that do not contain themselves\". Thus consistent systems of naive set theory must include some limitations on the principles which can be used to form sets.",
        "Cantor's theory",
        "Some believe that Georg Cantor's set theory was not actually implicated in the set-theoretic paradoxes (see Frápolli 1991). One difficulty in determining this with certainty is that Cantor did not provide an axiomatization of his system. By 1899, Cantor was aware of some of the paradoxes following from unrestricted interpretation of his theory, for instance Cantor's paradox and the Burali-Forti paradox, and did not believe that they discredited his theory. Cantor's paradox can actually be derived from the above (false) assumption—that any property  may be used to form a set—using for  \" is a cardinal number\". Frege explicitly axiomatized a theory in which a formalized version of naive set theory can be interpreted, and it is this formal theory which Bertrand Russell actually addressed",
        "a cardinal number\". Frege explicitly axiomatized a theory in which a formalized version of naive set theory can be interpreted, and it is this formal theory which Bertrand Russell actually addressed when he presented his paradox, not necessarily a theory Cantorwho, as mentioned, was aware of several paradoxespresumably had in mind.",
        "Axiomatic theories\nAxiomatic set theory was developed in response to these early attempts to understand sets, with the goal of determining precisely what operations were allowed and when.\n\nConsistency\nA naive set theory is not necessarily inconsistent, if it correctly specifies the sets allowed to be considered. This can be done by the means of definitions, which are implicit axioms. It is possible to state all the axioms explicitly, as in the case of Halmos' Naive Set Theory, which is actually an informal presentation of the usual axiomatic Zermelo–Fraenkel set theory. It is \"naive\" in that the language and notations are those of ordinary informal mathematics, and in that it does not deal with consistency or completeness of the axiom system.",
        "Likewise, an axiomatic set theory is not necessarily consistent: not necessarily free of paradoxes. It follows from Gödel's incompleteness theorems that a sufficiently complicated first order logic system (which includes most common axiomatic set theories) cannot be proved consistent from within the theory itself – even if it actually is consistent. However, the common axiomatic systems are generally believed to be consistent; by their axioms they do exclude some paradoxes, like Russell's paradox. Based on Gödel's theorem, it is just not known – and never can be – if there are no paradoxes at all in these theories or in any first-order set theory.",
        "The term naive set theory is still today also used in some literature to refer to the set theories studied by Frege and Cantor, rather than to the informal counterparts of modern axiomatic set theory.",
        "Utility\nThe choice between an axiomatic approach and other approaches is largely a matter of convenience. In everyday mathematics the best choice may be informal use of axiomatic set theory.  References to particular axioms typically then occur only when demanded by tradition, e.g. the axiom of choice is often mentioned when used. Likewise, formal proofs occur only when warranted by exceptional circumstances. This informal usage of axiomatic set theory can have (depending on notation) precisely the appearance of naive set theory as outlined below. It is considerably easier to read and write (in the formulation of most statements, proofs, and lines of discussion) and is less error-prone than a strictly formal approach.",
        "Sets, membership and equality \nIn naive set theory, a set is described as a well-defined collection of objects. These objects are called the elements or members of the set. Objects can be anything: numbers, people, other sets, etc. For instance, 4 is a member of the set of all even integers. Clearly, the set of even numbers is infinitely large; there is no requirement that a set be finite.\n\nThe definition of sets goes back to Georg Cantor. He wrote in his 1915 article Beiträge zur Begründung der transfiniten Mengenlehre:",
        "The definition of sets goes back to Georg Cantor. He wrote in his 1915 article Beiträge zur Begründung der transfiniten Mengenlehre:\n\n“Unter einer 'Menge' verstehen wir jede Zusammenfassung M von bestimmten wohlunterschiedenen Objekten unserer Anschauung oder unseres Denkens (welche die 'Elemente' von M genannt werden) zu einem Ganzen.” – Georg Cantor\n“A set is a gathering together into a whole of definite, distinct objects of our perception or of our thought—which are called elements of the set.” – Georg Cantor",
        "Note on consistency \nIt does not follow from this definition how sets can be formed, and what operations on sets again will produce a set. The term \"well-defined\" in \"well-defined collection of objects\" cannot, by itself, guarantee the consistency and unambiguity of what exactly constitutes and what does not constitute a set. Attempting to achieve this would be the realm of axiomatic set theory or of axiomatic class theory.",
        "The problem, in this context, with informally formulated set theories, not derived from (and implying) any particular axiomatic theory, is that there may be several widely differing formalized versions, that have both different sets and different rules for how new sets may be formed, that all conform to the original informal definition. For example, Cantor's verbatim definition allows for considerable freedom in what constitutes a set. On the other hand, it is unlikely that Cantor was particularly interested in sets containing cats and dogs, but rather only in sets containing purely mathematical objects. An example of such a class of sets could be the von Neumann universe. But even when fixing the class of sets under consideration, it is not always clear which rules for set formation are",
        "objects. An example of such a class of sets could be the von Neumann universe. But even when fixing the class of sets under consideration, it is not always clear which rules for set formation are allowed without introducing paradoxes.",
        "For the purpose of fixing the discussion below, the term \"well-defined\" should instead be interpreted as an intention, with either implicit or explicit rules (axioms or definitions), to rule out inconsistencies. The purpose is to keep the often deep and difficult issues of consistency away from the, usually simpler, context at hand. An explicit ruling out of all conceivable inconsistencies (paradoxes) cannot be achieved for an axiomatic set theory anyway, due to Gödel's second incompleteness theorem, so this does not at all hamper the utility of naive set theory as compared to axiomatic set theory in the simple contexts considered below. It merely simplifies the discussion. Consistency is henceforth taken for granted unless explicitly mentioned.",
        "Membership \nIf x is a member of a set A, then it is also said that x belongs to A, or that x is in A. This is denoted by x ∈ A. The symbol ∈ is a derivation from the lowercase Greek letter epsilon, \"ε\", introduced by Giuseppe Peano in 1889 and is the first letter of the word ἐστί (means \"is\"). The symbol ∉ is often used to write x ∉ A, meaning \"x is not in A\".",
        "Equality \nTwo sets A and B are defined to be equal when they have precisely the same elements, that is, if every element of A is an element of B and every element of B is an element of A. (See axiom of extensionality.) Thus a set is completely determined by its elements; the description is immaterial. For example, the set with elements 2, 3, and 5 is equal to the set of all prime numbers less than 6.\nIf the sets A and B are equal, this is denoted symbolically as A = B (as usual).",
        "Empty set \nThe empty set, denoted as  and sometimes , is a set with no members at all. Because a set is determined completely by its elements, there can be only one empty set. (See axiom of empty set.) Although the empty set has no members, it can be a member of other sets. Thus , because the former has no members and the latter has one member. In mathematics, the only sets with which one needs to be concerned can be built up from the empty set alone.",
        "Specifying sets \nThe simplest way to describe a set is to list its elements between curly braces (known as defining a set extensionally). Thus  denotes the set whose only elements are  and .\n(See axiom of pairing.)\nNote the following points:\nThe order of elements is immaterial; for example, .\nRepetition (multiplicity) of elements is irrelevant; for example, .\n(These are consequences of the definition of equality in the previous section.)\n\nThis notation can be informally abused by saying something like  to indicate the set of all dogs, but this example would usually be read by mathematicians as \"the set containing the single element dogs\".\n\nAn extreme (but correct) example of this notation is , which denotes the empty set.",
        "An extreme (but correct) example of this notation is , which denotes the empty set.\n\nThe notation , or sometimes , is used to denote the set containing all objects for which the condition  holds (known as defining a set intensionally).\nFor example,  denotes the set of real numbers,  denotes the set of everything with blonde hair.",
        "This notation is called set-builder notation (or \"set comprehension\", particularly in the context of Functional programming).\nSome variants of set builder notation are:\n denotes the set of all  that are already members of  such that the condition  holds for . For example, if  is the set of integers, then  is the set of all even integers. (See axiom of specification.)\n denotes the set of all objects obtained by putting members of the set  into the formula . For example,  is again the set of all even integers. (See axiom of replacement.)\n is the most general form of set builder notation. For example, {{math|{{mset|xs owner | x is a dog}}}} is the set of all dog owners.",
        "Subsets \nGiven two sets A and B, A is a subset of B if every element of A is also an element of B.\nIn particular, each set B is a subset of itself; a subset of B that is not equal to B is called a proper subset.\n\nIf A is a subset of B, then one can also say that B is a superset of A, that A is contained in B, or that B contains A. In symbols,  means that A is a subset of B, and  means that B is a superset of A.\nSome authors use the symbols ⊂ and ⊃ for subsets, and others use these symbols only for proper subsets. For clarity, one can explicitly use the symbols ⊊ and ⊋ to indicate non-equality.",
        "As an illustration, let R be the set of real numbers, let Z be the set of integers, let O be the set of odd integers, and let P be the set of current or former U.S. Presidents.\nThen O is a subset of Z, Z is a subset of R, and (hence) O is a subset of R, where in all cases subset may even be read as proper subset.\nNot all sets are comparable in this way. For example, it is not the case either that R is a subset of P nor that P is a subset of R.",
        "It follows immediately from the definition of equality of sets above that, given two sets A and B,  if and only if  and . In fact this is often given as the definition of equality. Usually when trying to prove that two sets are equal, one aims to show these two inclusions. The empty set is a subset of every set (the statement that all elements of the empty set are also members of any set A is vacuously true).\n\nThe set of all subsets of a given set A is called the power set of A and is denoted by  or ; the \"\" is sometimes in a script font: . If the set A has n elements, then  will have  elements.",
        "The set of all subsets of a given set A is called the power set of A and is denoted by  or ; the \"\" is sometimes in a script font: . If the set A has n elements, then  will have  elements.\n\n Universal sets and absolute complements \nIn certain contexts, one may consider all sets under consideration as being subsets of some given universal set.\nFor instance, when investigating properties of the real numbers R (and subsets of R), R may be taken as the universal set. A true universal set is not included in standard set theory (see Paradoxes below), but is included in some non-standard set theories.",
        "Given a universal set U and a subset A of U, the complement of A (in U''') is defined as\n.\nIn other words, AC (\"A-complement\"; sometimes simply A, \"A-prime\" ) is the set of all members of U which are not members of A.\nThus with R, Z and O defined as in the section on subsets, if Z is the universal set, then OC is the set of even integers, while if R is the universal set, then OC is the set of all real numbers that are either even integers or not integers at all.\n\n Unions, intersections, and relative complements \nGiven two sets A and B, their union is the set consisting of all objects which are elements of A or of B or of both (see axiom of union). It is denoted by .\n\nThe intersection of A and B is the set of all objects which are both in A and in B. It is denoted by .",
        "The intersection of A and B is the set of all objects which are both in A and in B. It is denoted by .\n\nFinally, the relative complement of B relative to A, also known as the set theoretic difference of A and B, is the set of all objects that belong to A but not to B. It is written as  or .\n\nSymbolically, these are respectively\n;\n;\n.\n\nThe set B doesn't have to be a subset of A for  to make sense; this is the difference between the relative complement and the absolute complement () from the previous section.",
        "The set B doesn't have to be a subset of A for  to make sense; this is the difference between the relative complement and the absolute complement () from the previous section.\n\nTo illustrate these ideas, let A be the set of left-handed people, and let B be the set of people with blond hair. Then  is the set of all left-handed blond-haired people, while  is the set of all people who are left-handed or blond-haired or both. , on the other hand, is the set of all people that are left-handed but not blond-haired, while  is the set of all people who have blond hair but aren't left-handed.\n\nNow let E be the set of all human beings, and let F be the set of all living things over 1000 years old. What is  in this case? No living human being is over 1000 years old, so  must be the empty set {}.",
        "For any set A, the power set  is a Boolean algebra under the operations of union and intersection.\n\n Ordered pairs and Cartesian products \nIntuitively, an ordered pair is simply a collection of two objects such that one can be distinguished as the first element and the other as the second element, and having the fundamental property that, two ordered pairs are equal if and only if their first elements are equal and their second elements are equal.\n\nFormally, an ordered pair with first coordinate a, and second coordinate b, usually denoted by (a, b), can be defined as the set \n\nIt follows that, two ordered pairs (a,b) and (c,d) are equal if and only if  and .\n\nAlternatively, an ordered pair can be formally thought of as a set {a,b} with a total order.",
        "It follows that, two ordered pairs (a,b) and (c,d) are equal if and only if  and .\n\nAlternatively, an ordered pair can be formally thought of as a set {a,b} with a total order.\n\n(The notation (a, b) is also used to denote an open interval on the real number line, but the context should make it clear which meaning is intended. Otherwise, the notation ]a, b[ may be used to denote the open interval whereas (a, b) is used for the ordered pair).\n\nIf A and B are sets, then the Cartesian product (or simply product) is defined to be:\n\nThat is,  is the set of all ordered pairs whose first coordinate is an element of A and whose second coordinate is an element of B.",
        "That is,  is the set of all ordered pairs whose first coordinate is an element of A and whose second coordinate is an element of B.\n\nThis definition may be extended to a set  of ordered triples, and more generally to sets of ordered n-tuples for any positive integer n.\nIt is even possible to define infinite Cartesian products, but this requires a more recondite definition of the product.\n\nCartesian products were first developed by René Descartes in the context of analytic geometry. If R denotes the set of all real numbers, then  represents the Euclidean plane and  represents three-dimensional Euclidean space.",
        "Some important sets \nThere are some ubiquitous sets for which the notation is almost universal. Some of these are listed below. In the list, a, b, and c refer to natural numbers, and r and s are real numbers.\n Natural numbers are used for counting. A blackboard bold capital N () often represents this set.\n Integers appear as solutions for x in equations like x + a = b. A blackboard bold capital Z () often represents this set (from the German Zahlen, meaning numbers).\n Rational numbers appear as solutions to equations like a + bx = c. A blackboard bold capital Q () often represents this set (for quotient, because R is used for the set of real numbers).",
        "Rational numbers appear as solutions to equations like a + bx = c. A blackboard bold capital Q () often represents this set (for quotient, because R is used for the set of real numbers).\n Algebraic numbers appear as solutions to polynomial equations (with integer coefficients) and may involve radicals (including ) and certain other irrational numbers. A Q with an overline () often represents this set. The overline denotes the operation of algebraic closure.\n Real numbers represent the \"real line\" and include all numbers that can be approximated by rationals. These numbers may be rational or algebraic but may also be transcendental numbers, which cannot appear as solutions to polynomial equations with rational coefficients. A blackboard bold capital R () often represents this set.",
        "Complex numbers are sums of a real and an imaginary number: . Here either  or  (or both) can be zero; thus, the set of real numbers and the set of strictly imaginary numbers are subsets of the set of complex numbers, which form an algebraic closure for the set of real numbers, meaning that every polynomial with coefficients in  has at least one root in this set. A blackboard bold capital C () often represents this set. Note that since a number  can be identified with a point  in the plane,  is basically \"the same\" as the Cartesian product  (\"the same\" meaning that any point in one determines a unique point in the other and for the result of calculations, it doesn't matter which one is used for the calculation, as long as multiplication rule is appropriate for ).",
        "Paradoxes in early set theory \n\nThe unrestricted formation principle of sets referred to as the axiom schema of unrestricted comprehension,\n\nis the source of several early appearing paradoxes:\n led, in the year 1897, to the Burali-Forti paradox, the first published antinomy.\n produced Cantor's paradox in 1897.\n yielded Cantor's second antinomy in the year 1899. Here the property  is true for all , whatever  may be, so  would be a universal set, containing everything.\n, i.e. the set of all sets that do not contain themselves as elements, gave Russell's paradox in 1902.\n\nIf the axiom schema of unrestricted comprehension is weakened to the axiom schema of specification or axiom schema of separation',",
        "If the axiom schema of unrestricted comprehension is weakened to the axiom schema of specification or axiom schema of separation',\n\nthen all the above paradoxes disappear. There is a corollary. With the axiom schema of separation as an axiom of the theory, it follows, as a theorem of the theory:\n\nOr, more spectacularly (Halmos' phrasing): There is no universe. Proof: Suppose that it exists and call it . Now apply the axiom schema of separation with  and for  use . This leads to Russell's paradox again. Hence  cannot exist in this theory.\n\nRelated to the above constructions is formation of the set\n\n,",
        "Related to the above constructions is formation of the set\n\n,\n\nwhere the statement following the implication certainly is false. It follows, from the definition of , using the usual inference rules (and some afterthought when reading the proof in the linked article below) both that  and  holds, hence . This is Curry's paradox.\n\nIt is (perhaps surprisingly) not the possibility of  that is problematic. It is again the axiom schema of unrestricted comprehension allowing  for . With the axiom schema of specification instead of unrestricted comprehension, the conclusion  does not hold and hence  is not a logical consequence.\n\nNonetheless, the possibility of  is often removed explicitly or, e.g. in ZFC, implicitly, by demanding the axiom of regularity to hold. One consequence of it is",
        "Nonetheless, the possibility of  is often removed explicitly or, e.g. in ZFC, implicitly, by demanding the axiom of regularity to hold. One consequence of it is\n\nor, in other words, no set is an element of itself.",
        "or, in other words, no set is an element of itself.\n\nThe axiom schema of separation is simply too weak (while unrestricted comprehension is a very strong axiom—too strong for set theory) to develop set theory with its usual operations and constructions outlined above. The axiom of regularity is of a restrictive nature as well. Therefore, one is led to the formulation of other axioms to guarantee the existence of enough sets to form a set theory. Some of these have been described informally above and many others are possible. Not all conceivable axioms can be combined freely into consistent theories. For example, the axiom of choice of ZFC is incompatible with the conceivable \"every set of reals is Lebesgue measurable\". The former implies the latter is false.\n\nSee also",
        "See also\n\n Algebra of sets\n Axiomatic set theory\n Internal set theory\n List of set identities and relations\n Set theory\n Set (mathematics)\n Partially ordered set\n\n Notes \n\nReferences\n Bourbaki, N., Elements of the History of Mathematics, John Meldrum (trans.), Springer-Verlag, Berlin, Germany, 1994.",
        "Notes \n\nReferences\n Bourbaki, N., Elements of the History of Mathematics, John Meldrum (trans.), Springer-Verlag, Berlin, Germany, 1994.\n\n Devlin, K.J., The Joy of Sets: Fundamentals of Contemporary Set Theory, 2nd edition, Springer-Verlag, New York, NY, 1993.\n María J. Frápolli|Frápolli, María J., 1991, \"Is Cantorian set theory an iterative conception of set?\". Modern Logic, v. 1 n. 4, 1991, 302–318.\n \n \n \n \n \n Kelley, J.L., General Topology, Van Nostrand Reinhold, New York, NY, 1955.\n van Heijenoort, J., From Frege to Gödel, A Source Book in Mathematical Logic, 1879-1931'', Harvard University Press, Cambridge, MA, 1967. Reprinted with corrections, 1977. .\n\nExternal links\n Beginnings of set theory page at St. Andrews\n Earliest Known Uses of Some of the Words of Mathematics (S)",
        "External links\n Beginnings of set theory page at St. Andrews\n Earliest Known Uses of Some of the Words of Mathematics (S)\n\nSet theory\nSystems of set theory"
    ],
    [
        "National Internet registry\nA national Internet registry (or NIR) is an organization under the umbrella of a regional Internet registry with the task of coordinating IP address allocations and other Internet resource management functions at a national level within a country or economic unit.\n\nNIRs operate primarily in the Asia Pacific region, under the authority of APNIC, the regional Internet registry for that region.\n\nThe following NIRs are currently operating in the APNIC region:",
        "NIRs operate primarily in the Asia Pacific region, under the authority of APNIC, the regional Internet registry for that region.\n\nThe following NIRs are currently operating in the APNIC region:\n\n IDNIC-APJII (Indonesia Network Information Centre-Asosiasi Penyelenggara Jasa Internet Indonesia)\n CNNIC, China Internet Network Information Center\n JPNIC, Japan Network Information Center\n KRNIC, Korea Internet & Security Agency\n TWNIC, Taiwan Network Information Center\n VNNIC, Vietnam Internet Network Information Center\n Indian Registry for Internet Names and Numbers\n\nThe following NIRs are currently operating in the Latin American (LACNIC) region:\nNIC Mexico\nNIC.br\n\nThere are no NIRs operating in the RIPE NCC region.",
        "The following NIRs are currently operating in the Latin American (LACNIC) region:\nNIC Mexico\nNIC.br\n\nThere are no NIRs operating in the RIPE NCC region.\n\nSee also \n Country code top-level domain\n Geolocation software\n Internet governance\n Local Internet registry\n\nReferences\n\nExternal links\n APNIC website\n NIC Mexico website\n NIC Chile website\n\nRegional Internet registries\nInternet Assigned Numbers Authority\nInternet Standards\nInternet governance"
    ],
    [
        "NPC (cable system)\nNPC (North Pacific Cable) is a submarine telecommunications cable system in the North Pacific Ocean linking the United States and Japan.\n\nIt has landing points in:\nMiura, Kanagawa Prefecture, Japan\nPacific City, Tillamook County, Oregon, United States\nSeward, Kenai Peninsula Borough, Alaska, United States (branch @ 420 Mbit/s)\n\nIt has a transmission capacity of 1,260 Mbit/s, and a total cable length of 5,200 miles (~8,400 km). The cable also included a spur to Alaska. The cable started operation in May 1991 and ceased operating in 2004.\n\nReferences\n\nExternal links \n https://www.fcc.gov/document/north-pacific-cable-system",
        "References\n\nExternal links \n https://www.fcc.gov/document/north-pacific-cable-system\n\nSubmarine communications cables in the Pacific Ocean\nJapan–United States relations\n1991 establishments in Alaska\n1991 establishments in Japan\n1991 establishments in Oregon\n2004 disestablishments in Alaska\n2004 disestablishments in Japan\n2004 disestablishments in Oregon"
    ],
    [
        "Omega Boost\nis a three dimensional shoot 'em up developed by Polyphony Digital and Cyberhead for the PlayStation. It was released in 1999 throughout Japan, North America, and Europe by Sony Computer Entertainment.\n\nThe game features mecha designs by Shoji Kawamori of Macross fame.\n\nBeing released late in the PlayStation's life, Omega Boost is said to have some of the best graphics on the console with parts of the game running at 60 frame/s. The game was criticized by some reviewers for being too short (Nine levels with nine unlockable special missions) and simplistic. However, it is still considered one of the best Macross-style mecha simulation games produced and is thought of by many as a sleeper hit due to its poor marketing.",
        "Omega Boost is so far the only non-racing game developed by Polyphony Digital, whose works comprise almost solely of racing simulators.\n\nGameplay\nThe gameplay takes place in waves, meaning that enemies will appear in the same groups and formations in the same order every playthrough. The player doesn't get to choose what order to engage an entire stage's enemies, just the ones in the current wave. This rail-shooter element does not hamper the player's freedom to fly where they choose in most stages. On some stages, the player has complete control of Omega Boost, specifically areas where they are in Planet ETA's atmosphere. Other stages limit the player in terms of speed (falling through the timeshift).",
        "The \"Boost\" part of the mech's name comes from Omega Boost's booster pack, allowing the player to move in any direction and circle strafe enemies with a scanning and lock-on feature. Omega Boost also learns the Viper Boost maneuver once it is levelled up. Viper Boost, when engaged, will cause Omega Boost to glow blue as it tears through enemies on screen. Destroying enemies will cause the gauge to refill incrementally.   However, the game can be completed without ever using Viper Boost.  If Viper Boost is used, the final ranking will have \"Pixy\" added onto the title, showing the attack during play.",
        "Story\nIn the past, an artificial intelligence named AlphaCore peacefully and silently co-existed with the human race, though its origins remain unknown. Eventually, the human race advanced to the point where they became aware of AlphaCore and its capabilities, and were shocked by what it was capable of. Fearing its power, humanity tried to 'dump' AlphaCore- presumably an attempt to destroy or manipulate the AI- but the action failed, only provoking the AlphaCore and starting a war between humans and machines. This war goes on into the distant future, with mankind steadily being outmatched by AlphaCore, who is capable of destroying entire cities easily.",
        "In this future, scientists devise a way to travel through time in order stop AlphaCore. However, AlphaCore discovers this plan and steals the time travel technology. It builds a giant shaft, the Timeshaft, on a desolate, mined out planet named ETA, and uses this to travel back in time and alter ENIAC, the first general-use computer created and considered by AlphaCore to be the first artificial intelligence. It plans to implant a virus into one of ENIAC's vacuum tubes, thus creating a predestination paradox and ensuring its survival in the present day.",
        "To counter AlphaCore's scheme, human scientists create the Omega Boost, a giant robot capable of traveling through time using the Direct Drive System (DDS). Lester J. Hemming, an experienced pilot and one of very few who can pilot the Omega Boost, is charged with traveling back in time to stop AlphaCore by finding ENIAC and replacing the AlphaCore infected vacuum tube, thus stopping AlphaCore before it even has a chance to exist.",
        "Omega Boost manages to pierce the defenses around planet ETA, encountering along the way a red 'knockoff' of Omega Boost made by AlphaCore named Beta Boost, and enters the Timeshaft encountering resistance around and inside. However, after traveling through, it finds itself not in the past of 1946, but near planet ETA a few decades into the future. The future ETA has become the home base of AlphaCore and turned into a cybernetic planet little resembling its present self, overrun with AlphaCore's machinery. Omega Boost proceeds to breach the planetary defenses, once again defeating Beta Boost (who has been given a version of the Viper Boost attack) and fending off the avatar of AlphaCore itself.",
        "Afterwards, the planet is destroyed but the Timeshaft survives, now its own structure akin to a space station, and AlphaCore attempts to escape through it to an unknown time. Omega Boost pursues it and destroys its three forms- crystal, serpent-like machine, and finally the true form, a vaguely fairylike humanoid with wings. After defeat, AlphaCore attempts to possess Omega Boost, but fails, disintegrating.\n\nThe end cutscene depicts Omega Boost arriving in 1946, crashing into a forest in North America. Lester locates ENIAC and, through a small probe, finds the vacuum tube infected by AlphaCore's virus, switching it out for the safe tube seen in the intro; thus preventing AlphaCore's rise to power and subsequent destruction of the human race.",
        "Development\nSimilarities between Omega Boost and Sega's Panzer Dragoon series led to a rumor that former members of Team Andromeda, dissolved in 1998, had joined Polyphony Digital. This rumor turned out to be true as the lead designer and programmer on Omega Boost was Yuji Yasuhara, who had worked on Panzer Dragoon Zwei.\n\nAmong the games created by Polyphony Digital, Omega Boost was the only shoot 'em up, while the others are vehicle racing simulators.\n\nAudio\n\nThe Omega Boost Original Soundtrack was released on June 19, 1999.",
        "Audio\n\nThe Omega Boost Original Soundtrack was released on June 19, 1999.\n\nThe album had a limited print and is considered very rare. As such, many fans of the game have found it easier to rip the soundtrack from the game disc itself, however, this leads to confusion over the official titles of the tracks, mainly because they are labeled as \"areas\" in-game instead of the official names given by the creators.",
        "The opening movie and ending credits in each version features different music. The Japanese version uses \"Shade\" by Feeder as its opening theme and final boss theme, and \"Ismeel\" by Dip in the Pool as its ending theme. The North American version licenses songs by Loudmouth, opening with \"Fly\" and closing with \"The Road\"; as well as featuring \"Otsegolation\" by Static-X, played during the title screen and the final boss. Finally, the European version uses \"Dreamer\" by Cast as the opening, final boss and ending themes.\n\nMerchandise\nA series of action figures was created by Blue Box Toys, featuring mecha from the game, including: Omega Boost and Beta Boost. A third figure, Herbarcher, was shown on the back of the boxes; however, it was never released.\n\nReception",
        "Reception\n\nOmega Boost received above-average reviews according to the review aggregation website GameRankings. However, Chris Charla of NextGen called it \"The textbook definition of a two-star game: competent, but totally uninspired.\" In Japan, Famitsu gave it a better score of 28 out of 40.\n\nNotes\n\nReferences\n\nExternal links\n\nOmega Boost at Game Set Watch\n\n1999 video games\nPlayStation (console) games\nPlayStation (console)-only games\nRail shooters\nScience fiction video games\nSingle-player video games\nSony Interactive Entertainment games\nVideo games about mecha\nVideo games developed in Japan\nPolyphony Digital games"
    ],
    [
        "Optical aberration",
        "In optics, aberration is a property of optical systems, such as lenses, that causes light to be spread out over some region of space rather than focused to a point. Aberrations cause the image formed by a lens to be blurred or distorted, with the nature of the distortion depending on the type of aberration. Aberration can be defined as a departure of the performance of an optical system from the predictions of paraxial optics. In an imaging system, it occurs when light from one point of an object does not converge into (or does not diverge from) a single point after transmission through the system. Aberrations occur because the simple paraxial theory is not a completely accurate model of the effect of an optical system on light, rather than due to flaws in the optical elements.",
        "An image-forming optical system with aberration will produce an image which is not sharp. Makers of optical instruments need to correct optical systems to compensate for aberration.\n\nAberration can be analyzed with the techniques of geometrical optics.  The articles on reflection, refraction and caustics discuss the general features of reflected and refracted rays.\n\nOverview\n\nWith an ideal lens, light from any given point on an object would pass through the lens and come together at a single point in the image plane (or, more generally, the image surface). Real lenses do not focus light exactly to a single point, however, even when they are perfectly made. These deviations from the idealized lens performance are called aberrations of the lens.",
        "Aberrations fall into two classes: monochromatic and chromatic. Monochromatic aberrations are caused by the geometry of the lens or mirror and occur both when light is reflected and when it is refracted. They appear even when using monochromatic light, hence the name.\n\nChromatic aberrations are caused by dispersion, the variation of a lens's refractive index with wavelength. Because of dispersion, different wavelengths of light come to focus at different points. Chromatic aberration does not appear when monochromatic light is used.\n\nMonochromatic aberrations\n\nThe most common monochromatic aberrations are:\n\nDefocus\nSpherical aberration\nComa\nAstigmatism\nField curvature\nImage distortion",
        "Monochromatic aberrations\n\nThe most common monochromatic aberrations are:\n\nDefocus\nSpherical aberration\nComa\nAstigmatism\nField curvature\nImage distortion\n\nAlthough defocus is technically the lowest-order of the optical aberrations, it is usually not considered as a lens aberration, since it can be corrected by moving the lens (or the image plane) to bring the image plane to the optical focus of the lens.\n\nIn addition to these aberrations, piston and tilt are effects which shift the position of the focal point.  Piston and tilt are not true optical aberrations, since when an otherwise perfect wavefront is altered by piston and tilt, it will still form a perfect, aberration-free image, only shifted to a different position.\n\nChromatic aberrations",
        "Chromatic aberrations\n\nChromatic aberration occurs when different wavelengths are not focussed to the same point.  Types of chromatic aberration are:\n\nAxial (or \"longitudinal\") chromatic aberration\nLateral (or \"transverse\") chromatic aberration\n\nTheory of monochromatic aberration",
        "In a perfect optical system in the classical theory of optics, rays of light proceeding from any object point unite in an image point; and therefore the object space is reproduced in an image space. The introduction of simple auxiliary terms, due to Gauss, named the focal lengths and focal planes, permits the determination of the image of any object for any system. The Gaussian theory, however, is only true so long as the angles made by all rays with the optical axis (the symmetrical axis of the system) are infinitely small, i.e., with infinitesimal objects, images and lenses; in practice these conditions may not be realized, and the images projected by uncorrected systems are, in general, ill-defined and often blurred if the aperture or field of view exceeds certain limits.",
        "The investigations of James Clerk Maxwell and Ernst Abbe showed that the properties of these reproductions, i.e., the relative position and magnitude of the images, are not special properties of optical systems, but necessary consequences of the supposition (per Abbe) of the reproduction of all points of a space in image points, and are independent of the manner in which the reproduction is effected. These authors showed, however, that no optical system can justify these suppositions, since they are contradictory to the fundamental laws of reflection and refraction.  Consequently, the Gaussian theory only supplies a convenient method of approximating reality; realistic optical systems fall short of this unattainable ideal. Currently, all that can be accomplished is the projection of a",
        "theory only supplies a convenient method of approximating reality; realistic optical systems fall short of this unattainable ideal. Currently, all that can be accomplished is the projection of a single plane onto another plane; but even in this, aberrations always occurs and it may be unlikely that these will ever be entirely corrected.",
        "Aberration of axial points (spherical aberration in the restricted sense)",
        "Let S (fig. 1) be any optical system, rays proceeding from an axis point O under an angle u1 will unite in the axis point O'1; and those under an angle u2 in the axis point O'2.  If there is refraction at a collective spherical surface, or through a thin positive lens, O'2 will lie in front of O'1 so long as the angle u2 is greater than u1 (under correction); and conversely with a dispersive surface or lenses (over correction).  The caustic, in the first case, resembles the sign > (greater than); in the second < (less than).  If the angle u1 is very small, O'1 is the Gaussian image; and O'1 O'2 is termed the longitudinal aberration, and O'1R the lateral aberration of the pencils with aperture u2. If the pencil with the angle u2 is that of the maximum aberration of all the pencils",
        "O'1 O'2 is termed the longitudinal aberration, and O'1R the lateral aberration of the pencils with aperture u2. If the pencil with the angle u2 is that of the maximum aberration of all the pencils transmitted, then in a plane perpendicular to the axis at O'1 there is a circular disk of confusion of radius O'1R, and in a parallel plane at O'2 another one of radius O'2R2; between these two is situated the disk of least confusion.",
        "The largest opening of the pencils, which take part in the reproduction of O, i.e., the angle u, is generally determined by the margin of one of the lenses or by a hole in a thin plate placed between, before, or behind the lenses of the system.  This hole is termed the stop or diaphragm; Abbe used the term aperture stop for both the hole and the limiting margin of the lens.  The component S1 of the system, situated between the aperture stop and the object O, projects an image of the diaphragm, termed by Abbe the entrance pupil; the exit pupil is the image formed by the component S2, which is placed behind the aperture stop.  All rays which issue from O and pass through the aperture stop also pass through the entrance and exit pupils, since these are images of the aperture stop.  Since the",
        "behind the aperture stop.  All rays which issue from O and pass through the aperture stop also pass through the entrance and exit pupils, since these are images of the aperture stop.  Since the maximum aperture of the pencils issuing from O is the angle u subtended by the entrance pupil at this point, the magnitude of the aberration will be determined by the position and diameter of the entrance pupil.  If the system be entirely behind the aperture stop, then this is itself the entrance pupil (front stop); if entirely in front, it is the exit pupil (back stop).",
        "If the object point be infinitely distant, all rays received by the first member of the system are parallel, and their intersections, after traversing the system, vary according to their perpendicular height of incidence, i.e. their distance from the axis.  This distance replaces the angle u in the preceding considerations; and the aperture, i.e., the radius of the entrance pupil, is its maximum value.",
        "Aberration of elements, i.e. smallest objects at right angles to the axis",
        "If rays issuing from O (fig. 1) are concurrent, it does not follow that points in a portion of a plane perpendicular at O to the axis will be also concurrent, even if the part of the plane be very small.  As the diameter of the lens increases (i.e., with increasing aperture), the neighboring point N will be reproduced, but attended by aberrations comparable in magnitude to ON. These aberrations are avoided if, according to Abbe, the sine condition, sin u'1/sin u1=sin u'2/sin u2, holds for all rays reproducing the point O. If the object point O is infinitely distant, u1 and u2 are to be replaced by h1 and h2, the perpendicular heights of incidence; the sine condition then becomes sin u'1/h1=sin u'2/h2. A system fulfilling this condition and free from spherical aberration is called",
        "replaced by h1 and h2, the perpendicular heights of incidence; the sine condition then becomes sin u'1/h1=sin u'2/h2. A system fulfilling this condition and free from spherical aberration is called aplanatic (Greek a-, privative, plann, a wandering).  This word was first used by Robert Blair to characterize a superior achromatism, and, subsequently, by many writers to denote freedom from spherical aberration as well.",
        "Since the aberration increases with the distance of the ray from the center of the lens, the aberration increases as the lens diameter increases (or, correspondingly, with the diameter of the aperture), and hence can be minimized by reducing the aperture, at the cost of also reducing the amount of light reaching the image plane.\n\nAberration of lateral object points (points beyond the axis) with narrow pencils — astigmatism",
        "A point O (fig. 2) at a finite distance from the axis (or with an infinitely distant object, a point which subtends a finite angle at the system) is, in general, even then not sharply reproduced if the pencil of rays issuing from it and traversing the system is made infinitely narrow by reducing the aperture stop; such a pencil consists of the rays which can pass from the object point through the now infinitely small entrance pupil.  It is seen (ignoring exceptional cases) that the pencil does not meet the refracting or reflecting surface at right angles; therefore it is astigmatic (Gr. a-, privative, stigmia, a point).  Naming the central ray passing through the entrance pupil the axis of the pencil or principal ray, it can be said: the rays of the pencil intersect, not in one point, but",
        "stigmia, a point).  Naming the central ray passing through the entrance pupil the axis of the pencil or principal ray, it can be said: the rays of the pencil intersect, not in one point, but in two focal lines, which can be assumed to be at right angles to the principal ray; of these, one lies in the plane containing the principal ray and the axis of the system, i.e. in the first principal section or meridional section, and the other at right angles to it, i.e. in the second principal section or sagittal section.  We receive, therefore, in no single intercepting plane behind the system, as, for example, a focusing screen, an image of the object point; on the other hand, in each of two planes lines O' and O\" are separately formed (in neighboring planes ellipses are formed), and in a plane",
        "example, a focusing screen, an image of the object point; on the other hand, in each of two planes lines O' and O\" are separately formed (in neighboring planes ellipses are formed), and in a plane between O' and O\" a circle of least confusion.  The interval O'O\", termed the astigmatic difference, increases, in general, with the angle W made by the principal ray OP with the axis of the system, i.e. with the field of view.  Two astigmatic image surfaces correspond to one object plane; and these are in contact at the axis point; on the one lie the focal lines of the first kind, on the other those of the second.  Systems in which the two astigmatic surfaces coincide are termed anastigmatic or stigmatic.",
        "Sir Isaac Newton was probably the discoverer of astigmation; the position of the astigmatic image lines was determined by Thomas Young; and the theory was developed by Allvar Gullstrand.  A bibliography by P. Culmann is given in Moritz von Rohr's Die Bilderzeugung in optischen Instrumenten.",
        "Aberration of lateral object points with broad pencils — coma",
        "By opening the stop wider, similar deviations arise for lateral points as have been already discussed for axial points; but in this case they are much more complicated.  The course of the rays in the meridional section is no longer symmetrical to the principal ray of the pencil; and on an intercepting plane there appears, instead of a luminous point, a patch of light, not symmetrical about a point, and often exhibiting a resemblance to a comet having its tail directed towards or away from the axis.  From this appearance it takes its name.  The unsymmetrical form of the meridional pencil—formerly the only one considered—is coma in the narrower sense only; other errors of coma have been treated by Arthur König and Moritz von Rohr, and later by Allvar Gullstrand.",
        "Curvature of the field of the image\n\nIf the above errors be eliminated, the two astigmatic surfaces united, and a sharp image obtained with a wide aperture—there remains the necessity to correct the curvature of the image surface, especially when the image is to be received upon a plane surface, e.g. in photography.  In most cases the surface is concave towards the system.\n\nDistortion of the image",
        "Even if the image is sharp, it may be distorted compared to ideal pinhole projection.  In pinhole projection, the magnification of an object is inversely proportional to its distance to the camera along the optical axis so that a camera pointing directly at a flat surface reproduces that flat surface. Distortion can be thought of as stretching the image non-uniformly, or, equivalently, as a variation in magnification across the field. While \"distortion\" can include arbitrary deformation of an image, the most pronounced modes of distortion produced by conventional imaging optics is \"barrel distortion\", in which the center of the image is magnified more than the perimeter (figure 3a). The reverse, in which the perimeter is magnified more than the center, is known as \"pincushion distortion\"",
        "in which the center of the image is magnified more than the perimeter (figure 3a). The reverse, in which the perimeter is magnified more than the center, is known as \"pincushion distortion\" (figure 3b). This effect is called lens distortion or image distortion, and there are algorithms to correct it.",
        "Systems free of distortion are called orthoscopic (orthos, right, skopein to look) or rectilinear (straight lines).",
        "This aberration is quite distinct from that of the sharpness of reproduction; in unsharp, reproduction, the question of distortion arises if only parts of the object can be recognized in the figure. If, in an unsharp image, a patch of light corresponds to an object point, the center of gravity of the patch may be regarded as the image point, this being the point where the plane receiving the image, e.g., a focusing screen, intersects the ray passing through the middle of the stop. This assumption is justified if a poor image on the focusing screen remains stationary when the aperture is diminished; in practice, this generally occurs. This ray, named by Abbe a principal ray (not to be confused with the principal rays of the Gaussian theory), passes through the center of the entrance pupil",
        "in practice, this generally occurs. This ray, named by Abbe a principal ray (not to be confused with the principal rays of the Gaussian theory), passes through the center of the entrance pupil before the first refraction, and the center of the exit pupil after the last refraction.  From this it follows that correctness of drawing depends solely upon the principal rays; and is independent of the sharpness or curvature of the image field. Referring to fig. 4, we have O'Q'/OQ = a' tan w'/a tan w = 1/N, where N is the scale or magnification of the image. For N to be constant for all values of w, a' tan w'/a tan w must also be constant. If the ratio a'/a be sufficiently constant, as is often the case, the above relation reduces to the condition of Airy, i.e. tan w'/ tan w= a constant. This",
        "w, a' tan w'/a tan w must also be constant. If the ratio a'/a be sufficiently constant, as is often the case, the above relation reduces to the condition of Airy, i.e. tan w'/ tan w= a constant. This simple relation (see Camb.  Phil.  Trans., 1830, 3, p. 1) is fulfilled in all systems which are symmetrical with respect to their diaphragm (briefly named symmetrical or holosymmetrical objectives), or which consist of two like, but different-sized, components, placed from the diaphragm in the ratio of their size, and presenting the same curvature to it (hemisymmetrical objectives); in these systems tan w' / tan w = 1.",
        "The constancy of a'/a necessary for this relation to hold was pointed out by R. H. Bow (Brit.  Journ.  Photog., 1861), and Thomas Sutton (Photographic Notes, 1862); it has been treated by O. Lummer and by M. von Rohr (Zeit. f.  Instrumentenk., 1897, 17, and 1898, 18, p. 4). It requires the middle of the aperture stop to be reproduced in the centers of the entrance and exit pupils without spherical aberration. M. von Rohr showed that for systems fulfilling neither the Airy nor the Bow-Sutton condition, the ratio a' cos w'/a tan w will be constant for one distance of the object.  This combined condition is exactly fulfilled by holosymmetrical objectives reproducing with the scale 1, and by hemisymmetrical, if the scale of reproduction be equal to the ratio of the sizes of the two",
        "combined condition is exactly fulfilled by holosymmetrical objectives reproducing with the scale 1, and by hemisymmetrical, if the scale of reproduction be equal to the ratio of the sizes of the two components.",
        "Zernike model of aberrations\n\nCircular wavefront profiles associated with aberrations may be mathematically modeled using Zernike polynomials.  Developed by Frits Zernike in the 1930s, Zernike's polynomials are orthogonal over a circle of unit radius. A complex, aberrated wavefront profile may be curve-fitted with Zernike polynomials to yield a set of fitting coefficients that individually represent different types of aberrations.  These Zernike coefficients are linearly independent, thus individual aberration contributions to an overall wavefront may be isolated and quantified separately.\n\nThere are even and odd Zernike polynomials. The even Zernike polynomials are defined as\n\nand the odd Zernike polynomials as",
        "There are even and odd Zernike polynomials. The even Zernike polynomials are defined as\n\nand the odd Zernike polynomials as\n\nwhere m and n are nonnegative integers with ,  Φ is the azimuthal angle in radians, and ρ is the normalized radial distance.  The radial polynomials  have no azimuthal dependence, and are defined as\n\nand  if  is odd.\n\nThe first few Zernike polynomials, multiplied by their respective fitting coefficients, are:\n\nwhere  is the normalized pupil radius with ,  is the azimuthal angle around the pupil with , and the fitting coefficients  are the wavefront errors in wavelengths.",
        "where  is the normalized pupil radius with ,  is the azimuthal angle around the pupil with , and the fitting coefficients  are the wavefront errors in wavelengths.\n\nAs in Fourier synthesis using sines and cosines, a wavefront may be perfectly represented by a sufficiently large number of higher-order Zernike polynomials.  However, wavefronts with very steep gradients or very high spatial frequency structure, such as produced by propagation through atmospheric turbulence or aerodynamic flowfields, are not well modeled by Zernike polynomials, which tend to low-pass filter fine spatial definition in the wavefront. In this case, other fitting methods such as fractals or singular value decomposition may yield improved fitting results.",
        "The circle polynomials were introduced by Frits Zernike to evaluate the point image of an aberrated optical system taking into account the effects of diffraction. The perfect point image in the presence of diffraction had already been described by Airy, as early as 1835. It took almost hundred years to arrive at a comprehensive theory and modeling of the point image of aberrated systems (Zernike and Nijboer). The  analysis by Nijboer and Zernike describes the intensity distribution close to the optimum focal plane. An extended theory that allows the calculation of the point image amplitude and  intensity over a much larger volume in the focal region was recently developed (Extended Nijboer-Zernike theory). This Extended Nijboer-Zernike theory of point image or 'point-spread function'",
        "and  intensity over a much larger volume in the focal region was recently developed (Extended Nijboer-Zernike theory). This Extended Nijboer-Zernike theory of point image or 'point-spread function' formation has found applications in general research on image formation, especially for systems with a high numerical aperture, and in characterizing optical systems with respect to their aberrations.",
        "Analytic treatment of aberrations",
        "The preceding review of the several errors of reproduction belongs to the Abbe theory of aberrations, in which definite aberrations are discussed separately; it is well suited to practical needs, for in the construction of an optical instrument certain errors are sought to be eliminated, the selection of which is justified by experience.  In the mathematical sense, however, this selection is arbitrary; the reproduction of a finite object with a finite aperture entails, in all probability, an infinite number of aberrations.  This number is only finite if the object and aperture are assumed to be infinitely small of a certain order; and with each order of infinite smallness, i.e. with each degree of approximation to reality (to finite objects and apertures), a certain number of aberrations",
        "be infinitely small of a certain order; and with each order of infinite smallness, i.e. with each degree of approximation to reality (to finite objects and apertures), a certain number of aberrations is associated.  This connection is only supplied by theories which treat aberrations generally and analytically by means of indefinite series.",
        "A ray proceeding from an object point O (fig. 5) can be defined by the coordinates (ξ, η).  Of this point O in an object plane I, at right angles to the axis, and two other coordinates (x, y), the point in which the ray intersects the entrance pupil, i.e. the plane II. Similarly the corresponding image ray may be defined by the points (ξ', η'), and (x', y'), in the planes I' and II'. The origins of these four plane coordinate systems may be collinear with the axis of the optical system; and the corresponding axes may be parallel.  Each of the four coordinates ξ', η', x', y' are functions of ξ, η, x, y; and if it be assumed that the field of view and the aperture be infinitely small, then ξ, η, x, y are of the same order of infinitesimals; consequently by expanding ξ', η', x', y' in",
        "of ξ, η, x, y; and if it be assumed that the field of view and the aperture be infinitely small, then ξ, η, x, y are of the same order of infinitesimals; consequently by expanding ξ', η', x', y' in ascending powers of ξ, η, x, y, series are obtained in which it is only necessary to consider the lowest powers.  It is readily seen that if the optical system be symmetrical, the origins of the coordinate systems collinear with the optical axis and the corresponding axes parallel, then by changing the signs of ξ, η, x, y, the values ξ', η', x', y' must likewise change their sign, but retain their arithmetical values; this means that the series are restricted to odd powers of the unmarked variables.",
        "The nature of the reproduction consists in the rays proceeding from a point O being united in another point O'; in general, this will not be the case, for ξ', η' vary if ξ, η be constant, but x, y variable.  It may be assumed that the planes I' and II' are drawn where the images of the planes I and II are formed by rays near the axis by the ordinary Gaussian rules; and by an extension of these rules, not, however, corresponding to reality, the Gauss image point O'0, with coordinates ξ'0, η'0, of the point O at some distance from the axis could be constructed.  Writing Dξ'=ξ'-ξ'0 and Dη'=η'-η'0, then Dξ' and Dη' are the aberrations belonging to ξ, η and x, y, and are functions of these magnitudes which, when expanded in series, contain only odd powers, for the same reasons as given above.",
        "then Dξ' and Dη' are the aberrations belonging to ξ, η and x, y, and are functions of these magnitudes which, when expanded in series, contain only odd powers, for the same reasons as given above.  On account of the aberrations of all rays which pass through O, a patch of light, depending in size on the lowest powers of ξ, η, x, y which the aberrations contain, will be formed in the plane I'. These degrees, named by J. Petzval the numerical orders of the image, are consequently only odd powers; the condition for the formation of an image of the mth order is that in the series for Dξ' and Dη' the coefficients of the powers of the 3rd, 5th...(m-2)th degrees must vanish.  The images of the Gauss theory being of the third order, the next problem is to obtain an image of 5th order, or to make",
        "the coefficients of the powers of the 3rd, 5th...(m-2)th degrees must vanish.  The images of the Gauss theory being of the third order, the next problem is to obtain an image of 5th order, or to make the coefficients of the powers of 3rd degree zero. This necessitates the satisfying of five equations; in other words, there are five alterations of the 3rd order, the vanishing of which produces an image of the 5th order.",
        "The expression for these coefficients in terms of the constants of the optical system, i.e. the radii, thicknesses, refractive indices and distances between the lenses, was solved by L. Seidel; in 1840, J. Petzval constructed his portrait objective, from similar calculations which have never been published. The theory was elaborated by S. Finterswalder, who also published a posthumous paper of Seidel containing a short view of his work; a simpler form was given by A. Kerber. A. Konig and M. von Rohr have represented Kerber's method, and have deduced the Seidel formulae from geometrical considerations based on the Abbe method, and have interpreted the analytical results geometrically.",
        "The aberrations can also be expressed by means of the characteristic function of the system and its differential coefficients, instead of by the radii, &c., of the lenses; these formulae are not immediately applicable, but give, however, the relation between the number of aberrations and the order. Sir William Rowan Hamilton (British Assoc.  Report, 1833, p. 360) thus derived the aberrations of the third order; and in later times the method was pursued by Clerk Maxwell (Proc. London Math. Soc., 1874–1875; (see also the treatises of R. S. Heath and L. A. Herman), M. Thiesen (Berlin. Akad.  Sitzber., 1890, 35, p. 804), H. Bruns (Leipzig. Math. Phys. Ber., 1895, 21, p. 410), and particularly successfully by K. Schwarzschild (Göttingen.  Akad.  Abhandl., 1905, 4, No. 1), who thus discovered",
        "1890, 35, p. 804), H. Bruns (Leipzig. Math. Phys. Ber., 1895, 21, p. 410), and particularly successfully by K. Schwarzschild (Göttingen.  Akad.  Abhandl., 1905, 4, No. 1), who thus discovered the aberrations of the 5th order (of which there are nine), and possibly the shortest proof of the practical (Seidel) formulae.  A. Gullstrand (vide supra, and Ann. d.  Phys., 1905, 18, p. 941) founded his theory of aberrations on the differential geometry of surfaces.",
        "The aberrations of the third order are: (1) aberration of the axis point; (2) aberration of points whose distance from the axis is very small, less than of the third order — the deviation from the sine condition and coma here fall together in one class; (3) astigmatism; (4) curvature of the field; (5) distortion.",
        "Aberration of the third order of axis points is dealt with in all text-books on optics.  It is very important in telescope design. In telescopes aperture is usually taken as the linear diameter of the objective. It is not the same as microscope aperture which is based on the entrance pupil or field of view as seen from the object and is expressed as an angular measurement. Higher order aberrations in telescope design can be mostly neglected. For microscopes it cannot be neglected. For a single lens of very small thickness and given power, the aberration depends upon the ratio of the radii r:r', and is a minimum (but never zero) for a certain value of this ratio; it varies inversely with the refractive index (the power of the lens remaining constant).  The total aberration of two or more",
        "and is a minimum (but never zero) for a certain value of this ratio; it varies inversely with the refractive index (the power of the lens remaining constant).  The total aberration of two or more very thin lenses in contact, being the sum of the individual aberrations, can be zero. This is also possible if the lenses have the same algebraic sign. Of thin positive lenses with n=1.5, four are necessary to correct spherical aberration of the third order. These systems, however, are not of great practical importance.  In most cases, two thin lenses are combined, one of which has just so strong a positive aberration (under-correction, vide supra) as the other a negative; the first must be a positive lens and the second a negative lens; the powers, however: may differ, so that the desired",
        "a positive aberration (under-correction, vide supra) as the other a negative; the first must be a positive lens and the second a negative lens; the powers, however: may differ, so that the desired effect of the lens is maintained. It is generally an advantage to secure a great refractive effect by several weaker than by one high-power lens.  By one, and likewise by several, and even by an infinite number of thin lenses in contact, no more than two axis points can be reproduced without aberration of the third order. Freedom from aberration for two axis points, one of which is infinitely distant, is known as Herschel's condition. All these rules are valid, inasmuch as the thicknesses and distances of the lenses are not to be taken into account.",
        "The condition for freedom from coma in the third order is also of importance for telescope objectives; it is known as Fraunhofer's condition. (4) After eliminating the aberration On the axis, coma and astigmatism, the relation for the flatness of the field in the third order is expressed by the Petzval equation, S1/r(n'−n) = 0, where r is the radius of a refracting surface, n and n' the refractive indices of the neighboring media, and S the sign of summation for all refracting surfaces.",
        "Practical elimination of aberrations\n\nThe classical imaging problem is to reproduce perfectly a finite plane (the object) onto another plane (the image) through a finite aperture. It is impossible to do so perfectly for more than one such pair of planes (this was proven with increasing generality by Maxwell in 1858, by Bruns in 1895, and by Carathéodory in 1926, see summary in Walther, A., J. Opt. Soc. Am. A 6, 415–422 (1989)). For a single pair of planes (e.g. for a single focus setting of an objective), however, the problem can in principle be solved perfectly. Examples of such a theoretically perfect system include the Luneburg lens and the Maxwell fish-eye.",
        "Practical methods solve this problem with an accuracy which mostly suffices for the special purpose of each species of instrument. The problem of finding a system which reproduces a given object upon a given plane with given magnification (insofar as aberrations must be taken into account) could be dealt with by means of the approximation theory; in most cases, however, the analytical difficulties were too great for older calculation methods but may be ameliorated by application of modern computer systems. Solutions, however, have been obtained in special cases. At the present time constructors almost always employ the inverse method: they compose a system from certain, often quite personal experiences, and test, by the trigonometrical calculation of the paths of several rays, whether the",
        "almost always employ the inverse method: they compose a system from certain, often quite personal experiences, and test, by the trigonometrical calculation of the paths of several rays, whether the system gives the desired reproduction (examples are given in A. Gleichen, Lehrbuch der geometrischen Optik, Leipzig and Berlin, 1902). The radii, thicknesses and distances are continually altered until the errors of the image become sufficiently small. By this method only certain errors of reproduction are investigated, especially individual members, or all, of those named above. The analytical approximation theory is often employed provisionally, since its accuracy does not generally suffice.",
        "In order to render spherical aberration and the deviation from the sine condition small throughout the whole aperture, there is given to a ray with a finite angle of aperture u* (width infinitely distant objects: with a finite height of incidence h*) the same distance of intersection, and the same sine ratio as to one neighboring the axis (u* or h* may not be much smaller than the largest aperture U or H to be used in the system). The rays with an angle of aperture smaller than u* would not have the same distance of intersection and the same sine ratio; these deviations are called zones, and the constructor endeavors to reduce these to a minimum. The same holds for the errors depending upon the angle of the field of view, w: astigmatism, curvature of field and distortion are eliminated",
        "the constructor endeavors to reduce these to a minimum. The same holds for the errors depending upon the angle of the field of view, w: astigmatism, curvature of field and distortion are eliminated for a definite value, w*, zones of astigmatism, curvature of field and distortion, attend smaller values of w. The practical optician names such systems: corrected for the angle of aperture u* (the height of incidence h*) or the angle of field of view w*. Spherical aberration and changes of the sine ratios are often represented graphically as functions of the aperture, in the same way as the deviations of two astigmatic image surfaces of the image plane of the axis point are represented as functions of the angles of the field of view.",
        "The final form of a practical system consequently rests on compromise; enlargement of the aperture results in a diminution of the available field of view, and vice versa. But the larger aperture will give the larger resolution. The following may be regarded as typical:\n Largest aperture; necessary corrections are — for the axis point, and sine condition; errors of the field of view are almost disregarded; example — high-power microscope objectives.",
        "Largest aperture; necessary corrections are — for the axis point, and sine condition; errors of the field of view are almost disregarded; example — high-power microscope objectives.\n Wide angle lens; necessary corrections are — for astigmatism, curvature of field and distortion; errors of the aperture only slightly regarded; examples — photographic widest angle objectives and oculars.  Between these extreme examples stands the normal lens: this is corrected more with regard to aperture; objectives for groups more with regard to the field of view.",
        "Long focus lenses have small fields of view and aberrations on axis are very important. Therefore zones will be kept as small as possible and design should emphasize simplicity. Because of this these lenses are the best for analytical computation.",
        "Chromatic or color aberration",
        "In optical systems composed of lenses, the position, magnitude and errors of the image depend upon the refractive indices of the glass employed (see Lens (optics) and Monochromatic aberration, above). Since the index of refraction varies with the color or wavelength of the light (see dispersion), it follows that a system of lenses (uncorrected) projects images of different colors in somewhat different places and sizes and with different aberrations; i.e. there are chromatic differences of the distances of intersection, of magnifications, and of monochromatic aberrations. If mixed light be employed (e.g. white light) all these images are formed and they cause a confusion, named chromatic aberration; for instance, instead of a white margin on a dark background, there is perceived a colored",
        "(e.g. white light) all these images are formed and they cause a confusion, named chromatic aberration; for instance, instead of a white margin on a dark background, there is perceived a colored margin, or narrow spectrum.  The absence of this error is termed achromatism, and an optical system so corrected is termed achromatic. A system is said to be chromatically under-corrected when it shows the same kind of chromatic error as a thin positive lens, otherwise it is said to be overcorrected.",
        "If, in the first place, monochromatic aberrations be neglected — in other words, the Gaussian theory be accepted — then every reproduction is determined by the positions of the focal planes, and the magnitude of the focal lengths, or if the focal lengths, as ordinarily happens, be equal, by three constants of reproduction.  These constants are determined by the data of the system (radii, thicknesses, distances, indices, etc., of the lenses); therefore their dependence on the refractive index, and consequently on the color, are calculable. The refractive indices for different wavelengths must be known for each kind of glass made use of. In this manner the conditions are maintained that any one constant of reproduction is equal for two different colors, i.e. this constant is achromatized.",
        "be known for each kind of glass made use of. In this manner the conditions are maintained that any one constant of reproduction is equal for two different colors, i.e. this constant is achromatized. For example, it is possible, with one thick lens in air, to achromatize the position of a focal plane of the magnitude of the focal length. If all three constants of reproduction be achromatized, then the Gaussian image for all distances of objects is the same for the two colors, and the system is said to be in stable achromatism.",
        "In practice it is more advantageous (after Abbe) to determine the chromatic aberration (for instance, that of the distance of intersection) for a fixed position of the object, and express it by a sum in which each component conlins the amount due to each refracting surface. In a plane containing the image point of one color, another colour produces a disk of confusion; this is similar to the confusion caused by two zones in spherical aberration. For infinitely distant objects the radius Of the chromatic disk of confusion is proportional to the linear aperture, and independent of the focal length (vide supra, Monochromatic Aberration of the Axis Point); and since this disk becomes the less harmful with an increasing image of a given object, or with increasing focal length, it follows that",
        "(vide supra, Monochromatic Aberration of the Axis Point); and since this disk becomes the less harmful with an increasing image of a given object, or with increasing focal length, it follows that the deterioration of the image is proportional to the ratio of the aperture to the focal length, i.e. the relative aperture. (This explains the gigantic focal lengths in vogue before the discovery of achromatism.)",
        "Examples:\n\nNewton failed to perceive the existence of media of different dispersive powers required by achromatism; consequently he constructed large reflectors instead of refractors. James Gregory and Leonhard Euler arrived at the correct view from a false conception of the achromatism of the eye; this was determined by Chester More Hall in 1728, Klingenstierna in 1754 and by Dollond in 1757, who constructed the celebrated achromatic telescopes. (See telescope.)",
        "Glass with weaker dispersive power (greater ) is named crown glass; that with greater dispersive power, flint glass. For the construction of an achromatic collective lens ( positive) it follows, by means of equation (4), that a collective lens I. of crown glass and a dispersive lens II. of flint glass must be chosen; the latter, although the weaker, corrects the other chromatically by its greater dispersive power. For an achromatic dispersive lens the converse must be adopted. This is, at the present day, the ordinary type, e.g., of telescope objective; the values of the four radii must satisfy the equations (2) and (4). Two other conditions may also be postulated: one is always the elimination of the aberration on the axis; the second either the Herschel or Fraunhofer Condition, the",
        "the equations (2) and (4). Two other conditions may also be postulated: one is always the elimination of the aberration on the axis; the second either the Herschel or Fraunhofer Condition, the latter being the best vide supra, Monochromatic Aberration). In practice, however, it is often more useful to avoid the second condition by making the lenses have contact, i.e. equal radii. According to P. Rudolph (Eder's Jahrb. f.  Photog., 1891, 5, p. 225; 1893, 7, p. 221), cemented objectives of thin lenses permit the elimination of spherical aberration on the axis, if, as above, the collective lens has a smaller refractive index; on the other hand, they permit the elimination of astigmatism and curvature of the field, if the collective lens has a greater refractive index (this follows from the",
        "has a smaller refractive index; on the other hand, they permit the elimination of astigmatism and curvature of the field, if the collective lens has a greater refractive index (this follows from the Petzval equation; see L. Seidel, Astr.  Nachr., 1856, p. 289). Should the cemented system be positive, then the more powerful lens must be positive; and, according to (4), to the greater power belongs the weaker dispersive power (greater ), that is to say, crown glass; consequently the crown glass must have the greater refractive index for astigmatic and plane images.  In all earlier kinds of glass, however, the dispersive power increased with the refractive index; that is,  decreased as  increased; but some of the Jena glasses by E. Abbe and O. Schott were crown glasses of high refractive",
        "however, the dispersive power increased with the refractive index; that is,  decreased as  increased; but some of the Jena glasses by E. Abbe and O. Schott were crown glasses of high refractive index, and achromatic systems from such crown glasses, with flint glasses of lower refractive index, are called the new achromats, and were employed by P. Rudolph in the first anastigmats (photographic objectives).",
        "Instead of making  vanish, a certain value can be assigned to it which will produce, by the addition of the two lenses, any desired chromatic deviation, e.g. sufficient to eliminate one present in other parts of the system. If the lenses I. and II. be cemented and have the same refractive index for one color, then its effect for that one color is that of a lens of one piece; by such decomposition of a lens it can be made chromatic or achromatic at will, without altering its spherical effect. If its chromatic effect () be greater than that of the same lens, this being made of the more dispersive of the two glasses employed, it is termed hyper-chromatic.",
        "For two thin lenses separated by a distance  the condition for achromatism is ; if  (e.g. if the lenses be made of the same glass), this reduces to , known as the condition for oculars.",
        "If a constant of reproduction, for instance the focal length, be made equal for two colors, then it is not the same for other colors, if two different glasses are employed.  For example, the condition for achromatism (4) for two thin lenses in contact is fulfilled in only one part of the spectrum, since  varies within the spectrum.  This fact was first ascertained by J. Fraunhofer, who defined the colors by means of the dark lines in the solar spectrum; and showed that the ratio of the dispersion of two glasses varied about 20% from the red to the violet (the variation for glass and water is about 50%).  If, therefore, for two colors, a and b, , then for a third color, c, the focal length is different; that is, if c lies between a and b, then , and vice versa; these algebraic results",
        "is about 50%).  If, therefore, for two colors, a and b, , then for a third color, c, the focal length is different; that is, if c lies between a and b, then , and vice versa; these algebraic results follow from the fact that towards the red the dispersion of the positive crown glass preponderates, towards the violet that of the negative flint.  These chromatic errors of systems, which are achromatic for two colors, are called the secondary spectrum, and depend upon the aperture and focal length in the same manner as the primary chromatic errors do.",
        "In fig. 6, taken from M. von Rohr's Theorie und Geschichte des photographischen Objectivs, the abscissae are focal lengths, and the ordinates wavelengths. The Fraunhofer lines used are shown in adjacent table.",
        "The focal lengths are made equal for the lines C and F. In the neighborhood of 550 nm the tangent to the curve is parallel to the axis of wavelengths; and the focal length varies least over a fairly large range of color, therefore in this neighborhood the color union is at its best. Moreover, this region of the spectrum is that which appears brightest to the human eye, and consequently this curve of the secondary on spectrum, obtained by making , is, according to the experiments of Sir G. G. Stokes (Proc.  Roy. Soc., 1878), the most suitable for visual instruments (optical achromatism,). In a similar manner, for systems used in photography, the vertex of the color curve must be placed in the position of the maximum sensibility of the plates; this is generally supposed to be at G'; and to",
        "similar manner, for systems used in photography, the vertex of the color curve must be placed in the position of the maximum sensibility of the plates; this is generally supposed to be at G'; and to accomplish this the F and violet mercury lines are united.  This artifice is specially adopted in objectives for astronomical photography (pure actinic achromatism). For ordinary photography, however, there is this disadvantage: the image on the focusing-screen and the correct adjustment of the photographic sensitive plate are not in register; in astronomical photography this difference is constant, but in other kinds it depends on the distance of the objects. On this account the lines D and G' are united for ordinary photographic objectives; the optical as well as the actinic image is",
        "constant, but in other kinds it depends on the distance of the objects. On this account the lines D and G' are united for ordinary photographic objectives; the optical as well as the actinic image is chromatically inferior, but both lie in the same place; and consequently the best correction lies in F (this is known as the actinic correction or freedom from chemical focus).",
        "Should there be in two lenses in contact the same focal lengths for three colours a, b, and c, i.e. , then the relative partial dispersion  must be equal for the two kinds of glass employed.  This follows by considering equation (4) for the two pairs of colors ac and bc. Until recently no glasses were known with a proportional degree of absorption; but R. Blair (Trans. Edin. Soc., 1791, 3, p. 3), P. Barlow, and F. S. Archer overcame the difficulty by constructing fluid lenses between glass walls. Fraunhofer prepared glasses which reduced the secondary spectrum; but permanent success was only assured on the introduction of the Jena glasses by E. Abbe and O. Schott.  In using glasses not having proportional dispersion, the deviation of a third colour can be eliminated by two lenses, if an",
        "assured on the introduction of the Jena glasses by E. Abbe and O. Schott.  In using glasses not having proportional dispersion, the deviation of a third colour can be eliminated by two lenses, if an interval be allowed between them; or by three lenses in contact, which may not all consist of the old glasses.  In uniting three colors an achromatism of a higher order is derived; there is yet a residual tertiary spectrum, but it can always be neglected.",
        "The Gaussian theory is only an approximation; monochromatic or spherical aberrations still occur, which will be different for different colors; and should they be compensated for one color, the image of another color would prove disturbing.  The most important is the chromatic difference of aberration of the axis point, which is still present to disturb the image, after par-axial rays of different colors are united by an appropriate combination of glasses.  If a collective system be corrected for the axis point for a definite wavelength, then, on account of the greater dispersion in the negative components — the flint glasses, — overcorrection will arise for the shorter wavelengths (this being the error of the negative components), and under-correction for the longer wavelengths (the",
        "components — the flint glasses, — overcorrection will arise for the shorter wavelengths (this being the error of the negative components), and under-correction for the longer wavelengths (the error of crown glass lenses preponderating in the red).  This error was treated by Jean le Rond d'Alembert, and, in special detail, by C. F. Gauss.  It increases rapidly with the aperture, and is more important with medium apertures than the secondary spectrum of par-axial rays; consequently, spherical aberration must be eliminated for two colors, and if this be impossible, then it must be eliminated for those particular wavelengths which are most effectual for the instrument in question (a graphical representation of this error is given in M. von Rohr, Theorie und Geschichte des photographischen",
        "for those particular wavelengths which are most effectual for the instrument in question (a graphical representation of this error is given in M. von Rohr, Theorie und Geschichte des photographischen Objectivs).",
        "The condition for the reproduction of a surface element in the place of a sharply reproduced point — the constant of the sine relationship must also be fulfilled with large apertures for several colors. E. Abbe succeeded in computing microscope objectives free from error of the axis point and satisfying the sine condition for several colors, which therefore, according to his definition, were aplanatic for several colors; such systems he termed apochromatic.  While, however, the magnification of the individual zones is the same, it is not the same for red as for blue; and there is a chromatic difference of magnification. This is produced in the same amount, but in the opposite sense, by the oculars, which Abbe used with these objectives (compensating oculars), so that it is eliminated in",
        "difference of magnification. This is produced in the same amount, but in the opposite sense, by the oculars, which Abbe used with these objectives (compensating oculars), so that it is eliminated in the image of the whole microscope. The best telescope objectives, and photographic objectives intended for three-color work, are also apochromatic, even if they do not possess quite the same quality of correction as microscope objectives do. The chromatic differences of other errors of reproduction seldom have practical importance.",
        "See also\n Aberrations of the eye\n \n Wavefront coding\n\nNotes\n\nReferences\n\nExternal links \n Microscope Objectives: Optical Aberrations section of Molecular Expressions website, Michael W. Davidson, Mortimer Abramowitz, Olympus America Inc., and The Florida State University\n\nGeometrical optics"
    ],
    [
        "Optical audio disc\nAn audio optical disc is an optical disc that stores sound information such as music or speech.\n\nIt may specifically refer to:\n\nAudio CDs\n Compact disc (CD), an optical disc used to store digital data (700 MB storage)\n Compact Disc Digital Audio (CD-DA), a CD that contains PCM encoded digital audio in the original \"Red Book\" CD-DA format\n 5.1 Music Disc, an extension to the Red Book standard that uses DTS Coherent Acoustics 5.1 surround sound\n Compressed audio optical disc, an optical disc storing MP3s and other compressed audio files as data, rather than in the Red Book format\n\nAudio DVDs\nDVD, 4 GB single layer, 8 GB double layer storage\n DVD-Audio, a DVD that plays audio\n Super Audio CD (SACD), a format which competes with DVD-Audio",
        "Audio DVDs\nDVD, 4 GB single layer, 8 GB double layer storage\n DVD-Audio, a DVD that plays audio\n Super Audio CD (SACD), a format which competes with DVD-Audio\n\nAudio Blu-rays\nBlu-ray, 25 GB single layer, 50 GB double layer\n BD-Audio, a Blu-ray disc that is capable of audio-only playback\n\nSee also\nCD-4 or Compatible discrete four-channel sound, a variety of quadrophonic audio for vinyl records"
    ],
    [
        "Parallel ATA\nParallel ATA (PATA), originally , also known as IDE, is a standard interface designed for IBM PC-compatible computers. It was first developed by Western Digital and Compaq in 1986 for compatible hard drives and CD or DVD drives. The connection is used for storage devices such as hard disk drives, floppy disk drives, and optical disc drives in computers.\n\nThe standard is maintained by the X3/INCITS committee. It uses the underlying  (ATA) and  Packet Interface (ATAPI) standards.",
        "The standard is maintained by the X3/INCITS committee. It uses the underlying  (ATA) and  Packet Interface (ATAPI) standards.\n\nThe Parallel ATA standard is the result of a long history of incremental technical development, which began with the original AT Attachment interface, developed for use in early PC AT equipment. The ATA interface itself evolved in several stages from Western Digital's original Integrated Drive Electronics (IDE) interface. As a result, many near-synonyms for ATA/ATAPI and its previous incarnations are still in common informal use, in particular Extended IDE (EIDE) and Ultra ATA (UATA).  After the introduction of SATA in 2003, the original ATA was renamed to Parallel ATA, or PATA for short.",
        "Parallel ATA cables have a maximum allowable length of . Because of this limit, the technology normally appears as an internal computer storage interface. For many years, ATA provided the most common and the least expensive interface for this application. It has largely been replaced by SATA in newer systems.",
        "History and terminology \nThe standard was originally conceived as the \"AT Bus Attachment,\" officially called \"AT Attachment\" and abbreviated \"ATA\" because its primary feature was a direct connection to the 16-bit ISA bus introduced with the IBM PC/AT. The original ATA specifications published by the standards committees use the name \"AT Attachment\". The \"AT\" in the IBM PC/AT referred to \"Advanced Technology\" so ATA has also been referred to as \"Advanced Technology Attachment\".  When a newer Serial ATA (SATA) was introduced in 2003, the original ATA was renamed to Parallel ATA, or PATA for short.",
        "Physical ATA interfaces became a standard component in all PCs, initially on host bus adapters, sometimes on a sound card but ultimately as two physical interfaces embedded in a Southbridge chip on a motherboard.  Called the \"primary\" and \"secondary\" ATA interfaces, they were assigned to base addresses 0x1F0 and 0x170 on ISA bus systems. They were replaced by SATA interfaces.\n\nIDE and ATA-1",
        "IDE and ATA-1 \n\nThe first version of what is now called the ATA/ATAPI interface was developed by Western Digital under the name Integrated Drive Electronics (IDE). Together with  Compaq Computer (the initial customer), they worked with various disk drive manufacturers to develop and ship early products with the goal of remaining software compatible with the existing IBM PC hard drive interface. The first such drives appeared internally in Compaq PCs in 1986\n and were first separately offered by Conner Peripherals as the CP342 in June 1987.",
        "The term Integrated Drive Electronics refers to the fact that the drive controller is integrated into the drive, as opposed to a separate controller situated at the other side of the connection cable to the drive. On an IBM PC compatible, CP/M machine, or similar, this was typically a card installed on a motherboard. The interface cards used to connect a parallel ATA drive to, for example, an ISA Slot, are not drive controllers: they are merely bridges between the host bus and the ATA interface. Since the original ATA interface is essentially just a 16-bit ISA bus in disguise, the bridge was especially simple in case of an ATA connector being located on an ISA interface card. The integrated controller presented the drive to the host computer as an array of 512-byte blocks with a",
        "was especially simple in case of an ATA connector being located on an ISA interface card. The integrated controller presented the drive to the host computer as an array of 512-byte blocks with a relatively simple command interface. This relieved the mainboard and interface cards in the host computer of the chores of stepping the disk head arm, moving the head arm in and out, and so on, as had to be done with earlier ST-506 and ESDI hard drives. All of these low-level details of the mechanical operation of the drive were now handled by the controller on the drive itself. This also eliminated the need to design a single controller that could handle many different types of drives, since the controller could be unique for the drive. The host need only to ask for a particular sector, or block,",
        "need to design a single controller that could handle many different types of drives, since the controller could be unique for the drive. The host need only to ask for a particular sector, or block, to be read or written, and either accept the data from the drive or send the data to it.",
        "The interface used by these drives was standardized in 1994 as ANSI standard X3.221-1994, AT Attachment Interface for Disk Drives. After later versions of the standard were developed, this became known as \"ATA-1\".\n\nA short-lived, seldom-used implementation of ATA was created for the IBM XT and similar machines that used the 8-bit version of the ISA bus. It has been referred to as \"XT-IDE\", \"XTA\" or \"XT Attachment\".\n\nEIDE and ATA-2 \n\nIn 1994, about the same time that the ATA-1 standard was adopted, Western Digital introduced drives under a newer name, Enhanced IDE (EIDE). These included most of the features of the forthcoming ATA-2 specification and several additional enhancements. Other manufacturers introduced their own variations of ATA-1 such as \"Fast ATA\" and \"Fast ATA-2\".",
        "The new version of the ANSI standard, AT Attachment Interface with Extensions ATA-2 (X3.279-1996), was approved in 1996. It included most of the features of the manufacturer-specific variants.\n\nATA-2 also was the first to note that devices other than hard drives could be attached to the interface:\n\nATAPI",
        "ATA-2 also was the first to note that devices other than hard drives could be attached to the interface:\n\nATAPI \n\nAs mentioned in the previous sections, ATA was originally designed for, and worked only with hard disk drives and devices that could emulate them. The introduction of ATAPI (ATA Packet Interface) by a group called the Small Form Factor committee (SFF) allowed ATA to be used for a variety of other devices that require functions beyond those necessary for hard disk drives. For example, any removable media device needs a \"media eject\" command, and a way for the host to determine whether the media is present, and these were not provided in the ATA protocol.",
        "The Small Form Factor committee approached this problem by defining ATAPI, the \"ATA Packet Interface\". ATAPI is actually a protocol allowing the ATA interface to carry SCSI commands and responses; therefore, all ATAPI devices are actually \"speaking SCSI\" other than at the electrical interface. In fact, some early ATAPI devices were simply SCSI devices with an ATA/ATAPI to SCSI protocol converter added on. The SCSI commands and responses are embedded in \"packets\" (hence \"ATA Packet Interface\") for transmission on the ATA cable. This allows any device class for which a SCSI command set has been defined to be interfaced via ATA/ATAPI.",
        "ATAPI devices are also \"speaking ATA\", as the ATA physical interface and protocol are still being used to send the packets. On the other hand, ATA hard drives and solid state drives do not use ATAPI.\n\nATAPI devices include CD-ROM and DVD-ROM drives, tape drives, and large-capacity floppy drives such as the Zip drive and SuperDisk drive.\n\nThe SCSI commands and responses used by each class of ATAPI device (CD-ROM, tape, etc.) are described in other documents or specifications specific to those device classes and are not within ATA/ATAPI or the T13 committee's purview.  One commonly used set is defined in the MMC SCSI command set.\n\nATAPI was adopted as part of ATA in INCITS 317-1998, AT Attachment with Packet Interface Extension (ATA/ATAPI-4).\n\nUDMA and ATA-4",
        "ATAPI was adopted as part of ATA in INCITS 317-1998, AT Attachment with Packet Interface Extension (ATA/ATAPI-4).\n\nUDMA and ATA-4 \n\nThe ATA/ATAPI-4 standard also introduced several \"Ultra DMA\" transfer modes. These initially supported speeds from 16 MByte/s to 33 MByte/second. In later versions, faster Ultra DMA modes were added, requiring new 80-wire cables to reduce crosstalk. The latest versions of Parallel ATA support up to 133 MByte/s.",
        "Ultra ATA \nUltra ATA, abbreviated UATA, is a designation that has been primarily used by Western Digital for different speed enhancements to the ATA/ATAPI standards. For example, in 2000 Western Digital published a document describing \"Ultra ATA/100\", which brought performance improvements for the then-current ATA/ATAPI-5 standard by improving maximum speed of the Parallel ATA interface from 66 to 100 MB/s. Most of Western Digital's changes, along with others, were included in the ATA/ATAPI-6 standard (2002).\n\nCurrent terminology \nThe terms \"integrated drive electronics\" (IDE), \"enhanced IDE\" and \"EIDE\" have come to be used interchangeably with ATA (now Parallel ATA, or PATA).",
        "Current terminology \nThe terms \"integrated drive electronics\" (IDE), \"enhanced IDE\" and \"EIDE\" have come to be used interchangeably with ATA (now Parallel ATA, or PATA).\n\nIn addition, there have been several generations of \"EIDE\" drives marketed, compliant with various versions of the ATA specification. An early \"EIDE\" drive might be compatible with ATA-2, while a later one with ATA-6.\n\nNevertheless, a request for an \"IDE\" or \"EIDE\" drive from a computer parts vendor will almost always yield a drive that will work with most Parallel ATA interfaces.",
        "Nevertheless, a request for an \"IDE\" or \"EIDE\" drive from a computer parts vendor will almost always yield a drive that will work with most Parallel ATA interfaces.\n\nAnother common usage is to refer to the specification version by the fastest mode supported. For example, ATA-4 supported Ultra DMA modes 0 through 2, the latter providing a maximum transfer rate of 33 megabytes per second. ATA-4 drives are thus sometimes called \"UDMA-33\" drives, and sometimes \"ATA-33\" drives. Similarly, ATA-6 introduced a maximum transfer speed of 100 megabytes per second, and some drives complying with this version of the standard are marketed as \"PATA/100\" drives.\n\nx86 BIOS size limitations",
        "Initially, the size of an ATA drive was stored in the system x86 BIOS using a type number (1 through 45) that predefined the C/H/S parameters and also often the landing zone, in which the drive heads are parked while not in use. Later, a \"user definable\" format called C/H/S or cylinders, heads, sectors was made available. These numbers were important for the earlier ST-506 interface, but were generally meaningless for ATA—the CHS parameters for later ATA large drives often specified impossibly high numbers of heads or sectors that did not actually define the internal physical layout of the drive at all. From the start, and up to ATA-2, every user had to specify explicitly how large every attached drive was. From ATA-2 on, an \"identify drive\" command was implemented that can be sent and",
        "drive at all. From the start, and up to ATA-2, every user had to specify explicitly how large every attached drive was. From ATA-2 on, an \"identify drive\" command was implemented that can be sent and which will return all drive parameters.",
        "Owing to a lack of foresight by motherboard manufacturers, the system BIOS was often hobbled by artificial C/H/S size limitations due to the manufacturer assuming certain values would never exceed a particular numerical maximum.\n\nThe first of these BIOS limits occurred when ATA drives reached sizes in excess of 504 MiB, because some motherboard BIOSes would not allow C/H/S values above 1024 cylinders, 16 heads, and 63 sectors. Multiplied by 512 bytes per sector, this totals  bytes which, divided by  bytes per MiB, equals 504 MiB (528 MB).",
        "The second of these BIOS limitations occurred at 1024 cylinders, 256 heads, and 63 sectors, and a problem in MS-DOS limited the number of heads to 255. This totals to  bytes (8032.5 MiB), commonly referred to as the 8.4 gigabyte barrier. This is again a limit imposed by x86 BIOSes, and not a limit imposed by the ATA interface.",
        "It was eventually determined that these size limitations could be overridden with a small program loaded at startup from a hard drive's boot sector. Some hard drive manufacturers, such as Western Digital, started including these override utilities with large hard drives to help overcome these problems. However, if the computer was booted in some other manner without loading the special utility, the invalid BIOS settings would be used and the drive could either be inaccessible or appear to the operating system to be damaged.\n\nLater, an extension to the x86 BIOS disk services called the \"Enhanced Disk Drive\" (EDD) was made available, which makes it possible to address drives as large as 264 sectors.",
        "Later, an extension to the x86 BIOS disk services called the \"Enhanced Disk Drive\" (EDD) was made available, which makes it possible to address drives as large as 264 sectors.\n\nInterface size limitations \nThe first drive interface used 22-bit addressing mode which resulted in a maximum drive capacity of two gigabytes. Later, the first formalized ATA specification used a 28-bit addressing mode through LBA28, allowing for the addressing of 228 () sectors (blocks) of 512 bytes each, resulting in a maximum capacity of 128 GiB (137 GB).",
        "ATA-6 introduced 48-bit addressing, increasing the limit to 128 PiB (144 PB). As a consequence, any ATA drive of capacity larger than about 137 GB must be an ATA-6 or later drive. Connecting such a drive to a host with an ATA-5 or earlier interface will limit the usable capacity to the maximum of the interface.\n\nSome operating systems, including Windows XP pre-SP1, and Windows 2000 pre-SP3, disable LBA48 by default, requiring the user to take extra steps to use the entire capacity of an ATA drive larger than about 137 gigabytes.",
        "Older operating systems, such as Windows 98, do not support 48-bit LBA at all.  However, members of the third-party group MSFN have modified the Windows 98 disk drivers to add unofficial support for 48-bit LBA to Windows 95 OSR2, Windows 98, Windows 98 SE and Windows ME.\n\nSome 16-bit and 32-bit operating systems supporting LBA48 may still not support disks larger than 2 TiB due to using 32-bit arithmetic only; a limitation also applying to many boot sectors.",
        "Some 16-bit and 32-bit operating systems supporting LBA48 may still not support disks larger than 2 TiB due to using 32-bit arithmetic only; a limitation also applying to many boot sectors.\n\nPrimacy and obsolescence \nParallel ATA (then simply called ATA or IDE) became the primary storage device interface for PCs soon after its introduction. In some systems, a third and fourth motherboard interface was provided, allowing up to eight ATA devices to be attached to the motherboard. Often, these additional connectors were implemented by inexpensive RAID controllers.",
        "Soon after the introduction of Serial ATA (SATA) in 2003, use of Parallel ATA declined. The first motherboards with built-in SATA interfaces usually had only a single PATA connector (for up to two PATA devices), along with multiple SATA connectors. Some PCs and laptops of the era have a SATA hard disk and an optical drive connected to PATA.\n\nAs of 2007, some PC chipsets, for example the Intel ICH10, had removed support for PATA. Motherboard vendors still wishing to offer Parallel ATA with those chipsets must include an additional interface chip. In more recent computers, the Parallel ATA interface is rarely used even if present, as four or more Serial ATA connectors are usually provided on the motherboard and SATA devices of all types are common.",
        "With Western Digital's withdrawal from the PATA market, hard disk drives with the PATA interface were no longer in production after December 2013 for other than specialty applications.\n\nParallel ATA interface",
        "Parallel ATA interface \n\nParallel ATA cables transfer data 16 bits at a time. The traditional cable uses 40-pin female connectors attached to a 40- or 80-conductor ribbon cable. Each cable has two or three connectors, one of which plugs into a host adapter interfacing with the rest of the computer system. The remaining connector(s) plug into storage devices, most commonly hard disk drives or optical drives. Each connector has 39 physical pins arranged into two rows (2.54 mm, -inch pitch), with a gap or key at pin 20. Earlier connectors may not have that gap, with all 40 pins available. Thus, later cables with the gap filled in are incompatible with earlier connectors, although earlier cables are compatible with later connectors.",
        "Round parallel ATA cables (as opposed to ribbon cables) were eventually made available for 'case modders' for cosmetic reasons, as well as claims of improved computer cooling and were easier to handle; however, only ribbon cables are supported by the ATA specifications.\n\n Pin 20\nIn the ATA standard, pin 20 is defined as a mechanical key and is not used. This pin's socket on the female connector is often obstructed, requiring pin 20 to be omitted from the male cable or drive connector; it is thus impossible to plug it in the wrong way round.\n\nHowever, some flash memory drives can use pin 20 as VCC_in to power the drive without requiring a special power cable; this feature can only be used if the equipment supports this use of pin 20.",
        "However, some flash memory drives can use pin 20 as VCC_in to power the drive without requiring a special power cable; this feature can only be used if the equipment supports this use of pin 20.\n\n Pin 28\nPin 28 of the gray (slave/middle) connector of an 80-conductor cable is not attached to any conductor of the cable. It is attached normally on the black (master drive end) and blue (motherboard end) connectors. This enables cable select functionality.\n\n Pin 34\nPin 34 is connected to ground inside the blue connector of an 80-conductor cable but not attached to any conductor of the cable, allowing for detection of such a cable. It is attached normally on the gray and black connectors.",
        "44-pin variant \nA 44-pin variant PATA connector is used for 2.5 inch drives inside laptops. The pins are closer together (2.0 mm pitch) and the connector is physically smaller than the 40-pin connector. The extra pins carry power.\n\n80-conductor variant",
        "80-conductor variant \n\nATA's cables have had 40 conductors for most of its history (44 conductors for the smaller form-factor version used for 2.5\" drives—the extra four for power), but an 80-conductor version appeared with the introduction of the UDMA/66 mode. All of the additional conductors in the new cable are grounds, interleaved with the signal conductors to reduce the effects of capacitive coupling between neighboring signal conductors, reducing crosstalk. Capacitive coupling is more of a problem at higher transfer rates, and this change was necessary to enable the 66 megabytes per second (MB/s) transfer rate of UDMA4 to work reliably. The faster UDMA5 and UDMA6 modes also require 80-conductor cables.",
        "Though the number of conductors doubled, the number of connector pins and the pinout remain the same as 40-conductor cables, and the external appearance of the connectors is identical. Internally, the connectors are different; the connectors for the 80-conductor cable connect a larger number of ground conductors to the ground pins, while the connectors for the 40-conductor cable connect ground conductors to ground pins one-to-one. 80-conductor cables usually come with three differently colored connectors (blue, black, and gray for controller, master drive, and slave drive respectively) as opposed to uniformly colored 40-conductor cable's connectors (commonly all gray). The gray connector on 80-conductor cables has pin 28 CSEL not connected, making it the slave position for drives",
        "as opposed to uniformly colored 40-conductor cable's connectors (commonly all gray). The gray connector on 80-conductor cables has pin 28 CSEL not connected, making it the slave position for drives configured cable select.",
        "Differences between connectors \n\nThe image on the right shows PATA connectors after removal of strain relief, cover, and cable. Pin one is at bottom left of the connectors, pin 2 is top left, etc., except that the lower image of the blue connector shows the view from the opposite side, and pin one is at top right.",
        "The connector is an insulation-displacement connector: each contact comprises a pair of points which together pierce the insulation of the ribbon cable with such precision that they make a connection to the desired conductor without harming the insulation on the neighboring conductors. The center row of contacts are all connected to the common ground bus and attach to the odd numbered conductors of the cable. The top row of contacts are the even-numbered sockets of the connector (mating with the even-numbered pins of the receptacle) and attach to every other even-numbered conductor of the cable. The bottom row of contacts are the odd-numbered sockets of the connector (mating with the odd-numbered pins of the receptacle) and attach to the remaining even-numbered conductors of the cable.",
        "Note the connections to the common ground bus from sockets 2 (top left), 19 (center bottom row), 22, 24, 26, 30, and 40 on all connectors. Also note (enlarged detail, bottom, looking from the opposite side of the connector) that socket 34 of the blue connector does not contact any conductor but unlike socket 34 of the other two connectors, it does connect to the common ground bus. On the gray connector, note that socket 28 is completely missing, so that pin 28 of the drive attached to the gray connector will be open. On the black connector, sockets 28 and 34 are completely normal, so that pins 28 and 34 of the drive attached to the black connector will be connected to the cable. Pin 28 of the black drive reaches pin 28 of the host receptacle but not pin 28 of the gray drive, while pin 34",
        "28 and 34 of the drive attached to the black connector will be connected to the cable. Pin 28 of the black drive reaches pin 28 of the host receptacle but not pin 28 of the gray drive, while pin 34 of the black drive reaches pin 34 of the gray drive but not pin 34 of the host. Instead, pin 34 of the host is grounded.",
        "The standard dictates color-coded connectors for easy identification by both installer and cable maker. All three connectors are different from one another. The blue (host) connector has the socket for pin 34 connected to ground inside the connector but not attached to any conductor of the cable. Since the old 40 conductor cables do not ground pin 34, the presence of a ground connection indicates that an 80 conductor cable is installed. The conductor for pin 34 is attached normally on the other types and is not grounded. Installing the cable backwards (with the black connector on the system board, the blue connector on the remote device and the gray connector on the center device) will ground pin 34 of the remote device and connect host pin 34 through to pin 34 of the center device. The",
        "board, the blue connector on the remote device and the gray connector on the center device) will ground pin 34 of the remote device and connect host pin 34 through to pin 34 of the center device. The gray center connector omits the connection to pin 28 but connects pin 34 normally, while the black end connector connects both pins 28 and 34 normally.",
        "Multiple devices on a cable \nIf two devices are attached to a single cable, one must be designated as Device 0 (in the past, commonly designated master) and the other as Device 1 (in the past, commonly designated as slave). This distinction is necessary to allow both drives to share the cable without conflict. The Device 0 drive is the drive that usually appears \"first\" to the computer's BIOS and/or operating system. In most personal computers the drives are often designated as \"C:\" for the Device 0 and \"D:\" for the Device 1 referring to one active primary partitions on each.\n\nThe terms device and drive are used interchangeably in the industry, as in master drive or master device.",
        "The terms device and drive are used interchangeably in the industry, as in master drive or master device.\n\nThe mode that a device must use is often set by a jumper setting on the device itself, which must be manually set to Device 0 (Master) or Device 1 (Slave). If there is a single device on a cable, it should be configured as Device 0. However, some certain era drives have a special setting called Single for this configuration (Western Digital, in particular). Also, depending on the hardware and software available, a Single drive on a cable will often work reliably even though configured as the Device 1 drive (most often seen where an optical drive is the only device on the secondary ATA interface).",
        "The words primary and secondary typically refers to the two IDE cables, which can have two drives each (primary master, primary slave, secondary master, secondary slave).\n\nCable select \n\nA drive mode called cable select was described as optional in ATA-1 and has come into fairly widespread use with ATA-5 and later. A drive set to \"cable select\" automatically configures itself as Device 0 or Device 1, according to its position on the cable. Cable select is controlled by pin 28. The host adapter grounds this pin; if a device sees that the pin is grounded, it becomes the Device 0 (master) device; if it sees that pin 28 is open, the device becomes the Device 1 (slave) device.",
        "This setting is usually chosen by a jumper setting on the drive called \"cable select\", usually marked CS, which is separate from the Device 0/1 setting.\n\nNote that if two drives are configured as Device 0 and Device 1 manually, this configuration does not need to correspond to their position on the cable. Pin 28 is only used to let the drives know their position on the cable; it is not used by the host when communicating with the drives. In other words, the manual master/slave setting using jumpers on the drives takes precedence and allows them to be freely placed on either connector of the ribbon cable.",
        "With the 40-conductor cable, it was very common to implement cable select by simply cutting the pin 28 wire between the two device connectors; putting the slave Device 1 device at the end of the cable, and the master Device 0 on the middle connector. This arrangement eventually was standardized in later versions. However, it had one drawback: if there is just one master device on a 2-drive cable, using the middle connector, this results in an unused stub of cable, which is undesirable for physical convenience and electrical reasons. The stub causes signal reflections, particularly at higher transfer rates.",
        "Starting with the 80-conductor cable defined for use in ATAPI5/UDMA4, the master Device 0 device goes at the far-from-the-host end of the  cable on the black connector, the slave Device 1 goes on the grey middle connector, and the blue connector goes to the host (e.g. motherboard IDE connector, or IDE card). So, if there is only one (Device 0) device on a two-drive cable, using the black connector, there is no cable stub to cause reflections (the unused connector is now in the middle of the ribbon). Also, cable select is now implemented in the grey middle device connector, usually simply by omitting the pin 28 contact from the connector body.",
        "Serialized, overlapped, and queued operations \nThe parallel ATA protocols up through ATA-3 require that once a command has been given on an ATA interface, it must complete before any subsequent command may be given. Operations on the devices must be serializedwith only one operation in progress at a timewith respect to the ATA host interface. A useful mental model is that the host ATA interface is busy with the first request for its entire duration, and therefore can not be told about another request until the first one is complete. The function of serializing requests to the interface is usually performed by a device driver in the host operating system.",
        "The ATA-4 and subsequent versions of the specification have included an \"overlapped feature set\" and a \"queued feature set\" as optional features, both being given the name \"Tagged Command Queuing\" (TCQ), a reference to a set of features from SCSI which the ATA version attempts to emulate. However, support for these is extremely rare in actual parallel ATA products and device drivers because these feature sets were implemented in such a way as to maintain software compatibility with its heritage as originally an extension of the ISA bus. This implementation resulted in excessive CPU utilization which largely negated the advantages of command queuing. By contrast, overlapped and queued operations have been common in other storage buses; in particular, SCSI's version of tagged command",
        "which largely negated the advantages of command queuing. By contrast, overlapped and queued operations have been common in other storage buses; in particular, SCSI's version of tagged command queuing had no need to be compatible with APIs designed for ISA, allowing it to attain high performance with low overhead on buses which supported first party DMA like PCI. This has long been seen as a major advantage of SCSI.",
        "The Serial ATA standard has supported native command queueing (NCQ) since its first release, but it is an optional feature for both host adapters and target devices. Many obsolete PC motherboards do not support NCQ, but modern SATA hard disk drives and SATA solid-state drives usually support NCQ, which is not the case for removable (CD/DVD) drives because the ATAPI command set used to control them prohibits queued operations.\n\nTwo devices on one cable—speed impact \nThere are many debates about how much a slow device can impact the performance of a faster device on the same cable. There is an effect, but the debate is confused by the blurring of two quite different causes, called here \"Lowest speed\" and \"One operation at a time\".",
        "\"Lowest speed\" \nOn early ATA host adapters, both devices' data transfers can be constrained to the speed of the slower device, if two devices of different speed capabilities are on the same cable.\n\nFor all modern ATA host adapters, this is not true, as modern ATA host adapters support independent device timing. This allows each device on the cable to transfer data at its own best speed. Even with earlier adapters without independent timing, this effect applies only to the data transfer phase of a read or write operation.",
        "\"One operation at a time\" \nThis is caused by the omission of both overlapped and queued feature sets from most parallel ATA products. Only one device on a cable can perform a read or write operation at one time; therefore, a fast device on the same cable as a slow device under heavy use will find it has to wait for the slow device to complete its task first.\n\nHowever, most modern devices will report write operations as complete once the data is stored in their onboard cache memory, before the data is written to the (slow) magnetic storage. This allows commands to be sent to the other device on the cable, reducing the impact of the \"one operation at a time\" limit.",
        "The impact of this on a system's performance depends on the application. For example, when copying data from an optical drive to a hard drive (such as during software installation), this effect probably will not matter. Such jobs are necessarily limited by the speed of the optical drive no matter where it is. But if the hard drive in question is also expected to provide good throughput for other tasks at the same time, it probably should not be on the same cable as the optical drive.\n\nHDD passwords and security",
        "HDD passwords and security \n\nATA devices may support an optional security feature which is defined in an ATA specification, and thus not specific to any brand or device. The security feature can be enabled and disabled by sending special ATA commands to the drive. If a device is locked, it will refuse all access until it is unlocked.\n\nA device can have two passwords: A User Password and a Master Password; either or both may be set. There is a Master Password identifier feature which, if supported and used, can identify the current Master Password (without disclosing it).\n\nA device can be locked in two modes: High security mode or Maximum security mode. Bit 8 in word 128 of the IDENTIFY response shows which mode the disk is in: 0 = High, 1 = Maximum.",
        "A device can be locked in two modes: High security mode or Maximum security mode. Bit 8 in word 128 of the IDENTIFY response shows which mode the disk is in: 0 = High, 1 = Maximum.\n\nIn High security mode, the device can be unlocked with either the User or Master password, using the \"SECURITY UNLOCK DEVICE\" ATA command. There is an attempt limit, normally set to 5, after which the disk must be power cycled or hard-reset before unlocking can be attempted again. Also in High security mode, the SECURITY ERASE UNIT command can be used with either the User or Master password.",
        "In Maximum security mode, the device can be unlocked only with the User password. If the User password is not available, the only remaining way to get at least the bare hardware back to a usable state is to issue the SECURITY ERASE PREPARE command, immediately followed by SECURITY ERASE UNIT. In Maximum security mode, the SECURITY ERASE UNIT command requires the Master password and will completely erase all data on the disk. Word 89 in the IDENTIFY response indicates how long the operation will take.\n\nWhile the ATA lock is intended to be impossible to defeat without a valid password, there are purported workarounds to unlock a device.",
        "While the ATA lock is intended to be impossible to defeat without a valid password, there are purported workarounds to unlock a device.\n\nFor sanitizing entire disks the built-in Secure Erase command is effective when implemented correctly. There have been a few reported instances of failures to erase some or all data.\n\nExternal parallel ATA devices",
        "Due to a short cable length specification and shielding issues it is extremely uncommon to find external PATA devices that directly use PATA for connection to a computer. A device connected externally needs additional cable length to form a U-shaped bend so that the external device may be placed alongside, or on top of the computer case, and the standard cable length is too short to permit this.  For ease of reach from motherboard to device, the connectors tend to be positioned towards the front edge of motherboards, for connection to devices protruding from the front of the computer case. This front-edge position makes extension out the back to an external device even more difficult. Ribbon cables are poorly shielded, and the standard relies upon the cabling to be installed inside a",
        "This front-edge position makes extension out the back to an external device even more difficult. Ribbon cables are poorly shielded, and the standard relies upon the cabling to be installed inside a shielded computer case to meet RF emissions limits.",
        "External hard disk drives or optical disk drives that have an internal PATA interface, use some other interface technology to bridge the distance between the external device and the computer. USB is the most common external interface, followed by Firewire. A bridge chip inside the external devices converts from the USB interface to PATA, and typically only supports a single external device without cable select or master/slave.\n\nCompact Flash interface",
        "Compact Flash interface \n\nCompact Flash in its IDE mode is essentially a miniaturized ATA interface, intended for use on devices that use flash memory storage. No interfacing chips or circuitry are required, other than to directly adapt the smaller CF socket onto the larger ATA connector. (Although most CF cards only support IDE mode up to PIO4, making them much slower in IDE mode than their CF capable speed)",
        "The ATA connector specification does not include pins for supplying power to a CF device, so power is inserted into the connector from a separate source. The exception to this is when the CF device is connected to a 44-pin ATA bus designed for 2.5-inch hard disk drives, commonly found in notebook computers, as this bus implementation must provide power to a standard hard disk drive.\n\nCF devices can be designated as devices 0 or 1 on an ATA interface, though since most CF devices offer only a single socket, it is not necessary to offer this selection to end users.  Although CF can be hot-pluggable with additional design methods, by default when wired directly to an ATA interface, it is not intended to be hot-pluggable.",
        "ATA standards versions, transfer rates, and features \nThe following table shows the names of the versions of the ATA standards and the transfer modes and rates supported by each. Note that the transfer rate for each mode (for example, 66.7 MB/s for UDMA4, commonly called \"Ultra-DMA 66\", defined by ATA-5) gives its maximum theoretical transfer rate on the cable. This is simply two bytes multiplied by the effective clock rate, and presumes that every clock cycle is used to transfer end-user data. In practice, of course, protocol overhead reduces this value.",
        "Congestion on the host bus to which the ATA adapter is attached may also limit the maximum burst transfer rate. For example, the maximum data transfer rate for conventional PCI bus is 133 MB/s, and this is shared among all active devices on the bus.",
        "In addition, no ATA hard drives existed in 2005 that were capable of measured sustained transfer rates of above 80 MB/s. Furthermore, sustained transfer rate tests do not give realistic throughput expectations for most workloads: They use I/O loads specifically designed to encounter almost no delays from seek time or rotational latency. Hard drive performance under most workloads is limited first and second by those two factors; the transfer rate on the bus is a distant third in importance. Therefore, transfer speed limits above 66 MB/s really affect performance only when the hard drive can satisfy all I/O requests by reading from its internal cache—a very unusual situation, especially considering that such data is usually already buffered by the operating system.",
        ", mechanical hard disk drives can transfer data at up to 524 MB/s, which is far beyond the capabilities of the PATA/133 specification. High-performance solid state drives can transfer data at up to 7000–7500 MB/s.\n\nOnly the Ultra DMA modes use CRC to detect errors in data transfer between the controller and drive. This is a 16-bit CRC, and it is used for data blocks only. Transmission of command and status blocks do not use the fast signaling methods that would necessitate CRC. For comparison, in Serial ATA, 32-bit CRC is used for both commands and data.\n\nFeatures introduced with each ATA revision\n\nSpeed of defined transfer modes\n\nRelated standards, features, and proposals\n\nATAPI Removable Media Device (ARMD)",
        "Features introduced with each ATA revision\n\nSpeed of defined transfer modes\n\nRelated standards, features, and proposals\n\nATAPI Removable Media Device (ARMD) \n\nATAPI devices with removable media, other than CD and DVD drives, are classified as ARMD (ATAPI Removable Media Device) and can appear as either a super-floppy (non-partitioned media) or a hard drive (partitioned media) to the operating system. These can be supported as bootable devices by a BIOS complying with the ATAPI Removable Media Device BIOS Specification, originally developed by Compaq Computer Corporation and Phoenix Technologies. It specifies provisions in the BIOS of a personal computer to allow the computer to be bootstrapped from devices such as Zip drives, Jaz drives, SuperDisk (LS-120) drives, and similar devices.",
        "These devices have removable media like floppy disk drives, but capacities more commensurate with hard drives, and programming requirements unlike either. Due to limitations in the floppy controller interface most of these devices were ATAPI devices, connected to one of the host computer's ATA interfaces, similarly to a hard drive or CD-ROM device. However, existing BIOS standards did not support these devices. An ARMD-compliant BIOS allows these devices to be booted from and used under the operating system without requiring device-specific code in the OS.",
        "A BIOS implementing ARMD allows the user to include ARMD devices in the boot search order. Usually an ARMD device is configured earlier in the boot order than the hard drive. Similarly to a floppy drive, if bootable media is present in the ARMD drive, the BIOS will boot from it; if not, the BIOS will continue in the search order, usually with the hard drive last.",
        "There are two variants of ARMD, ARMD-FDD and ARMD-HDD. Originally ARMD caused the devices to appear as a sort of very large floppy drive, either the primary floppy drive device 00h or the secondary device 01h. Some operating systems required code changes to support floppy disks with capacities far larger than any standard floppy disk drive. Also, standard-floppy disk drive emulation proved to be unsuitable for certain high-capacity floppy disk drives such as Iomega Zip drives. Later the ARMD-HDD, ARMD-\"Hard disk device\", variant was developed to address these issues. Under ARMD-HDD, an ARMD device appears to the BIOS and the operating system as a hard drive.",
        "ATA over Ethernet \nIn August 2004, Sam Hopkins and Brantley Coile of Coraid specified a lightweight ATA over Ethernet protocol to carry ATA commands over Ethernet instead of directly connecting them to a PATA host adapter. This permitted the established block protocol to be reused in storage area network (SAN) applications.\n\nSee also \n\n Advanced Host Controller Interface (AHCI)\n CE-ATA Consumer Electronics (CE) ATA\n FATA (hard drive)\n INT 13H for BIOS Enhanced Disk Drive Specification (SFF-8039i)\n IT8212, a low-end Parallel ATA controller\n Master/slave (technology)\n List of device bandwidths\n\nReferences\n\nExternal links \n CE-ATA Workgroup\n\nAT Attachment\nComputer storage buses\nComputer connectors\nComputer hardware standards"
    ],
    [
        "PC-1\nPC-1 (Pacific Crossing 1) is a submarine telecommunications cable system in the North Pacific Ocean linking the United States to Japan.\n\nIt has landing points in:\nShima, Mie Prefecture, Japan\nAjigaura, Hitachinaka, Ibaraki Prefecture, Japan\nHarbour Pointe, Snohomish County, Washington, United States\nGrover Beach, San Luis Obispo County, California, United States\n\nIt has a design transmission capacity of 640 Gbit/s, with 180 Gbit/s lit (as of February 2006), and a total cable length of 20,890 km.  It started operation in January 2001.\n\nSince 2009, PC-1 has been owned by Japanese carrier NTT.\n\nReferences",
        "Since 2009, PC-1 has been owned by Japanese carrier NTT.\n\nReferences\n \n \n\nSubmarine communications cables in the Pacific Ocean\nJapan–United States relations\nCommunications in Washington (state)\nCommunications in California\n2001 establishments in California\n2001 establishments in Japan\n2001 establishments in Oregon"
    ],
    [
        "Printer (computing)\nIn computing, a printer is a peripheral machine which makes a persistent representation of graphics or text, usually on paper. While most output is human-readable, bar code printers are an example of an expanded use for printers. Different types of printers include 3D printers, inkjet printers, laser printers, and thermal printers.\n\nHistory\nThe first computer printer designed was a mechanically driven apparatus by Charles Babbage for his difference engine in the 19th century; however, his mechanical printer design was not built until 2000.",
        "The first patented printing mechanism for applying a marking medium to a recording medium or more particularly an electrostatic inking apparatus and a method for electrostatically depositing ink on controlled areas of a receiving medium, was in 1962 by C. R. Winston, Teletype Corporation, using continuous inkjet printing. The ink was a red stamp-pad ink manufactured by Phillips Process Company of Rochester, NY under the name Clear Print. This patent (US3060429) led to the Teletype Inktronic Printer product delivered to customers in late 1966.\n\nThe first compact, lightweight digital printer was the EP-101, invented by Japanese company Epson and released in 1968, according to Epson.",
        "The first compact, lightweight digital printer was the EP-101, invented by Japanese company Epson and released in 1968, according to Epson.\n\nThe first commercial printers generally used mechanisms from electric typewriters and Teletype machines. The demand for higher speed led to the development of new systems specifically for computer use. In the 1980s there were daisy wheel systems similar to typewriters, line printers that produced similar output but at much higher speed, and dot-matrix systems that could mix text and graphics but produced relatively low-quality output. The plotter was used for those requiring high-quality line art like blueprints.",
        "The introduction of the low-cost laser printer in 1984, with the first HP LaserJet, and the addition of PostScript in next year's Apple LaserWriter set off a revolution in printing known as desktop publishing. Laser printers using PostScript mixed text and graphics, like dot-matrix printers, but at quality levels formerly available only from commercial typesetting systems. By 1990, most simple printing tasks like fliers and brochures were now created on personal computers and then laser printed; expensive offset printing systems were being dumped as scrap. The HP Deskjet of 1988 offered the same advantages as a laser printer in terms of flexibility, but produced somewhat lower-quality output (depending on the paper) from much less-expensive mechanisms. Inkjet systems rapidly displaced",
        "same advantages as a laser printer in terms of flexibility, but produced somewhat lower-quality output (depending on the paper) from much less-expensive mechanisms. Inkjet systems rapidly displaced dot-matrix and daisy-wheel printers from the market. By the 2000s, high-quality printers of this sort had fallen under the $100 price point and became commonplace.",
        "The rapid improvement of internet email through the 1990s and into the 2000s has largely displaced the need for printing as a means of moving documents, and a wide variety of reliable storage systems means that a \"physical backup\" is of little benefit today.\n\nStarting around 2010, 3D printing became an area of intense interest, allowing the creation of physical objects with the same sort of effort as an early laser printer required to produce a brochure. As of the 2020s, 3D printing has become a widespread hobby due to the abundance of cheap 3D printer kits, with the most common process being Fused deposition modeling.",
        "Types\nPersonal printers are mainly designed to support individual users, and may be connected to only a single computer. These printers are designed for low-volume, short-turnaround print jobs, requiring minimal setup time to produce a hard copy of a given document. However, they are generally slow devices ranging from 6 to around 25 pages per minute (ppm), and the cost per page is relatively high. However, this is offset by the on-demand convenience. Some printers can print documents stored on memory cards or from digital cameras and scanners.",
        "Networked or shared printers are \"designed for high-volume, high-speed printing\".  They are usually shared by many users on a network and can print at speeds of 45 to around 100 ppm. The Xerox 9700 could achieve 120 ppm.\nAn ID Card printer is used for printing plastic ID cards. These can now be customised with important features such as holographic overlays, HoloKotes and watermarks. This is either a direct to card printer (the more feasible option, or a retransfer printer.",
        "A virtual printer is a piece of computer software whose user interface and API resembles that of a printer driver, but which is not connected with a physical computer printer. A virtual printer can be used to create a file which is an image of the data which would be printed, for archival purposes or as input to another program, for example to create a PDF or to transmit to another system or user.",
        "A barcode printer is a computer peripheral for printing barcode labels or tags that can be attached to, or printed directly on, physical objects. Barcode printers are commonly used to label cartons before shipment, or to label retail items with UPCs or EANs.\n\nA 3D printer is a device for making a three-dimensional object from a 3D model or other electronic data source through additive processes in which successive layers of material (including plastics, metals, food, cement, wood, and other materials) are laid down under computer control.  It is called a printer by analogy with an inkjet printer which produces a two-dimensional document by a similar process of depositing a layer of ink on paper.",
        "ID Card printers",
        "A card printer is an electronic desktop printer with single card feeders which print and personalize plastic cards.  In this respect they differ from, for example, label printers which have a continuous supply feed.  Card dimensions are usually 85.60 × 53.98 mm, standardized under ISO/IEC 7810 as ID-1.  This format is also used in EC-cards, telephone cards, credit cards, driver's licenses and health insurance cards.  This is commonly known as the bank card format.  Card printers are controlled by corresponding printer drivers or by means of a specific programming language. Generally card printers are designed with laminating, striping, and punching functions, and use  desktop or web-based software. The hardware features of a card printer differentiate a card printer from the more",
        "printers are designed with laminating, striping, and punching functions, and use  desktop or web-based software. The hardware features of a card printer differentiate a card printer from the more traditional printers, as ID cards are usually made of PVC plastic and require laminating and punching. Different card printers can accept different card thickness and dimensions.",
        "The principle is the same for practically all card printers: the plastic card is passed through a thermal print head at the same time as a color ribbon.  The color from the ribbon is transferred onto the card through the heat given out from the print head.  The standard performance for card printing is 300 dpi (300 dots per inch, equivalent to 11.8 dots per mm). There are different printing processes, which vary in their detail:\n Thermal transfer Mainly used to personalize pre-printed plastic cards in monochrome.  The color is \"transferred\" from the (monochrome) color ribbon onto the card.",
        "Thermal transfer Mainly used to personalize pre-printed plastic cards in monochrome.  The color is \"transferred\" from the (monochrome) color ribbon onto the card.\n Dye sublimation This process uses four panels of color according to the CMYK color ribbon.  The card to be printed passes under the print head several times each time with the corresponding ribbon panel.  Each color in turn is diffused (sublimated) directly onto the card.  Thus it is possible to produce a high depth of color (up to 16 million shades) on the card.  Afterwards a transparent overlay (O) also known as a topcoat (T) is placed over the card to protect it from mechanical wear and tear and to render the printed image UV resistant.",
        "Reverse image technology The standard for high-security card applications that use contact and contactless smart chip cards.  The technology prints images onto the underside of a special film that fuses to the surface of a card through heat and pressure.  Since this process transfers dyes and resins directly onto a smooth, flexible film, the print-head never comes in contact with the card surface itself.  As such, card surface interruptions such as smart chips, ridges caused by internal RFID antennae and debris do not affect print quality. Even printing over the edge is possible.",
        "Thermal rewrite print process In contrast to the majority of other card printers, in the thermal rewrite process the card is not personalized through the use of a color ribbon, but by activating a thermal sensitive foil within the card itself.  These cards can be repeatedly personalized, erased and rewritten.  The most frequent use of these are in chip-based student identity cards, whose validity changes every semester.",
        "Common printing problems: Many printing problems are caused by physical defects in the card material itself, such as deformation or warping of the card that is fed into the machine in the first place. Printing irregularities can also result from chip or antenna embedding that alters the thickness of the plastic and interferes with the printer's effectiveness. Other issues are often caused by operator errors, such as users attempting to feed non-compatible cards into the card printer, while other printing defects may result from environmental abnormalities such as dirt or contaminants on the card or in the printer. Reverse transfer printers are less vulnerable to common printing problems than direct-to-card printers, since with these printers the card does not come into direct contact",
        "card or in the printer. Reverse transfer printers are less vulnerable to common printing problems than direct-to-card printers, since with these printers the card does not come into direct contact with the printhead.",
        "Variations in card printers:\n Broadly speaking there are three main types of card printers, differing mainly by the method used to print onto the card. They are:\n Near to Edge. This term designates the cheapest type of printing by card printers. These printers print up to 5 mm from the edge of the card stock.\n Direct to Card, also known as \"Edge to Edge Printing\". The print-head comes in direct contact with the card. This printing type is the most popular nowadays, mostly due to cost factor. The majority of identification card printers today are of this type.",
        "Reverse Transfer, also known as \"High Definition Printing\" or \"Over the Edge Printing\". The print-head prints to a transfer film backwards (hence the reverse) and then the printed film is rolled onto the card with intense heat (hence the transfer). The term \"over the edge\" is due to the fact that when the printer prints onto the film it has a \"bleed\", and when rolled onto the card the bleed extends to completely over the edge of the card, leaving no border.",
        "Different ID Card Printers use different encoding techniques to facilitate disparate business environments and to support security initiatives. Known encoding techniques are:\n Contact Smart Card – The Contact Smart Cards use RFID technology and require direct contact to a conductive plate to register admission or transfer of information. The transmission of commands, data, and card status held between the two physical contact points.\n Contactless Smart Card – Contactless Smart Cards exhibit integrated circuit that can store and process data while communicating with the terminal via Radio Frequency. Unlike Contact Smart Card, contact less cards feature intelligent re-writable microchip that can be transcribed through radio waves.",
        "HiD Proximity – HID's proximity technology allows fast, accurate reading while offering card or key tag read ranges from 4” to 24” inches (10 cm to 60.96 cm), dependent on the type of proximity reader being used. Since these cards and key tags do not require physical contact with the reader, they are virtually maintenance and wear-free.\n ISO Magnetic Stripe - A magnetic stripe card is a type of card capable of storing data by modifying the magnetism of tiny iron-based magnetic particles on a band of magnetic material on the card. The magnetic stripe, sometimes called swipe card or magstripe, is read by physical contact and swiping past a magnetic reading head.",
        "Software",
        "There are basically two categories of card printer software: desktop-based, and web-based (online). The biggest difference between the two is whether or not a customer has a printer on their network that is capable of printing identification cards. If a business already owns an ID card printer, then a desktop-based badge maker is probably suitable for their needs. Typically, large organizations who have high employee turnover will have their own printer. A desktop-based badge maker is also required if a company needs their IDs make instantly. An example of this is the private construction site that has restricted access. However, if a company does not already have a local (or network) printer that has the features they need, then the web-based option is a perhaps a more affordable",
        "site that has restricted access. However, if a company does not already have a local (or network) printer that has the features they need, then the web-based option is a perhaps a more affordable solution. The web-based solution is good for small businesses that do not anticipate a lot of rapid growth, or organizations who either can not afford a card printer, or do not have the resources to learn how to set up and use one. Generally speaking, desktop-based solutions involve software, a database (or spreadsheet) and can be installed on a single computer or network.",
        "Other options\nAlongside the basic function of printing cards, card printers can also read and encode magnetic stripes as well as contact and contact free RFID chip cards (smart cards).  Thus card printers enable the encoding of plastic cards both visually and logically. Plastic cards can also be laminated after printing.  Plastic cards are laminated after printing to achieve a considerable increase in durability and a greater degree of counterfeit prevention. Some card printers come with an option to print both sides at the same time, which cuts down the time taken to print and less margin of error. In such printers one side of id card is printed and then the card is flipped in the flip station and other side is printed.",
        "Applications\nAlongside the traditional uses in time attendance and access control (in particular with photo personalization), countless other applications have been found for plastic cards, e.g. for personalized customer and members' cards, for sports ticketing and in local public transport systems for the production of season tickets, for the production of school and college identity cards as well as for the production of national ID cards.\n\nTechnology\nThe choice of print technology has a great effect on the cost of the printer and cost of operation, speed, quality and permanence of documents, and noise. Some printer technologies do not work with certain types of physical media, such as carbon paper or transparencies.",
        "A second aspect of printer technology that is often forgotten is resistance to alteration: liquid ink, such as from an inkjet head or fabric ribbon, becomes absorbed by the paper fibers, so documents printed with liquid ink are more difficult to alter than documents printed with toner or solid inks, which do not penetrate below the paper surface.\n\nCheques can be printed with liquid ink or on special cheque paper with toner anchorage so that alterations may be detected. The machine-readable lower portion of a cheque must be printed using MICR toner or ink. Banks and other clearing houses employ automation equipment that relies on the magnetic flux from these specially printed characters to function properly.\n\nModern print technology",
        "Modern print technology\n\nThe following printing technologies are routinely found in modern printers:\n\nToner-based printers\n\nA laser printer rapidly produces high quality text and graphics. As with digital photocopiers and multifunction printers (MFPs), laser printers employ a xerographic printing process but differ from analog photocopiers in that the image is produced by the direct scanning of a laser beam across the printer's photoreceptor.\n\nAnother toner-based printer is the LED printer which uses an array of LEDs instead of a laser to cause toner adhesion to the print drum.\n\nLiquid inkjet printers\n\nInkjet printers operate by propelling variably sized droplets of liquid ink onto almost any sized page. They are the most common type of computer printer used by consumers.",
        "Liquid inkjet printers\n\nInkjet printers operate by propelling variably sized droplets of liquid ink onto almost any sized page. They are the most common type of computer printer used by consumers.\n\nSolid ink printers",
        "Solid ink printers, also known as phase-change ink or hot-melt ink printers, are a type of thermal transfer printer, graphics sheet printer or 3D printer . They use solid sticks, crayons, pearls or granular ink materials.  Common inks are CMYK-colored ink, similar in consistency to candle wax, which are melted and fed into a piezo crystal operated print-head. A Thermal transfer printhead jets the liquid ink on a rotating, oil coated drum. The paper then passes over the print drum, at which time the image is immediately transferred, or transfixed, to the page. Solid ink printers are most commonly used as color office printers and are excellent at printing on transparencies and other non-porous media. Solid ink is also called phase-change or hot-melt ink was first used by Data Products and",
        "used as color office printers and are excellent at printing on transparencies and other non-porous media. Solid ink is also called phase-change or hot-melt ink was first used by Data Products and Howtek, Inc., in 1984.   Solid ink printers can produce excellent results with text and images. Some solid ink printers have evolved to print 3D models, for example, Visual Impact Corporation of Windham, NH was started by retired Howtek employee, Richard Helinski whose 3D patents US4721635 and then US5136515 was licensed to Sanders Prototype, Inc., later named Solidscape, Inc.  Acquisition and operating costs are similar to laser printers. Drawbacks of the technology include high energy consumption and long warm-up times from a cold state. Also, some users complain that the resulting prints are",
        "costs are similar to laser printers. Drawbacks of the technology include high energy consumption and long warm-up times from a cold state. Also, some users complain that the resulting prints are difficult to write on, as the wax tends to repel inks from pens, and are difficult to feed through automatic document feeders, but these traits have been significantly reduced in later models. This type of thermal transfer printer is only available from one manufacturer, Xerox, manufactured as part of their Xerox Phaser office printer line. Previously, solid ink printers were manufactured by Tektronix, but Tektronix sold the printing business to Xerox in 2001.",
        "Dye-sublimation printers\n\nA dye-sublimation printer (or dye-sub printer) is a printer that employs a printing process that uses heat to transfer dye to a medium such as a plastic card, paper, or canvas. The process is usually to lay one color at a time using a ribbon that has color panels. Dye-sub printers are intended primarily for high-quality color applications, including color photography; and are less well-suited for text. While once the province of high-end print shops, dye-sublimation printers are now increasingly used as dedicated consumer photo printers.\n\nThermal printers",
        "Thermal printers\n\nThermal printers work by selectively heating regions of special heat-sensitive paper. Monochrome thermal printers are used in cash registers, ATMs, gasoline dispensers and some older inexpensive fax machines. Colors can be achieved with special papers and different temperatures and heating rates for different colors; these colored sheets are not required in black-and-white output. One example is Zink (a portmanteau of \"zero ink\").\n\nObsolete and special-purpose printing technologies \n\nThe following technologies are either obsolete, or limited to special applications though most were, at one time, in widespread use.",
        "Impact printers",
        "Impact printers rely on a forcible impact to transfer ink to the media. The impact printer uses a print head that either hits the surface of the ink ribbon, pressing the ink ribbon against the paper (similar to the action of a typewriter), or, less commonly, hits the back of the paper, pressing the paper against the ink ribbon (the IBM 1403 for example).  All but the dot matrix printer rely on the use of fully formed characters, letterforms that represent each of the characters that the printer was capable of printing. In addition, most of these printers were limited to monochrome, or sometimes two-color, printing in a single typeface at one time, although bolding and underlining of text could be done by \"overstriking\", that is, printing two or more impressions either in the same",
        "sometimes two-color, printing in a single typeface at one time, although bolding and underlining of text could be done by \"overstriking\", that is, printing two or more impressions either in the same character position or slightly offset. Impact printers varieties include typewriter-derived printers, teletypewriter-derived printers, daisywheel printers, dot matrix printers, and line printers. Dot-matrix printers remain in common use  in businesses where multi-part forms are printed. An overview of impact printing contains a detailed description of many of the technologies used.",
        "Typewriter-derived printers\n\nSeveral different computer printers were simply computer-controllable versions of existing electric typewriters. The Friden Flexowriter and IBM Selectric-based printers were the most-common examples. The Flexowriter printed with a conventional typebar mechanism while the Selectric used IBM's well-known \"golf ball\" printing mechanism. In either case, the letter form then struck a ribbon which was pressed against the paper, printing one character at a time. The maximum speed of the Selectric printer (the faster of the two) was 15.5 characters per second.\n\nTeletypewriter-derived printers",
        "Teletypewriter-derived printers\n\nThe common teleprinter could easily be interfaced with the computer and became very popular except for those computers manufactured by IBM. Some models used a \"typebox\" that was positioned, in the X- and Y-axes, by a mechanism, and the selected letter form was struck by a hammer. Others used a type cylinder in a similar way as the Selectric typewriters used their type ball. In either case, the letter form then struck a ribbon to print the letterform. Most teleprinters operated at ten characters per second although a few achieved 15 CPS.\n\nDaisy wheel printers",
        "Daisy wheel printers\n\nDaisy wheel printers operate in much the same fashion as a typewriter. A hammer strikes a wheel with petals, the \"daisy wheel\", each petal containing a letter form at its tip. The letter form strikes a ribbon of ink, depositing the ink on the page and thus printing a character. By rotating the daisy wheel, different characters are selected for printing. These printers were also referred to as letter-quality printers because they could produce text which was as clear and crisp as a typewriter. The fastest letter-quality printers printed at 30 characters per second.\n\nDot-matrix printers",
        "Dot-matrix printers\n\nThe term dot matrix printer is used for impact printers that use a matrix of small pins to transfer ink to the page. The advantage of dot matrix over other impact printers is that they can produce graphical images in addition to text; however the text is generally of poorer quality than impact printers that use letterforms (type).\n\nDot-matrix printers can be broadly divided into two major classes:\n Ballistic wire printers\n Stored energy printers\n\nDot matrix printers can either be character-based or line-based (that is, a single horizontal series of pixels across the page), referring to the configuration of the print head.",
        "In the 1970s and '80s, dot matrix printers were one of the more common types of printers used for general use, such as for home and small office use. Such printers normally had either 9 or 24 pins on the print head (early 7 pin printers also existed, which did not print descenders). There was a period during the early home computer era when a range of printers were manufactured under many brands such as the Commodore VIC-1525 using the Seikosha Uni-Hammer system.  This used a single solenoid with an oblique striker that would be actuated 7 times for each column of 7 vertical pixels while the head was moving at a constant speed. The angle of the striker would align the dots vertically even though the head had moved one dot spacing in the time.  The vertical dot position was controlled by a",
        "was moving at a constant speed. The angle of the striker would align the dots vertically even though the head had moved one dot spacing in the time.  The vertical dot position was controlled by a synchronized longitudinally ribbed platen behind the paper that rotated rapidly with a rib moving vertically seven dot spacings in the time it took to print one pixel column. 24-pin print heads were able to print at a higher quality and started to offer additional type styles and were marketed as Near Letter Quality by some vendors. Once the price of inkjet printers dropped to the point where they were competitive with dot matrix printers, dot matrix printers began to fall out of favour for general use.",
        "Some dot matrix printers, such as the NEC P6300, can be upgraded to print in color. This is achieved through the use of a four-color ribbon mounted on a mechanism (provided in an upgrade kit that replaces the standard black ribbon mechanism after installation) that raises and lowers the ribbons as needed. Color graphics are generally printed in four passes at standard resolution, thus slowing down printing considerably. As a result, color graphics can take up to four times longer to print than standard monochrome graphics, or up to 8-16 times as long at high resolution mode.",
        "Dot matrix printers are still commonly used in low-cost, low-quality applications such as cash registers, or in demanding, very high volume applications like invoice printing. Impact printing, unlike laser printing, allows the pressure of the print head to be applied to a stack of two or more forms to print multi-part documents such as sales invoices and credit card receipts using continuous stationery with carbonless copy paper. It also has security advantages as ink impressed into a paper matrix by force is harder to erase invisibly. Dot-matrix printers were being superseded even as receipt printers after the end of the twentieth century.\n\nLine printers\n\nLine printers print an entire line of text at a time. Four principal designs exist.",
        "Line printers\n\nLine printers print an entire line of text at a time. Four principal designs exist.\n\n Drum printers, where a horizontally mounted rotating drum carries the entire character set of the printer repeated in each printable character position.  The IBM 1132 printer is an example of a drum printer. Drum printers are also found in adding machines and other numeric printers (POS), the dimensions are compact as only a dozen characters need to be supported.",
        "Chain or train printers, where the character set is arranged multiple times around a linked chain or a set of character slugs in a track traveling horizontally past the print line.  The IBM 1403 is perhaps the most popular and comes in both chain and train varieties.  The band printer is a later variant where the characters are embossed on a flexible steel band.  The LP27 from Digital Equipment Corporation is a band printer.\nBar printers, where the character set is attached to a solid bar that moves horizontally along the print line, such as the IBM 1443.",
        "Bar printers, where the character set is attached to a solid bar that moves horizontally along the print line, such as the IBM 1443.\nA fourth design, used mainly on very early printers such as the IBM 402, features independent type bars, one for each printable position. Each bar contains the character set to be printed.  The bars move vertically to position the character to be printed in front of the print hammer.",
        "In each case, to print a line, precisely timed hammers strike against the back of the paper at the exact moment that the correct character to be printed is passing in front of the paper. The paper presses forward against a ribbon which then presses against the character form and the impression of the character form is printed onto the paper.  Each system could have slight timing issues, which could cause minor misalignment of the resulting printed characters.  For drum or typebar printers, this appeared as vertical misalignment, with characters being printed slightly above or below the rest of the line.  In chain or bar printers, the misalignment was horizontal, with printed characters being crowded closer together or farther apart.  This was much less noticeable to human vision than",
        "of the line.  In chain or bar printers, the misalignment was horizontal, with printed characters being crowded closer together or farther apart.  This was much less noticeable to human vision than vertical misalignment, where characters seemed to bounce up and down in the line, so they were considered as higher quality print.",
        "Comb printers, also called line matrix printers, represent the fifth major design. These printers are a hybrid of dot matrix printing and line printing. In these printers, a comb of hammers prints a portion of a row of pixels at one time, such as every eighth pixel. By shifting the comb back and forth slightly, the entire pixel row can be printed, continuing the example, in just eight cycles. The paper then advances, and the next pixel row is printed. Because far less motion is involved than in a conventional dot matrix printer, these printers are very fast compared to dot matrix printers and are competitive in speed with formed-character line printers while also being able to print dot matrix graphics.  The Printronix P7000  series of line matrix printers are still manufactured as of",
        "and are competitive in speed with formed-character line printers while also being able to print dot matrix graphics.  The Printronix P7000  series of line matrix printers are still manufactured as of 2013.",
        "Line printers are the fastest of all impact printers and are used for bulk printing in large computer centres. A line printer can print at 1100 lines per minute or faster, frequently printing pages more rapidly than many current laser printers.  On the other hand, the mechanical components of line printers operate with tight tolerances and require regular preventive maintenance (PM) to produce a top quality print. They are virtually never used with personal computers and have now been replaced by high-speed laser printers.  The legacy of line printers lives on in many operating systems, which use the abbreviations \"lp\", \"lpr\", or \"LPT\" to refer to printers.",
        "Liquid ink electrostatic printers \nLiquid ink electrostatic printers use a chemical coated paper, which is charged by the print head according to the image of the document. The paper is passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image. This process was developed from the process of electrostatic copying. Color reproduction is very accurate, and because there is no heating the scale distortion is less than  ±0.1%. (All laser printers have an accuracy of ±1%.)",
        "Worldwide, most survey offices used this printer before color inkjet plotters become popular. Liquid ink electrostatic printers were mostly available in  width and also 6 color printing. These were also used to print large billboards. It was first introduced by Versatec, which was later bought by Xerox. 3M also used to make these printers.\n\nPlotters",
        "Plotters \n\nPen-based plotters were an alternate printing technology once common in engineering and architectural firms. Pen-based plotters rely on contact with the paper (but not impact, per se) and special purpose pens that are mechanically run over the paper to create text and images. Since the pens output continuous lines, they were able to produce technical drawings of higher resolution than was achievable with dot-matrix technology.  Some plotters used roll-fed paper, and therefore had a minimal restriction on the size of the output in one dimension. These plotters were capable of producing quite sizable drawings.\n\nOther printers \n\nA number of other sorts of printers are important for historical reasons, or for special purpose uses.",
        "Other printers \n\nA number of other sorts of printers are important for historical reasons, or for special purpose uses.\n\n Digital minilab (photographic paper)\n Electrolytic printers\n Spark printer\n Barcode printer multiple technologies, including: thermal printing, inkjet printing, and laser printing barcodes\n Billboard / sign paint spray printers\n Laser etching (product packaging) industrial printers\n Microsphere (special paper)\n\nAttributes\n\nConnectivity \n\nPrinters can be connected to computers in many ways: directly by a dedicated data cable such as the USB, through a short-range radio like Bluetooth, a local area network using cables (such as the Ethernet) or radio (such as WiFi), or on a standalone basis without a computer, using a memory card or other portable data storage device.",
        "Printer control languages \nMost printers other than line printers accept control characters or unique character sequences to control various printer functions.  These may range from shifting from lower to upper case or from black to red ribbon on typewriter printers to switching fonts and changing character sizes and colors on raster printers. Early printer controls were not standardized, with each manufacturer's equipment having its own set.  The IBM Personal Printer Data Stream (PPDS) became a commonly used command set for dot-matrix printers.",
        "Today, most printers accept one or more page description languages (PDLs). Laser printers with greater processing power frequently offer support for variants of Hewlett-Packard's Printer Command Language (PCL), PostScript or XML Paper Specification. Most inkjet devices support manufacturer proprietary PDLs such as ESC/P. The diversity in mobile platforms have led to various standardization efforts around device PDLs such as the Printer Working Group (PWG's) PWG Raster.\n\nPrinting speed",
        "Printing speed \n\nThe speed of early printers was measured in units of characters per minute (cpm) for character printers, or lines per minute (lpm) for line printers. Modern printers are measured in pages per minute (ppm). These measures are used primarily as a marketing tool, and are not as well standardised as toner yields. Usually pages per minute refers to sparse monochrome office documents, rather than dense pictures which usually print much more slowly, especially color images. Speeds in ppm usually apply to A4 paper in most countries in the world, and letter paper size, about 6% shorter, in North America.\n\nPrinting mode \n\nThe data received by a printer may be:",
        "Printing mode \n\nThe data received by a printer may be:\n\n A string of characters\n A bitmapped image\n A vector image\n A computer program written in a page description language, such as PCL or PostScript\n\nSome printers can process all four types of data, others not.\n\n Character printers, such as daisy wheel printers, can handle only plain text data or rather simple point plots.\n Pen plotters typically process vector images. Inkjet based plotters can adequately reproduce all four.\n Modern printing technology, such as laser printers and inkjet printers, can adequately reproduce all four. This is especially true of printers equipped with support for PCL or PostScript, which includes the vast majority of printers produced today.",
        "Today it is possible to print everything (even plain text) by sending ready bitmapped images to the printer. This allows better control over formatting, especially among machines from different vendors. Many printer drivers do not use the text mode at all, even if the printer is capable of it.\n\nMonochrome, color and photo printers\nA monochrome printer can only produce monochrome images, with only shades of a single color.  Most printers can produce only two colors, black (ink) and white (no ink). With half-tonning techniques, however, such a printer can produce acceptable grey-scale images too",
        "A color printer can produce images of multiple colors. A photo printer is a color printer that can produce images that mimic the color range (gamut) and resolution of prints made from photographic film.\n\nPage yield\n\nThe page yield is the number of pages that can be printed from a toner cartridge or ink cartridge—before the cartridge needs to be refilled or replaced.\nThe actual number of pages yielded by a specific cartridge depends on a number of factors.\n\nFor a fair comparison, many laser printer manufacturers use the ISO/IEC 19752 process to measure the toner cartridge yield.",
        "For a fair comparison, many laser printer manufacturers use the ISO/IEC 19752 process to measure the toner cartridge yield.\n\nEconomics\nIn order to fairly compare operating expenses of printers with a relatively small ink cartridge to printers with a larger, more expensive toner cartridge that typically holds more toner and so prints more pages before the cartridge needs to be replaced, many people prefer to estimate operating expenses in terms of cost per page (CPP).",
        "Retailers often apply the \"razor and blades\" model: a company may sell a printer at cost and make profits on the ink cartridge, paper, or some other replacement part. This has caused legal disputes regarding the right of companies other than the printer manufacturer to sell compatible ink cartridges. To protect their business model, several manufacturers invest heavily in developing new cartridge technology and patenting it.",
        "Other manufacturers, in reaction to the challenges from using this business model, choose to make more money on printers and less on ink, promoting the latter through their advertising campaigns. Finally, this generates two clearly different proposals: \"cheap printer – expensive ink\" or \"expensive printer – cheap ink\". Ultimately, the consumer decision depends on their reference interest rate or their time preference. From an economics viewpoint, there is a clear trade-off between cost per copy and cost of the printer.\n\nPrinter steganography",
        "Printer steganography \n\nPrinter steganography is a type of steganography – \"hiding data within data\" – produced by color printers, including Brother, Canon, Dell, Epson, HP, IBM, Konica Minolta, Kyocera, Lanier, Lexmark, Ricoh, Toshiba and Xerox brand color laser printers, where tiny yellow dots are added to each page. The dots are barely visible and contain encoded printer serial numbers, as well as date and time stamps.\n\nManufacturers and market share\nAs of 2020-2021, the largest worldwide vendor of printers is Hewlett-Packard, followed by Canon, Brother, Seiko Epson and Kyocera. Other known vendors include NEC, Ricoh, Xerox, Lexmark, OKI, Sharp, Konica Minolta, Samsung, Kodak, Dell, Toshiba, Star Micronics, Citizen and Panasonic.\n\nSee also",
        "See also\n\n Campus card\n Cardboard modeling\n Dye-sublimation printer\n History of printing\n Label printer\n List of printer companies\n Print (command)\n Printer driver\n Print screen\n Print server\n Printer friendly (also known as a printable version)\n Printer point\n Printer (publishing)\n Printmaking\n Smart card\n Typewriter ribbon\n 3D printing\n\nReferences\n\nExternal links\n \n\nComputer printers\nOffice equipment\nTypography\nArticles containing video clips"
    ],
    [
        "Procom\nProcom, ProCom or PROCOM may refer to:\n\n Processes of Compounds (PROCOM), a process simulation software package; see \n ProCom, the Promotion Commission of the World Association for Waterborne Transport Infrastructure\n Procom Technology, a company acquired by Sun Microsystems in 2005\n Protective Security Command (ProCom), a Singapore counter terrorism police unit"
    ],
    [
        "R10000\nThe R10000, code-named \"T5\", is a RISC microprocessor implementation of the MIPS IV instruction set architecture (ISA) developed by MIPS Technologies, Inc. (MTI), then a division of Silicon Graphics, Inc. (SGI). The chief designers are Chris Rowen and Kenneth C. Yeager. The R10000 microarchitecture is known as ANDES, an abbreviation for Architecture with Non-sequential Dynamic Execution Scheduling. The R10000 largely replaces the R8000 in the high-end and the R4400 elsewhere. MTI was a fabless semiconductor company; the R10000 was fabricated by NEC and Toshiba. Previous fabricators of MIPS microprocessors such as Integrated Device Technology (IDT) and three others did not fabricate the R10000 as it was more expensive to do so than the R4000 and R4400.\n\nHistory",
        "History \n\nThe R10000 was introduced in January 1996 at clock frequencies of 175 MHz and 195 MHz. A 150 MHz version was introduced in the O2 product line in 1997, but discontinued shortly after due to customer preference for the 175 MHz version. The R10000 was not available in large volumes until later in the year due to fabrication problems at MIPS's foundries. The 195 MHz version was in short supply throughout 1996, and was priced at US$3,000 as a result.\n\nOn 25 September 1996, SGI announced that R10000s fabricated by NEC between March and the end of July that year were faulty, drawing too much current and causing systems to shut down during operation. SGI recalled 10,000 R10000s that had shipped in systems as a result, which impacted the company's earnings.",
        "In 1997, a version of R10000 fabricated in a 0.25 µm process enabled the microprocessor to reach 250 MHz.\n\nUsers\nUsers of the R10000 include:\n SGI:\n Workstations: Indigo2 (IMPACT Generation), Octane, O2\n Servers: Challenge, Origin 2000\n Supercomputers: Onyx, Onyx2\n NEC, in its Cenju-4 supercomputer\n Siemens Nixdorf, in its servers run under SINIX\n Tandem Computers, in its Himalaya fault-tolerant servers\n\nDescription \n\nThe R10000 is a four-way superscalar design that implements register renaming and executes instructions out-of-order. Its design is a departure from previous MTI microprocessors such as the R4000, which is a much simpler scalar in-order design that relies largely on high clock rates for performance.",
        "The R10000 fetches four instructions every cycle from its instruction cache. These instructions are decoded and then placed into the integer, floating-point or load/store instruction queues depending on the type of the instruction. The decode unit is assisted by the pre-decoded instructions from the instruction cache, which append five bits to every instruction to enable the unit to quickly identify which execution unit the instruction is executed in, and rearrange the format of the instruction to optimize the decode process.",
        "Each of the instruction queues can accept up to four instructions from the decoder, avoiding any bottlenecks. The instruction queues issue their instructions to their execution units dynamically depending on the availability of operands and resources. Each of the queues except for the load/store queue can issue up to two instructions every cycle to its execution units. The load/store queue can only issue one instruction. The R10000 can thus issue up to five instructions every cycle.",
        "Integer unit",
        "The integer unit consists of the integer register file and three pipelines, two integer, one load store. The integer register file is 64 bits wide and contains 64 entries, of which 32 are architectural registers and 32 are rename registers which implement register renaming. The register file has seven read ports and three write ports. Both integer pipelines have an adder and a logic unit. However, only the first pipeline has a barrel shifter and hardware for confirming the prediction of conditional branches. The second pipeline is used to access the multiplier and divider. Multiplies are pipelined, and have a six-cycle latency for 32-bit integers and ten for 64-bit integers. Division is not pipelined. The divider uses a non-restoring algorithm that produces one bit per cycle. Latencies",
        "and have a six-cycle latency for 32-bit integers and ten for 64-bit integers. Division is not pipelined. The divider uses a non-restoring algorithm that produces one bit per cycle. Latencies for 32-bit and 64-bit divides are 35 and 67 cycles, respectively.",
        "Floating-point unit\nThe floating-point unit (FPU) consists of four functional units, an adder, a multiplier, divide unit and square root unit. The adder and multiplier are pipelined, but the divide and square root units are not. Adds and multiplies have a latency of three cycles and the adder and multiplier can accept a new instruction every cycle. The divide unit has a 12- or 19-cycle latency, depending on whether the divide is single precision or double precision, respectively.",
        "The square root unit executes square root and reciprocal square root instructions. Square root instructions have an 18- or 33-cycle latency for single precision or double precision, respectively. A new square root instruction can be issued to the divide unit every 20 or 35 cycles for single precision and double precision respectively. Reciprocal square roots have longer latencies, 30 to 52 cycles for single precision (32-bit) and double precision (64-bit) respectively.\n\nThe floating-point register file contains sixty-four 64-bit registers, of which thirty-two are architectural and the remaining are rename registers.\nThe adder has its own dedicated read and write ports, whereas the multiplier shares its with the divider and square root unit.",
        "The divide and square root units use the SRT algorithm. The MIPS IV ISA has a multiply–add instruction. This instruction is implemented by the R10000 with a bypass — the result of the multiply can bypass the register file and be delivered to the add pipeline as an operand, thus it is not a fused multiply–add, and has a four-cycle latency.\n\nCaches\nThe R10000 has two comparatively large on-chip caches, a 32 KB instruction cache and a 32 KB data cache. The instruction cache is two-way set-associative and has a 128-byte line size. Instructions are partially decoded by appending four bits to each instruction (which have a length of 32 bits) before they are placed in the cache.",
        "The 32 KB data cache is dual-ported through two-way interleaving. It consists of two 16 KB banks, and each bank are two-way set-associative. The cache has 64-byte lines, uses the write-back protocol, and is virtually indexed and physically tagged to enable the cache to be indexed in the same clock cycle and to maintain coherency with the secondary cache.",
        "The external secondary unified cache supported capacities between 512 KB and 16 MB. It is implemented with commodity synchronous static random access memorys (SSRAMs). The cache is accessed via its own 128-bit bus that is protected by 9-bits of error correcting code (ECC). The cache and bus operate at the same clock rate as the R10000, whose maximum frequency was 200 MHz. At 200 MHz, the bus yielded a peak bandwidth of 3.2 GB/s. The cache is two-way set associative, but to avoid a high pin count, the R10000 predicts which way is accessed.",
        "Addressing\nMIPS IV is a 64-bit architecture, but to reduce cost the R10000 does not implement the entire physical or virtual address. Instead, it has a 40-bit physical address and a 44-bit virtual address, thus it is capable of addressing 1 TB of physical memory and 16 TB of virtual memory.\n\nAvalanche system bus\nThe R10000 uses the Avalanche bus, a 64-bit bus that operates at frequencies up to 100 MHz. Avalanche is a multiplexed address and data bus, so at 100 MHz it yields a maximum theoretical bandwidth of 800 MB/s, but its peak bandwidth is 640 MB/s as it requires some cycles to transmit addresses.",
        "The system interface controller supports glue-less symmetrical multiprocessing (SMP) of up to four microprocessors. Systems using the R10000 with external logic can scale to hundreds of processors. An example of such a system is the Origin 2000.",
        "Fabrication\nThe R10000 consists of approximately 6.8 million transistors, of which approximately 4.4 million are contained in the primary caches. The die measures 16.640 by 17.934 mm, for a die area of 298.422 mm2. It is fabricated in a 0.35 µm process and packaged in 599-pad ceramic land grid array (LGA). Before the R10000 was introduced, the Microprocessor Report, covering the 1994 Microprocessor Forum, reported that it was packaged in a 527-pin ceramic pin grid array (CPGA); and that vendors also investigated the possibility of using a 339-pin multi-chip module (MCM) containing the microprocessor die and 1 MB of cache.",
        "Derivatives \nThe R10000 was extended by multiple successive derivatives. All derivatives after the R12000 have their clock frequency kept as low as possible to maintain power dissipation in the 15 to 20 W range so they can be densely packaged in SGI's high performance computing (HPC) systems.\n\nR12000",
        "R12000 \n\nThe R12000 is a derivative of the R10000 started by MIPS and completed by SGI. It was fabricated by NEC and Toshiba. The version fabricated by NEC is called the VR12000. The microprocessor was introduced in November 1998. It is available at 270, 300 and 360 MHz. The R12000 was developed as a stop-gap solution following the cancellation of the \"Beast\" project, which intended to deliver a successor to the R10000. R12000 users include NEC, Siemens-Nixdorf, SGI and Tandem Computers (and later Compaq, after their acquisition of Tandem).",
        "The R12000 improves upon the R10000 microarchitecture by: inserting an extra pipeline stage to improve clock frequency by resolving a critical path; increasing the number of entries in the branch history table, improving prediction; modifying the instruction queues so they take into account the age of a queued instruction, enabling older instructions to be executed before newer ones if possible.",
        "The R12000 was fabricated by NEC and Toshiba in a 0.25 µm CMOS process with four levels of aluminum interconnect. The use of a new process does not mean that the R12000 was a simple die shrink with a tweaked microarchitecture; the layout of the die is optimized to take advantage of the 0.25 µm process. The NEC fabricated VR12000 contained 7.15 million transistors and measured 15.7 by 14.6 mm (229.22 mm2).\n\nR12000A \nThe R12000A is a derivative of the R12000 developed by SGI. Introduced in July 2000, it operates at 400 MHz and was fabricated by NEC a 0.18 µm process with aluminum interconnects.",
        "R12000A \nThe R12000A is a derivative of the R12000 developed by SGI. Introduced in July 2000, it operates at 400 MHz and was fabricated by NEC a 0.18 µm process with aluminum interconnects.\n\nR14000 \nThe R14000 is a further development of the R12000 announced in July 2001. The R14000 operates at 500 MHz, enabled by the 0.13 µm CMOS process with five levels of copper interconnect it is fabricated with. It features improvements to the microarchitecture of the R12000 by supporting double data rate (DDR) SSRAMs for the secondary cache and a 200 MHz system bus.\n\nR14000A",
        "R14000A \n\nThe R14000A is a further development of the R14000 announced in February 2002. It operates at 600 MHz, dissipates approximately 17 W, and was fabricated by NEC Corporation in a 0.13 µm CMOS process with seven levels of copper interconnect.\n\nR16000 \n\nThe R16000, code-named \"N0\", is the last derivative of the R10000. It is developed by SGI and fabricated by NEC in their 0.11 µm process with eight levels of copper interconnect. The microprocessor was introduced on 9 January 2003, debuting at 700 MHz for the Fuel and also used in their Onyx4 Ultimate Vision. In April 2003, a 600 MHz version was introduced for the Origin 350. Improvements are 64 KB instruction and data caches.\n\nR16000A",
        "R16000A \n\nThe R16000A refers to R16000 microprocessors with clock rates higher than 700 MHz. The first R16000A is an 800 MHz version, introduced on 4 February 2004. Later, a 900 MHz version was introduced, and this version was, for some time, the fastest publicly known R16000A—SGI later revealed there were 1.0 GHz R16000s shipped to selected customers. R16000 users included HP and SGI. SGI used the microprocessor in their Fuel and Tezro workstations; and the Origin 3000 servers and supercomputers. HP used the R16000A in their NonStop Himalaya S-Series fault-tolerant servers inherited from Tandem via Compaq.\n\nR18000",
        "R18000 \n\nThe R18000 is a canceled further development of the R10000 microarchitecture that featured major improvements by Silicon Graphics, Inc. described at the Hot Chips symposium in 2001. The R18000 was designed specifically for SGI's ccNUMA servers and supercomputers. Each node would have two R18000s connected via a multiplexed bus to a system controller, which would interface the microprocessors to their local memory and the rest of the system via a hypercube network.",
        "The R18000 improved the floating-point instruction queues and revised the floating-point unit to feature two multiply–add units, quadrupling the peak FLOPS count. Division and square-root would be performed in separate non-pipelined units in parallel to the multiply–add units. The system interface and memory hierarchy was also significantly reworked. It would have a 52-bit virtual address and a 48-bit physical address. The bidirectional multiplexed address and data system bus of the earlier models would be replaced by two unidirectional DDR links, a 64-bit multiplexed address and write path and a 128-bit read path. The paths could be shared with another R18000 through multiplexing. The bus could also be configured in the SysAD or Avalanche configuration for backwards compatibility with",
        "and a 128-bit read path. The paths could be shared with another R18000 through multiplexing. The bus could also be configured in the SysAD or Avalanche configuration for backwards compatibility with R10000 systems.",
        "The R18000 would have a 1 MB four-way set-associative secondary cache to be included on-die; supplemented by an optional tertiary cache built from single data rate (SDR) or double data rate (DDR) SSRAM or DDR SDRAM with capacities of 2 to 64 MB. The L3 cache would have its cache tags, equivalent to 400 KB, located on-die to reduce latency. The L3 cache would be accessed via a 144-bit bus, of which 128 bits are for data and 16 bits for ECC. The L3 cache's clock rate would be programmable.\n\nThe R18000 was to be fabricated in NEC's UX5 process, a 0.13 µm CMOS process with nine levels of copper interconnect. It would have used 1.2 V power supply and dissipated less heat than contemporary server microprocessors in order to be densely packed into systems.\n\nNotes",
        "References \n Fu, Tim et al. (31 August 2001). \"R18000: The Latest SGI Superscalar Microprocessor\". Hot Chips XIII.\n Halfhill, Tom R. (November 1994). \"T5: Brute Force\". Byte Magazine.\n Heinrich, Joe (29 January 1997). \"MIPS R10000 Microprocessor User's Manual\".\n Kanellos, Michael; Kawamoto, Dawn (9 April 1998). \"Silicon Graphics scraps MIPS plans\". CNET News.\n MIPS Technologies, Incorporated. (October 1994). \"R10000 Microprocessor Product Review\".\n Morgan, Timothy Prickett (16 April 2003). \"SGI Announces Origin 350 Midrange HPC Server\". IT Jungle.\n NEC Corporation (24 November 1998). NEC Markets World's Highest Class Performance Microprocessor. (Press release).",
        "NEC Corporation (24 November 1998). NEC Markets World's Highest Class Performance Microprocessor. (Press release).\n \n Vasseghi, N. et al. (November 1996). \"200-MHz superscalar RISC microprocessor\". IEEE Journal of Solid-State Circuits 31 (11): pp. 1675–1686.\n Yeager, Kenneth C. (August 1995). \"R10000 Superscalar Microprocessor\". Hot Chips VII.",
        "MIPS implementations\nMIPS microprocessors\nSuperscalar microprocessors\n64-bit microprocessors"
    ],
    [
        "R3000\nThe R3000 is a 32-bit RISC microprocessor chipset developed by MIPS Computer Systems that implemented the MIPS I instruction set architecture (ISA). Introduced in June 1988, it was the second MIPS implementation, succeeding the R2000 as the flagship MIPS microprocessor. It operated at 20, 25 and 33.33 MHz.\n\nThe MIPS 1 instruction set is small compared to those of the contemporary 80x86 and 680x0 architectures, encoding only more commonly used operations and supporting few addressing modes. Combined with its fixed instruction length and only three different types of instruction formats, this simplified instruction decoding and processing. It employed a 5-stage instruction pipeline, enabling execution at a rate approaching one instruction per cycle, unusual for its time.",
        "This MIPS generation supports up to four co-processors. In addition to the CPU core, the R3000 microprocessor includes a Control Processor (CP), which contains a Translation Lookaside Buffer and a Memory Management Unit. The CP works as a coprocessor. Besides the CP, the R3000 can also support an external R3010 numeric coprocessor, along with two other external coprocessors.\n\nThe R3000 CPU does not include level 1 cache. Instead, its on-chip cache controller operates external data and instruction caches of up to 256 KB each. It can access both caches during the same clock cycle.",
        "The R3000 found much success and was used by many companies in their workstations and servers. Users included:\n Ardent Computer\n Atari COJAG (A modified Atari Jaguar for arcade systems).\n Digital Equipment Corporation (DEC) for their DECstation workstations and multiprocessor DECsystem servers.\n Evans & Sutherland for their Vision (ESV) series workstations.\n LSI Logic for their CW4003 RISC processor core and DCAM-101 system-on-a-chip.\n MIPS Computer Systems for their MIPS RISC/os Unix workstations and servers.\n NEC for their RISC EWS4800 workstations and UP4800 servers.\n Prime Computer\n Pyramid Technology\n Seiko Epson\n Silicon Graphics for their Professional IRIS, Personal IRIS and Indigo workstations, and the multiprocessor Power Series visualization systems.",
        "Prime Computer\n Pyramid Technology\n Seiko Epson\n Silicon Graphics for their Professional IRIS, Personal IRIS and Indigo workstations, and the multiprocessor Power Series visualization systems.\n Sony for their PlayStation and PlayStation 2 (SCPH-10000 to SCPH-700XX - clocked at 37.5 MHz for use as an I/O CPU and at 33.8 MHz for compatibility with PlayStation games) video game consoles, and NEWS workstations, as well as the Bemani System 573 Analog arcade unit, which runs on the R3000A.\n Tandem Computers for their NonStop Cyclone/R and CLX/R fault-tolerant servers.\n Whitechapel Workstations for their Hitech-20 workstation.\n New Horizons Probe",
        "The R3000 was also used as an embedded microprocessor. When advances in technology rendered it obsolete for high-performance systems, it found continued use in lower-cost designs. Companies such as LSI Logic developed derivatives of the R3000 specifically for embedded systems.",
        "The R3000 was a further development of the R2000 with minor improvements including larger TLB and a faster bus to the external caches. The R3000 die contained 115,000 transistors and measured about 75,000 square mils (48 mm2). MIPS was a fabless semiconductor company, so the R3000 was fabricated by MIPS partners including Integrated Device Technology (IDT), LSI Logic, NEC Corporation, Performance Semiconductor, and others. It was fabricated in a 1.2 μm complementary metal–oxide–semiconductor (CMOS) process with two levels of aluminium interconnect.",
        "Derivatives of the R3000 for non-embedded applications include:\n R3000A - A further development by MIPS introduced in 1989. It operated at clock frequencies up to 40 MHz.\n PR3400 - Developed by Performance Semiconductor, introduced in May 1991, also at up to 40 MHz. It integrated the Performance Semiconductor PR3000A and PR3010A onto a single die.",
        "Derivatives of the R3000 for embedded applications include:\n CW4003, DCAM-101 - Aimed at digital camera applications, the CW4003 core featured a \"multiplier-addition bolt-on\" (MABO) unit for accelerated integer arithmetic and a pixel-processing accelerator (PPA) unit accessible via the coprocessor 2 interface. The DCAM-101 combined the CW4003 core with modules interfacing to a camera sensor, display, storage and other peripherals, also incorporating a JPEG compression/decompression unit.\n PR31500, PR31700 - Microprocessors from Philips Semiconductors used in the Philips Velo handheld PC range. The 75 MHz PR31700 was fabricated in a 350 nm process, delivered in a 208-pin LQFP, it operated at 3.3 V and dissipated only 350 mW.",
        "RISController - A family of low-end microcontrollers from IDT. Models include the R3041, R3051, R3052, R3071 and R3081.\n TX3900 - A microcontroller from Toshiba.\n Mongoose-V - A radiation-hardened and expanded 10–15 MHz CPU for use on spacecraft, it is still in use today in applications such as NASA's New Horizons space probe.",
        "References\n\n \"MIPS Technologies R3000\"\n\nFurther reading\n\nMIPS implementations\nMIPS microprocessors\n32-bit microprocessors"
    ],
    [
        "R4000\nThe R4000 is a microprocessor developed by MIPS Computer Systems that implements the MIPS III instruction set architecture (ISA). Officially announced on 1 October 1991, it was one of the first 64-bit microprocessors and the first MIPS III implementation. In the early 1990s, when RISC microprocessors were expected to replace CISC microprocessors such as the Intel i486, the R4000 was selected to be the microprocessor of the Advanced Computing Environment (ACE), an industry standard that intended to define a common RISC platform. ACE ultimately failed for a number of reasons, but the R4000 found success in the workstation and server markets.",
        "Models \nThere are three configurations of the R4000: the R4000PC, an entry-level model with no support for a secondary cache; the R4000SC, a model with secondary cache but no multiprocessor capability; and the R4000MC, a model with secondary cache and support for the cache coherency protocols required by multiprocessor systems.",
        "Description \nThe R4000 is a scalar superpipelined microprocessor with an eight-stage integer pipeline. During the first stage (IF), a virtual address for an instruction is generated and the instruction translation lookaside buffer (TLB) begins the translation of the address to a physical address. In the second stage (IS), translation is completed and the instruction is fetched from an internal 8 KB instruction cache. The instruction cache is direct-mapped and virtually indexed, physically tagged. It has a 16- or 32-byte line size. Architecturally, it could be expanded to 32 KB.",
        "During the third stage (RF), the instruction is decoded and the register file is read. The MIPS III defines two register files, one for the integer unit and the other for floating-point. Each register file is 64 bits wide and contained 32 entries. The integer register file has two read ports and one write port, while the floating-point register file has two read ports and two write ports. Execution begins at stage four (EX) for both integer and floating-point instructions; and is written back to the register files when completed in stage eight (WB). Results may be bypassed if possible.",
        "Integer execution \nThe R4000 has an arithmetic logic unit (ALU), a shifter, multiplier and divider and load aligner for executing integer instructions. The ALU consists of a 64-bit carry-select adder and a logic unit and is pipelined. The shifter is a 32-bit barrel shifter. It performs 64-bit shifts in two cycles, stalling the pipeline as a result. This design was chosen to save die area. The multiplier and divider are not pipelined and have significant latencies: multiplies have a 10- or 20-cycle latency for 32-bit or 64-bit integers, respectively; whereas divides have a 69- or 133-cycle latency for 32-bit or 64-bit integers, respectively. Most instructions have a single cycle latency. The ALU adder is also used for calculating virtual addresses for loads, stores and branches.",
        "Load and store instructions are executed by the integer pipeline, and access the on-chip 8 KB data cache.\n\nFloating-point execution \nThe R4000 has an on-die IEEE 754-1985-compliant floating-point unit (FPU), referred to as the R4010. The FPU is a coprocessor designated CP1 (the MIPS ISA defined four coprocessors, designated CP0 to CP3). The FPU can operate in two modes, 32- or 64-bit which are selected by setting a bit, the FR bit, in the CPU status register. In 32-bit mode, the 32 floating-point registers become 32 bits wide when used to hold single-precision floating-point numbers. When used to hold double-precision numbers, there are 16 floating-point registers (the registers are paired).",
        "The FPU can operate in parallel with the ALU unless there is a data or resource dependency, which causes it to stall. It contains three sub-units: an adder, a multiplier and a divider. The multiplier and divider can execute an instruction in parallel with the adder, but they use the adder in their final stages of execution, thus imposing limits to overlapping execution. Thus, under certain conditions, it can execute up to three instructions at any time, one in each unit. The FPU is capable of retiring one instruction per cycle.",
        "The adder and multiplier are pipelined. The multiplier has a four-stage multiplier pipeline. It is clocked at twice the clock frequency of the microprocessor for adequate performance and uses dynamic logic to achieve the high clock frequency. Division has a 23- or 36-cycle latency for single- or double-precision operations and square-root has a 54- or 112-cycle latency. Division and square-root uses the SRT algorithm.",
        "Memory management \nThe memory management unit (MMU) uses a 48-entry translation lookaside buffer to translate virtual addresses. The R4000 uses a 64-bit virtual address, but only implements 40 of the 64 bits, allowing 1 TB of virtual memory; the remaining bits are checked to ensure that they contain zero. The R4000 uses a 36-bit physical address, thus is able to address 64 GB of physical memory.",
        "Secondary cache \nThe R4000 (SC and MC configurations only) supports an external secondary cache with a capacity of 128 KB to 4 MB. The cache is accessed via a dedicated 128-bit data bus. The secondary cache can be configured either as a unified cache or as a split instruction and data cache. In the latter configuration, each cache can have a capacity of 128 KB to 2 MB. The secondary cache is physically indexed, physically tagged and has a programmable line size of 128, 256, 512 or 1,024 bytes. The cache controller is on-die. The cache is built from standard static random access memory (SRAM). The data and tag buses are ECC-protected.",
        "System bus \nThe R4000 uses a 64-bit system bus called the SysAD bus. The SysAD bus was an address and data multiplexed bus, that is, it used the same set of wires to transfer data and addresses. While this reduces bandwidth, it is also less expensive than providing a separate address bus, which requires more pins and increases the complexity of the system.  The SysAD bus can be configured to operate at half, a third or a quarter of the internal clock frequency. The SysAD bus generates its clock signal by dividing the operating frequency.",
        "Transistor count, die dimensions and process details \nThe R4000 contains 1.2 million transistors. It was designed for a 1.0 μm two-layer metal complementary metal–oxide–semiconductor (CMOS) process. As MIPS was a fabless company, the R4000 was fabricated by partners in their own processes, which had a 0.8 μm minimum feature size.\n\nClocking \nThe R4000 generates the various clock signals from a master clock signal generated externally. For the operating frequency, the R4000 multiplies the master clock signal by two by use of an on-die phase-locked loop (PLL).",
        "Packaging \nThe R4000PC is packaged in a 179-pin ceramic pin grid array (CPGA). The R4000SC and R4000MC are packaged in a 447-pin ceramic staggered pin grid array (SPGA). The pin out of the R4000MC is different from the R4000SC, with some pins which are unused on the R4000SC used for signals to implement cache coherency on the R4000MC. The pin-out of the R4000PC is similar to that of the PGA-packaged R4200 and R4600 microprocessors. This characteristic enables a properly designed system to use any of the three microprocessors.\n\nR4400",
        "R4400 \n\nThe R4400 is a further development of the R4000. It was announced in early November 1992. Samples of the microprocessor had been shipped to selected customers before then, with general availability in January 1993. The R4400 operates at clock frequencies of 100, 133, 150, 200, and 250 MHz. The only major improvement from the R4000 is larger primary caches, which were doubled in capacity to 16 KB each from 8 KB each. It contained 2.3 million transistors.",
        "The R4400 was licensed by Integrated Device Technology (IDT), LSI Logic, NEC, Performance Semiconductor, Siemens AG and Toshiba. IDT, NEC, Siemens and Toshiba fabricated and marketed the microprocessor. LSI Logic used the R4400 in custom products. Performance Semiconductor sold their logic division to Cypress Semiconductor where the MIPS microprocessor products were discontinued.",
        "NEC marketed their version as the VR4400. The first version, a 150 MHz part, was announced in November 1992. Early versions were fabricated in a 0.6 μm process. In mid-1995, a 250 MHz part began sampling. It was fabricated in a 0.35 μm four-layer-metal process. NEC also produced the MR4401, a ceramic multi-chip module (MCM) that contained a VR4400SC with ten 1 Mbit SRAM chips that implemented a 1 MB secondary cache. The MCM was pin-compatible with the R4x00PC. The first version, a 150 MHz part, was announced in 1994. In 1995, a 200 MHz part was announced.",
        "Toshiba marketed their version as the TC86R4400. A 200 MHz part containing 2.3 million transistors and measuring 134 mm2 fabricated in a 0.3 μm process was introduced in mid-1994. The R4400PC was priced at , the R4400SC at , and the R4400MC at  in quantities of 10,000.\n\nUsage \nThe R4400 is used by:",
        "Carrera Computers in their Windows NT personal computers and workstations\n Concurrent Computer Corporation in their real-time multiprocessor Maxion systems\n DeskStation Technology in their Windows NT personal computers and DeskStation Tyne workstation\n Digital Equipment Corporation in their DECstation 5000/260 workstation and server\n NEC Corporation in their RISCstation workstations, RISCserver servers, and Cenju-3 supercomputer\n NeTPower in their Windows NT workstations and servers\n Pyramid Technology used the R4400MC in their Nile Series servers\n Siemens Nixdorf Informationssysteme (SNI) in their RM-series UNIX servers and SR2000 mainframe\n Silicon Graphics in their Onyx, Indigo, Indigo2, and Indy workstations; and in their Challenge server",
        "Siemens Nixdorf Informationssysteme (SNI) in their RM-series UNIX servers and SR2000 mainframe\n Silicon Graphics in their Onyx, Indigo, Indigo2, and Indy workstations; and in their Challenge server\n Tandem Computers in their NonStop Himalaya fault-tolerant servers",
        "Chipsets \nThe R4000 and R4400 microprocessors were interfaced to the system by custom ASICs or by commercially available chipsets. System vendors such as SGI developed their own ASICs for their systems. Commercial chipsets were developed, fabricated and marketed by companies such as Toshiba with their the Tiger Shark chipset, which provided a i486-compatible bus.\n\nNotes\n\nReferences \n Heinrich, Joe. MIPS R4000 Microprocessor User's Manual, Second Edition.\n Sunil Mirapuri, Michael Woodacre, Nader Vasseghi, \"The Mips R4000 Processor,\" IEEE Micro, vol. 12. no. 2, pp. 10–22, March/April 1992\n\nAdvanced RISC Computing\nMIPS implementations\nMIPS microprocessors\nSuperscalar microprocessors\n64-bit computers\n64-bit microprocessors"
    ],
    [
        "R5000\nThe R5000 is a 64-bit, bi-endian, superscalar, in-order execution 2-issue design microprocessor, that implements the MIPS IV instruction set architecture (ISA) developed by Quantum Effect Design (QED) in 1996. The project was funded by MIPS Technologies, Inc (MTI), also the licensor. MTI then licensed the design to Integrated Device Technology (IDT), NEC, NKK, and Toshiba. The R5000 succeeded the QED R4600 and R4700 as their flagship high-end embedded microprocessor. IDT marketed its version of the R5000 as the 79RV5000, NEC as VR5000, NKK as the NR5000, and Toshiba as the TX5000. The R5000 was sold to PMC-Sierra when the company acquired QED. Derivatives of the R5000 are still in production today for embedded systems.",
        "Users\nUsers of the R5000 in workstation and server computers were Silicon Graphics, Inc. (SGI) and Siemens-Nixdorf. SGI used the R5000 in their O2 and Indy low-end workstations. The R5000 was also used in embedded systems such as network routers and high-end printers. The R5000 found its way into the arcade gaming industry, R5000 powered mainboards were used by Atari and Midway. Initially the Cobalt Qube and Cobalt RaQ used a derivative model, the RM5230 and RM5231. The Qube 2700 used the RM5230 microprocessor, whereas the Qube 2 used the RM5231. The original RaQ systems were equipped with RM5230 or RM5231 CPUs but later models used AMD K6-2 chips and then eventually Intel Pentium III CPUs for the final models.",
        "History\nThe original roadmap called for 200 MHz operation in early 1996, 250 MHz in late 1996, succeeded in 1997 by R5000A. The R5000 was introduced in January 1996 and failed to achieve 200 MHz, topping out at 180 MHz. When positioned as a low-end workstation microprocessor, the competition included the IBM and Motorola PowerPC 604, the HP PA-7300LC and the Intel Pentium Pro.\n\nDescription",
        "Description\n\nThe R5000 is a two-way superscalar design that executes instructions in-order. The R5000 could simultaneously issue an integer and a floating-point instruction. It had one simple pipeline for integer instructions and another for floating-point to save transistors and die area to reduce cost. The R5000 did not perform dynamic branch prediction for cost reasons. Instead it uses a static approach, utilizing the hints encoded by the compiler in the branch-likely instructions first introduced in the MIPS II architecture to determine how likely a branch is taken.",
        "The R5000 had large L1 caches, a distinct characteristic of QED, whose designers favored simple designs with large caches. The R5000 had two L1 caches, one for instructions and the other for data. Both have a capacity of 32 KB. The caches are two-way set-associative, have a 32-byte line size, and are virtually indexed, physically tagged. Instructions were predecoded as they enter the instruction cache by appending four bits to each instruction. These four bits specify whether can be issued together and which execution unit they are executed by. This assisted superscalar instruction issue by moving some of the dependency and conflict checking out of the critical path.",
        "The integer unit executes most instructions with a one cycle latency and throughput except for multiply and divide. 32-bit multiplies have a five-cycle latency and a four-cycle throughput. 64-bit multiplies have an extra four cycles of latency and half the throughput. Divides have a 36-cycle latency and throughput for 32-bit integers, and for 64-bit integers, they are increased to 68 cycles.",
        "The floating-point unit (FPU) was a fast single-precision (32-bit) design, for reduced cost and to benefit SGI, whose mid-range 3D graphics workstations relied mostly on single-precision math for 3D graphics applications. It was fully pipelined, which made it significantly better than that of the R4700. The R5000 implements the multiply-add instruction of the MIPS IV ISA. Single-precision adds, multiplies and multiply-adds have a four-cycle latency and a one cycle throughput. Single-precision divides have a 21-cycle latency and a 19-cycle throughput, while square roots have a 26-cycle latency and a 38-cycle throughput. Division and square-root was not pipelined. Instructions that operate on double precision numbers have a significantly higher latency and lower throughput except for add,",
        "and a 38-cycle throughput. Division and square-root was not pipelined. Instructions that operate on double precision numbers have a significantly higher latency and lower throughput except for add, which has identical latency and throughput with single-precision add. Multiply and multiply-add have a five-cycle latency and a two-cycle throughput. Divide has a 36-cycle latency and a 34-cycle throughput. Square root has a 68-cycle latency and a 66-cycle throughput.",
        "The R5000 had an integrated L2 cache controller that supported capacities of 512 KB, 1 MB and 2 MB. The L2 cache shares the SysAD bus with the external interface. The cache was built with custom synchronous SRAMs (SSRAMs). The microprocessor uses the SysAD bus that is also used by several other MIPS microprocessors. The bus is multiplexed (address and data share the same set of wires) and can operate at clock frequencies up to 100 MHz. The initial R5000 did not support multiprocessing, but the package reserved eight pins for the future addition of this feature.",
        "QED was a fabless company and did not fabricate their own designs. The R5000 was fabricated by IDT, NEC and NKK. All three companies fabricated the R5000 in a 0.35 μm complementary metal–oxide–semiconductor (CMOS) process, but with different process features. IDT fabricated the R5000 in a process with two levels of polysilicon and three levels of aluminium interconnect. The two levels of polysilicon enabled IDT to use a four-transistor SRAM cell, resulting in a transistor count of 3.6 million and a die that measured 8.7 mm by 9.7 mm (84.39 mm2). NEC and NKK fabricated the R5000 in a process with one level of polysilicon and three levels of aluminium interconnect. Without an extra level of polysilicon, both companies had to use a six-transistor SRAM cell, resulting in a transistor count of",
        "one level of polysilicon and three levels of aluminium interconnect. Without an extra level of polysilicon, both companies had to use a six-transistor SRAM cell, resulting in a transistor count of 5.0 million and a larger die with an area of around 87 mm2. Die sizes in the range of 80 to 90 mm2 were claimed by MTI. 0.8 million of the transistors in both versions were for logic, and the remainder contained in the caches. It was packaged in a 272-ball plastic ball grid array (BGA) or 272-pin plastic pin grid array (PGA). It was not pin-compatible with any previous MIPS microprocessor.",
        "Derivatives\nIn the late 1990s, Quantum Effect Design acquired a license to manufacture and sell MIPS microprocessors from MTI and became a microprocessor vendor, changing its name to Quantum Effect Devices to reflect its new business model. The company's first products were members of the RM52xx family, which initially consisted of two models, the RM5230 and RM5260. These were announced on 24 March 1997. The RM5230 was initially available at 100 and 133 MHz, and the RM5260 at 133 and 150 MHz. On 29 September 1997, new 150 and 175 MHz RM5230s were introduced, as were 175 and 200 MHz RM5260s.",
        "Both the RM5230 and RM5260 are derivatives of the R5000 and differ in the size of their primary caches (16 KB each instead of 32 KB), the width of their system interfaces (the RM5230 has a 32-bit 67 MHz SysAD bus, and the RM5260 a 64-bit 75 MHz SysAD bus), and the addition of multiply-add and three-operand multiply instructions for digital signal processing applications. These microprocessors were fabricated by the Taiwan Semiconductor Manufacturing Company (TSMC) in its 0.35 μm process with three levels of interconnect. They were packaged by Amkor Technology in its Power-Quad 4 packages, the RM5230 in a 128-pin version, and the RM5260 in a 208-pin version.",
        "The RM52xx family was later joined by the RM5270, which was announced at the Embedded Systems Conference on 29 September 1997. Intended for high-end embedded applications, the RM5270 was available at 150 and 200 MHz. Improvements were the addition of an on-chip secondary cache controller that supported up to 2 MB of cache. The SysAD bus is 64 bits wide and can operate at 100 MHz. It was packaged in a 304-pin Super-BGA (SBGA) that was pin-compatible with the RM7000 and was offered as a migration path to the RM7000.",
        "On 20 July 1998, the RM52x1 family was announced. The family consisted of the RM5231, RM5261, and RM5271. These microprocessors were derivatives of the corresponding devices from the RM52x0 family fabricated in a 0.25 μm process with four levels of metal. The RM5231 was initially available at 150, 200, and 250 MHz; whereas the RM5261 and RM5271 were available at 250 and 266 MHz. On 6 July 1999, a 300 MHz RM5271 was introduced, priced at US$140 in quantities of 10,000. The RM52x1 improved upon the previous family with larger 32 KB primary caches and a faster SysAD bus that supported clock rates up to 125 MHz.",
        "After QED was acquired by PMC-Sierra, the RM52xx and RM52x1 families were continued as PMC-Sierra products. PMC-Sierra introduced two RM52x1 derivatives, the RM5231A and RM5261A, on 4 April 2001. These microprocessors were fabricated by TSMC in its 0.18 μm process and differ from the previous devices by featuring higher clock rates and lower power consumption. The RM5231A was available at clock rates of 250 to 350 MHz, and the RM5261A from 250 to 400 MHz.",
        "R5900 used in Sony's PlayStation 2 is a modified version of R5000 CPU dubbed the Emotion Engine with a customized instruction/data cache arrangement and Sony's proprietary 107 vector SIMD Multimedia Extensions(MMI). Its custom FPU is not IEEE 754 compliant unlike FPUs used by R5000. It also has a second MIPS core which acted as a sync controller for specialized vector coprocessors, important for 3D math which at the time was principally computed on the CPU.\n\nReferences",
        "Computergram (8 January 1996). \"MIPS Ready With R5000 Successor to the 4600/4700\". Computer Business Review.\n Gwennap, Linley (22 January 1996). \"R5000 Improves FP for MIPS Midrange\". Microprocessor Report, 10 (1).\n Halfhill, Tom R. (April 1996). \"R5000 Cuts 3-D Cost\". Byte.\n Halfhill, Tom R. (May 1996). \"Mips R5000: Fast, Affordable 3-D\". Byte, 161–162.\n MIPS Technologies, Inc. MIPS R5000 Microprocessor Technical Backgrounder.\n PMC-Sierra, Inc. (4 April 2001). \"PMC-Sierra Ships Third Generation R5200A MIPS Microprocessors\". Press release.\n Quantum Effect Devices (24 March 1997). \"QED Introduces RM52xx Microprocessor Family\". Press release.\n Quantum Effect Devices (29 September 1997). \"QED Introduces RM5270 Superscalar 64-bit Microprocessor\". Press release.",
        "Quantum Effect Devices (29 September 1997). \"QED Introduces RM5270 Superscalar 64-bit Microprocessor\". Press release.\n Quantum Effect Devices (20 July 1998). \"QED Introduces The RM52x1 Microprocessor Family\". Press release.\n Quantum Effect Devices (6 July 1999). \"QED's RM5271 Available Immediately at 300MHz\". Press release.",
        "MIPS implementations\nQuantum Effect Devices microprocessors\nSuperscalar microprocessors\n64-bit microprocessors"
    ],
    [
        "R8000\nThe R8000 is a microprocessor chipset developed by MIPS Technologies, Inc. (MTI), Toshiba, and Weitek. It was the first implementation of the MIPS IV instruction set architecture. The R8000 is also known as the TFP, for Tremendous Floating-Point, its name during development.",
        "History\nDevelopment of the R8000 started in the early 1990s at Silicon Graphics, Inc. (SGI). The R8000 was specifically designed to provide the performance of circa 1990s supercomputers with a microprocessor instead of a central processing unit (CPU) built from many discrete components such as gate arrays. At the time, the performance of traditional supercomputers was not advancing as rapidly as reduced instruction set computer (RISC) microprocessors. It was predicted that RISC microprocessors would eventually match the performance of more expensive and larger supercomputers at a fraction of the cost and size, making computers with this level of performance more accessible and enabling deskside workstations and servers to replace supercomputers in many situations.",
        "First details of the R8000 emerged in April 1992 in an announcement by MIPS Computer Systems detailing future MIPS microprocessors. In March 1992, SGI announced it was acquiring MIPS Computer Systems, which became a subsidiary of SGI called MIPS Technologies, Inc. (MTI) in mid-1992. Development of the R8000 was transferred to MTI, where it continued. The R8000 was expected to be introduced in 1993, but it was delayed until mid-1994. The first R8000, a 75 MHz part, was introduced on 7 June 1994. It was priced at US$2,500 at the time. In mid-1995, a 90 MHz part appeared in systems from SGI. The R8000's high cost and narrow market (technical and scientific computing) restricted its market share, and although it was popular in its intended market, it was largely replaced with the cheaper and",
        "R8000's high cost and narrow market (technical and scientific computing) restricted its market share, and although it was popular in its intended market, it was largely replaced with the cheaper and generally better performing R10000 introduced January 1996.",
        "Users of the R8000 were SGI, who used it in their Power Indigo2 workstation, Power Challenge server, Power ChallengeArray cluster and Power Onyx visualization system. In the November 1994 TOP500 list, 50 systems out of 500 used the R8000. The highest ranked R8000-based systems were four Power Challenges at positions 154 to 157. Each had 18 R8000s.\n\nDescription\nThe chip set consisted of the R8000 microprocessor, the R8010 floating-point unit, two Tag RAMs, and the streaming cache. The R8000 is superscalar, capable of issuing up to four instructions per cycle, and executes instructions in program order. It has a five-stage integer pipeline.\n\nR8000",
        "R8000\n\nThe R8000 controlled the chip set and executed integer instructions. It contained the integer execution units, integer register file, primary caches and hardware for instruction fetch, branch prediction the translation lookaside buffers (TLBs).",
        "In stage one, four instructions are fetched from the instruction cache. The instruction cache is 16 kB large, direct-mapped, virtually tagged and virtually indexed, and has a 32-byte line size. Instruction decoding and register reads occur during stage two, and branch instructions are resolved as well, leading to a one-cycle branch mispredict penalty. Load and store instructions begin execution in stage three, and integer instructions in stage four. Integer execution was delayed until stage four so that integer instructions which use the result of a load as an operand may be issued in the cycle after the load. Results are written to the integer register file in stage five.",
        "The integer register file has nine read ports and four write ports. Four read ports supply operands to the two integer execution units (the branch unit was considered part of an integer unit).  Another four read ports supply operands to the two address generators.  Four ports are needed, rather than two, because of the base(register) + index(register) address style added in the MIPS IV ISA.  The R8000 issues at most one integer store per cycle, and one final read port delivers the integer store data.\n\nTwo register file write ports are used to write results from the two integer functional units.  The R8000 issues two integer loads per cycle, and the other two write ports are used to write the results of integer loads to the register file.",
        "The level 1 data cache was organized as two redundant arrays, each of which had one read port and one write port.  Integer stores were written to both arrays.  Two loads could be processed in parallel, one on each array.\n\nInteger functional units consisted of two integer units, a shift unit, a multiply-divide unit, and two address generator units. Multiply and divide instructions are executed in the multiply-divide unit, which is not pipelined. As a result, the latency for a multiply instruction is four cycles for 32-bit operands and six cycles for 64-bit. The latency for a divide instruction depends on the number of significant digits in the result and thus it varies from 21 to 73 cycles.",
        "Loads and stores",
        "Loads and stores begin execution in stage three. The R8000 has two address generation units (AGUs) that calculate virtual address for loads and stores. In stage four, the virtual addresses are translated to physical addresses by a dual-ported TLB that contains 384 entries and is three-way set associative. The 16 kB data cache is accessed in the same cycle. It is dual-ported, and is accessed via two 64-bit buses. It can service two loads or one load and one store per cycle. The cache is not protected by parity or by error correcting code (ECC). In the event of a cache miss, the data must be loaded from the streaming cache with an eight-cycle penalty. The cache is virtually indexed, physically tagged, direct mapped, has a 32-byte line size and uses a write-through with allocate protocol. If",
        "from the streaming cache with an eight-cycle penalty. The cache is virtually indexed, physically tagged, direct mapped, has a 32-byte line size and uses a write-through with allocate protocol. If the loads hit in the data cache, the result is written to the integer register file in stage five.",
        "R8010\n\nThe R8010 executed floating-point instructions provided by an instruction queue on the R8000. The queue decoupled the floating-point pipeline from the integer pipeline, implementing a limited form of out-of-order execution by allowing floating-point instructions to execute when possible after or before the integer instructions from the same group are issued. The pipelines were decoupled to help mitigate some of the streaming cache latency.",
        "It contained the floating-point register file, a load queue, a store queue, and two identical floating-point units. All instructions except for divide and square-root are pipelined. The R8010 implements an iterative division and square-root algorithm that uses the multiplier for a key part, requiring the pipeline to be stalled the unit for the duration of the operation.\n\nArithmetic instructions except for compares have a four-cycle latency. Single and double precision divides have latencies of 14 and 20 cycles, respectively; and single and double precision square-roots have latencies of 14 and 23 cycles, respectively.",
        "Streaming cache and Tag RAMs\nThe streaming cache is an external 1 to 16 MB cache that serves as the R8000's L2 unified cache and the R8010's L1 data cache. It operates at the same clock rate as the R8000 and is built from commodity synchronous static RAMs. This scheme was used to attain sustained floating point performance, which requires frequent access to data. A small low-latency primary cache would not contain enough data and frequently miss, necessitating long latency refiles that reduce performance.",
        "The streaming cache is two-way interleaved. It has two independent banks, each containing data from even or odd addresses. It can therefore perform two reads, two writes, or a read and a write every cycle, provided that the two accesses are to separate banks. Each bank is accessed via two 64-bit unidirectional buses, one for reads, and the other for writes. This scheme was used to avoid bus turnover, which is required by bidirectional buses. By avoiding bus turnover, the cache can be read from in one cycle and then written to in the next cycle without an intervening cycle for turnover, resulting in improved performance.",
        "The streaming cache's tags are contained on two Tag RAM chips, one for each bank. Both chips contain identical data. Each chip contains 1.189 Mbit of cache tags implemented by four-transistor SRAM cells. The chips are implemented in a 0.7 μm BiCMOS process with two levels of polysilicon and two levels of aluminium interconnect. BiCMOS circuitry was used in the decoders and combined sense amplifier and comparator portions of the chip to reduce cycle time. Each Tag RAM is 14.8 mm by 14.8 mm large, packaged in a 155-pin CPGA, and dissipates 3 W at 75 MHz. In addition to providing the cache tags, the Tag RAMs are responsible for the streaming cache being four-way set associative. To avoid high a pin count, the cache tags are four-way set associative and logic selects which set to access after",
        "the Tag RAMs are responsible for the streaming cache being four-way set associative. To avoid high a pin count, the cache tags are four-way set associative and logic selects which set to access after lookup instead of the usual way of implementing set-associative caches.",
        "Access to the streaming cache is pipelined to mitigate some of the latency. The pipeline has five stages: in stage one, addresses are sent to the Tag RAMs, which are accessed in stage two. Stage three is for the signals from the Tag RAMs to propagate to the SSRAMs. In stage four, the SSRAMs are accessed and data is returned to the R8000 or R8010 in stage five.",
        "Physical\nThe R8000 contained 2.6 million transistors and measured 17.34 mm by 17.30 mm (299.98 mm²). The R8010 contained 830,000 transistors. In total, the two chips contained 3.43 million transistors. Both were fabricated by Toshiba in their VHMOSIII process, a 0.7 μm, triple-layer metal complementary metal–oxide–semiconductor (CMOS) process. Both are packaged in 591-pin ceramic pin grid array (CPGA) packages. Both chips used a 3.3 V power supply, and the R8000 dissipated 13 W at 75 MHz.\n\nNotes",
        "References\n \"Aligning Instructions for Multiple Dispatch\". Sidebar in Pountain, Dick, \"The Last Bastion\", Byte, August 1994.\n Dongarra, Jack J.; Meuer, Hans W. and Strohmaier, Erich (9 November 1994). TOP500 Supercomputer Sites.\n Gwennap, Linley (15 February 1993). \"SGI Provides Overview of TFP CPU\". Microprocessor Report, vol. 7, no. 2.\n Gwennap, Linley (23 August 1993). \"TFP Designed for Tremendous Floating Point\". Microprocessor Report, vol. 7, no. 11.\n Hsu, Peter Yan-Tek (2 June 1994). Design of the R8000 Microprocessor.\n MIPS Technologies, Inc. (August 1994). R8000 Microprocessor Chip Set Product Overview.\n Pountain, Dick (September 1994). \"The Last Bastion\". Byte.",
        "MIPS Technologies, Inc. (August 1994). R8000 Microprocessor Chip Set Product Overview.\n Pountain, Dick (September 1994). \"The Last Bastion\". Byte.\n Shen, John Paul; Lipasti, Mikko H. (2005). Modern Processor Design: Fundamentals of Superscalar Microprocessors. McGraw-Hill Professional. pp. 418–419.\n Unekawa, Yasuo et al. (1993). \"A 110MHz / 1Mbit synchronous Tag RAM\". Symposium on VLSI Circuits. pp. 15–16.",
        "Further reading \n\n Ikumi, N. et al. (February 1994). \"A 300 MIPS, 300 MFLOPS four-issue CMOS superscalar microprocessor\". ISSCC Digest of Technical Papers.\n Unekawa, Y. et al. (April 1994). \"A 110-MHz/1-Mb synchronous TagRAM\". IEEE Journal of Solid-State Circuits 29 (4): pp. 403–410.\n How VLIW almost disappeared\n Peter Hsu Design of the R8000 IEEE Micro 1994\n\nMIPS implementations\nMIPS microprocessors\nSuperscalar microprocessors\n64-bit microprocessors"
    ],
    [
        "REC\nREC or Rec is a shortening of recording, the process of capturing data onto a storage medium.\n\nREC may also refer to:\n\nEducational institutes\n Regional Engineering College, colleges of engineering and technology education in India\n Rajalakshmi Engineering College (), Thandalam, Chennai, India",
        "Educational institutes\n Regional Engineering College, colleges of engineering and technology education in India\n Rajalakshmi Engineering College (), Thandalam, Chennai, India\n\nOrganizations\n Railway Executive Committee, in Britain\n REC Limited, an infrastructure finance company in India\n Reformed Episcopal Church, an Anglican church in the United States and Canada\n Regional Economic Communities, in Africa\n Regional electricity companies, the fourteen companies created when the electricity market in the UK was privatised\n Renewable Energy Corporation, a solar power company with headquarters in Norway\n REC Silicon (no)\n Research Ethics Committee, a type of ethics committee\n Rock Eisteddfod Challenge, an Australian abstinence program\n Rural Electrification Corporation",
        "Television, film, and fiction\n Rec (film series), a Spanish horror film series\n Rec (film), the first film in the series\n Rec (manga), a Japanese manga series; also refers to anime based on it\n\nPlaces\n Reç, a settlement in Albania\n Reč, a town in Montenegro\n Recife/Guararapes–Gilberto Freyre International Airport, of which the IATA code is REC\n Rectory Road railway station, of which the National Rail station code is REC\n\nOther uses\nrec.*, a newsgroup hierarchy\nRecitation, as abbreviated on course schedules\n Renewable Energy Certificate (United States), tradable environmental commodities\n Rec., the debut extended play by South Korean singer Yuju"
    ],
    [
        "RVA\nRVA may refer to:\n Régie des Voies Aériennes de la République Démocratique du Congo\n Richmond, Virginia\n RVA Magazine, an art, music and opinion magazine for Richmond, VA\n Relative Virtual Address, a concept in the COFF format\n Rift Valley Academy, a boarding school outside of Nairobi, Kenya"
    ],
    [
        "Seqlock\nA seqlock (short for sequence lock) is a special locking mechanism used in Linux for supporting fast writes of shared variables between two parallel operating system routines. The semantics stabilized as of version 2.5.59, and they are present in the 2.6.x stable kernel series. The seqlocks were developed by Stephen Hemminger  and originally called frlocks, based on earlier work by Andrea Arcangeli. The first implementation was in the x86-64 time code where it was needed to synchronize with user space where it was not possible to use a real lock.",
        "It is a reader–writer consistent mechanism which avoids the problem of writer starvation. A seqlock consists of storage for saving a sequence number in addition to a lock. The lock is to support synchronization between two writers and the counter is for indicating consistency in readers. In addition to updating the shared data, the writer increments the sequence number, both after acquiring the lock and before releasing the lock. Readers read the sequence number before and after reading the shared data. If the sequence number is odd on either occasion, a writer had taken the lock while the data was being read and it may have changed. If the sequence numbers are different, a writer has changed the data while it was being read. In either case readers simply retry (using a loop) until they",
        "was being read and it may have changed. If the sequence numbers are different, a writer has changed the data while it was being read. In either case readers simply retry (using a loop) until they read the same even sequence number before and after.",
        "The reader never blocks, but it may have to retry if a write is in progress; this speeds up the readers in the case where the data was not modified, since they do not have to acquire the lock as they would with a traditional read–write lock.  Also, writers do not wait for readers, whereas with traditional read–write locks they do, leading to potential resource starvation in a situation where there are a number of readers (because the writer must wait for there to be no readers).  Because of these two factors, seqlocks are more efficient than traditional read–write locks for the situation where there are many readers and few writers. The drawback is that if there is too much write activity or the reader is too slow, they might livelock (and the readers may starve).",
        "The technique will not work for data that contains pointers, because any writer could invalidate a pointer that a reader has already followed. Updating the memory block being pointed-to is fine using seqlocks, but updating the pointer itself is not allowed. In a case where the pointers themselves must be updated or changed, using read-copy-update synchronization is preferred.\n\nThis was first applied to system time counter updating. Each time interrupt updates the time of the day; there may be many readers of the time for operating system internal use and applications, but writes are relatively infrequent and only occur one at a time. The BSD timecounter code for instance appears to use a similar technique.",
        "One subtle issue of using seqlocks for a time counter is that it is impossible to step through it with a debugger. The retry logic will trigger all the time because the debugger is slow enough to make the read race occur always.\n\nSee also \n Synchronization\n Spinlock\n\nReferences \n\n fast reader/writer lock for gettimeofday 2.5.30\n Effective synchronisation on Linux systems\n Driver porting: mutual exclusion with seqlocks\n Simple seqlock implementation\n Improved seqlock algorithm with lock-free readers\n Seqlocks and Memory Models(slides)\n\nConcurrency control\nLinux kernel"
    ],
    [
        "Shared graphics memory\nIn computer architecture, shared graphics memory refers to a design where the graphics chip does not have its own dedicated memory, and instead shares the main system RAM with the CPU and other components.",
        "This design is used with many integrated graphics solutions to reduce the cost and complexity of the motherboard design, as no additional memory chips are required on the board. There is usually some mechanism (via the BIOS or a jumper setting) to select the amount of system memory to use for graphics, which means that the graphics system can be tailored to only use as much RAM as is actually required, leaving the rest free for applications. A side effect of this is that when some RAM is allocated for graphics, it becomes effectively unavailable for anything else, so an example computer with 512 MiB RAM set up with 64 MiB graphics RAM will appear to the operating system and user to only have 448 MiB RAM installed.",
        "The disadvantage of this design is lower performance because system RAM usually runs slower than dedicated graphics RAM, and there is more contention as the memory bus has to be shared with the rest of the system. It may also cause performance issues with the rest of the system if it is not designed with the fact in mind that some RAM will be 'taken away' by graphics.  \n\nA similar approach that gave similar results is the boost up of graphics used in some SGi computers, most notably the O2/O2+. The memory in these machines is simply one fast pool (2.1 GB per second in 1996) shared between system and graphics.  Sharing is performed on demand, including pointer redirection communication between main system and graphics subsystem. This is called Unified Memory Architecture (UMA).",
        "History \nMost early personal computers used a shared memory design with graphics hardware sharing memory with the CPU. Such designs saved money as a single bank of DRAM could be used for both display and program. Examples of this include the Apple II computer, the Commodore 64, the Radio Shack Color Computer, the Atari ST, and the Apple Macintosh.\n\nA notable exception was the IBM PC. Graphics display was facilitated by the use of an expansion card with its own memory plugged into an ISA slot. \n\nThe first IBM PC to use the SMA was the IBM PCjr, released in 1984. Video memory was shared with the first 128KiB of RAM. The exact size of the video memory could be reconfigured by software to meet the needs of the current program.",
        "An early hybrid system was the Commodore Amiga which could run as a shared memory system, but would load executable code preferentially into non-shared \"fast RAM\" if it was available.\n\nSee also \n IBM PCjr\n Video memory\n Shared memory, in general, other than graphics\n\nExternal links \n PC Magazine Definition for SMA\n IBM PCjr information\n\nMemory management\n\nro:Arhitectură cu memorie partajată\nru:SMA"
    ],
    [
        "Software\nSoftware is a set of computer programs and associated documentation and data. This is in contrast to hardware, from which the system is built and which actually performs the work.",
        "At the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed",
        "displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to \"jump\" to a different instruction or is interrupted by the operating system. , most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.",
        "The majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler, an interpreter, or a combination of the two. Software may also be written in a low-level assembly language that has a strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler.\n\nHistory",
        "History\n\nAn algorithm for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine. She created proofs to show how the engine would calculate Bernoulli numbers. Because of the proofs and the algorithm, she is considered the first computer programmer.",
        "The first theory about software, prior to the creation of computers as we know them today, was proposed by Alan Turing in his 1936 essay, On Computable Numbers, with an Application to the Entscheidungsproblem (decision problem). This eventually led to the creation of the academic fields of computer science and software engineering; both fields study software and its creation. Computer science is the theoretical study of computer and software (Turing's essay is an example of computer science), whereas software engineering is the application of engineering principles to development of software.",
        "In 2000, Fred Shapiro, a librarian at the Yale Law School, published a letter revealing that John Wilder Tukey's 1958 paper \"The Teaching of Concrete Mathematics\" contained the earliest known usage of the term \"software\" found in a search of JSTOR's electronic archives, predating the Oxford English Dictionary's citation by two years. This led many to credit Tukey with coining the term, particularly in obituaries published that same year, although Tukey never claimed credit for any such coinage. In 1995, Paul Niquette claimed he had originally coined the term in October 1953, although he could not find any documents supporting his claim. The earliest known publication of the term \"software\" in an engineering context was in August 1953 by Richard R. Carhart, in a Rand Corporation Research",
        "not find any documents supporting his claim. The earliest known publication of the term \"software\" in an engineering context was in August 1953 by Richard R. Carhart, in a Rand Corporation Research Memorandum.",
        "Types\n\nOn virtually all computer platforms, software can be grouped into a few broad categories.",
        "Purpose, or domain of use\nBased on the goal, computer software can be divided into:\n Application software uses the computer system to perform special functions beyond the basic operation of the computer itself. There are many different types of application software because the range of tasks that can be performed with a modern computer is so large—see list of software.\n System software manages hardware behaviour, as to provide basic functionalities that are required by users, or for other software to run properly, if at all. System software is also designed for providing a platform for running application software, and it includes the following:",
        "Operating systems are essential collections of software that manage resources and provide common services for other software that runs \"on top\" of them. Supervisory programs, boot loaders, shells and window systems are core parts of operating systems. In practice, an operating system comes bundled with additional software (including application software) so that a user can potentially do some work with a computer that only has one operating system.\n Device drivers operate or control a particular type of device that is attached to a computer. Each device needs at least one corresponding device driver; because a computer typically has at minimum at least one input device and at least one output device, a computer typically needs more than one device driver.",
        "Utilities are computer programs designed to assist users in the maintenance and care of their computers.\n Malicious software, or malware, is software that is developed to harm or disrupt computers. Malware is closely associated with computer-related crimes, though some malicious programs may have been designed as practical jokes.",
        "Nature or domain of execution\n Desktop applications such as web browsers and Microsoft Office and LibreOffice and WordPerfect, as well as smartphone and tablet applications (called \"apps\").\n JavaScript scripts are pieces of software traditionally embedded in web pages that are run directly inside the web browser when a web page is loaded without the need for a web browser plugin. Software written in other programming languages can also be run within the web browser if the software is either translated into JavaScript, or if a web browser plugin that supports that language is installed; the most common example of the latter is ActionScript scripts, which are supported by the Adobe Flash plugin.\n Server software, including:",
        "Server software, including:\n Web applications, which usually run on the web server and output dynamically generated web pages to web browsers, using e.g. PHP, Java, ASP.NET, or even JavaScript that runs on the server. In modern times these commonly include some JavaScript to be run in the web browser as well, in which case they typically run partly on the server, partly in the web browser.\n Plugins and extensions are software that extends or modifies the functionality of another piece of software, and require that software be used in order to function.",
        "Embedded software resides as firmware within embedded systems, devices dedicated to a single use or a few uses such as cars and televisions (although some embedded devices such as wireless chipsets can themselves be part of an ordinary, non-embedded computer system such as a PC or smartphone). In the embedded system context there is sometimes no clear distinction between the system software and the application software. However, some embedded systems run embedded operating systems, and these systems do retain the distinction between system software and application software (although typically there will only be one, fixed application which is always run).",
        "Microcode is a special, relatively obscure type of embedded software which tells the processor itself how to execute machine code, so it is actually a lower level than machine code. It is typically proprietary to the processor manufacturer, and any necessary correctional microcode software updates are supplied by them to users (which is much cheaper than shipping replacement processor hardware). Thus an ordinary programmer would not expect to ever have to deal with it.",
        "Programming tools\n\nProgramming tools are also software in the form of programs or applications that developers use to create, debug, maintain, or otherwise support software.",
        "Software is written in one or more programming languages; there are many programming languages in existence, and each has at least one implementation, each of which consists of its own set of programming tools. These tools may be relatively self-contained programs such as compilers, debuggers, interpreters, linkers, and text editors, that can be combined to accomplish a task; or they may form an integrated development environment (IDE), which combines much or all of the functionality of such self-contained tools. IDEs may do this by either invoking the relevant individual tools or by re-implementing their functionality in a new way. An IDE can make it easier to do specific tasks, such as searching in files in a particular project. Many programming language implementations provide the",
        "their functionality in a new way. An IDE can make it easier to do specific tasks, such as searching in files in a particular project. Many programming language implementations provide the option of using both individual tools or an IDE.",
        "Topics\n\nArchitecture",
        "People who use modern general purpose computers (as opposed to embedded systems, analog computers and supercomputers) usually see three layers of software performing a variety of tasks: platform, application, and user software.\n Platform software: The platform includes the firmware, device drivers, an operating system, and typically a graphical user interface which, in total, allow a user to interact with the computer and its peripherals (associated equipment). Platform software often comes bundled with the computer. On a PC one will usually have the ability to change the platform software.",
        "Application software: Application software is what most people think of when they think of software. Typical examples include office suites and video games. Application software is often purchased separately from computer hardware. Sometimes applications are bundled with the computer, but that does not change the fact that they run as independent applications. Applications are usually independent programs from the operating system, though they are often tailored for specific platforms. Most users think of compilers, databases, and other \"system software\" as applications.",
        "User-written software: End-user development tailors systems to meet users' specific needs. User software includes spreadsheet templates and word processor templates. Even email filters are a kind of user software. Users create this software themselves and often overlook how important it is. Depending on how competently the user-written software has been integrated into default application packages, many users may not be aware of the distinction between the original packages, and what has been added by co-workers.",
        "Execution\n\nComputer software has to be \"loaded\" into the computer's storage (such as the hard drive or memory). Once the software has loaded, the computer is able to execute the software. This involves passing instructions from the application software, through the system software, to the hardware which ultimately receives the instruction as machine code. Each instruction causes the computer to carry out an operation—moving data, carrying out a computation, or altering the control flow of instructions.",
        "Data movement is typically from one place in memory to another. Sometimes it involves moving data between memory and registers which enable high-speed data access in the CPU. Moving data, especially large amounts of it, can be costly; this is sometimes avoided by using \"pointers\" to data instead. Computations include simple operations such as incrementing the value of a variable data element. More complex computations may involve many operations and data elements together.\n\nQuality and reliability",
        "Quality and reliability\n\nSoftware quality is very important, especially for commercial and system software. If software is faulty, it can delete a person's work, crash the computer and do other unexpected things. Faults and errors are called \"bugs\" which are often discovered during alpha and beta testing. Software is often also a victim to what is known as software aging, the progressive performance degradation resulting from a combination of unseen bugs.",
        "Many bugs are discovered and fixed through software testing. However, software testing rarely—if ever—eliminates every bug; some programmers say that \"every program has at least one more bug\" (Lubarsky's Law). In the waterfall method of software development, separate testing teams are typically employed, but in newer approaches, collectively termed agile software development, developers often do all their own testing, and demonstrate the software to users/clients regularly to obtain feedback. Software can be tested through unit testing, regression testing and other methods, which are done manually, or most commonly, automatically, since the amount of code to be tested can be large. Programs containing command software enable hardware engineering and system operations to function much",
        "manually, or most commonly, automatically, since the amount of code to be tested can be large. Programs containing command software enable hardware engineering and system operations to function much easier together.",
        "License\n\nThe software's license gives the user the right to use the software in the licensed environment, and in the case of free software licenses, also grants other rights such as the right to make copies.\n\nProprietary software can be divided into two types:\n freeware, which includes the category of \"free trial\" software or \"freemium\" software (in the past, the term shareware was often used for free trial/freemium software). As the name suggests, freeware can be used for free, although in the case of free trials or freemium software, this is sometimes only true for a limited period of time or with limited functionality.\n software available for a fee, which can only be legally used on purchase of a license.",
        "Open-source software comes with a free software license, granting the recipient the rights to modify and redistribute the software.\n\nPatents",
        "Software patents, like other types of patents, are theoretically supposed to give an inventor an exclusive, time-limited license for a detailed idea (e.g. an algorithm) on how to implement a piece of software, or a component of a piece of software. Ideas for useful things that software could do, and user requirements, are not supposed to be patentable, and concrete implementations (i.e. the actual software packages implementing the patent) are not supposed to be patentable either—the latter are already covered by copyright, generally automatically. So software patents are supposed to cover the middle area, between requirements and concrete implementation. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements",
        "middle area, between requirements and concrete implementation. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid—although since all useful software has effects on the physical world, this requirement may be open to debate. Meanwhile, American copyright law was applied to various aspects of the writing of the software code.",
        "Software patents are controversial in the software industry with many people holding different views about them. One of the sources of controversy is that the aforementioned split between initial ideas and patent does not seem to be honored in practice by patent lawyers—for example the patent for aspect-oriented programming (AOP), which purported to claim rights over any programming tool implementing the idea of AOP, howsoever implemented. Another source of controversy is the effect on innovation, with many distinguished experts and companies arguing that software is such a fast-moving field that software patents merely create vast additional litigation costs and risks, and actually retard innovation. In the case of debates about software patents outside the United States, the argument",
        "that software patents merely create vast additional litigation costs and risks, and actually retard innovation. In the case of debates about software patents outside the United States, the argument has been made that large American corporations and patent lawyers are likely to be the primary beneficiaries of allowing or continue to allow software patents.",
        "Design and implementation\n\nDesign and implementation of software vary depending on the complexity of the software. For instance, the design and creation of Microsoft Word took much more time than designing and developing Microsoft Notepad because the former has much more basic functionality.",
        "Software is usually developed in integrated development environments (IDE) like Eclipse, IntelliJ and Microsoft Visual Studio that can simplify the process and compile the software. As noted in a different section, software is usually created on top of existing software and the application programming interface (API) that the underlying software provides like GTK+, JavaBeans or Swing. Libraries (APIs) can be categorized by their purpose. For instance, the Spring Framework is used for implementing enterprise applications, the Windows Forms library is used for designing graphical user interface (GUI) applications like Microsoft Word, and Windows Communication Foundation is used for designing web services. When a program is designed, it relies upon the API. For instance, a Microsoft Windows",
        "(GUI) applications like Microsoft Word, and Windows Communication Foundation is used for designing web services. When a program is designed, it relies upon the API. For instance, a Microsoft Windows desktop application might call API functions in the .NET Windows Forms library like Form1.Close() and Form1.Show() to close or open the application. Without these APIs, the programmer needs to write these functionalities entirely themselves. Companies like Oracle and Microsoft provide their own APIs so that many applications are written using their software libraries that usually have numerous APIs in them.",
        "Data structures such as hash tables, arrays, and binary trees, and algorithms such as quicksort, can be useful for creating software.\n\nComputer software has special economic characteristics that make its design, creation, and distribution different from most other economic goods.\n\nA person who creates software is called a programmer, software engineer or software developer, terms that all have a similar meaning. More informal terms for programmer also exist such as \"coder\" and \"hacker\"although use of the latter word may cause confusion, because it is more often used to mean someone who illegally breaks into computer systems.\n\nSee also\n Computer program\n Independent software vendor\n Open-source software\n Outline of software\n Software asset management\n Software release life cycle",
        "See also\n Computer program\n Independent software vendor\n Open-source software\n Outline of software\n Software asset management\n Software release life cycle\n\nReferences\n\nSources\n\nExternal links\n Software at Encyclopædia Britannica"
    ],
    [
        "Stuart Langridge\nStuart Langridge (also known as 'Aq' or 'Zippy' ) is a podcaster, developer and author. He became a member of the Web Standards Project's DOM Scripting Task Force, an invited expert on the W3C HTML Working Group and is an acknowledged commentator on W3C Document Object Model and JavaScript techniques.\n\nPodcasts \nLangridge is known as a presenter of the now defunct LugRadio, which was a free software podcast in the UK. Along with Jono Bacon, he was the longest-serving member of the team and often served to incite discussion about issues that more directly related to software freedom. In LugRadio he frequently advocated freedom, yet despite this often attracted criticism for using proprietary software.",
        "Langridge was involved in the Shot of Jaq podcast, in collaboration with his former Lugradio co-host Jono Bacon. He's now a part of the Bad Voltage podcast, together with Jono Bacon and Jeremy Garcia (founder of LinuxQuestions.org). Bryan Lunduke (founder of Jupiter Broadcasting) was also a founder member of the Bad Voltage podcast but has since moved on due to other commitments. The podcast first aired in October 2013.\n\nProgramming \nHe has worked on projects including Jokosher, a multi-track audio editor for GNOME, and Jackfield, a program to run Mac OS X Dashboard widgets under GNOME.",
        "Programming \nHe has worked on projects including Jokosher, a multi-track audio editor for GNOME, and Jackfield, a program to run Mac OS X Dashboard widgets under GNOME.\n\nCareer \nIn January 2009 Langridge joined Canonical as a developer and left the company in 2013 to work as a freelancer for Kryogenix's consulting. At Canonical he worked on the Desktop Couch for Ubuntu in his role as Canonical Ltd. staffer.\n\nAuthor \nLangridge has written two books for technical publisher SitePoint, DHTML Utopia, and Run Your Own Web Server Using Linux & Apache (with Tony Steidler-Dennison) as well as writing the Stylish Scripting weblog during 2005.\n\nReferences",
        "References\n\nExternal links \nStuart Langridge's website and blog\nLugRadio\nJokosher\nJackfield\nThe Bad Voltage community forum with link to the current and all past podcast episodes\n\nLiving people\nOpen source people\nCopyright activists\nFree software programmers\n1976 births\nUbuntu (operating system) people"
    ],
    [
        "T-V-H\nT-V-H (Thailand-Vietnam-Hong Kong) is a submarine telecommunications cable system in the South China Sea linking Thailand, Vietnam and Hong Kong.\n\nIt has landing points in:\nSi Racha, Chonburi Province, Thailand\nVũng Tàu, Bà Rịa–Vũng Tàu province, Vietnam\nDeep Water Bay, Southern District, Hong Kong\n\nIt has a transmission capacity of 565 Mbit/s and a total cable length of . It started operation on 8 February 1996.\n\nService disruptions\nOn 25 March 2007, the cable network was broken due to local pirates who cut the cable to sell for scrap metals. It was estimated that the link would take three months to repair.\n\nReferences\n\nExternal links",
        "References\n\nExternal links\n \n\nSubmarine communications cables in the Pacific Ocean\nThailand–Vietnam relations\nChina–Thailand relations\nChina–Vietnam relations\n1996 establishments in Hong Kong\n1996 establishments in Thailand\n1996 establishments in Vietnam"
    ],
    [
        "Telecommunications in Antigua and Barbuda\nTelecommunications in Antigua and Barbuda are via media in the telecommunications industry.\n\nTelephone\nTelephones – main lines in use: 37,500 (2006)\ncountry comparison to the world: 168\n\nTelephones – mobile cellular: 110,200 (2006) (APUA PCS, Cable & Wireless, Digicel)\ncountry comparison to the world: 177\n\nTelephone system:\ndomestic: good automatic telephone system\ninternational: 3 fiber optic submarine cables (2 to Saint Kitts and 1 to Guadeloupe); satellite earth station – 1 Intelsat (Atlantic Ocean)\n\nRadio\nRadio broadcast stations: AM 4, FM 6, shortwave 0 (2002)\n\nRadios: 36,000 (1997)\n\nTelevision\nTelevision broadcast stations: 2 (1997) (including ABS-TV)\n\nTelevisions: 31,000 (1997)",
        "Radio\nRadio broadcast stations: AM 4, FM 6, shortwave 0 (2002)\n\nRadios: 36,000 (1997)\n\nTelevision\nTelevision broadcast stations: 2 (1997) (including ABS-TV)\n\nTelevisions: 31,000 (1997)\n\nInternet\nInternet Service Providers (ISPs): Cable & Wireless, Antigua Computer Technologies (ACT), Antigua Public Utilities Authority (APUA INET)\n\nInternet hosts: 2,215 (2008)\ncountry comparison to the world: 140\n\nInternet users: 60,000 (2007)\ncountry comparison to the world: 158\n\nCountry codes: AG\n\nDemographics\n\nSee also\nAntigua and Barbuda\nHistory of telecommunication\nList of telecommunications terminology\nOutline of telecommunication\n\nReferences\n\nExternal links\n Antigua and Barbuda, SubmarineCableMap.com\n\n \nAntigua and Barbuda\nAntigua"
    ],
    [
        "Telecommunications in Cambodia\nTelecommunications in Cambodia include telephone, radio, television, and Internet services, which are regulated by the Ministry of Posts and Telecommunications. Transport and posts were restored throughout most of the country in the early 1980s during the People's Republic of Kampuchea regime after being disrupted under Democratic Kampuchea (Khmer Rouge).",
        "In January 1987, the Soviet-aided Intersputnik space communications station began operation in Phnom Penh and established two-way telecommunication links between the Cambodian capital and the cities of Moscow, Hanoi, Vientiane and Paris. The completion of the earth satellite station restored the telephone and telex links among Phnom Penh, Hanoi, and other countries for the first time since 1975. Although telecommunications services were initially limited to the government, these advances in communications helped break down the country's isolation, both internally and internationally.\n\nToday, with the availability of mobile phones, communications are open to all, though the country's Prime Minister Hun Sen decreed that 3G mobile phones would not be allowed to support video calling.",
        "Telephones\n\nAs of Q1 2020, Cambodia's mobile connection is at 21.4 million. Smart Axiata, a leading telecommunications company, in 2019 conducted a live trial of its 5G network with support from China's Huawei. The company said it expects to begin rolling out 5G services in Cambodia by the end of 2019.\n\nGSMA predicted that by 2025, Cambodia will have approximately 24.3 million total mobile connections with smartphone connections up to 69%. The market is predicted to adopt 1.6 million of 5G connections within 5 years from 2020. Though so, it's believed that 4G still have room for growth and will continue to be the majority network connection.",
        "The government state communications corporation is Telecom Cambodia, founded in 2006 as an expansion of the telecom operating department of the Ministry of Posts and Telecommunications.\n\nMobile networks\n\nRadio and television\n\nAs of 2019, Cambodian broadcasters were a mixture of state-owned, joint public-private, and privately owned companies.\n\nRadio stations\nAs of 2019, there were roughly 84 radio broadcast stations: 1 state-owned broadcaster with multiple stations and a large mixture of public and private broadcasters. Several international broadcasters are also available.\n\nPhnom Penh",
        "Phnom Penh\n\n Apsara Radio FM 97\n Angel Radio 96.3Mhz Kampot\n BBC World Service 100.0 MHz\n Dance Radio 96.6Mhz \n DAP Radio FM 93.75\n Family FM 99.5\n Hang Meas Radio FM 104.5\n Koh Santepheap Daily FM 87.75\n National Radio Kampuchea\n Phnom Penh Radio FM 103\n Radio FM 90.5\n Radio Beehive FM 105\n DaunPenh eFM 87.50Mhz\n ABC News FM 107.5\n Lotus Radio FM 100.5hz\n Radio Free Asia\n Radio Khmer FM 107\n Radio Love FM 97.5\n Radio Town FM 102.3 MHz\n Raksmey Hang Meas Radio FM 95.7000\n Royal Cambodia Armed Forces Radio FM 98\n Voice of America Khmer\n Women's Media Centre of Cambodia (WMC) Radio FM 102",
        "Provincial stations\nThere are radio stations in each of the following provinces: Banteay Meanchey, Battambang, Kampong Cham, Kampong Thom, Kampot, Kandal, Pailin, Preah Vihear, Siem Reap, Sihanoukville and Svay Rieng.\n\nTelevision\nCambodia has 27 TV broadcast stations with most operating on multiple channels, including 1 state-operated station broadcasting from multiple locations, 11 stations either jointly operated or privately owned with some broadcasting from several locations. Multi-channel cable and satellite systems are also available. There is one Chinese joint venture television station with the Ministry of Interior. Several television and radio operators broadcast online only (often via Facebook).\n\nBroadcast and cable networks",
        "Broadcast and cable networks\n\n PNN TV\n Apsara Television (TV11)\n Bayon Television\n Bayon News Television\n Cambodia Cable Television (CCTV)\n Cambodian News Channel (CNC)\n Cambodian Television Network (CTN)\n CTV 8 HD\n Hang Meas HDTV\n Khmer Television 9 HDTV (TV9 HDTV)\n My TV\n National Television of Cambodia (TVK)\n One TV (Royal Media Entertainment Corporation, LTD)\n Phnom Penh Municipal Cable Television (PPCTV Co., LTD)\n Phnom Penh Television (TV3)\n TV5 Cambodia\n One News",
        "Provincial television stations\n Kandal Province - Broadcasting on channel 27, Bayon Television is Cambodia's only UHF channel. A private television company belonging to Prime Minister Hun Sen, it also operates Bayon Radio FM 95 MHz. It was established in January 1998.\n Mondulkiri - Established in 1999, relays TVK on channel 10.\n Preah Vihear - Established in 2006, broadcasts on channel 7.\n Ratanakiri - Established in 1993, relays TVK on channel 7.\n Siem Reap - Established in 2002, relays TV3 on channel 12.\n\nMost viewed channels",
        "Most viewed channels\n\nInternet\n the number of internet users in Cambodia rose to 15.8 million, about 98.5% of the population. According to the Telecommunications Regulator of Cambodia (TRC), the number of registered SIM cards rose by 9.4 percent during the first half of the year, reaching 20.8 million. The SIM card market is saturated, with Cambodia now having more active SIM cards than people. According to TRC, there are six telecommunications firms in the country: Cellcard, Smart Axiata, Metfone, Seatel, Cootel, and qb. Three companies, Metfone, Cellcard, and Smart, account for 90% of users. TRC noted that, as of February 2019, Facebook had seven million users in Cambodia.\n\nList of Internet service providers",
        "AngkorNet\n AZ (Online)\n Cambo Technology (ISP) Co., Ltd.\n Cambodia Internet Corp\n Cambotech\n Camintel\n Camnet (Telecom Cambodia)\n CB (Cambodian Broadband)\n CDC\n Cellcard (Mobitel)\n CooTel\n Chuan Wei \n CIDC IT\n Citylink\n Digi ISP\n Dragon Royal Telecom\n EmCom\n Everyday\n Ezecom\n GTD\n Hiway Telecom\n Home Internet\n Kingtel Communications Limited\n MaxBIT\n MekongNet (Angkor Data Communication Group)\n Metfone\n Mobilastic\n Neocom ISP (NTC)\n NTC - NeocomISP Limited\n Open Net\n PCP\n PP Net Phone\n PPCTV Broadband Internet Service\n SingMeng Telemedia\n yes SEATEL Cambodia\n SINET (S.I Group Co., Ltd)\n Smart @Home\n TeleSURF\n Telecom Cambodia\n TODAY ISP (Today Communication Co., Ltd)\n Turbotech\n Vimean Seile\n Wicam\n WIP\n Wireless Internet Provider\n WirelessIP\n Y5Net (BDKTel Co,LTD)\n Mekong CLOUD Cambodia",
        "Internet censorship and surveillance\n\nIn its Freedom on the Net 2013 report, Freedom House gives Cambodia a \"Freedom on the Net Status\" of \"partly free\".",
        "Compared to traditional media in Cambodia, new media, including online news, social networks and personal blogs, enjoy more freedom and independence from government censorship and restrictions. However, the government does proactively block blogs and websites, either on moral grounds, or for hosting content deemed critical of the government. The government restricts access to sexually explicit content, but does not systematically censor online political discourse. Since 2011 three blogs hosted overseas have been blocked for perceived antigovernment content. In 2012, government ministries threatened to shutter internet cafes too near schools—citing moral concerns—and instituted surveillance of cafe premises and cell phone subscribers as a security measure.",
        "Early in 2011, very likely at the urging of the Ministry of Posts and Telecommunications, all Cambodian ISPs blocked the hosting service Blogspot, apparently in reaction to a December 2010 post on KI-Media, a blog run by Cambodians from both inside and outside the country. The site, which is often critical of the administration, described the prime minister and other officials as \"traitors\" after opposition leader Sam Rainsy alleged they had sold land to Vietnam at a contested national border. All ISPs but one subsequently restored service to the sites following customer complaints. In February 2011, however, multiple ISPs reinstated blocks on individual Blogspot sites, including KI-Media, Khmerization—another critical citizen journalist blog—and a blog by the Khmer political cartoonist",
        "2011, however, multiple ISPs reinstated blocks on individual Blogspot sites, including KI-Media, Khmerization—another critical citizen journalist blog—and a blog by the Khmer political cartoonist Sacrava.",
        "There are no government restrictions on access to the Internet or credible reports that the government monitors e-mail or Internet chat rooms without appropriate legal authority. During 2012 NGOs expressed concern about potential online restrictions. In February and November, the government published two circulars, which, if implemented fully, would require Internet cafes to install surveillance cameras and restrict operations within major urban centers. Activists also reported concern about a draft “cybercrimes” law, noting that it could be used to restrict online freedoms. The government maintained it would only regulate criminal activity.",
        "The constitution provides for freedom of speech and press; however, these rights were not always respected in practice. The 1995 press law prohibits prepublication censorship or imprisonment for expressing opinions; however, the government uses the penal code to prosecute citizens on defamation, disinformation, and incitement charges. The penal code does not prescribe imprisonment for defamation, but does for incitement or spreading disinformation, which carry prison sentences of up to three years. Judges also can order fines, which may lead to jail time if not paid. The constitution requires that free speech not adversely affect public security.",
        "The constitution declares that the king is “inviolable,” and a Ministry of Interior directive conforming to the defamation law reiterates these limits and prohibits publishers and editors from disseminating stories that insult or defame government leaders and institutions. The continued criminalization of defamation and disinformation and a broad interpretation of criminal incitement constrains freedom of expression.\n\nThe law provides for the privacy of residence and correspondence and prohibits illegal searches; however, NGOs report that police routinely conduct searches and seizures without warrants.",
        "The law provides for the privacy of residence and correspondence and prohibits illegal searches; however, NGOs report that police routinely conduct searches and seizures without warrants.\n\nCorruption remains pervasive and governmental human rights bodies are generally ineffective. A weak judiciary that sometimes fails to provide due process or fair trial procedures is a serious problem. The courts lack human and financial resources and, as a result, are not truly independent and are subject to corruption and political influence.\n\nOn 17 February 2021, the Cambodian government announced its plans to launch a censorship scheme called \"National Internet Gateway\" which heavily resembles China's Great Firewall, and it will get launched in February 2022.\n\nSee also",
        "See also\n\n Media of Cambodia\n Ministry of Posts and Telecommunications (Cambodia)\n\nReferences\n\nExternal links\n Networks: Cambodia \n Ministry of Posts and Telecommunications \n Telecom Cambodia\n Television stations in Cambodia\n \"3G phones banned in anti-porn drive\", China Daily (Associated Press), 26 May 2006. Retrieved 24 October 2013."
    ],
    [
        "Telecommunications in Cameroon\nTelecommunications in Cameroon include radio, television, fixed and mobile telephones, and the Internet.\n\nHistory \nDuring German rule, It was set up in the protectorate of Kamerun the first telegraph line, the first telephone line, and the first wireless telegraph. However, the country remained undeveloped in telecommunications. During First World War, Germans followed a scorched-earth policy that meant the destruction of communication lines, included telephone and telegraph.",
        "In British Cameroon, from 1916 to 1950s, communications in the country relied on flag post runners that had been described as \"human telephone lines\". Paths followed by the runners served as a base of the development of telegraph lines in the territory. For instance, the line from Buea-Kumba to Ossidinge used the same paths that the mail runners. In the mid-1930s, the wiring of British Cameroon received more support.\n\nRadio and television",
        "Radio and television\n\n Radio stations:\n state-owned Cameroon Radio Television (CRTV); one private radio broadcaster; about 70 privately owned, unlicensed radio stations operating, but subject to closure at any time; foreign news services are required to partner with a state-owned national station (2007);\n 2 AM, 9 FM, and 3 shortwave stations (2001).\n Television stations:\n state-owned Cameroon Radio Television (CRTV), 2 private TV broadcasters (2007);\n one station (2001).\n\nBBC World Service radio is available via local relays (98.4 FM in Yaounde, the capital).",
        "BBC World Service radio is available via local relays (98.4 FM in Yaounde, the capital).\n\nThe government maintains tight control over broadcast media. State-owned Cameroon Radio Television (CRTV), operates both a TV and a radio network. It was the only officially recognized and fully licensed broadcaster until August 2007 when the government issued licenses to two private TV and one private radio broadcasters.",
        "Approximately 375 privately owned radio stations were operating in 2012, three-fourths of them in Yaounde and Douala. The government requires nonprofit rural radio stations to submit applications to broadcast, but they were exempt from licensing fees. Commercial radio and television broadcasters must submit a licensing application and pay an application fee and thereafter pay a high annual licensing fee. Several rural community radio stations function with foreign funding. The government prohibits these stations from discussing politics.\n\nIn spite of the government's tight control, Reporters Without Borders reported in its 2011 field survey that \"[i]t is clear from the diversity of the media and the outspoken reporting style that press freedom is a reality\".\n\nTelephones",
        "Calling code: +237\n International call prefix: 00\n Main lines:\n 737,400 lines in use, 88th in the world (2012);\n 130,700 lines in use (2006).\n Mobile cellular:\n 13.1 million lines, 64th in the world (2012);\n   4.5 million lines (2007).\n Telephone system: system includes cable, microwave radio relay, and tropospheric scatter; Camtel, the monopoly provider of fixed-line service, provides connections for only about 3 per 100 persons; equipment is old and outdated, and connections with many parts of the country are unreliable; mobile-cellular usage, in part a reflection of the poor condition and general inadequacy of the fixed-line network, has increased sharply, reaching a subscribership base of 50 per 100 persons (2011).",
        "Communications cables: South Atlantic 3/West Africa Submarine Cable (SAT-3/WASC) fiber-optic cable system provides connectivity to Europe and Asia (2011); Africa Coast to Europe (ACE), cable system connecting countries along the west coast of Africa to each other and to Portugal and France, is planned.\n Satellite earth stations: 2 Intelsat (Atlantic Ocean) (2011).",
        "Internet\n\n Top-level domain: .cm\n Internet users:\n 1.1 million users, 113th in the world; 5.7% of the population, 184th in the world (2012).\n 985,565 users (2011);\n 749,600 users, 106th in the world (2009).\n Fixed broadband: 1,006 subscriptions, 180th in the world; less than 0.05% of the population, 190th in the world (2012).\n Wireless broadband: Unknown (2012).\n Internet hosts:\n 10,207 hosts, 134th in the world (2012);\n        69 hosts (2008).\n IPv4: 137,728 addresses allocated, less than 0.05% of the world total, 6.8 addresses per 1000 people (2012).\n Internet service providers (ISPs): \n Creolink Communications",
        "A number of projects are underway that will improve Internet access, telecommunications, and Information and communications technology (ICT) in general:\n Implementation of the e-post project, connecting 234 post offices throughout the country;\n Extension of the national optical fiber network, installation of the initial 3,200 km of fiber is complete and studies for the installation of an additional 3,400 km are underway;\n Construction of multipurpose community telecentres, some 115 telecentres are operating with an additional 205 under construction;\n Construction of metropolitan optical loops, the urban optical loop of Douala is complete and construction of the Yaounde loop is underway;\n Construction of submarine cable landing points;\n Establishment of public key infrastructure (PKI);",
        "Construction of submarine cable landing points;\n Establishment of public key infrastructure (PKI);\n Construction of a regional technology park to support the development of ICTs.",
        "Internet censorship and surveillance\n\nThere are no government restrictions on access to the Internet or reports that the government monitors e-mail or Internet chat rooms.",
        "Although the law provides for freedom of speech and press, it also criminalizes media offenses, and the government restricts freedoms of speech and press. Government officials threaten, harass, arrest, and deny equal treatment to individuals or organizations that criticize government policies or express views at odds with government policy. Individuals who criticize the government publicly or privately sometimes face reprisals. Press freedom is constrained by strict libel laws that suppress criticism. These laws authorize the government, at its discretion and the request of the plaintiff, to criminalize a civil libel suit or to initiate a criminal libel suit in cases of alleged libel against the president and other high government officials. Such crimes are punishable by prison terms and",
        "criminalize a civil libel suit or to initiate a criminal libel suit in cases of alleged libel against the president and other high government officials. Such crimes are punishable by prison terms and heavy fines.",
        "Although the constitution and law prohibit arbitrary interference with privacy, family, home, or correspondence, these rights are subject to restriction for the \"higher interests of the state\", and there are credible reports that police and gendarmes harass citizens, conduct searches without warrants, and open or seize mail with impunity.\n\nSee also\n\n Cameroon Radio Television, government-controlled national broadcaster.\n Commonwealth Telecommunications Organisation\n List of terrestrial fibre optic cable projects in Africa\n Media of Cameroon\n Cameroon\n\nReferences\n\nExternal links\n Antic.cm, top-level domain registry for Cameroon (.cm).\n Ministry of Posts and Telecommunications, Cameroon (MINPOSTEL) . English translation."
    ],
    [
        "Telecommunications in Canada\nPresent-day telecommunications in Canada include telephone, radio, television, and internet usage. In the past, telecommunications included telegraphy available through Canadian Pacific and Canadian National.\n\nHistory \n\nThe history of telegraphy in Canada dates back to the Province of Canada.  While the first telegraph company was the Toronto, Hamilton and Niagara Electro-Magnetic Telegraph Company, founded in 1846, it was the Montreal Telegraph Company, controlled by Hugh Allan and founded a year later, that dominated in Canada during the technology's early years.",
        "Following the 1852 Telegraph Act, Canada's first permanent transatlantic telegraph link was a submarine cable built in 1866 between Ireland and Newfoundland. Telegrams were sent through networks built by Canadian Pacific and Canadian National.",
        "In 1868 Montreal Telegraph began facing competition from the newly established Dominion Telegraph Company. 1880 saw the Great North Western Telegraph Company established to connect Ontario and Manitoba but within a year it was taken over by Western Union, leading briefly to that company's control of almost all telegraphy in Canada. In 1882, Canadian Pacific transmitted its first commercial telegram over telegraph lines they had erected alongside its tracks, breaking Western Union's monopoly. Great North Western Telegraph, facing bankruptcy, was taken over in 1915 by Canadian Northern.\n\nBy the end of World War II, Canadians communicated by telephone, more than any other country. In 1967 the CP and CN networks were merged to form CNCP Telecommunications.",
        "By the end of World War II, Canadians communicated by telephone, more than any other country. In 1967 the CP and CN networks were merged to form CNCP Telecommunications.\n\nAs of 1951, approximately 7000 messages were sent daily from the United States to Canada.  An agreement with Western Union required that U.S. company to route messages in a specified ratio of 3:1, with three telegraphic messages transmitted to Canadian National for every message transmitted to Canadian Pacific. The agreement was complicated by the fact that some Canadian destinations were served by only one of the two networks.\n\nFixed-line telephony \n\nTelephones - fixed lines: total subscriptions: 13.926 million (2020)\n Subscriptions per 100 inhabitants: 36.9 (2020 est.)",
        "Fixed-line telephony \n\nTelephones - fixed lines: total subscriptions: 13.926 million (2020)\n Subscriptions per 100 inhabitants: 36.9 (2020 est.)\n\nTelephones - mobile cellular:  36,093,021 (2020)\n Subscriptions per 100 inhabitants: 95.63 (2020 est.)\n\nTelephone system: (2019)\n Domestic: Nearly 37 per 100 fixed-line and 96 per 100 mobile-cellular teledensity; domestic satellite system with about 300 earth stations (2020)\n International: country code - +1; submarine cables provide links within the Americas and Europe; satellite earth stations - 7 (5 Intelsat - 4 trans-Atlantic Ocean and 1 trans-Pacific Ocean, and 2 Intersputnik - (Atlantic Ocean region)\n\nCall signs",
        "Call signs \n\nITU prefixes: Letter combinations available for use in Canada as the first two letters of a television or radio station's call sign are CF, CG, CH, CI, CJ, CK, CY, CZ, VA, VB, VC, VD, VE, VF, VG, VO, VX, VY, XJ, XK, XL, XM, XN and XO. Only CF, CH, CI, CJ and CK are currently in common use, although four radio stations in St. John's, Newfoundland and Labrador retained call letters beginning with VO when Newfoundland joined Canadian Confederation in 1949. Stations owned by the Canadian Broadcasting Corporation use CB through a special agreement with the government of Chile. Some codes beginning with VE and VF are also in use to identify radio repeater transmitters.\n\nRadio",
        "Radio \n\nAs of 2016, there were over 1,100 radio stations and audio services broadcasting in Canada. Of these, 711 are private commercial radio stations. These commercial stations account for over three quarters of radio stations in Canada. The remainder of the radio stations are a mix of public broadcasters, such as CBC Radio, as well as campus, community, and Aboriginal stations.\n\nTelevision \n\nAs of 2018, 762 TV services were broadcasting in Canada. This includes both conventional television stations and discretionary services.\n\nCable and satellite television services are available throughout Canada. The largest cable providers are Bell Canada, Rogers Cable, Shaw Cable, Vidéotron, Telus and Cogeco, while the two licensed satellite providers are Bell Satellite TV and Shaw Direct.",
        "Internet \n\nBell, Rogers, Telus, and Shaw are among the bigger ISPs in Canada. Depending on your location, Bell and Rogers would be the big internet service providers in Eastern provinces, while Shaw and Telus are the main players competing in western provinces.",
        "Internet service providers: there are more than 44 ISPs in Canada, including Beanfield, Bell Canada, Cable Axion, Cablevision (Canada), Chebucto Community Net, Cogeco, Colbanet, Craig Wireless, Dery Telecom, Eastlink (company), Electronic Box, Everus Communications, Guest-tek, Information Gateway Services, Internet Access Solutions, Internex Online, Inukshuk Wireless, Jet2.net, Look Communications, Managed Network Systems, Inc., Mountain Cablevision, National Capital FreeNet, Novus Entertainment, Ontera, Persona Communications, Primus Canada, Project Chapleau, Qiniq, Rally Internet, Rogers Hi-Speed Internet, Rose Media, Rush Communications Ltd., SaskTel, Seaside Communications, Shaw Communications, Source Cable, SSI Micro, TAO (collective), TekSavvy, Telus, Telus Internet, Vidéotron,",
        "Hi-Speed Internet, Rose Media, Rush Communications Ltd., SaskTel, Seaside Communications, Shaw Communications, Source Cable, SSI Micro, TAO (collective), TekSavvy, Telus, Telus Internet, Vidéotron, Vmedia, Web Community Resource Networks, Wireless Nomad, YourLink",
        "Internet Exchange Points: There are multiple Internet Exchange Points in Canada, the largest of which are in Calgary, Montreal, Toronto and Vancouver.  Most ISP's peer at one or more of these Exchanges, except for Bell Canada.  The Toronto Internet Exchange ranks as one of the largest internet exchanges in the world.\nCountry codes: .CA, CDN, 124\nInternet users: 33 million users \nInternet hosts: 8.7 million (2012-2017)\nPercentage of households with Internet access: 87(2016)\nTotal households with high speed connection: 67%  (2014)\nTotal users of home online banking: 68% (2016)",
        "Mobile networks \n\nThe three major mobile network operators are Rogers Wireless (10.4 million subscribers), Bell Mobility (9.8 million) and Telus Mobility (9.5 million), which have a combined 86% of market share.",
        "Administration and Government \nFederally, telecommunications are overseen by the Canadian Radio-television and Telecommunications Commission ()–CRTC as outlined under the provisions of both the Telecommunications Act and Radiocommunication Acts.  CRTC further works with Innovation, Science and Economic Development Canada (formerly Industry Canada) on various technical aspects including: allocating frequencies and call signs, managing the broadcast spectrum, and regulating other technical issues such as interference with electronics equipment. As Canada comprises a part of the North American Numbering Plan for area codes, the Canadian Numbering Administration Consortium within Canada is responsible for allocating and managing area codes in Canada.\n\nSee also",
        "See also \n\n Media in Canada\n List of newspapers in Canada\n List of mobile network operators of the Americas\n List of telephone operating companies\n List of area codes in Canada\n List of postcode areas in Canada\n Canadian Association of Broadcasters\n Canadian Broadcast Standards Council\n Canadian Broadcasting Corporation\n Canadian Communications Foundation\n\nReferences\n\nFurther reading \n\nBibliography\n\n Canada’s internet infrastructure Internet Exchange Points (IXPs), Canadian Internet Registration Authority (CIRA)\n Canadian Area Codes, Canadian Numbering Administration Consortium, Inc. (CNAC)\n Canada, SubmarineCableMap.com\n National Broadband Internet Service Availability Map, Innovation, Science and Economic Development Canada",
        "External links \nThe Canadian Communications Foundation – A History of Canadian Broadcasting\nFind Service Providers in Canada, CRTC (Mobile, Phone, Internet, or free/paid TV) by location\nCompare Internet & Phone Plans in Canada – Free Online Comparison Tool"
    ],
    [
        "The Apache Software Foundation\nThe Apache Software Foundation ( ; ASF) is an American nonprofit corporation (classified as a 501(c)(3) organization in the United States) to support a number of open-source software projects. The ASF was formed from a group of developers of the Apache HTTP Server, and incorporated on March 25, 1999.  it includes approximately 1000 members.",
        "The Apache Software Foundation is a decentralized open source community of developers. The software they produce is distributed under the terms of the Apache License, a permissive open-source license for free and open-source software (FOSS). The Apache projects are characterized by a collaborative, consensus-based development process and an open and pragmatic software license, which is to say that it allows developers, who receive the software freely, to redistribute it under non-free terms.  Each project is managed by a self-selected team of technical experts who are active contributors to the project. The ASF is a meritocracy, implying that membership of the foundation is granted only to volunteers who have actively contributed to Apache projects. The ASF is considered a",
        "to the project. The ASF is a meritocracy, implying that membership of the foundation is granted only to volunteers who have actively contributed to Apache projects. The ASF is considered a second-generation open-source organization, in that commercial support is provided without the risk of platform lock-in.",
        "Among the ASF's objectives are: to provide legal protection to volunteers working on Apache projects, and to prevent the \"Apache\" brand name from being used by other organizations without permission.\n\nThe ASF also holds several ApacheCon conferences each year, highlighting Apache projects and related technology.",
        "History",
        "The history of the Apache Software Foundation is linked to the Apache HTTP Server, development beginning in February 1993. A group of eight developers started working on enhancing the NCSA HTTPd daemon. They came to be known as the Apache Group. On March 25, 1999, the Apache Software Foundation was formed. The first official meeting of the Apache Software Foundation was held on April 13, 1999. The initial members of the Apache Software Foundation consisted of the Apache Group: Brian Behlendorf, Ken Coar, Miguel Gonzales, Mark Cox, Lars Eilebrecht, Ralf S. Engelschall, Roy T. Fielding, Dean Gaudet, Ben Hyde, Jim Jagielski, Alexei Kosut, Martin Kraemer, Ben Laurie, Doug MacEachern, Aram Mirzadeh, Sameer Parekh, Cliff Skolnick, Marc Slemko, William (Bill) Stoddard, Paul Sutton, Randy Terbush",
        "Ben Hyde, Jim Jagielski, Alexei Kosut, Martin Kraemer, Ben Laurie, Doug MacEachern, Aram Mirzadeh, Sameer Parekh, Cliff Skolnick, Marc Slemko, William (Bill) Stoddard, Paul Sutton, Randy Terbush and Dirk-Willem van Gulik. After a series of additional meetings to elect board members and resolve other legal matters regarding incorporation, the effective incorporation date of the Apache Software Foundation was set to June 1, 1999.",
        "Co-founder Brian Behlendorf states how the name 'Apache' was chosen: \"I suggested the name Apache partly because the web technologies at the time that were launching were being called cyber this or spider that or something on those themes and I was like we need something a little more interesting, a little more romantic, not to be a cultural appropriator or anything like that, I had just seen a documentary about Geronimo and the last days of a Native American tribe called the Apaches, right, who succumbed to the invasion from the West, from the United States, and they were the last tribe to give up their territory and for me that almost romantically represented what I felt we were doing with this web-server project...\"\n\nProjects",
        "Projects\n\nApache divides its software development activities into separate semi-autonomous areas called \"top-level projects\" (formally known as a \"Project Management Committee\" in the bylaws), some of which have a number of sub-projects. Unlike some other organizations that host FOSS projects, before a project is hosted at Apache it has to be licensed to the ASF with a grant or contributor agreement. In this way, the ASF gains the necessary intellectual property rights for the development and distribution of all its projects.",
        "Board of directors\nThe Board of Directors of The Apache Software Foundation (ASF) is responsible for management and oversight of the business and affairs of the corporation in accordance with the Bylaws. This includes management of the corporate assets (funds, intellectual property, trademarks, and support equipment), appointment of a President and corporate officers managing the core operations of the ASF, and allocation of corporate resources for the benefit of Apache projects. Technical decision-making authority for every Apache project is assigned to their independent project management committee; the participants in each project provide direction, not the board.\nThe board is elected annually by the ASF membership.\n\nSince March 17, 2021, the board of directors has been:",
        "Since March 17, 2021, the board of directors has been:\n\n Bertrand Delacretaz\n Roy T. Fielding\n Sharan Foga\n Justin Mclean\n Sam Ruby\n Craig L. Russell\n Roman Shaposhnik\n Sander Striker\n Sheng Wu\n\nSee also\n\n List of Apache Software Foundation projects\n Apache Attic\n Apache Incubator\n Log4Shell\n CNCF\n Linux Foundation\n\nNotes\n\nFurther reading\n Wikinomics: How Mass Collaboration Changes Everything (2006); Don Tapscott, Anthony D. Williams.\n\nExternal links\n\n \n ApacheCon website\n“Trillions and Trillions Served”  Feature documentary by the Apache Software Foundation detailing its history and impact on the open-source software community (2020)",
        "ApacheCon website\n“Trillions and Trillions Served”  Feature documentary by the Apache Software Foundation detailing its history and impact on the open-source software community (2020)\n\n1999 establishments in Maryland\n501(c)(3) organizations\n \nFree and open-source software organizations\nNon-profit organizations based in Maryland\nSoftware companies established in 1999"
    ],
    [
        "Video game\nA video game or computer game is an electronic game that involves interaction with a user interface or input device (such as a joystick, controller, keyboard, or motion sensing device) to generate visual feedback from a display device, most commonly shown in a video format on a television set, computer monitor, flat-panel display or touchscreen on handheld devices, or a virtual reality headset. Most modern video games are audiovisual, with audio complement delivered through speakers or headphones, and sometimes also with other types of sensory feedback (e.g., haptic technology that provides tactile sensations), and some video games also allow microphone and webcam inputs for in-game chatting and livestreaming.",
        "Video games are typically categorized according to their hardware platform, which traditionally includes arcade video games, console games, and computer (PC) games; the latter also encompasses LAN games, online games, and browser games. More recently, the video game industry has expanded onto mobile gaming through mobile devices (such as smartphones and tablet computers), virtual and augmented reality systems, and remote cloud gaming. Video games are also classified into a wide range of genres based on their style of gameplay and target audience.",
        "The first video game prototypes in the 1950s and 1960s were simple extensions of electronic games using video-like output from large, room-sized mainframe computers. The first consumer video game was the arcade video game Computer Space in 1971. In 1972 came the iconic hit game Pong and the first home console, the Magnavox Odyssey. The industry grew quickly during the \"golden age\" of arcade video games from the late 1970s to early 1980s but suffered from the crash of the North American video game market in 1983 due to loss of publishing control and saturation of the market. Following the crash, the industry matured, was dominated by Japanese companies such as Nintendo, Sega, and Sony, and established practices and methods around the development and distribution of video games to prevent a",
        "the industry matured, was dominated by Japanese companies such as Nintendo, Sega, and Sony, and established practices and methods around the development and distribution of video games to prevent a similar crash in the future, many of which continue to be followed. In the 2000s, the core industry centered on \"AAA\" games, leaving little room for riskier experimental games. Coupled with the availability of the Internet and digital distribution, this gave room for independent video game development (or \"indie games\") to gain prominence into the 2010s. Since then, the commercial importance of the video game industry has been increasing. The emerging Asian markets and proliferation of smartphone games in particular are altering player demographics towards casual gaming and increasing",
        "of the video game industry has been increasing. The emerging Asian markets and proliferation of smartphone games in particular are altering player demographics towards casual gaming and increasing monetization by incorporating games as a service.",
        "Today, video game development requires numerous interdisciplinary skills, vision, teamwork, and liaisons between different parties, including developers, publishers, distributors, retailers, hardware manufacturers, and other marketers, to successfully bring a game to its consumers. , the global video game market had estimated annual revenues of  across hardware, software, and services, which is three times the size of the global music industry and four times that of the film industry in 2019, making it a formidable heavyweight across the modern entertainment industry. The video game market is also a major influence behind the electronics industry, where personal computer component, console, and peripheral sales, as well as consumer demands for better game performance, have been powerful",
        "also a major influence behind the electronics industry, where personal computer component, console, and peripheral sales, as well as consumer demands for better game performance, have been powerful driving factors for hardware design and innovation.",
        "Origins",
        "Early video games use interactive electronic devices with various display formats. The earliest example is from 1947—a \"cathode-ray tube amusement device\" was filed for a patent on 25 January 1947, by Thomas T. Goldsmith Jr. and Estle Ray Mann, and issued on 14 December 1948, as U.S. Patent 2455992. Inspired by radar display technology, it consists of an analog device allowing a user to control the parabolic arc of a dot on the screen to simulate a missile being fired at targets, which are paper drawings fixed to the screen. Other early examples include Christopher Strachey's draughts game, the Nimrod computer at the 1951 Festival of Britain; OXO, a tic-tac-toe computer game by Alexander S. Douglas for the EDSAC in 1952; Tennis for Two, an electronic interactive game engineered by William",
        "Nimrod computer at the 1951 Festival of Britain; OXO, a tic-tac-toe computer game by Alexander S. Douglas for the EDSAC in 1952; Tennis for Two, an electronic interactive game engineered by William Higinbotham in 1958; and Spacewar!, written by Massachusetts Institute of Technology students Martin Graetz, Steve Russell, and Wayne Wiitanen's on a DEC PDP-1 computer in 1961. Each game has different means of display: NIMROD has a panel of lights to play the game of Nim, OXO has a graphical display to play tic-tac-toe, Tennis for Two has an oscilloscope to display a side view of a tennis court, and Spacewar! has the DEC PDP-1's vector display to have two spaceships battle each other.",
        "These preliminary inventions paved the way for the origins of video games today. Ralph H. Baer, while working at Sanders Associates in 1966, devised a control system to play a rudimentary game of table tennis on a television screen. With the company's approval, Baer built the prototype \"Brown Box\". Sanders patented Baer's inventions and licensed them to Magnavox, which commercialized it as the first home video game console, the Magnavox Odyssey, released in 1972. Separately, Nolan Bushnell and Ted Dabney, inspired by seeing Spacewar! running at Stanford University, devised a similar version running in a smaller coin-operated arcade cabinet using a less expensive computer. This was released as Computer Space, the first arcade video game, in 1971. Bushnell and Dabney went on to form Atari,",
        "in a smaller coin-operated arcade cabinet using a less expensive computer. This was released as Computer Space, the first arcade video game, in 1971. Bushnell and Dabney went on to form Atari, Inc., and with Allan Alcorn, created their second arcade game in 1972, the hit ping pong-style Pong, which was directly inspired by the table tennis game on the Odyssey. Sanders and Magnavox sued Atari for infringement of Baer's patents, but Atari settled out of court, paying for perpetual rights to the patents. Following their agreement, Atari made a home version of Pong, which was released by Christmas 1975. The success of the Odyssey and Pong, both as an arcade game and home machine, launched the video game industry. Both Baer and Bushnell have been titled \"Father of Video Games\" for their",
        "1975. The success of the Odyssey and Pong, both as an arcade game and home machine, launched the video game industry. Both Baer and Bushnell have been titled \"Father of Video Games\" for their contributions.",
        "Terminology\nThe term \"video game\" was developed to distinguish this class of electronic games that were played on some type of video display rather than on a teletype printer, audio speaker or similar device. This also distinguished from many handheld electronic games like Merlin which commonly used LED lights for indicators but did not use these in combination for imaging purposes.",
        "\"Computer game\" may also be used as a descriptor, as all these types of games essentially require the use of a computer processor, and in some cases, it is used interchangeably with \"video game\". Particularly in the United Kingdom and Western Europe, this is common due to the historic relevance of domestically produced microcomputers. Other terms used include digital game, for example by the Australian Bureau of Statistics. However, the term \"computer game\" can also be used to more specifically refer to games played primarily on personal computers or other type of flexible hardware systems (also known as a PC game), as a way distinguish them from console games, arcade games or mobile games. Other terms such as \"television game\" or \"telegame\" had been used in the 1970s and early 1980s,",
        "(also known as a PC game), as a way distinguish them from console games, arcade games or mobile games. Other terms such as \"television game\" or \"telegame\" had been used in the 1970s and early 1980s, particularly for the home gaming consoles that rely on connection to a television set. In Japan, where consoles like the Odyssey were first imported and then made within the country by the large television manufacturers such as Toshiba and Sharp Corporation, such games are known as \"TV games\", or TV geemu or terebi geemu. \"Electronic game\" may also be used to refer to video games, but this also incorporates devices like early handheld electronic games that lack any video output. and the term \"TV game\" is still commonly used into the 21st century.",
        "The first appearance of the term \"video game\" emerged around 1973. The Oxford English Dictionary cited a 10 November 1973 BusinessWeek article as the first printed use of the term. Though Bushnell believed the term came from a vending magazine review of Computer Space in 1971, a review of the major vending magazines Vending Times and Cashbox showed that the term came much earlier, appearing first around March 1973 in these magazines in mass usage including by the arcade game manufacturers. As analyzed by video game historian Keith Smith, the sudden appearance suggested that the term had been proposed and readily adopted by those involved. This appeared to trace to Ed Adlum, who ran Cashboxs coin-operated section until 1972 and then later founded RePlay Magazine, covering the coin-op",
        "been proposed and readily adopted by those involved. This appeared to trace to Ed Adlum, who ran Cashboxs coin-operated section until 1972 and then later founded RePlay Magazine, covering the coin-op amusement field, in 1975. In a September 1982 issue of RePlay, Adlum is credited with first naming these games as \"video games\": \"RePlay's Eddie Adlum worked at 'Cash Box' when 'TV games' first came out. The personalities in those days were Bushnell, his sales manager Pat Karns and a handful of other 'TV game' manufacturers like Henry Leyser and the McEwan brothers. It seemed awkward to call their products 'TV games', so borrowing a word from Billboards description of movie jukeboxes, Adlum started to refer to this new breed of amusement machine as 'video games.' The phrase stuck.\" Adlum",
        "their products 'TV games', so borrowing a word from Billboards description of movie jukeboxes, Adlum started to refer to this new breed of amusement machine as 'video games.' The phrase stuck.\" Adlum explained in 1985 that up until the early 1970s, amusement arcades typically had non-video arcade games such as pinball machines and electro-mechanical games. With the arrival of video games in arcades during the early 1970s, there was initially some confusion in the arcade industry over what term should be used to describe the new games. He \"wrestled with descriptions of this type of game,\" alternating between \"TV game\" and \"television game\" but \"finally woke up one day\" and said, \"what the hell... video game!\"",
        "For many years, the traveling Videotopia exhibit served as the closest representation of such a vital resource. In addition to collecting home video game consoles, the Electronics Conservancy organization set out to locate and restore 400 antique arcade cabinets after realizing that the majority of these games had been destroyed and feared the loss of their historical significance. Video games have significantly began to be seen in the real-world as a purpose to present history in a way of understanding the methodology and terms that are being compared. Researchers have looked at how historical representations affect how the public perceives the past, and digital humanists encourage historians to use video games as primary materials. Video games, considering their past and age, have over",
        "representations affect how the public perceives the past, and digital humanists encourage historians to use video games as primary materials. Video games, considering their past and age, have over time progressed as what a video game really means. Whether played through a monitor, TV, or a hand-held device, there are many ways that video games are being displayed for users to enjoy. People have drawn comparisons between flow-state-engaged video gamers and pupils in conventional school settings. In traditional, teacher-led classrooms, students have little say in what they learn, are passive consumers of the information selected by teachers, are required to follow the pace and skill level of the group (group teaching), and receive brief, imprecise, normative feedback on their work. Video",
        "consumers of the information selected by teachers, are required to follow the pace and skill level of the group (group teaching), and receive brief, imprecise, normative feedback on their work. Video games, as they continue to develop into better graphic definition and genre's, create new terminology when something unknown tends to become known. Yearly, consoles are being created to compete against other brands with similar functioning features that tends to lead the consumer into which they'd like to purchase. Now, companies have moved towards games only the specific console can play to grasp the consumer into purchasing their product compared to when video games first began, there was little to no variety. In 1989, a console war begun with Nintendo, one of the biggest in gaming was up",
        "the consumer into purchasing their product compared to when video games first began, there was little to no variety. In 1989, a console war begun with Nintendo, one of the biggest in gaming was up against target, Sega with their brand new Master System which, failed to compete, allowing the Nintendo Emulator System to be one of the most consumed product in the world. More technology continued to be created, as the computer began to be used in people's houses for more than just office and daily use. Games began being implemented into computers and have progressively grown since then with coded robots to play against you. Early games like tic-tac-toe, solitaire, and Tennis for Two were great ways to bring new gaming to another system rather than one specifically meant for gaming.",
        "Definition\nWhile many games readily fall into a clear, well-understood definition of video games, new genres and innovations in game development have raised the question of what are the essential factors of a video game that separate the medium from other forms of entertainment.",
        "The introduction of interactive films in the 1980s with games like Dragon's Lair, featured games with full motion video played off a form of media but only limited user interaction. This had required a means to distinguish these games from more traditional board games that happen to also use external media, such as the Clue VCR Mystery Game which required players to watch VCR clips between turns. To distinguish between these two, video games are considered to require some interactivity that affects the visual display.",
        "Most video games tend to feature some type of victory or winning conditions, such as a scoring mechanism or a final boss fight. The introduction of walking simulators (adventure games that allow for exploration but lack any objectives) like Gone Home, and empathy games (video games that tend to focus on emotion) like That Dragon, Cancer brought the idea of games that did not have any such type of winning condition and raising the question of whether these were actually games. These are still commonly justified as video games as they provide a game world that the player can interact with by some means.",
        "The lack of any industry definition for a video game by 2021 was an issue during the case Epic Games v. Apple which dealt with video games offered on Apple's iOS App Store. Among concerns raised were games like Fortnite Creative and Roblox which created metaverses of interactive experiences, and whether the larger game and the individual experiences themselves were games or not in relation to fees that Apple charged for the App Store. Judge Yvonne Gonzalez Rogers, recognizing that there was yet an industry standard definition for a video game, established for her ruling that \"At a bare minimum, videogames appear to require some level of interactivity or involvement between the player and the medium\" compared to passive entertainment like film, music, and television, and \"videogames are",
        "videogames appear to require some level of interactivity or involvement between the player and the medium\" compared to passive entertainment like film, music, and television, and \"videogames are also generally graphically rendered or animated, as opposed to being recorded live or via motion capture as in films or television\". Rogers still concluded that what is a video game \"appears highly eclectic and diverse\".",
        "Video game terminology",
        "The gameplay experience varies radically between video games, but many common elements exist. Most games will launch into a title screen and give the player a chance to review options such as the number of players before starting a game. Most games are divided into levels which the player must work the avatar through, scoring points, collecting power-ups to boost the avatar's innate attributes, all while either using special attacks to defeat enemies or moves to avoid them. This information is relayed to the player through a type of on-screen user interface such as a heads-up display atop the rendering of the game itself. Taking damage will deplete their avatar's health, and if that falls to zero or if the avatar otherwise falls into an impossible-to-escape location, the player will lose",
        "rendering of the game itself. Taking damage will deplete their avatar's health, and if that falls to zero or if the avatar otherwise falls into an impossible-to-escape location, the player will lose one of their lives. Should they lose all their lives without gaining an extra life or \"1-UP\", then the player will reach the \"game over\" screen. Many levels as well as the game's finale end with a type of boss character the player must defeat to continue on. In some games, intermediate points between levels will offer save points where the player can create a saved game on storage media to restart the game should they lose all their lives or need to stop the game and restart at a later time. These also may be in the form of a passage that can be written down and reentered at the title screen.",
        "Product flaws include software bugs which can manifest as glitches which may be exploited by the player; this is often the foundation of speedrunning a video game. These bugs, along with cheat codes, Easter eggs, and other hidden secrets that were intentionally added to the game can also be exploited. On some consoles, cheat cartridges allow players to execute these cheat codes, and user-developed trainers allow similar bypassing for computer software games. Both of which might make the game easier, give the player additional power-ups, or change the appearance of the game.\n\nComponents",
        "Components\n\nTo distinguish from electronic games, a video game is generally considered to require a platform, the hardware which contains computing elements, to process player interaction from some type of input device and displays the results to a video output display.\n\nPlatform",
        "Platform\n\nVideo games require a platform, a specific combination of electronic components or computer hardware and associated software, to operate. The term system is also commonly used. Games are typically designed to be played on one or a limited number of platforms, and exclusivity to a platform is used as a competitive edge in the video game market. However, games may be developed for alternative platforms than intended, which are described as ports or conversions. These also may be remasters - where most of the original game's source code is reused and art assets, models, and game levels are updated for modern systems – and remakes, where in addition to asset improvements, significant reworking of the original game and possibly from scratch is performed.",
        "The list below is not exhaustive and excludes other electronic devices capable of playing video games such as PDAs and graphing calculators.",
        "PC games",
        "PC games involve a player interacting with a personal computer (PC) connected to a video monitor. Personal computers are not dedicated game platforms, so there may be differences running the same game on different hardware. Also, the openness allows some features to developers like reduced software cost, increased flexibility, increased innovation, emulation, creation of modifications or mods, open hosting for online gaming (in which a person plays a video game with people who are in a different household) and others. A gaming computer is a PC or laptop intended specifically for gaming, typically using high-performance, high-cost components. In additional to personal computer gaming, there also exist games that work on mainframe computers and other similarly shared systems, with users",
        "using high-performance, high-cost components. In additional to personal computer gaming, there also exist games that work on mainframe computers and other similarly shared systems, with users logging in remotely to use the computer.",
        "Home console\nA console game is played on a home console, a specialized electronic device that connects to a common television set or composite video monitor. Home consoles are specifically designed to play games using a dedicated hardware environment, giving developers a concrete hardware target for development and assurances of what features will be available, simplifying development compared to PC game development. Usually consoles only run games developed for it, or games from other platform made by the same company, but never games developed by its direct competitor, even if the same game is available on different platforms. It often comes with a specific game controller. Major console platforms include Xbox, PlayStation and Nintendo.\nHandheld console",
        "Handheld console\nA handheld game console is a small, self-contained electronic device that is portable and can be held in a user's hands. It features the console, a small screen, speakers and buttons, joystick or other game controllers in a single unit. Like consoles, handhelds are dedicated platforms, and share almost the same characteristics. Handheld hardware usually is less powerful than PC or console hardware. Some handheld games from the late 1970s and early 1980s could only play one game. In the 1990s and 2000s, a number of handheld games used cartridges, which enabled them to be used to play many different games. The handheld console has waned in the 2010s as mobile device gaming has become a more dominant factor.\nArcade video game",
        "An arcade video game generally refers to a game played on an even more specialized type of electronic device that is typically designed to play only one game and is encased in a special, large coin-operated cabinet which has one built-in console, controllers (joystick, buttons, etc.), a CRT screen, and audio amplifier and speakers. Arcade games often have brightly painted logos and images relating to the theme of the game. While most arcade games are housed in a vertical cabinet, which the user typically stands in front of to play, some arcade games use a tabletop approach, in which the display screen is housed in a table-style cabinet with a see-through table top. With table-top games, the users typically sit to play. In the 1990s and 2000s, some arcade games offered players a choice of",
        "screen is housed in a table-style cabinet with a see-through table top. With table-top games, the users typically sit to play. In the 1990s and 2000s, some arcade games offered players a choice of multiple games. In the 1980s, video arcades were businesses in which game players could use a number of arcade video games. In the 2010s, there are far fewer video arcades, but some movie theaters and family entertainment centers still have them.",
        "Browser game\nA browser game takes advantages of standardizations of technologies for the functionality of web browsers across multiple devices providing a cross-platform environment. These games may be identified based on the website that they appear, such as with Miniclip games. Others are named based on the programming platform used to develop them, such as Java and Flash games.\nMobile game\nWith the introduction of smartphones and tablet computers standardized on the iOS and Android operating systems, mobile gaming has become a significant platform. These games may use unique features of mobile devices that are not necessary present on other platforms, such as accelerometers, global positing information and camera devices to support augmented reality gameplay.\nCloud gaming",
        "Cloud gaming\nCloud gaming requires a minimal hardware device, such as a basic computer, console, laptop, mobile phone or even a dedicated hardware device connected to a display with good Internet connectivity that connects to hardware systems by the cloud gaming provider. The game is computed and rendered on the remote hardware, using a number of predictive methods to reduce the network latency between player input and output on their display device. For example, the Xbox Cloud Gaming and PlayStation Now platforms use dedicated custom server blade hardware in cloud computing centers.\nVirtual reality",
        "Virtual reality\n Virtual reality (VR) games generally require players to use a special head-mounted unit that provides stereoscopic screens and motion tracking to immerse a player within virtual environment that responds to their head movements. Some VR systems include control units for the player's hands as to provide a direct way to interact with the virtual world. VR systems generally require a separate computer, console, or other processing device that couples with the head-mounted unit.\nEmulation",
        "Emulation\nAn emulator enables games from a console or otherwise different system to be run in a type of virtual machine on a modern system, simulating the hardware of the original and allows old games to be played. While emulators themselves have been found to be legal in United States case law, the act of obtaining the game software that one does not already own may violate copyrights. However, there are some official releases of emulated software from game manufacturers, such as Nintendo with its Virtual Console or Nintendo Switch Online offerings.\nBackward compatibility",
        "Backward compatibility\nBackward compatibility is similar in nature to emulation in that older games can be played on newer platforms, but typically directly though hardware and build-in software within the platform. For example, the PlayStation 2 is capable of playing original PlayStation games simply by inserting the original game media into the newer console, while Nintendo's Wii could play GameCube titles as well in the same manner.",
        "Game media",
        "Early arcade games, home consoles, and handheld games were dedicated hardware units with the game's logic built into the electronic componentry of the hardware. Since then, most video game platforms are considered programmable, having means to read and play multiple games distributed on different types of media or formats. Physical formats include ROM cartridges, magnetic storage including magnetic-tape data storage and floppy discs, optical media formats including CD-ROM and DVDs, and flash memory cards. Furthermore digital distribution over the Internet or other communication methods as well as cloud gaming alleviate the need for any physical media. In some cases, the media serves as the direct read-only memory for the game, or it may be the form of installation media that is used to",
        "as cloud gaming alleviate the need for any physical media. In some cases, the media serves as the direct read-only memory for the game, or it may be the form of installation media that is used to write the main assets to the player's platform's local storage for faster loading periods and later updates.",
        "Games can be extended with new content and software patches through either expansion packs which are typically available as physical media, or as downloadable content nominally available via digital distribution. These can be offered freely or can be used to monetize a game following its initial release. Several games offer players the ability to create user-generated content to share with others to play. Other games, mostly those on personal computers, can be extended with user-created modifications or mods that alter or add onto the game; these often are unofficial and were developed by players from reverse engineering of the game, but other games provide official support for modding the game.\n\nInput device",
        "Video game can use several types of input devices to translate human actions to a game. Most common are the use of game controllers like gamepads and joysticks for most consoles, and as accessories for personal computer systems along keyboard and mouse controls. Common controls on the most recent controllers include face buttons, shoulder triggers, analog sticks, and directional pads (\"d-pads\"). Consoles typically include standard controllers which are shipped or bundled with the console itself, while peripheral controllers are available as a separate purchase from the console manufacturer or third-party vendors. Similar control sets are built into handheld consoles and onto arcade cabinets. Newer technology improvements have incorporated additional technology into the controller or the",
        "third-party vendors. Similar control sets are built into handheld consoles and onto arcade cabinets. Newer technology improvements have incorporated additional technology into the controller or the game platform, such as touchscreens and motion detection sensors that give more options for how the player interacts with the game. Specialized controllers may be used for certain genres of games, including racing wheels, light guns and dance pads. Digital cameras and motion detection can capture movements of the player as input into the game, which can, in some cases, effectively eliminate the control, and on other systems such as virtual reality, are used to enhance immersion into the game.",
        "Display and output\n\nBy definition, all video games are intended to output graphics to an external video display, such as cathode-ray tube televisions, newer liquid-crystal display (LCD) televisions and built-in screens, projectors or computer monitors, depending on the type of platform the game is played on. Features such as color depth, refresh rate, frame rate, and screen resolution are a combination of the limitations of the game platform and display device and the program efficiency of the game itself. The game's output can range from fixed displays using LED or LCD elements, text-based games, two-dimensional and three-dimensional graphics, and augmented reality displays.",
        "The game's graphics are often accompanied by sound produced by internal speakers on the game platform or external speakers attached to the platform, as directed by the game's programming. This often will include sound effects tied to the player's actions to provide audio feedback, as well as background music for the game.\n\nSome platforms support additional feedback mechanics to the player that a game can take advantage of. This is most commonly haptic technology built into the game controller, such as causing the controller to shake in the player's hands to simulate a shaking earthquake occurring in game.\n\nClassifications\nVideo games are frequently classified by a number of factors related to how one plays them.\n\nGenre",
        "Classifications\nVideo games are frequently classified by a number of factors related to how one plays them.\n\nGenre\n\nA video game, like most other forms of media, may be categorized into genres. However, unlike film or television which use visual or narrative elements, video games are generally categorized into genres based on their gameplay interaction, since this is the primary means which one interacts with a video game. The narrative setting does not impact gameplay; a shooter game is still a shooter game, regardless of whether it takes place in a fantasy world or in outer space. An exception is the horror game genre, used for games that are based on narrative elements of horror fiction, the supernatural, and psychological horror.",
        "Genre names are normally self-describing in terms of the type of gameplay, such as action game, role playing game, or shoot 'em up, though some genres have derivations from influential works that have defined that genre, such as roguelikes from Rogue, Grand Theft Auto clones from Grand Theft Auto III, and battle royale games from the film Battle Royale. The names may shift over time as players, developers and the media come up with new terms; for example, first-person shooters were originally called \"Doom clones\" based on the 1993 game. A hierarchy of game genres exist, with top-level genres like \"shooter game\" and \"action game\" that broadly capture the game's main gameplay style, and several subgenres of specific implementation, such as within the shooter game first-person shooter and",
        "like \"shooter game\" and \"action game\" that broadly capture the game's main gameplay style, and several subgenres of specific implementation, such as within the shooter game first-person shooter and third-person shooter. Some cross-genre types also exist that fall until multiple top-level genres such as action-adventure game.",
        "Mode\n\nA video game's mode describes how many players can use the game at the same type. This is primarily distinguished by single-player video games and multiplayer video games. Within the latter category, multiplayer games can be played in a variety of ways, including locally at the same device, on separate devices connected through a local network such as LAN parties, or online via separate Internet connections. Most multiplayer games are based on competitive gameplay, but many offer cooperative and team-based options as well as asymmetric gameplay. Online games use server structures that can also enable massively multiplayer online games (MMOs) to support hundreds of players at the same time.",
        "A small number of video games are zero-player games, in which the player has very limited interaction with the game itself. These are most commonly simulation games where the player may establish a starting state and then let the game proceed on its own, watching the results as a passive observer, such as with many computerized simulations of Conway's Game of Life.\n\nTypes\nMost video games are intended for entertainment purposes. Different game types include:",
        "Types\nMost video games are intended for entertainment purposes. Different game types include:\n\n Core games\n Core or hard-core games refer to the typical perception of video games, developed for entertainment purposes. These games typically require a fair amount of time to learn and master, in contrast to casual games, and thus are most appealing to gamers rather than a broader audience. Most of the AAA video game industry is based around the delivery of core games.",
        "Casual games",
        "In contrast to core games, casual games are designed for ease of accessibility, simple to understand gameplay and quick to grasp rule sets, and aimed at mass market audience. They frequently support the ability to jump in and out of play on demand, such as during commuting or lunch breaks. Numerous browser and mobile games fall into the casual game area, and casual games often are from genres with low intensity game elements such as match three, hidden object, time management, and puzzle games. Causal games frequently use social-network game mechanics, where players can enlist the help of friends on their social media networks for extra turns or moves each day. Popular casual games include Tetris and Candy Crush Saga. More recent, starting in the late 2010s, are hyper-casual games which",
        "on their social media networks for extra turns or moves each day. Popular casual games include Tetris and Candy Crush Saga. More recent, starting in the late 2010s, are hyper-casual games which use even more simplistic rules for short but infinitely replayable games, such as Flappy Bird.",
        "Educational games",
        "Education software has been used in homes and classrooms to help teach children and students, and video games have been similarly adapted for these reasons, all designed to provide a form of interactivity and entertainment tied to game design elements. There are a variety of differences in their designs and how they educate the user. These are broadly split between edutainment games that tend to focus on the entertainment value and rote learning but are unlikely to engage in critical thinking, and educational video games that are geared towards problem solving through motivation and positive reinforcement while downplaying the entertainment value. Examples of educational games include The Oregon Trail and the Carmen Sandiego series. Further, games not initially developed for educational",
        "reinforcement while downplaying the entertainment value. Examples of educational games include The Oregon Trail and the Carmen Sandiego series. Further, games not initially developed for educational purposes have found their way into the classroom after release, such as that feature open worlds or virtual sandboxes like Minecraft, or offer critical thinking skills through puzzle video games like SpaceChem.",
        "Serious games",
        "Further extending from educational games, serious games are those where the entertainment factor may be augmented, overshadowed, or even eliminated by other purposes for the game. Game design is used to reinforce the non-entertainment purpose of the game, such as using video game technology for the game's interactive world, or gamification for reinforcement training. Educational games are a form of serious games, but other types of games include fitness games that incorporate significant physical exercise to help keep the player fit (such as Wii Fit), simulator games that resemble fight simulators to pilot aircraft (such as Microsoft Flight Simulator), advergames that are built around the advertising of a product (such as Pepsiman), and newsgames aimed at conveying a specific advocacy",
        "to pilot aircraft (such as Microsoft Flight Simulator), advergames that are built around the advertising of a product (such as Pepsiman), and newsgames aimed at conveying a specific advocacy message (such as NarcoGuerra).",
        "Art games\nAlthough video games have been considered an art form on their own, games may be developed to try to purposely communicate a story or message, using the medium as a work of art. These art or arthouse games are designed to generate emotion and empathy from the player by challenging societal norms and offering critique through the interactivity of the video game medium. They may not have any type of win condition and are designed to let the player explore through the game world and scenarios. Most art games are indie games in nature, designed based on personal experiences or stories through a single developer or small team. Examples of art games include Passage, Flower, and That Dragon, Cancer.",
        "Content rating",
        "Video games can be subject to national and international content rating requirements. Like with film content ratings, video game ratings typing identify the target age group that the national or regional ratings board believes is appropriate for the player, ranging from all-ages, to a teenager-or-older, to mature, to the infrequent adult-only games. Most content review is based on the level of violence, both in the type of violence and how graphic it may be represented, and sexual content, but other themes such as drug and alcohol use and gambling that can influence children may also be identified. A primary identifier based on a minimum age is used by nearly all systems, along with additional descriptors to identify specific content that players and parents should be aware of.",
        "The regulations vary from country to country but generally are voluntary systems upheld by vendor practices, with penalty and fines issued by the ratings body on the video game publisher for misuse of the ratings. Among the major content rating systems include:\n Entertainment Software Rating Board (ESRB) that oversees games released in the United States. ESRB ratings are voluntary and rated along a E (Everyone), E10+ (Everyone 10 and older), T (Teen), M (Mature), and AO (Adults Only). Attempts to mandate video games ratings in the U.S. subsequently led to the landmark Supreme Court case, Brown v. Entertainment Merchants Association in 2011 which ruled video games were a protected form of art, a key victory for the video game industry.",
        "Pan European Game Information (PEGI) covering the United Kingdom, most of the European Union and other European countries, replacing previous national-based systems. The PEGI system uses content rated based on minimum recommended ages, which include 3+, 8+, 12+, 16+, and 18+.",
        "Australian Classification Board (ACB) oversees the ratings of games and other works in Australia, using ratings of G (General), PG (Parental Guidance), M (Mature), MA15+ (Mature Accompanied), R18+ (Restricted), and X (Restricted for pornographic material). ACB can also deny to give a rating to game (RC – Refused Classification). The ACB's ratings are enforceable by law, and importantly, games cannot be imported or purchased digitally in Australia if they have failed to gain a rating or were given the RC rating, leading to a number of notable banned games.\n Computer Entertainment Rating Organization (CERO) rates games for Japan. Their ratings include A (all ages), B (12 and older), C (15 and over), D (17 and over), and Z (18 and over).",
        "Additionally, the major content system provides have worked to create the International Age Rating Coalition (IARC), a means to streamline and align the content ratings system between different region, so that a publisher would only need to complete the content ratings review for one provider, and use the IARC transition to affirm the content rating for all other regions.",
        "Certain nations have even more restrictive rules related to political or ideological content. Within Germany, until 2018, the Unterhaltungssoftware Selbstkontrolle (Entertainment Software Self-Regulation) would refuse to classify, and thus allow sale, of any game depicting Nazi imagery, and thus often requiring developers to replace such imagery with fictional ones. This ruling was relaxed in 2018 to allow for such imagery for \"social adequacy\" purposes that applied to other works of art. China's video game segment is mostly isolated from the rest of the world due to the government's censorship, and all games published there must adhere to strict government review, disallowing content such as smearing the image of the Chinese Communist Party. Foreign games published in China often require",
        "and all games published there must adhere to strict government review, disallowing content such as smearing the image of the Chinese Communist Party. Foreign games published in China often require modification by developers and publishers to meet these requirements.",
        "Development\n\nVideo game development and authorship, much like any other form of entertainment, is frequently a cross-disciplinary field. Video game developers, as employees within this industry are commonly referred, primarily include programmers and graphic designers. Over the years this has expanded to include almost every type of skill that one might see prevalent in the creation of any movie or television program, including sound designers, musicians, and other technicians; as well as skills that are specific to video games, such as the game designer. All of these are managed by producers.",
        "In the early days of the industry, it was more common for a single person to manage all of the roles needed to create a video game. As platforms have become more complex and powerful in the type of material they can present, larger teams have been needed to generate all of the art, programming, cinematography, and more. This is not to say that the age of the \"one-man shop\" is gone, as this is still sometimes found in the casual gaming and handheld markets, where smaller games are prevalent due to technical limitations such as limited RAM or lack of dedicated 3D graphics rendering capabilities on the target platform (e.g., some PDAs).",
        "Video games are programmed like any other piece of computer software. Prior to the mid-1970s, arcade and home consoles were programmed by assembling discrete electro-mechanical components on circuit boards, which limited games to relatively simple logic. By 1975, low-cost microprocessors were available at volume to be used for video game hardware, which allowed game developers to program more detailed games, widening the scope of what was possible. Ongoing improvements in computer hardware technology has expanded what has become possible to create in video games, coupled with convergence of common hardware between console, computer, and arcade platforms to simplify the development process. Today, game developers have a number of commercial and open source tools available for use to make",
        "common hardware between console, computer, and arcade platforms to simplify the development process. Today, game developers have a number of commercial and open source tools available for use to make games, often which are across multiple platforms to support portability, or may still opt to create their own for more specialized features and direct control of the game. Today, many games are built around a game engine that handles the bulk of the game's logic, gameplay, and rendering. These engines can be augmented with specialized engines for specific features, such as a physics engine that simulates the physics of objects in real-time. A variety of middleware exists to help developers to access other features, such as for playback of videos within games, network-oriented code for games",
        "simulates the physics of objects in real-time. A variety of middleware exists to help developers to access other features, such as for playback of videos within games, network-oriented code for games that communicate via online services, matchmaking for online games, and similar features. These features can be used from a developers' programming language of choice, or they may opt to also use game development kits that minimize the amount of direct programming they have to do but can also limit the amount of customization they can add into a game. Like all software, video games usually undergo quality testing before release to assure there are no bugs or glitches in the product, though frequently developers will release patches and updates.",
        "With the growth of the size of development teams in the industry, the problem of cost has increased. Development studios need the best talent, while publishers reduce costs to maintain profitability on their investment. Typically, a video game console development team ranges from 5 to 50 people, and some exceed 100. In May 2009, Assassin's Creed II was reported to have a development staff of 450. The growth of team size combined with greater pressure to get completed projects into the market to begin recouping production costs has led to a greater occurrence of missed deadlines, rushed games and the release of unfinished products.",
        "While amateur and hobbyist game programming had existed since the late 1970s with the introduction of home computers, a newer trend since the mid-2000s is indie game development. Indie games are made by small teams outside any direct publisher control, their games being smaller in scope than those from the larger \"AAA\" game studios, and are often experiment in gameplay and art style. Indie game development are aided by larger availability of digital distribution, including the newer mobile gaming marker, and readily-available and low-cost development tools for these platforms.\n\nGame theory and studies",
        "Although departments of computer science have been studying the technical aspects of video games for years, theories that examine games as an artistic medium are a relatively recent development in the humanities. The two most visible schools in this emerging field are ludology and narratology. Narrativists approach video games in the context of what Janet Murray calls \"Cyberdrama\". That is to say, their major concern is with video games as a storytelling medium, one that arises out of interactive fiction. Murray puts video games in the context of the Holodeck, a fictional piece of technology from Star Trek, arguing for the video game as a medium in which the player is allowed to become another person, and to act out in another world. This image of video games received early widespread",
        "from Star Trek, arguing for the video game as a medium in which the player is allowed to become another person, and to act out in another world. This image of video games received early widespread popular support, and forms the basis of films such as Tron, eXistenZ and The Last Starfighter.",
        "Ludologists break sharply and radically from this idea. They argue that a video game is first and foremost a game, which must be understood in terms of its rules, interface, and the concept of play that it deploys. Espen J. Aarseth argues that, although games certainly have plots, characters, and aspects of traditional narratives, these aspects are incidental to gameplay. For example, Aarseth is critical of the widespread attention that narrativists have given to the heroine of the game Tomb Raider, saying that \"the dimensions of Lara Croft's body, already analyzed to death by film theorists, are irrelevant to me as a player, because a different-looking body would not make me play differently... When I play, I don't even see her body, but see through it and past it.\" Simply put,",
        "are irrelevant to me as a player, because a different-looking body would not make me play differently... When I play, I don't even see her body, but see through it and past it.\" Simply put, ludologists reject traditional theories of art because they claim that the artistic and socially relevant qualities of a video game are primarily determined by the underlying set of rules, demands, and expectations imposed on the player.",
        "While many games rely on emergent principles, video games commonly present simulated story worlds where emergent behavior occurs within the context of the game. The term \"emergent narrative\" has been used to describe how, in a simulated environment, storyline can be created simply by \"what happens to the player.\" However, emergent behavior is not limited to sophisticated games. In general, any place where event-driven instructions occur for AI in a game, emergent behavior will exist. For instance, take a racing game in which cars are programmed to avoid crashing, and they encounter an obstacle in the track: the cars might then maneuver to avoid the obstacle causing the cars behind them to slow or maneuver to accommodate the cars in front of them and the obstacle. The programmer never",
        "in the track: the cars might then maneuver to avoid the obstacle causing the cars behind them to slow or maneuver to accommodate the cars in front of them and the obstacle. The programmer never wrote code to specifically create a traffic jam, yet one now exists in the game.",
        "Intellectual property for video games\n\nMost commonly, video games are protected by copyright, though both patents and trademarks have been used as well.\n\nThough local copyright regulations vary to the degree of protection, video games qualify as copyrighted visual-audio works, and enjoy cross-country protection under the Berne Convention. This typically only applies to the underlying code, as well as to the artistic aspects of the game such as its writing, art assets, and music. Gameplay itself is generally not considered copyrightable; in the United States among other countries, video games are considered to fall into the idea–expression distinction in that it is how the game is presented and expressed to the player that can be copyrighted, but not the underlying principles of the game.",
        "Because gameplay is normally ineligible for copyright, gameplay ideas in popular games are often replicated and built upon in other games. At times, this repurposing of gameplay can be seen as beneficial and a fundamental part of how the industry has grown by building on the ideas of others. For example Doom (1993) and Grand Theft Auto III (2001) introduced gameplay that created popular new game genres, the first-person shooter and the Grand Theft Auto clone, respectively, in the few years after their release. However, at times and more frequently at the onset of the industry, developers would intentionally create video game clones of successful games and game hardware with few changes, which led to the flooded arcade and dedicated home console market around 1978. Cloning is also a major",
        "intentionally create video game clones of successful games and game hardware with few changes, which led to the flooded arcade and dedicated home console market around 1978. Cloning is also a major issue with countries that do not have strong intellectual property protection laws, such as within China. The lax oversight by China's government and the difficulty for foreign companies to take Chinese entities to court had enabled China to support a large grey market of cloned hardware and software systems. The industry remains challenged to distinguish between creating new games based on refinements of past successful games to create a new type of gameplay, and intentionally creating a clone of a game that may simply swap out art assets.",
        "Industry",
        "History",
        "The early history of the video game industry, following the first game hardware releases and through 1983, had little structure. Video games quickly took off during the golden age of arcade video games from the late 1970s to early 1980s, but the newfound industry was mainly composed of game developers with little business experience. This led to numerous companies forming simply to create clones of popular games to try to capitalize on the market. Due to loss of publishing control and oversaturation of the market, the North American home video game market crashed in 1983, dropping from revenues of around  in 1983 to  by 1985. Many of the North American companies created in the prior years closed down. Japan's growing game industry was briefly shocked by this crash but had sufficient",
        "of around  in 1983 to  by 1985. Many of the North American companies created in the prior years closed down. Japan's growing game industry was briefly shocked by this crash but had sufficient longevity to withstand the short-term effects, and Nintendo helped to revitalize the industry with the release of the Nintendo Entertainment System in North America in 1985. Along with it, Nintendo established a number of core industrial practices to prevent unlicensed game development and control game distribution on their platform, methods that continue to be used by console manufacturers today.",
        "The industry remained more conservative following the 1983 crash, forming around the concept of publisher-developer dichotomies, and by the 2000s, leading to the industry centralizing around low-risk, triple-A games and studios with large development budgets of at least  or more. The advent of the Internet brought digital distribution as a viable means to distribute games, and contributed to the growth of more riskier, experimental independent game development as an alternative to triple-A games in the late 2000s and which has continued to grow as a significant portion of the video game industry.",
        "Industry roles\nVideo games have a large network effect that draw on many different sectors that tie into the larger video game industry. While video game developers are a significant portion of the industry, other key participants in the market include:",
        "Publishers: Companies generally that oversee bringing the game from the developer to market. This often includes performing the marketing, public relations, and advertising of the game. Publishers frequently pay the developers ahead of time to make their games and will be involved in critical decisions about the direction of the game's progress, and then pay the developers additional royalties or bonuses based on sales performances. Other smaller, boutique publishers may simply offer to perform the publishing of a game for a small fee and a portion of the sales, and otherwise leave the developer with the creative freedom to proceed. A range of other publisher-developer relationships exist between these points.",
        "Distributors: Publishers often are able to produce their own game media and take the role of distributor, but there are also third-party distributors that can mass-produce game media and distribute to retailers. Digital storefronts like Steam and the iOS App Store also serve as distributors and retailers in the digital space.",
        "Retailers: Physical storefronts, which include large online retailers, department and electronic stores, and specialty video game stores, sell games, consoles, and other accessories to consumers. This has also including a trade-in market in certain regions, allowing players to turn in used games for partial refunds or credit towards other games. However, with the uprising of digital marketplaces and e-commerce revolution, retailers have been performing worse than in the past.",
        "Hardware manufacturers: The video game console manufacturers produce console hardware, often through a value chain system that include numerous component suppliers and contract manufacturer that assemble the consoles. Further, these console manufacturers typically require a license to develop for their platform and may control the production of some games, such as Nintendo does with the use of game cartridges for its systems. In exchange, the manufacturers may help promote games for their system and may seek console exclusivity for certain games. For games on personal computers, a number of manufacturers are devoted to high-performance \"gaming computer\" hardware, particularly in the graphics card area; several of the same companies overlap with component supplies for consoles. A range of",
        "are devoted to high-performance \"gaming computer\" hardware, particularly in the graphics card area; several of the same companies overlap with component supplies for consoles. A range of third-party manufacturers also exist to provide equipment and gear for consoles post-sale, such as additional controllers for console or carrying cases and gear for handheld devices.",
        "Journalism: While journalism around video games used to be primarily print-based, and focused more on post-release reviews and gameplay strategy, the Internet has brought a more proactive press that use web journalism, covering games in the months prior to release as well as beyond, helping to build excitement for games ahead of release.\n Influencers: With the rising importance of social media, video game companies have found that the opinions of influencers using streaming media to play through their games has had a significant impact on game sales, and have turned to use influencers alongside traditional journalism as a means to build up attention to their game before release.",
        "Esports: Esports is a major function of several multiplayer games with numerous professional leagues established since the 2000s, with large viewership numbers, particularly out of southeast Asia since the 2010s.\n Trade and advocacy groups: Trade groups like the Entertainment Software Association were established to provide a common voice for the industry in response to governmental and other advocacy concerns. They frequently set up the major trade events and conventions for the industry such as E3.",
        "Gamers: Proactive hobbyists who are players and consumers of video games. While their representation in the industry is primarily seen through game sales, many companies follow gamers' comments on social media or on user reviews and engage with them to work to improve their products in addition to other feedback from other parts of the industry. Demographics of the larger player community also impact parts of the market; while once dominated by younger men, the market shifted in the mid-2010s towards women and older players who generally preferred mobile and causal games, leading to further growth in those sectors.",
        "Major regional markets\n\nThe industry itself grew out from both the United States and Japan in the 1970s and 1980s before having a larger worldwide contribution. Today, the video game industry is predominantly led by major companies in North America (primarily the United States and Canada), Europe, and southeast Asia including Japan, South Korea, and China. Hardware production remains an area dominated by Asian companies either directly involved in hardware design or part of the production process, but digital distribution and indie game development of the late 2000s has allowed game developers to flourish nearly anywhere and diversify the field.\n\nGame sales",
        "Game sales\n\nAccording to the market research firm Newzoo, the global video game industry drew estimated revenues of over  in 2020. Mobile games accounted for the bulk of this, with a 48% share of the market, followed by console games at 28% and personal computer games at 23%.",
        "Sales of different types of games vary widely between countries due to local preferences. Japanese consumers tend to purchase much more handheld games than console games and especially PC games, with a strong preference for games catering to local tastes. Another key difference is that, though having declined in the West, arcade games remain an important sector of the Japanese gaming industry. In South Korea, computer games are generally preferred over console games, especially MMORPG games and real-time strategy games. Computer games are also popular in China.\n\nEffects on society\n\nCulture",
        "Video game culture is a worldwide new media subculture formed around video games and game playing. As computer and video games have increased in popularity over time, they have had a significant influence on popular culture. Video game culture has also evolved over time hand in hand with internet culture as well as the increasing popularity of mobile games. Many people who play video games identify as gamers, which can mean anything from someone who enjoys games to someone who is passionate about it. As video games become more social with multiplayer and online capability, gamers find themselves in growing social networks. Gaming can both be entertainment as well as competition, as a new trend known as electronic sports is becoming more widely accepted. In the 2010s, video games and",
        "in growing social networks. Gaming can both be entertainment as well as competition, as a new trend known as electronic sports is becoming more widely accepted. In the 2010s, video games and discussions of video game trends and topics can be seen in social media, politics, television, film and music. The COVID-19 pandemic during 2020–2021 gave further visibility to video games as a pastime to enjoy with friends and family online as a means of social distancing.",
        "Since the mid-2000s there has been debate whether video games qualify as art, primarily as the form's interactivity interfered with the artistic intent of the work and that they are designed for commercial appeal. A significant debate on the matter came after film critic Roger Ebert published an essay \"Video Games can never be art\", which challenged the industry to prove him and other critics wrong. The view that video games were an art form was cemented in 2011 when the U.S. Supreme Court ruled in the landmark case Brown v. Entertainment Merchants Association that video games were a protected form of speech with artistic merit. Since then, video game developers have come to use the form more for artistic expression, including the development of art games, and the cultural heritage of",
        "form of speech with artistic merit. Since then, video game developers have come to use the form more for artistic expression, including the development of art games, and the cultural heritage of video games as works of arts, beyond their technical capabilities, have been part of major museum exhibits, including The Art of Video Games at the Smithsonian American Art Museum and toured at other museums from 2012 to 2016.",
        "Video games will inspire sequels and other video games within the same franchise, but also have influenced works outside of the video game medium. Numerous television shows (both animated and live-action), films, comics and novels have been created based on existing video game franchises. Because video games are an interactive medium there has been trouble in converting them to these passive forms of media, and typically such works have been critically panned or treated as children's media. For example, until 2019, no video game film had ever been received a \"Fresh\" rating on Rotten Tomatoes, but the releases of Detective Pikachu (2019) and Sonic the Hedgehog (2020), both receiving \"Fresh\" ratings, shows signs of the film industry having found an approach to adapt video games for the",
        "but the releases of Detective Pikachu (2019) and Sonic the Hedgehog (2020), both receiving \"Fresh\" ratings, shows signs of the film industry having found an approach to adapt video games for the large screen. That said, some early video game-based films have been highly successful at the box office, such as 1995's Mortal Kombat and 2001's Lara Croft: Tomb Raider.",
        "More recently since the 2000s, there has also become a larger appreciation of video game music, which ranges from chiptunes composed for limited sound-output devices on early computers and consoles, to fully-scored compositions for most modern games. Such music has frequently served as a platform for covers and remixes, and concerts featuring video game soundtracks performed by bands or orchestras, such as Video Games Live, have also become popular. Video games also frequently incorporate licensed music, particularly in the area of rhythm games, furthering the depth of which video games and music can work together.",
        "Further, video games can serve as a virtual environment under full control of a producer to create new works. With the capability to render 3D actors and settings in real-time, a new type of work machinima (short for \"machine cinema\") grew out from using video game engines to craft narratives. As video game engines gain higher fidelity, they have also become part of the tools used in more traditional filmmaking. Unreal Engine has been used as a backbone by Industrial Light & Magic for their StageCraft technology for shows like The Mandalorian.",
        "Separately, video games are also frequently used as part of the promotion and marketing for other media, such as for films, anime, and comics. However, these licensed games in the 1990s and 2000s often had a reputation for poor quality, developed without any input from the intellectual property rights owners, and several of them are considered among lists of games with notably negative reception, such as Superman 64. More recently, with these licensed games being developed by triple-A studios or through studios directly connected to the licensed property owner, there has been a significant improvement in the quality of these games, with an early trendsetting example of Batman: Arkham Asylum.\n\nBeneficial uses",
        "Beneficial uses\n\nBesides their entertainment value, appropriately-designed video games have been seen to provide value in education across several ages and comprehension levels. Learning principles found in video games have been identified as possible techniques with which to reform the U.S. education system. It has been noticed that gamers adopt an attitude while playing that is of such high concentration, they do not realize they are learning, and that if the same attitude could be adopted at school, education would enjoy significant benefits. Students are found to be \"learning by doing\" while playing video games while fostering creative thinking.",
        "Video games are also believed to be beneficial to the mind and body. It has been shown that action video game players have better hand–eye coordination and visuo-motor skills, such as their resistance to distraction, their sensitivity to information in the peripheral vision and their ability to count briefly presented objects, than nonplayers. Researchers found that such enhanced abilities could be acquired by training with action games, involving challenges that switch attention between different locations, but not with games requiring concentration on single objects. A 2018 systematic review found evidence that video gaming training had positive effects on cognitive and emotional skills in the adult population, especially with young adults. A 2019 systematic review also added support",
        "found evidence that video gaming training had positive effects on cognitive and emotional skills in the adult population, especially with young adults. A 2019 systematic review also added support for the claim that video games are beneficial to the brain, although the beneficial effects of video gaming on the brain differed by video games types.",
        "Organisers of video gaming events, such as the organisers of the D-Lux video game festival in Dumfries, Scotland, have emphasised the positive aspects video games can have on mental health. Organisers, mental health workers and mental health nurses at the event emphasised the relationships and friendships that can be built around video games and how playing games can help people learn about others as a precursor to discussing the person's mental health. A study in 2020 from Oxford University also suggested that playing video games can be a benefit to a person's mental health. The report of 3,274 gamers, all over the age of 18, focused on the games Animal Crossing: New Horizons and Plants vs Zombies: Battle for Neighborville and used actual play-time data. The report found that those that",
        "gamers, all over the age of 18, focused on the games Animal Crossing: New Horizons and Plants vs Zombies: Battle for Neighborville and used actual play-time data. The report found that those that played more games tended to report greater \"wellbeing\". Also in 2020, computer science professor Regan Mandryk of the University of Saskatchewan said her research also showed that video games can have health benefits such as reducing stress and improving mental health. The university's research studied all age groups – \"from pre-literate children through to older adults living in long term care homes\" – with a main focus on 18 to 55-year-olds.",
        "A study of gamers attitudes towards gaming which was reported about in 2018 found that millennials use video games as a key strategy for coping with stress. In the study of 1,000 gamers, 55% said that it \"helps them to unwind and relieve stress ... and half said they see the value in gaming as a method of escapism to help them deal with daily work pressures\".\n\nControversies",
        "Video games have caused controversy since the 1970s. Parents and children's advocates regularly raise concerns that violent video games can influence young players into performing those violent acts in real life, and events such as the Columbine High School massacre in 1999 in which the perpetrators specifically alluded to using video games to plot out their attack, raised further fears. Medical experts and mental health professionals have also raised concerned that video games may be addictive, and the World Health Organization has included \"gaming disorder\" in the 11th revision of its International Statistical Classification of Diseases. Other health experts, including the American Psychiatric Association, have stated that there is insufficient evidence that video games can create",
        "International Statistical Classification of Diseases. Other health experts, including the American Psychiatric Association, have stated that there is insufficient evidence that video games can create violent tendencies or lead to addictive behavior, though agree that video games typically use a compulsion loop in their core design that can create dopamine that can help reinforce the desire to continue to play through that compulsion loop and potentially lead into violent or addictive behavior. Even with case law establishing that video games qualify as a protected art form, there has been pressure on the video game industry to keep their products in check to avoid over-excessive violence particularly for games aimed at younger children. The potential addictive behavior around games,",
        "pressure on the video game industry to keep their products in check to avoid over-excessive violence particularly for games aimed at younger children. The potential addictive behavior around games, coupled with increased used of post-sale monetization of video games, has also raised concern among parents, advocates, and government officials about gambling tendencies that may come from video games, such as controversy around the use of loot boxes in many high-profile games.",
        "Numerous other controversies around video games and its industry have arisen over the years, among the more notable incidents include the 1993 United States Congressional hearings on violent games like Mortal Kombat which lead to the formation of the ESRB ratings system, numerous legal actions taken by attorney Jack Thompson over violent games such as Grand Theft Auto III and Manhunt from 2003 to 2007, the outrage over the \"No Russian\" level from Call of Duty: Modern Warfare 2 in 2009 which allowed the player to shoot a number of innocent non-player characters at an airport, and the Gamergate harassment campaign in 2014 that highlighted misogyny from a portion of the player demographic. The industry as a whole has also dealt with issues related to gender, racial, and LGBTQ+ discrimination",
        "harassment campaign in 2014 that highlighted misogyny from a portion of the player demographic. The industry as a whole has also dealt with issues related to gender, racial, and LGBTQ+ discrimination and mischaracterization of these minority groups in video games. A further issue in the industry is related to working conditions, as development studios and publishers frequently use \"crunch time\", required extended working hours, in the weeks and months ahead of a game's release to assure on-time delivery.",
        "Collecting and preservation\n\nPlayers of video games often maintain collections of games. More recently there has been interest in retrogaming, focusing on games from the first decades. Games in retail packaging in good shape have become collectors items for the early days of the industry, with some rare publications having gone for over  . Separately, there is also concern about the preservation of video games, as both game media and the hardware to play them degrade over time. Further, many of the game developers and publishers from the first decades no longer exist, so records of their games have disappeared. Archivists and preservations have worked within the scope of copyright law to save these games as part of the cultural history of the industry.",
        "There are many video game museums around the world, including the National Videogame Museum in Frisco, Texas, which serves as the largest museum wholly dedicated to the display and preservation of the industry's most important artifacts. Europe hosts video game museums such as the Computer Games Museum in Berlin and the Museum of Soviet Arcade Machines in Moscow and Saint-Petersburg. The Museum of Art and Digital Entertainment in Oakland, California is a dedicated video game museum focusing on playable exhibits of console and computer games. The Video Game Museum of Rome is also dedicated to preserving video games and their history. The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games",
        "preserving video games and their history. The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games and game-related historical materials in the world, including a  exhibit which allows guests to play their way through the history of video games. The Smithsonian Institution in Washington, DC has three video games on permanent display: Pac-Man, Dragon's Lair, and Pong.",
        "The Museum of Modern Art has added a total of 20 video games and one video game console to its permanent Architecture and Design Collection since 2012. In 2012, the Smithsonian American Art Museum ran an exhibition on \"The Art of Video Games\". However, the reviews of the exhibit were mixed, including questioning whether video games belong in an art museum.\n\nSee also\n\n Lists of video games\n List of accessories to video games by system\n Outline of video games\n\nNotes\n\nReferences\n\nSources\n\nFurther reading",
        "See also\n\n Lists of video games\n List of accessories to video games by system\n Outline of video games\n\nNotes\n\nReferences\n\nSources\n\nFurther reading \n\n \n \n \n \n \n \n \n \n \n \n\n The Ultimate History of Video Games: From Pong to Pokemon--The Story Behind the Craze That Touched Our Lives and Changed the World by Steven L. Kent, Crown, 2001, \n The Ultimate History of Video Games, Volume 2: Nintendo, Sony, Microsoft, and the Billion-Dollar Battle to Shape Modern Gaming by Steven L. Kent, Crown, 2021,\n\nExternal links\n\n Video games bibliography by the French video game research association Ludoscience\n The Virtual Museum of Computing (VMoC) (archived 10 October 2014)\n\n \nGames and sports introduced in 1947\nDigital media\nAmerican inventions"
    ],
    [
        "Wireless broadband\nWireless broadband is a telecommunications technology that provides high-speed wireless Internet access or computer networking access over a wide area. The term encompasses both fixed and mobile broadband.\n\nThe term broadband\nOriginally the word \"broadband\" had a technical meaning, but became a marketing term for any kind of relatively high-speed computer network or Internet access technology.\nAccording to the 802.16-2004 standard, broadband means \"having instantaneous bandwidths greater than 1 MHz and supporting data rates greater than about 1.5 Mbit/s.\"\nThe Federal Communications Commission (FCC) recently re-defined the definition to mean download speeds of at least 25 Mbit/s and upload speeds of at least 3 Mbit/s.\n\nTechnology and speeds",
        "Technology and speeds\n\nA wireless broadband network is an outdoor fixed and/or mobile wireless network providing point-to-multipoint or point-to-point terrestrial wireless links for broadband services.\n\nWireless networks can feature data rates exceeding 1 Gbit/s. Many fixed wireless networks are exclusively half-duplex (HDX), however, some licensed and unlicensed systems can also operate at full-duplex (FDX) allowing communication in both directions simultaneously.\n\nOutdoor fixed wireless broadband networks commonly utilize a priority TDMA based protocol in order to divide communication into timeslots. This timeslot technique eliminates many of the issues common to 802.11 Wi-Fi protocol in outdoor networks such as the hidden node problem.",
        "Few wireless Internet service providers (WISPs) provide download speeds of over 100 Mbit/s; most broadband wireless access (BWA) services are estimated to have a range of  from a tower. Technologies used include Local Multipoint Distribution Service (LMDS) and Multichannel Multipoint Distribution Service (MMDS), as well as heavy use of the industrial, scientific and medical (ISM) radio bands and one particular access technology was standardized by IEEE 802.16, with products known as WiMAX.",
        "WiMAX is highly popular in Europe but has not met full acceptance in the United States because cost of deployment does not meet return on investment figures. In 2005 the Federal Communications Commission adopted a Report and Order that revised the FCC's rules to open the 3650 MHz band for terrestrial wireless broadband operations.\n\nAnother system that is popular with cable internet service providers uses point-to-multipoint wireless links that extend the existing wired network using a transparent radio connection. This allows the same DOCSIS modems to be used for both wired and wireless customers.",
        "Development of Wireless Broadband in the United States\nOn November 14, 2007 the Commission released Public Notice DA 07–4605 in which the Wireless Telecommunications Bureau announced the start date for licensing and registration process for the 3650–3700 MHz band.\nIn 2010 the FCC adopted the TV White Space Rules (TVWS) and allowed some of the better no line of sight frequency (700 MHz) into the FCC Part-15 Rules. The Wireless Internet Service Providers Association, a national association of WISPs, petitioned the FCC and won.",
        "Initially, WISPs were only found in rural areas not covered by cable or DSL. These early WISPs would employ a high-capacity T-carrier, such as a T1 or DS3 connection, and then broadcast the signal from a high elevation, such as at the top of a water tower. To receive this type of Internet connection, consumers mount a small dish to the roof of their home or office and point it to the transmitter. Line of sight is usually necessary for WISPs operating in the 2.4 and 5 GHz bands with 900 MHz offering better NLOS (non-line-of-sight) performance.",
        "Residential Wireless Internet\nProviders of fixed wireless broadband services typically provide equipment to customers and install a small antenna or dish somewhere on the roof. This equipment is usually deployed as a service and maintained by the company providing that service. Fixed wireless services have become particularly popular in many rural areas where cable, DSL or other typical home internet services are not available.",
        "Business Wireless Internet\nMany companies in the US and worldwide have started using wireless alternatives to incumbent and local providers for internet and voice service. These providers tend to offer competitive services and options in areas where there is a difficulty getting affordable Ethernet connections from terrestrial providers such as ATT, Comcast, Verizon and others. Also, companies looking for full diversity between carriers for critical uptime requirements may seek wireless alternatives to local options.\n\nDemand for spectrum",
        "Demand for spectrum\n\nTo cope with increased demand for wireless broadband, increased spectrum would be needed. Studies began in 2009, and while some unused spectrum was available, it appeared broadcasters would have to give up at least some spectrum. This led to strong objections from the broadcasting community. In 2013, auctions were planned, and for now any action by broadcasters is voluntary.",
        "Mobile wireless broadband\nCalled mobile broadband, wireless broadband technologies include services from mobile phone service providers such as Verizon Wireless, Sprint Corporation, and AT&T Mobility, and T-Mobile which allow a more mobile version of Internet access. Consumers can purchase a PC card, laptop card, USB equipment, or mobile broadband modem, to connect their PC or laptop to the Internet via cell phone towers. This type of connection would be stable in almost any area that could also receive a strong cell phone connection. These connections can cost more for portable convenience as well as having speed limitations in all but urban environments.",
        "On June 2, 2010, after months of discussion, AT&T became the first wireless Internet provider in the US to announce plans to charge according to usage. As the only iPhone service in the United States, AT&T experienced the problem of heavy Internet use more than other providers. About 3 percent of AT&T smart phone customers account for 40 percent of the technology's use. 98 percent of the company's customers use less than 2 gigabytes (4000 page views, 10,000 emails or 200 minutes of streaming video), the limit under the $25 monthly plan, and 65 percent use less than 200 megabytes, the limit for the $15 plan. For each gigabyte in excess of the limit, customers would be charged $10 a month starting June 7, 2010, though existing customers would not be required to change from the $30 a month",
        "the $15 plan. For each gigabyte in excess of the limit, customers would be charged $10 a month starting June 7, 2010, though existing customers would not be required to change from the $30 a month unlimited service plan. The new plan would become a requirement for those upgrading to the new iPhone technology later in the summer.",
        "Licensing",
        "A wireless connection can be either licensed or unlicensed. In the US, licensed connections use a private spectrum the user has secured rights to from the Federal Communications Commission (FCC). The unlicensed mobile wireless broadband, in US operates on CBRS Which has three tiers. Tier 1 – Incumbent Access, reserved for US Federals Government, Tier 2 – Priority Access, a paid access with priority on the spectrum, Tier 3 – General Authorized Access (GAA), a shared spectrum. In other countries, spectrum is licensed from the country's national radio communications authority (such as the ACMA in Australia or Nigerian Communications Commission in Nigeria (NCC)). Licensing is usually expensive and often reserved for large companies who wish to guarantee private access to spectrum for use in",
        "in Australia or Nigerian Communications Commission in Nigeria (NCC)). Licensing is usually expensive and often reserved for large companies who wish to guarantee private access to spectrum for use in point to point communication. Because of this, most wireless ISP's use unlicensed spectrum which is publicly shared.",
        "See also\n Clearwire\n CorDECT\n HIPERMAN\n Skyriver, provider in California\n WiBro, provider in South Korea\n iBurst\n 802.20\n Connect card\n Policies promoting wireless broadband\n\nReferences\n\nExternal links\n\n \n\n Wireless Internet Service Provider Association\n Wireless Internet Service Provider Ontario\n\nWireless networking\nBroadband"
    ],
    [
        "Ziply Fiber",
        "Northwest Fiber, LLC, doing business as Ziply Fiber, is an American telecommunications company based in Kirkland, Washington. Ziply is a subsidiary of WaveDivision Capital, a private investment company, which is also Kirkland-based. The company started operations on May 1, 2020 when it completed its acquisition of Frontier Communications Northwest operations and assets for $1.4 billion; Frontier sold its Northwest operations after filing for bankruptcy protection in April 2020. Ziply Fiber's footprint covers the Pacific Northwest region, specifically the states of Washington, Oregon, Idaho and Montana. Its key offerings include fiber internet and phone for residential customers, Business Fiber Internet, and Ziply Voice services for small businesses; and a variety of internet, networking",
        "Montana. Its key offerings include fiber internet and phone for residential customers, Business Fiber Internet, and Ziply Voice services for small businesses; and a variety of internet, networking and voice solutions for enterprise customers. The company will also continue to support DSL and grandfathered TV customers. Ziply has stated that it plans on investing $500 million  to improve its network and service throughout its footprint. This includes bringing fiber to nearly 85%  of its network, which mainly encompasses rural communities. Currently approximately 30% have access to fiber as of June 2020.",
        "Ziply Fiber serves more than 500,000 customers across the four states and has a workforce of approximately 1,000 employees. It has major offices in Everett, Washington; Beaverton, Oregon, and Hayden, Idaho.\n\nHistory\n\nGeneral Telephone Company of the Northwest, Inc. was founded in 1964 and later became GTE Northwest, Incorporated. GTE Northwest originally served Idaho, Montana, Oregon, Washington.\n\nIn 1993, GTE acquired Continental Telephone (ConTel); as a result, ConTel operations in Oregon, Washington, and Idaho were merged into GTE Northwest. In 1994, GTE sold ConTel of Idaho to Citizens Communications (which later became Frontier Communications). In 1995, GTE sold operations in Montana to Citizens Communications, and absorbed ConTel of Oregon into GTE's existing Oregon operations.",
        "In 2000, Bell Atlantic acquired GTE, forming Verizon. As a result, GTE Northwest was renamed Verizon Northwest, Inc. It continued to provide local telephone service to former GTE regions and some ConTel regions in Idaho, Washington, and Oregon.\n\nVerizon Northwest, along with several other Verizon wireline operating companies, were acquired by Frontier Communications on July 1, 2010. The company's name was changed to '''Frontier Communications Northwest, Inc.",
        "In May 2019, WaveDivision Capital announced that it would buy Frontier Communications Northwest operations for nearly $1.4 billion as part of a new service. WaveDivision Capital (“WDC”) was founded in 2003 by broadband entrepreneur Steven Weed. It is a private investment firm focused on the broadband industry, and its stated goal is “to bring better internet connections to more homes and businesses throughout North America”. Weed was the CEO of Wave Broadband until 2018, when the company was sold and he departed. Despite having similar names and key players WaveDivision Capital and Wave Broadband are not related. WaveDivision Capital formed Northwest Fiber to acquire the Frontier Communications Northwest operations, which was approved by federal and state regulators on February 19, 2020,",
        "are not related. WaveDivision Capital formed Northwest Fiber to acquire the Frontier Communications Northwest operations, which was approved by federal and state regulators on February 19, 2020, with the new service receiving the name of \"Ziply Fiber\". Frontier Communications Northwest division ceased operations on April 30, 2020, and was replaced the following day by Ziply Fiber. As of 2020, the company has almost 1000 employees and serves over 500,000 customers.",
        "References\n\nSources\nVerizon Northwest, Inc.\n\nExternal links\n \n Home | WaveDivision Capital\n\nCommunications in Washington (state)\nCompanies based in Kirkland, Washington\nFrontier Communications\nAmerican companies established in 2020"
    ]
]