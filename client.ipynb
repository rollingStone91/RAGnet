{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2dfced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from datasets import load_dataset\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "# DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1399b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# 自定义 LangChain 的 Embeddings 类封装\n",
    "class LlamaCppEmbeddings(Embeddings):\n",
    "    def __init__(self, model_path: str):\n",
    "        self.llm = Llama(model_path=model_path, embedding=True)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        # return [self.llm.embed(text)[\"data\"][0][\"embedding\"] for text in texts]\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            result = self.llm.embed(text)\n",
    "            if isinstance(result, list) and isinstance(result[0], list):\n",
    "                embeddings.append(result[0])\n",
    "            else:\n",
    "                embeddings.append(result)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        # return self.llm.embed(text)[\"data\"][0][\"embedding\"]\n",
    "        result = self.llm.embed(text)\n",
    "        return result[0] if isinstance(result, list) and isinstance(result[0], list) else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec073e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    \"\"\"\n",
    "    轻量级rag客户端，负责数据集加载、向量存储构建与检索。\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path: str = \"./models/Qwen3-Embedding/Qwen3-Embedding-0.6B-Q8_0.gguf\", \n",
    "                vectorstore_path: str = \"faiss_db\"): # dashscope_api_key: str,使用api调用embedding模型\n",
    "        os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"TRUE\")\n",
    "        self.vectorstore_path = vectorstore_path\n",
    "        # self.embeddings = DashScopeEmbeddings(\n",
    "        #     model=\"text-embedding-v1\",\n",
    "        #     dashscope_api_key=dashscope_api_key\n",
    "        # )\n",
    "        self.embeddings = LlamaCppEmbeddings(model_path=model_path)\n",
    "        self.db: FAISS = None\n",
    "        self.retriever = None\n",
    "\n",
    "    def _chunk_text(self, text: str, chunk_size=800, overlap= 200) -> list[str]:\n",
    "        \"\"\"\n",
    "        将文本分块处理，使用递归字符分割器。\n",
    "        \"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    # 读取PDF文件并提取文本内容\n",
    "    def _read_pdfs(self, pdf_paths: List[str]) -> List[Document]:\n",
    "        docs = []\n",
    "        for path in pdf_paths:\n",
    "            loader = PyPDFLoader(path)\n",
    "            pages = loader.load_and_split()\n",
    "            docs.extend(pages)\n",
    "        return docs\n",
    "\n",
    "    # 读取JSON文件夹中的所有文件\n",
    "    def _load_json_folder(self, folder_path: str) -> List[Document]:\n",
    "        docs = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if not filename.endswith('.json'):\n",
    "                continue\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            with open(filepath, encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            content = f\"{data.get('title', '')}\\n{data.get('content', '')}\".strip()\n",
    "            if content:\n",
    "                docs.append(Document(page_content=content, metadata={'source': filepath}))\n",
    "        return docs\n",
    "    \n",
    "    # 在线读取数据集\n",
    "    def _streaming_load_dataset(self, sample_size=100, language='en', date_version='20231101') -> List[str]:\n",
    "        # 启用streaming模式在线读取huggingface datasets\n",
    "        dataset = load_dataset(\"wikimedia/wikipedia\", f'{date_version}.{language}', streaming=True)\n",
    "        docs = []\n",
    "        for i, item in enumerate(dataset['train']):\n",
    "            if i >= sample_size:\n",
    "                break\n",
    "            text = item.get('text', '')\n",
    "            title = item.get('title', '')\n",
    "            if not text:\n",
    "                continue\n",
    "            # # 抽取前 5000 字，避免过长\n",
    "            # snippet = text[:5000]\n",
    "            meta = {'source': f'wikipedia://{language}/{item.get(\"id\")}'}\n",
    "            docs.append(Document(page_content=f\"{title}\\n{text}\", metadata=meta))\n",
    "        print(f\"Streamed {len(docs)} Wikipedia docs.\")\n",
    "        return docs\n",
    "    \n",
    "    def build_vectorstore(self, sample_size=100, batch_size=10, \n",
    "                          streaming=False, folder_path=None, pdf_paths:List[str]=None):\n",
    "        docs = []\n",
    "        if streaming:\n",
    "            # 在线读取数据集\n",
    "            docs.extend(self._streaming_load_dataset(sample_size))\n",
    "        elif folder_path is not None and pdf_paths is None:\n",
    "            # 从指定文件夹加载JSON文件\n",
    "            docs.extend(self._load_json_folder(folder_path))\n",
    "        elif pdf_paths is not None:\n",
    "            # 从PDF文件加载\n",
    "            docs.extend(self._read_pdfs(pdf_paths))\n",
    "\n",
    "        # 分块并批量处理\n",
    "        for i, doc in enumerate(docs):\n",
    "            texts, metadatas, all_chunk= [], [], []\n",
    "            all_chunk = self._chunk_text(doc.page_content)\n",
    "            for j, chunk in enumerate(all_chunk):\n",
    "                texts.append(chunk)\n",
    "                metadatas.append(doc.metadata)\n",
    "                # 每 batch_size 保存一次，防止内存溢出\n",
    "                if len(texts) >= batch_size or j == len(all_chunk) - 1:\n",
    "                    if self.db is None:\n",
    "                        self.db = FAISS.from_texts(texts, embedding=self.embeddings, metadatas=metadatas)\n",
    "                    else:\n",
    "                        self.db.add_texts(texts, metadatas=metadatas)\n",
    "                    texts.clear()\n",
    "                    metadatas.clear()\n",
    "            print(f\"Processed {i+1}/{len(docs)} articles...\")\n",
    "\n",
    "        # 保存向量库\n",
    "        if self.db:\n",
    "            self.db.save_local(self.vectorstore_path)\n",
    "            print(f\"Vectorstore saved to {self.vectorstore_path}\")\n",
    "        else:\n",
    "            print(\"No data processed.\")\n",
    "\n",
    "    def load_vectorstore(self) -> None:\n",
    "        \"\"\"\n",
    "        加载已保存的向量存储，并初始化检索器。\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.vectorstore_path):\n",
    "            raise FileNotFoundError(f\"Vectorstore directory '{self.vectorstore_path}' not found.\")\n",
    "        self.db = FAISS.load_local(\n",
    "            self.vectorstore_path,\n",
    "            embeddings=self.embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(f\"Vectorstore {self.vectorstore_path} loaded.\")\n",
    "\n",
    "    def retrieve(self, query:str, top_k=4, use_mmr=False):\n",
    "        \"\"\"\n",
    "        通过query在FAISS向量库中检索k个最相似文档，\n",
    "        返回每个Document对象、其特征向量及相似度得分。\n",
    "        \"\"\"\n",
    "        # 检查向量库是否已加载\n",
    "        if self.db is None:\n",
    "            raise ValueError(\"Vectorstore尚未加载，请先调用load_vectorstore或build_vectorstore\")\n",
    "\n",
    "        query_vec = np.array(self.embeddings.embed_query(query), dtype=np.float32).tolist()\n",
    "\n",
    "        if use_mmr:\n",
    "            # 执行MMR搜索\n",
    "            docs = self.db.max_marginal_relevance_search(query, k=top_k, fetch_k=top_k * 2)\n",
    "            # 手动计算相似度得分\n",
    "            doc_texts = [doc.page_content for doc in docs]\n",
    "            doc_vecs = np.array(self.embeddings.embed_documents(doc_texts), dtype=np.float32)\n",
    "            scores = [float(np.dot(query_vec, dv) / (np.linalg.norm(query_vec) * np.linalg.norm(dv)))\n",
    "                      for dv in doc_vecs]\n",
    "        else:\n",
    "            # 执行相似度搜索\n",
    "            docs_and_scores = self.db.similarity_search_with_score(query, k=top_k)\n",
    "            docs, scores = zip(*docs_and_scores)\n",
    "            docs = list(docs)\n",
    "            scores = list(scores)\n",
    "            doc_texts = [doc.page_content for doc in docs]\n",
    "            doc_vecs = self.embeddings.embed_documents(doc_texts)\n",
    "\n",
    "        # 将特征向量转换为列表形式\n",
    "        doc_vecs = [vec.tolist() for vec in doc_vecs]\n",
    "\n",
    "        return docs, doc_vecs, scores, query_vec\n",
    "\n",
    "    # 这一段query包含了调用llm生成答案部分，一种是调用ollama部署的llm，一种是调用api并使用agent工具\n",
    "    # def query(self, question: str) -> str:\n",
    "    #     \"\"\"\n",
    "    #     基于已加载的向量存储进行查询，并返回生成的答案。\n",
    "    #     \"\"\"\n",
    "    #     if self.retriever is None:\n",
    "    #         raise RuntimeError(\"Retriever not initialized. Call 'load_vectorstore()' first.\")\n",
    "    #     llm = ChatOllama(model=self.ollama_model)\n",
    "    #     qa_chain = RetrievalQA.from_chain_type(\n",
    "    #         llm=llm,\n",
    "    #         retriever=self.retriever,\n",
    "    #         chain_type=\"stuff\",\n",
    "    #         return_source_documents=False\n",
    "    #     )\n",
    "    #     result = qa_chain.invoke({\"query\": question})\n",
    "    #     return result[\"result\"]\n",
    "        # 创建检索工具\n",
    "        # retrieval_tool = create_retriever_tool(\n",
    "        #     self.retriever,\n",
    "        #     name=\"pdf_extractor\",\n",
    "        #     description=\"Tool to answer queries based on the processed PDF content.\"\n",
    "        # )\n",
    "\n",
    "        # prompt = ChatPromptTemplate.from_messages([\n",
    "        #     (\"system\", \n",
    "        #      \"\"\"\n",
    "        #      你是AI助手，请根据提供的上下文回答问题，确保提供所有细节，\n",
    "        #      如果答案不在上下文中，请说 '答案不在上下文中'，不要提供错误的答案\n",
    "        #      \"\"\"),\n",
    "        #     (\"human\", \"{input}\"),\n",
    "        #     (\"placeholder\", \"{agent_scratchpad}\")\n",
    "        # ])\n",
    "        # agent = create_tool_calling_agent(llm, [retrieval_tool], prompt)\n",
    "        # executor = AgentExecutor(agent=agent, tools=[retrieval_tool], verbose=False)\n",
    "        # result = executor.invoke({\"input\": question})\n",
    "        # return result.get('output', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2ce5240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 36 key-value pairs and 310 tensors from ./models/Qwen3-Embedding/Qwen3-Embedding-0.6B-Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen3 Embedding 0.6b\n",
      "llama_model_loader: - kv   3:                           general.basename str              = qwen3-embedding\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 0.6B\n",
      "llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   7:                  general.base_model.0.name str              = Qwen3 0.6B Base\n",
      "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-0.6...\n",
      "llama_model_loader: - kv  10:                               general.tags arr[str,5]       = [\"transformers\", \"sentence-transforme...\n",
      "llama_model_loader: - kv  11:                          qwen3.block_count u32              = 28\n",
      "llama_model_loader: - kv  12:                       qwen3.context_length u32              = 32768\n",
      "llama_model_loader: - kv  13:                     qwen3.embedding_length u32              = 1024\n",
      "llama_model_loader: - kv  14:                  qwen3.feed_forward_length u32              = 3072\n",
      "llama_model_loader: - kv  15:                 qwen3.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  16:              qwen3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  18:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  19:                 qwen3.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  20:               qwen3.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  21:                         qwen3.pooling_type u32              = 3\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,151669]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,151669]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.eot_token_id u32              = 151645\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = true\n",
      "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:  113 tensors\n",
      "llama_model_loader: - type q8_0:  197 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 603.87 MiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: special tokens cache size = 26\n",
      "load: token to piece cache size = 0.9311 MB\n",
      "print_info: arch             = qwen3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 1024\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 3072\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 0.6B\n",
      "print_info: model params     = 595.78 M\n",
      "print_info: general.name     = Qwen3 Embedding 0.6b\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151669\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 310 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =   603.87 MiB\n",
      "...........................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    56.00 MiB\n",
      "llama_kv_cache_unified: size =   56.00 MiB (   512 cells,  28 layers,  1 seqs), K (f16):   28.00 MiB, V (f16):   28.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   298.23 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Qwen3 Embedding 0.6b', 'general.architecture': 'qwen3', 'qwen3.rope.freq_base': '1000000.000000', 'general.base_model.0.name': 'Qwen3 0.6B Base', 'qwen3.attention.layer_norm_rms_epsilon': '0.000001', 'general.type': 'model', 'general.basename': 'qwen3-embedding', 'general.size_label': '0.6B', 'general.license': 'apache-2.0', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Qwen', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen3-0.6B-Base', 'tokenizer.ggml.pre': 'qwen2', 'qwen3.block_count': '28', 'qwen3.context_length': '32768', 'qwen3.embedding_length': '1024', 'qwen3.feed_forward_length': '3072', 'qwen3.attention.head_count': '16', 'qwen3.attention.head_count_kv': '8', 'tokenizer.ggml.add_bos_token': 'false', 'qwen3.attention.key_length': '128', 'qwen3.attention.value_length': '128', 'qwen3.pooling_type': '3', 'tokenizer.ggml.model': 'gpt2', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eot_token_id': '151645', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.add_eos_token': 'true', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith(\\'<tool_response>\\') and message.content.endswith(\\'</tool_response>\\')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set content = message.content %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in message.content %}\\n                {%- set content = message.content.split(\\'</think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = message.content.split(\\'</think>\\')[0].rstrip(\\'\\\\n\\').split(\\'<think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\\\n\\\\n</think>\\\\n\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for message in messages[::-1] %}\n",
      "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in message.content %}\n",
      "                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore ./common_sense_db loaded.\n"
     ]
    }
   ],
   "source": [
    "client = Client(vectorstore_path=\"./common_sense_db\")\n",
    "# client.build_vectorstore(batch_size=10, streaming=False, folder_path=\"./classified/common_sense\")\n",
    "client.load_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abfc0866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     165.61 ms\n",
      "llama_perf_context_print: prompt eval time =      61.21 ms /     4 tokens (   15.30 ms per token,    65.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      65.17 ms /     5 tokens\n",
      "llama_perf_context_print:        load time =     165.61 ms\n",
      "llama_perf_context_print: prompt eval time =      85.99 ms /     4 tokens (   21.50 ms per token,    46.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      91.01 ms /     5 tokens\n",
      "llama_perf_context_print:        load time =     165.61 ms\n",
      "llama_perf_context_print: prompt eval time =     828.39 ms /   147 tokens (    5.64 ms per token,   177.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     835.96 ms /   148 tokens\n",
      "llama_perf_context_print:        load time =     165.61 ms\n",
      "llama_perf_context_print: prompt eval time =     676.16 ms /    92 tokens (    7.35 ms per token,   136.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     683.99 ms /    93 tokens\n",
      "llama_perf_context_print:        load time =     165.61 ms\n",
      "llama_perf_context_print: prompt eval time =     692.85 ms /    98 tokens (    7.07 ms per token,   141.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     699.27 ms /    99 tokens\n",
      "llama_perf_context_print:        load time =     165.61 ms\n",
      "llama_perf_context_print: prompt eval time =    1066.03 ms /   196 tokens (    5.44 ms per token,   183.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1073.09 ms /   197 tokens\n",
      "llama_perf_context_print:        load time =     165.61 ms\n",
      "llama_perf_context_print: prompt eval time =     706.87 ms /   142 tokens (    4.98 ms per token,   200.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     713.31 ms /   143 tokens\n"
     ]
    }
   ],
   "source": [
    "docs, doc_vecs, scores, q_vec = client.retrieve(\"What is Chinese\", top_k=5, use_mmr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf1082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
